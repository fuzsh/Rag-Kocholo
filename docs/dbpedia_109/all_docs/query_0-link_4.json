{
    "id": "dbpedia_109_0",
    "rank": 4,
    "data": {
        "url": "https://docs.dhis2.org/pt/full/develop/dhis-core-version-master/developer-manual.html",
        "read_more_link": "",
        "language": "en",
        "title": "Manual do desenvolvedor",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://docs.dhis2.org/pt/full/resources/images/dhis2-logo-rgb-positive.png",
            "https://docs.dhis2.org/pt/full/develop/dhis-core-version-master/resources/images/program_rules/program-rule-model.jpg",
            "https://docs.dhis2.org/pt/full/develop/dhis-core-version-master/resources/images/enrollments/enrollments-pi-relationship.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Official documentation of the DHIS2 platform",
        "meta_lang": "",
        "meta_favicon": "../../resources/images/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://docs.dhis2.org/pt/full/develop/dhis-core-version-master/developer-manual.html",
        "text": "VisÃ£o Geral\n\nThe Web API is a component which makes it possible for external systems to access and manipulate data stored in an instance of DHIS2. More precisely, it provides a programmatic interface to a wide range of exposed data and service methods for applications such as third-party software clients, web portals and internal DHIS2 modules.\n\nIntroduÃ§Ã£o\n\nThe Web API adheres to many of the principles behind the REST architectural style. To mention some important ones:\n\nThe fundamental building blocks are referred to as resources. A resource can be anything exposed to the Web, from a document to a business process - anything a client might want to interact with. The information aspects of a resource can be retrieved or exchanged through resource representations. A representation is a view of a resource's state at any given time. For instance, the visualizations resource in DHIS2 represents visualizations of aggregated data for a certain set of parameters. This resource can be retrieved in a variety of representation formats including JSON and CSV.\n\nAll resources can be uniquely identified by a URI (also referred to as URL). All resources have a default representation. You can indicar que estÃ¡ interessado em uma representaÃ§Ã£o especÃ­fica por supplying an Accept HTTP header, a file extension or a format query parameter. So in order to retrieve a CSV representation of an analytics data response you can supply an Accept: application/csv header or append .csv or ?format=csv to your request URL.\n\nInteractions with the API requires the correct use of HTTP methods or verbos. Isso implica que, para um recurso, deve emitir um GET request when you want to retrieve it, POST request when you want to create one, PUT when you want to update it and DELETE when you want to remove it.\n\nAutenticaÃ§Ã£o\n\nThe DHIS2 Web API supports three protocols for authentication:\n\nAutenticaÃ§Ã£o BÃ¡sica\n\nPersonal Access Tokens (PAT)\n\nOAuth 2\n\nYou can verify and get information about the currently authenticated user by making a GET request to the following URL:\n\nAnd more information about authorities (and if a user has a certain authority) by using the endpoints:\n\nAutenticaÃ§Ã£o BÃ¡sica\n\nThe DHIS2 Web API supports Basic authentication. Basic authentication is a technique for clients to send login credentials over HTTP to a web server. Technically speaking, the username is appended with a colon and the password, Base64-encoded, prefixed Basic and supplied as the value of the Authorization HTTP header. More formally that is:\n\nMost network-aware development environments provide support for Basic authentication, such as Apache HttpClient and Spring RestTemplate. An important note is that this authentication scheme provides no security since the username and password are sent in plain text and can be easily observed by an attacker. Using Basic is recommended only if the server is using SSL/TLS (HTTPS) to encrypt communication with clients. Consider this a hard requirement in order to provide secure interactions with the Web API.\n\nAutenticaÃ§Ã£o de dois fatores\n\nDHIS2 supports two-factor authentication. This can be enabled per user. When enabled, users will be asked to enter a 2FA code when logging in. You can read more about 2FA here.\n\nPersonal Access Token\n\nPersonal access tokens (PATs) are an alternative to using passwords for authentication to DHIS2 when using the API.\n\nPATs can be a more secure alternative to HTTP Basic Authentication, and should be your preferred choice when creating a new app/script etc.\n\nHTTP Basic Authentication is considered insecure because, among other things, it sends your username and password in clear text. It may be deprecated in future DHIS2 versions or made opt-in, meaning that basic authentication would need to be explicitly enabled in the configuration.\n\nImportant security concerns!\n\nYour PATs will automatically inherit all the permissions and authorizations your user has. It is therefore extremely important that you limit the access granted to your token depending on how you intend to use it, see Configuring your token.\n\nIf you only want the token to have access to a narrow and specific part of the server, it is advised to rather create a new special user that you assign only the roles/authorities you want it to have access to.\n\nCreating a token\n\nExistem duas opÃ§Ãµes para criar um novo PAT: * A. Crie um token na interface do utilizador na pÃ¡gina de perfil da sua conta. * B. Create a token via the API.\n\nA. Creating a token on the account's page\n\nLog in with your username and password, go to your profile page (Click top right corner, and chose \"Edit profile\" from the dropdown). On your user profile page, choose \"Personal access tokens\" from the left side menu. You should now be on the \"Manage personal access tokens\" page and see the text: \"You don't have any active personal access tokens\". Click \"Generate new token\" to make a new token. A \"Generate new token\" popup will be shown and present you with two choices:\n\n1. Server/script context:\n\n\"Este tipo Ã© usado para integraÃ§Ãµes e scripts que nunca serÃ£o acedidos por um browser\".\n\nIf you plan to use the token in an application, a script or similar, this type should be your choice.\n\n2. Browser context:\n\n\"Este tipo Ã© usado para aplicaÃ§Ãµes, tais como portais pÃºblicos, que serÃ£o acedidos com um navegador\".\n\nIf you need to link to DHIS2 on a webpage, or e.g. embed in an iframe, this is probably the type of token you want.\n\nConfiguring your token\n\nAfter choosing what token type you want, you can configure different access constraints on your token. By constraint, we mean how to limit and narrow down how your token can be used. This can be of crucial importance if you plan on using the token in a public environment, e.g. on a public dashboard on another site, embedded in an iframe. Since tokens always have the same access/authorities that your user currently has, taking special care is needed if you intend to use it in any environment you don't have 100% control over.\n\nNB: If anyone else gets their hands on your token, they can do anything your user can do. It is not possible to distinguish between actions performed using the token and other actions performed by your user.\n\nImportant: It is strongly advised that you create a separate unique user with only the roles/authorities you want the token to have if you plan on using PAT tokens in a non-secure and/or public environment, e.g. on a PC or server, you don't have 100% control over, or \"embedded\" in a webpage on another server.\n\nThe different constraint types are as follows:\n\nTempo de expiraÃ§Ã£o\n\nEndereÃ§os IP permitidos\n\nMÃ©todos HTTP permitidos\n\nReferenciadores HTTP permitidos\n\nExpiry time\n\nExpiry time simply sets for how long you want your token to be usable, the default is 30 days. After the expiry time, the token will simply return a 401 (Unauthorized) message. You can set any expiry time you want, but it is strongly advised that you set an expiry time that is reasonable for your use case.\n\nAllowed IP addresses\n\nEsta Ã© uma lista separada por vÃ­rgulas, de endereÃ§os IPs que deseja limitar por onde os token request podem chegar.\n\nImportant: IP address validation relies on the X-Forwarded-For header, which can be spoofed. For security, make sure a load balancer or reverse proxy overwrites this header.\n\nAllowed HTTP methods\n\nA comma-separated list of HTTP methods you want your token to be able to use. If you only need your token to view data, not modify or delete, selecting only the GET HTTP method makes sense.\n\nAllowed HTTP referrers\n\nHTTP referer is a header added to the request, when you click on a link, this says which site/page you were on when you clicked the link. Read more about the HTTP referer header here: https://en.wikipedia.org/wiki/HTTP_referer\n\nThis can be used to limit the use of a \"public\" token embedded on another page on another site. Making sure that the referer header match the site hostname in should come from, can help avoid abuse of the token, e.g. if someone posts it on a public forum.\n\nImportant: this is not a security feature. The referer header can easily be spoofed. This setting is intended to discourage unauthorized third-party developers from connecting to public access instances.\n\nSaving your token:\n\nWhen you are done configuring your token, you can save it by clicking the \"Generate new token\" button, on the bottom right of the pop-up. When doing so the token will be saved and a secret token key will be generated on the server. The new secret token key will be shown on the bottom of the PAT token list with a green background, and the text \"Newly created token\". The secret token key will look similar to this:\n\nImportant: This generated secret token key will only be shown once, so it is important that you copy the token key now and save it in a secure place for use later. The secret token key will be securely hashed on the server, and only the hash of this secret token key will be saved to the database. This is done to minimize the security impact if someone gets unauthorized access to the database, similar to the way passwords are handled.\n\nB. Creating a token via the API\n\nExemplo de como criar um novo Personal Access Token com a API:\n\nNB: Remember the empty JSON body ({}) in the payload!\n\nIsso retornarÃ¡ uma resposta contendo um token semelhante a este:\n\nImportant: The token key will only be shown once here in this response. You need to copy and save this is in a secure place for use later!\n\nO token em si consiste em trÃªs partes: 1. Prefixo: (d2pat_) indica que tipo de token Ã© este. 2. Bytes aleatÃ³rios codificados em Base64: (5xVA12xyUbWNedQxy4ohH77WlxRGVvZZ) 3. Soma de verificaÃ§Ã£o CRC32: (1151814092) a parte da soma de verificaÃ§Ã£o Ã© preenchida com 0 para garantir que fique sempre com dez caracteres.\n\nConfigure your token via the API:\n\nPara alterar qualquer uma das restriÃ§Ãµes em seu token, pode emitir a seguinte solicitaÃ§Ã£o de API HTTP.\n\nNB: Only the constraints are possible to modify after the token is created!\n\nUsing your Personal Access Token\n\nTo issue a request with your newly created token, use the Authorization header accordingly. The Authorization header format is:\n\nExemplo:\n\nDeleting your Personal Access Token\n\nYou can delete your PATs either in the UI on your profile page where you created it, or via the API like this:\n\nOAuth2\n\nDHIS2 supports the OAuth2 authentication protocol. OAuth2 is an open standard for authorization which allows third-party clients to connect on behalf of a DHIS2 user and get a reusable bearer token for subsequent requests to the Web API. DHIS2 does not support fine-grained OAuth2 roles but rather provides applications access based on user roles of the DHIS2 user.\n\nEach client for which you want to allow OAuth 2 authentication must be registered in DHIS2. To add a new OAuth2 client go to Apps > Settings > OAuth2 Clients in the user interface, click Add new and enter the desired client name and the grant types.\n\nAdicionar um cliente usando a API Web\n\nAn OAuth2 client can be added through the Web API. As an example, we can send a payload like this:\n\nA carga Ãºtil pode ser enviada com o seguinte comando:\n\nUsaremos esse cliente como base para nossos prÃ³ximos exemplos de tipo de concessÃ£o.\n\nSenha do tipo de concessÃ£o\n\nThe simplest of all grant types is the password grant type. This grant type is similar to basic authentication in the sense that it requires the client to collect the user's username and password. As an example we can use our demo server:\n\nIsso lhe darÃ¡ uma resposta semelhante a esta:\n\nFor now, we will concentrate on the access_token, which is what we will use as our authentication (bearer) token. As an example, we will get all data elements using our token:\n\nGrant type refresh_token\n\nIn general the access tokens have limited validity. You can have a look at the expires_in property of the response in the previous example to understand when a token expires. To get a fresh access_token you can make another round trip to the server and use refresh_token which allows you to get an updated token without needing to ask for the user credentials one more time.\n\nA resposta serÃ¡ exatamente a mesma de quando vocÃª recebe um token para comeÃ§ar.\n\nConceder tipo autorizaÃ§Ã£o_code\n\nAuthorized code grant type is the recommended approach if you don't want to store the user credentials externally. It allows DHIS2 to collect the username/password directly from the user instead of the client collecting them and then authenticating on behalf of the user. Please be aware that this approach uses the redirectUris part of the client payload.\n\nStep 1: Visit the following URL using a web browser. If you have more than one redirect URIs, you might want to add &redirect_uri=http://www.example.org to the URL:\n\nStep 2: After the user has successfully logged in and accepted your client access, it will redirect back to your redirect uri like this:\n\nStep 3: This step is similar to what we did in the password grant type, using the given code, we will now ask for an access token:\n\nMensagens de erro e informaÃ§Ãµes\n\nThe Web API uses a consistent format for all error/warning and informational messages:\n\nHere we can see from the message that the user tried to access a resource I did not have access to. It uses the http status code 403, the HTTP status message forbidden and a descriptive message.\n\nTabela: Propriedades do WebMessage\n\nNome DescriÃ§Ã£o httpStatus Mensagem de status HTTP para esta resposta, consulte RFC 2616 (SeÃ§Ã£o 10) para obter mais informaÃ§Ãµes. httpStatusCode CÃ³digo de status HTTP para esta resposta, consulte RFC 2616 (SeÃ§Ã£o 10) para obter mais informaÃ§Ãµes. status DHIS2 status, possible values are OK | WARNING | ERROR, where OK means everything was successful, ERROR means that operation did not complete and WARNING means the operation was partially successful, if the message contains a response property, please look there for more information. mensagem Uma mensagem amigÃ¡vel, informando se a operaÃ§Ã£o foi um sucesso ou nÃ£o. devMessage Uma mensagem mais tÃ©cnica e amigÃ¡vel ao desenvolvedor (nÃ£o estÃ¡ em uso no momento). response Extension point for future extensions of the WebMessage format.\n\nThroughout the Web API, we refer to dates and periods. The date format is:\n\nFor instance, if you want to express March 20, 2014, you must use 2014-03-20.\n\nThe period format is described in the following table (also available on the API endpoint /api/periodTypes)\n\nTabela: Formato de perÃ­odo\n\nIntervalo Formato Exemplo DescriÃ§Ã£o Dia aaaaMMdd 20040315 MarÃ§o 15, 2004 Semana yyyyWn 2004W10 Week 10 2004 Week Wednesday yyyyWedWn 2015WedW5 Semana 5, iniciando na Quarta-feira Week Thursday yyyyThuWn 2015ThuW6 Week 6 with start Thursday Week Saturday yyyySatWn 2015SatW7 Week 7 with start Saturday Week Sunday yyyySunWn 2015SunW8 Week 8 with start Sunday Bi-week yyyyBiWn 2015BiW1 Week 1-2 20015 MÃªs yyyyMM 200403 March 2004 Bi-month yyyyMMB 200401B January-February 2004 Quarter yyyyQn 2004Q1 January-March 2004 Six-month yyyySn 2004S1 January-June 2004 Six-month April yyyyAprilSn 2004AprilS1 April-September 2004 Ano yyyy 2004 2004 Financial Year April yyyyApril 2004April Apr 2004-Mar 2005 Financial Year July yyyyJuly 2004July July 2004-June 2005 Financial Year Oct yyyyOct 2004Oct Oct 2004-Sep 2005\n\nIn some parts of the API, like for the analytics resource, you can utilize relative periods in addition to fixed periods (defined above). The relative periods are relative to the current date and allow e.g. for creating dynamic reports. The available relative period values are:\n\nOs recursos de query do Analytics oferecem suporte a parÃ¢metros extras para expressar perÃ­odos.\n\nDefault pe dimension will fall back to:\n\neventDate para /analytics/events/query\n\nenrollmentDate para /analytics/enrollments/query\n\nAdding conditions on one or more date fields and combining them are allowed.\n\nEm recursos que oferecem suporte a perÃ­odos de data personalizados, hÃ¡ parÃ¢metros de consulta extras que serÃ£o combinados para expressar condiÃ§Ãµes na dimensÃ£o de tempo.\n\nPerÃ­odo de data customizado events query resource enrollment query resource eventDate [x] [ ] enrollmentDate [x] [x] scheduledDate [x] [ ] incidentDate [x] [x] lastUpdated [x] [x]\n\nConditions can be expressed in the following form:\n\nanalytics/events/query/...?...&eventDate=2021&...\n\nIt's possible to combine more time fields in the same query:\n\nanalytics/events/query/...?...&eventDate=2021&incidentDate=202102&...\n\nAll of these conditions can be combined with pe dimension:\n\nanalytics/events/query/...?...&dimension=pe:TODAY&enrollmentDate=2021&incidentDate=202102&...\n\nSupported formats are described in \"date and period format\" above. An extra format is provided to express a range of dates: yyyyMMdd_yyyyMMdd and yyyy-MM-dd_yyyy-MM-dd.\n\nIn the example bellow, the endpoint will return events that are scheduled to happen between 20210101 and 20210104:\n\nanalytics/events/query/...?...&dimension=pe:TODAY&enrollmentDate=2021&incidentDate=202102&scheduledDate=20210101_20210104&...\n\nAuthorities\n\nSystem authority ids and names can be listed using:\n\nEle retorna o seguinte formato:\n\nMetadata\n\nEsquemas de identificador\n\nThis section provides an explanation of the identifier scheme concept. Identifier schemes are used to map metadata objects to other metadata during import, and to render metadata as part of exports. Note that not all schemes work for all API calls, and not all schemes can be used for both input and output. This is outlined in the sections explaining the various API endpoints.\n\nThe full set of identifier scheme object types available are listed below, using the name of the property to use in queries:\n\nidScheme\n\ndataElementIdScheme\n\ncategoryOptionComboIdScheme\n\norgUnitIdScheme\n\nprogramIdScheme\n\nprogramStageIdScheme\n\ntrackedEntityIdScheme\n\ntrackedEntityAttributeIdScheme\n\nThe general idScheme applies to all types of objects. It can be overridden by specific object types.\n\nThe default scheme for all parameters is UID (stable DHIS2 identifiers). The supported identifier schemes are described in the table below.\n\nTabela: Scheme Values\n\nScheme DescriÃ§Ã£o ID, UID Corresponde ao identificador estÃ¡vel DHIS2, este Ã© o esquema de identificaÃ§Ã£o padrÃ£o. CÃDIGO CorrespondÃªncia ao CÃ³digo DHIS2, usado principalmente para trocar dados com um sistema externo. NOME Match on DHIS2 Name, please note that this uses what is available as object.name, and not the translated name. Also note that names are not always unique, and in that case, they can not be used. ATTRIBUTE:ID Match on metadata attribute, this attribute needs to be assigned to the type you are matching on, and also that the unique property is set to true. The main usage of this is also to exchange data with external systems, it has some advantages over CODE since multiple attributes can be added, so it can be used to synchronize with more than one system.\n\nNote that identifier schemes is not an independent feature but needs to be used in combination with resources such as data value import, metadata import and GeoJson import.\n\nAs an example, to specify CODE as the general id scheme and override with UID for organisation unit id scheme you can use these query parameters:\n\nAs another example, to specify an attribute for the organisation unit id scheme, code for the data element id scheme and use the default UID id scheme for all other objects you can use these parameters:\n\nNavegando na API da Web\n\nThe entry point for browsing the Web API is /api. This resource provides links to all available resources. Four resource representation formats are consistently available for all resources: HTML, XML, JSON, and JSONP. Some resources will have other formats available, like MS Excel, PDF, CSV, and PNG. To explore the API from a web browser, navigate to the /api entry point and follow the links to your desired resource, for instance /api/dataElements. For all resources which return a list of elements certain query parameters can be used to modify the response:\n\nQuery parameters Parameter Option values Default option DescriÃ§Ã£o paging true | false verdade Indicates whether to return lists of elements in pages. page number 1 Defines which page number to return. pageSize number 50 Defines the number of elements to return for each page. order property:asc/iasc/desc/idesc Order the output using a specified order, only properties that are both persisted and simple (no collections, idObjects etc) are supported. iasc and idesc are case insensitive sorting. If it is wanted to sort for more than one property, separate them using a comma.\n\nAn example of how these parameters can be used to get a full list of data element groups in XML response format is:\n\nYou can query for elements on the name property instead of returning a full list of elements using the query query variable. In this example we query for all data elements with the word \"anaemia\" in the name:\n\nPode obter pÃ¡ginas especÃ­ficas e tamanhos de pÃ¡gina de objetos como este:\n\nPode desativar completamente a paginaÃ§Ã£o assim:\n\nPara ordenar o resultado com base em uma propriedade especÃ­fica:\n\nTo order the result based on created datetime property first (descending order) and then by name property (ascending order):\n\nPode encontrar um objecto com base em seu ID em todos os tipos de objecto por meio de o recurso identifiableObjects:\n\nTraduÃ§Ã£o\n\nDHIS2 supports translations of database content, such as data elements, indicators, and programs. All metadata objects in the Web API have properties meant to be used for display / UI purposes, which include displayName, displayShortName, displayDescription and displayFormName (for data elements and tracked entity attributes).\n\nTranslate options Parameter Values DescriÃ§Ã£o translate true | false Translate display* properties in metadata output (displayName, displayShortName, displayDescription, and displayFormName for data elements and tracked entity attributes). Default value is true. locale Locale to use Translate metadata output using a specified locale (requires translate=true).\n\nAPI de traduÃ§Ã£o\n\nThe translations for an object is rendered as part of the object itself in the translations array. Note that the translations array in the JSON/XML payloads is normally pre-filtered for you, which means they can not directly be used to import/export translations (as that would normally overwrite locales other than current users).\n\nExemplo de elemento de dados com matriz de traduÃ§Ã£o filtrada na localidade do usuÃ¡rio:\n\nExemplo de elemento de dados com traduÃ§Ãµes desativadas:\n\nNote that even if you get the unfiltered result, and are using the appropriate type endpoint i..e /api/dataElements we do not allow updates, as it would be too easy to make mistakes and overwrite the other available locales.\n\nTo read and update translations you can use the special translations endpoint for each object resource. These can be accessed by GET or PUT on the appropriate /api/<object-type>/<object-id>/translations endpoint.\n\nAs an example, for a data element with identifier FTRrcoaog83, you could use /api/dataElements/FTRrcoaog83/translations to get and update translations. The fields available are property with options NAME, SHORT_NAME, FORM_NAME, DESCRIPTION, locale which supports any valid locale ID and the translated property value.\n\nExemplo de propriedade NAME para a localidade francesa:\n\nThis payload would then be added to a translation array, and sent back to the appropriate endpoint:\n\nFor a data element with ID FTRrcoaog83 you can PUT this to /api/dataElements/FTRrcoaog83/translations. Make sure to send all translations for the specific object and not just for a single locale (if not you will potentially overwrite existing locales for other locales).\n\nThe status code will be 204 No Content if the data value was successfully saved or updated, or 404 Not Found if there was a validation error (e.g. more than one SHORT_NAME for the same locale).\n\nVersÃµes da API da web\n\nThe Web API is versioned starting from DHIS 2.25. The API versioning follows the DHIS2 major version numbering. As an example, the API version for DHIS 2.33 is 33.\n\nYou can access a specific API version by including the version number after the /api component, as an example like this:\n\nSe omitir a parte da versÃ£o da URL, o sistema usarÃ¡ a versÃ£o actual VersÃ£o da API. Como exemplo, para DHIS 2.25, ao omitir a parte da API, o sistema usarÃ¡ a versÃ£o 25 da API. Ao desenvolver clientes de API, Ã© recomendado usar versÃµes de API explÃ­citas (em vez de omitir a API versÃ£o), pois isso protegerÃ¡ o cliente de mudanÃ§as imprevistas na API.\n\nThe last three API versions will be supported. As an example, DHIS version 2.27 will support API version 27, 26 and 25.\n\nNote that the metadata model is not versioned and that you might experience changes e.g. in associations between objects. These changes will be documented in the DHIS2 major version release notes.\n\nFiltro de objeto de metadados\n\nTo filter the metadata there are several filter operations that can be applied to the returned list of metadata. The format of the filter itself is straight-forward and follows the pattern property:operator:value, where property is the property on the metadata you want to filter on, operator is the comparison operator you want to perform and value is the value to check against (not all operators require value).\n\nPlease see the schema section to discover which properties are available. In addition to the listed properties filters can apply to custom attribute values by using the attribute's ID as property name.\n\nRecursive filtering, ie. filtering on associated objects or collection of objects, is supported as well.\n\nAvailable Operators Operator Types Value required DescriÃ§Ã£o eq string | boolean | integer | float | enum | collection (checks for size) | date verdade Equality !eq string | boolean | integer | float | enum | collection (checks for size) | date verdade Inequality ieq string verdade Case insensitive string, match exact ne string | boolean | integer | float | enum | collection (checks for size) | date verdade Inequality like string verdade Case sensitive string, match anywhere !like string verdade Case sensitive string, not match anywhere $like string verdade Case sensitive string, match start !$like string verdade Case sensitive string, not match start like$ string verdade Case sensitive string, match end !like$ string verdade Case sensitive string, not match end ilike string verdade Case insensitive string, match anywhere !ilike string verdade Case insensitive string, not match anywhere $ilike string verdade Case insensitive string, match start !$ilike string verdade Case insensitive string, not match start ilike$ string verdade Case insensitive string, match end !ilike$ string verdade Case insensitive string, not match end gt string | boolean | integer | float | collection (checks for size) | date verdade Greater than ge string | boolean | integer | float | collection (checks for size) | date verdade Greater than or equal lt string | boolean | integer | float | collection (checks for size) | date verdade Less than le string | boolean | integer | float | collection (checks for size) | date verdade Less than or equal null all falso Property is null !null all falso Property is not null empty collection falso Collection is empty token string verdade Match on multiple tokens in search property !token string verdade Not match on multiple tokens in search property in string | boolean | integer | float | date verdade Find objects matching 1 or more values !in string | boolean | integer | float | date verdade Find objects not matching 1 or more values\n\nOperators will be applied as logical and query. If you need a or query, you can have a look at the in filter and the section below. The filtering mechanism allows for recursion. See below for some examples.\n\nObtenha elementos de dados com propriedade id ID1 ou ID2:\n\nGet data elements, ignoring case, with name property MyDataElement:\n\nGet all data elements which have a data set with id ID1:\n\nGet all data elements with aggregation operator sum and value type int:\n\nYou can do filtering within collections, e.g. to get data elements which are members of the ANC data element group you can use the following query using the id property of the associated data element groups:\n\nTo get data elements with a particular attribute value for a metadata attribute, a filter for the attribute ID and the attribute value can be specified using the same collection query syntax:\n\nGet data elements which have any option set:\n\nSince all operators are and by default, you can't find a data element matching more than one id, for that purpose you can use the in operator.\n\nOperadores lÃ³gicos\n\nAs mentioned in the section before, the default logical operator applied to the filters is AND which means that all object filters must be matched. There are however cases where you want to match on one of several filters (maybe id and code field) and in those cases, it is possible to switch the root logical operator from AND to OR using the rootJunction parameter.\n\nExample: Normal filtering where both id and code must match to have a result returned\n\nExample: Filtering where the logical operator has been switched to OR and now only one of the filters must match to have a result returned\n\nFiltro de token identificÃ¡vel\n\nIn addition to the specific property based filtering mentioned above, we also have token based AND filtering across a set of properties: id, code, and name (also shortName if available). These properties are commonly referred to as identifiable. The idea is to filter metadata whose id, name, code or short name containing something.\n\nExample: Filter all data elements containing 2nd in any of the following: id,name,code, shortName\n\nTambÃ©m Ã© possÃ­vel especificar vÃ¡rios valores de filtragem.\n\nExample: Get all data elements where ANC visit is found in any of the identifiable properties. The system returns all data elements where both tokens (ANC and visit) are found anywhere in identifiable properties.\n\nIt is also possible to combine the identifiable filter with property-based filter and expect the rootJunction to be applied.\n\nIndexable only filter for tracked entity attributes\n\nFor tracked entity attributes, there is a special filter in addition to the previous mentioned filtering capabilities. Some of the tracked entity attributes are candidates for creating a trigram index for better lookup performance. Using the indexableOnly parameter set to true, the results can be filtered to include only the attributes that are trigram indexable.\n\nExample: Get all tracked entity attributes that are indexable.\n\nAdditional filters along with the indexableOnly parameter can be specified.\n\nExample: Get all tracked entity attributes where ANC is found in any of the name property. The system returns the tracked entity attributes where the name matches the provided keyword as well as if the attribute is indexable.\n\nFiltro de campo de metadados\n\nIn many situations, the default views of the metadata can be too verbose. A client might only need a few fields from each object and want to remove unnecessary fields from the response. To discover which fields are available for each object please see the schema section. In addition to the listed properties custom attributes can be included for top level objects by using the attribute's ID as property name.\n\nThe format for include/exclude allows for infinite recursion. To filter at the \"root\" level you can just use the name of the field, i.e. ?fields=id,name which would only display the id and name fields for every object. For objects that are either collections or complex objects with properties on their own, you can use the format ?fields=id,name,dataSets[id,name] which would return id, name of the root, and the id and name of every data set on that object. Negation can be done with the exclamation operator, and we have a set of presets of field select. Both XML and JSON formats are supported.\n\n** Exemplo **: Obtenha id ename no recurso de indicadores:\n\nExample: Get id and name from data elements, and id and name from the associated data sets:\n\nExample: Get id, name and the value of a user defined attribute with ID DnrLSdo4hMl for organisation units:\n\nThe attribute is then included as property DnrLSdo4hMl of each matching object in the response. This can be renamed using the rename transformer as shown in the next section.\n\nTo exclude a field from the output you can use the exclamation ! operator. This is allowed anywhere in the query and will simply not include that property as it might have been inserted in some of the presets.\n\nA few presets (selected fields groups) are available and can be applied using the : operator.\n\nProperty operators Operator DescriÃ§Ã£o <field-name> Include property with name, if it exists. <object>[<field-name>, ...] Includes a field within either a collection (will be applied to every object in that collection), or just on a single object. !<field-name>, <object>[!<field-name> Do not include this field name, it also works inside objects/collections. Useful when you use a preset to include fields. *, <object>[*] Include all fields on a certain object, if applied to a collection, it will include all fields on all objects on that collection. :<preset> Alias to select multiple fields. Three presets are currently available, see the table below for descriptions.\n\nField presets Preset DescriÃ§Ã£o all All fields of the object * Alias for all identifiable Includes id, name, code, created, lastUpdated and lastUpdatedBy fields nameable Includes id, name, shortName, code, description, created and lastUpdated fields persisted Returns all persisted property on an object, does not take into consideration if the object is the owner of the relation. owner Returns all persisted property on an object where the object is the owner of all properties, this payload can be used to update through the API.\n\nExample: Include all fields from data sets except organisation units:\n\n** Exemplo **: inclui apenas id, nome e a coleÃ§Ã£o de unidades de organizaÃ§Ã£o de um conjunto de dados, mas exclui a id de unidades de organizaÃ§Ã£o:\n\n** Exemplo **: Incluir propriedades que podem ser nomeadas de todos os indicadores:\n\nTransformadores de campo\n\nField transforms can be used to transform properties. The syntax is described below.\n\nIsso renomearÃ¡ a propriedade id para i e a propriedade name para n.\n\nMultiple transformers can be applied to a single property by repeating the transformer operator:\n\nThe supported transformer operators are described in the table below.\n\nAvailable Transformers Nome Arguments DescriÃ§Ã£o size Gives sizes of strings (length) and collections isEmpty Is string or collection empty isNotEmpty Is string or collection not empty rename Arg1: name Renames the property name paging Arg1: page,Arg2: pageSize Pages a collection, default pageSize is 50. pluck Optional Arg1: fieldName Converts an array of objects to an array of a selected field of that object. By default, the first field that is returned by the collection is used (normally the ID). keyBy Optional Arg1: fieldName Converts an array of objects to an object where the fieldName (default id) is used as the key. This can be useful for quick lookups in JavaScript for example\n\nExemplos\n\nExamples of transformer usage are found below.\n\nGet the size of a collection:\n\nTest if a collection is empty:\n\nTest if a collection is not empty:\n\nRename properties:\n\nApply paging to a collection:\n\nGet array with IDs of organisation units:\n\nGet array with names of organisation units:\n\nKey the dataElements array by the id field:\n\nKey the dataElements array by the valueType field, since multiple hits this will results in arrays (of data elements):\n\nMetadados criam, lÃªem, atualizam, excluem, validam\n\nAll metadata entities in DHIS2 have their own API endpoint which supports CRUD operations (create, read, update and delete). The endpoint URLs follows this format:\n\nThe entityName uses the camel-case notation. As an example, the endpoint for data elements is:\n\nNOTE: When updating objects, all existing property values will be overwritten, even if the new value is null. Please use JSON Patch API in case you want do partial update to an object.\n\nOs seguintes parÃ¢metros de consulta de solicitaÃ§Ã£o estÃ£o disponÃ­veis em todos os terminais de metadados.\n\nAvailable Query Filters Param Modelo Requerido Options (default first) DescriÃ§Ã£o preheatCache boolean falso true | false Turn cache-map preheating on/off. This is on by default, turning this off will make initial load time for importer much shorter (but will make the import itself slower). This is mostly used for cases where you have a small XML/JSON file you want to import, and don't want to wait for cache-map preheating. importStrategy enum falso CREATE_AND_UPDATE | CREATE | UPDATE | DELETE Import strategy to use, see below for more information.\n\nCriaÃ§Ã£o e actualizaÃ§Ã£o de objectos\n\nFor creating new objects you will need to know the endpoint, the type format, and make sure that you have the required authorities. As an example, we will create and update a constant. To figure out the format, we can use the new schema endpoint for getting format description. So we will start with getting that info:\n\nFrom the output, you can see that the required authorities for create are F_CONSTANT_ADD, and the important properties are: name and value. From this, we can create a JSON payload and save it as a file called constant.json:\n\nO mesmo conteÃºdo de uma carga Ãºtil XML:\n\nWe are now ready to create the new constant by sending a POST request to the constants endpoint with the JSON payload using curl:\n\nA specific example of posting the constant to the demo server:\n\nSe tudo correr bem, verÃ¡ uma saÃ­da semelhante a:\n\nO processo serÃ¡ exatamente o mesmo para actualizaÃ§Ã£o, faz suas alteraÃ§Ãµes para a carga JSON/XML, descubra o ID da constante e, em seguida, envie uma solicitaÃ§Ã£o PUT para o endpoint incluindo o ID:\n\nExcluindo objetos\n\nDeleting objects is very straight forward, you will need to know the ID and the endpoint of the type you want to delete, let's continue our example from the last section and use a constant. Let's assume that the id is abc123, then all you need to do is the send the DELETE request to the endpoint + id:\n\nUma exclusÃ£o bem-sucedida deve retornar o status HTTP 204 (sem conteÃºdo).\n\nAdicionar e remover objetos em coleÃ§Ãµes\n\nO recurso de coleÃ§Ãµes permite modificar coleÃ§Ãµes de objectos.\n\nAdicionar ou remover objetos Ãºnicos\n\nPara adicionar ou remover objectos de ou para uma coleÃ§Ã£o de objectos, pode usar o seguinte padronizar:\n\nDeve usar o mÃ©todo POST para adicionar e o mÃ©todo DELETE para remover um objecto. Quando hÃ¡ uma relaÃ§Ã£o muitos-para-muitos entre objectos, deve primeiro determinar qual objecto possui o relacionamento. Se nÃ£o for claro qual objecto Ã© este, tente a chamada nos dois sentidos para ver qual funciona.\n\nOs componentes do padrÃ£o sÃ£o:\n\nobjecto de coleÃ§Ã£o: o tipo de objectos que possui a coleÃ§Ã£o que deseja modificar.\n\nID do objecto de coleÃ§Ã£o: O identificador do objecto que possui o coleÃ§Ã£o que deseja modificar.\n\nnome da coleÃ§Ã£o: o nome da coleÃ§Ã£o que deseja modificar.\n\nID do objecto: O identificador do objeto que deseja adicionar ou remover da coleÃ§Ã£o.\n\nComo exemplo, para remover um elemento de dados com identificador IDB de um grupo de elementos de dados com identificador IDA, pode fazer um DELETE solicitar:\n\nPara adicionar uma opÃ§Ã£o de categoria com identificador IDB a uma categoria com identificador IDA vocÃª pode fazer um POST solicitar:\n\nAdicionar ou remover vÃ¡rios objetos\n\nPode adicionar ou remover vÃ¡rios objectos de uma coleÃ§Ã£o em uma solicitaÃ§Ã£o com uma carga como esta:\n\nUsando essa carga, vocÃª pode adicionar, substituir ou excluir itens:\n\nAdicionando itens:\n\nSubstituindo itens:\n\nExcluir Itens:\n\nAdicionar e remover objetos em uma Ãºnica solicitaÃ§Ã£o\n\nYou can both add and remove objects from a collection in a single POST request to the following URL:\n\nO formato da carga Ãºtil Ã©:\n\nValidando cargas Ãºteis\n\nDHIS 2 supports system wide validation of metadata payloads, which means that create and update operations on the API endpoints will be checked for valid payload before allowing changes to be made. To find out what validations are in place for a specific endpoint, have a look at the /api/schemas endpoint, i.e. to figure out which constraints a data element have, you would go to /api/schemas/dataElement.\n\nTambÃ©m pode validar sua carga manualmente enviando-a para o ponto final do esquema. Se quisesse validar a constante do create secÃ§Ã£o antes, enviaria assim:\n\nUm exemplo simples (nÃ£o validante) seria:\n\nWhich will yield the result:\n\nFor our API endpoints that deal with metadata, we support partial updates (PATCH) using the JSON patch standard. The payload basically outlines a set of operation you want applied to a existing metadata object. For JSON patch details and examples, see jsonpatch.com. Three operators are supported: add, remove and replace.\n\nBelow is a few examples relevant to DHIS2. Note that any update to a payload should be thought of as a HTTP PUT operation, i.e. any mutation must result in a valid PUT metadata payload.\n\nThe default importReportMode for JSON patch is ERRORS_NOT_OWNER which implies that when updating any property which is not owned by that particular object (for example trying to add a indicator group directly to an indicator) you will get an error.\n\nAs per the JSON patch specification you must always use the mimetype application/json-patch+json when sending patches.\n\nExemplos\n\nAdd new data element to a data element group\n\nRemove all data element associations from a data element group\n\nChange domain and value type of a data element\n\nRemove a specific orgUnit from an orgUnit group\n\nBlocked add dataElementGroup to dataElement\n\nRemove collection item by id\n\nPatch request with invalid path\n\nIf path property is invalid or does not exist the patch service will return an error as below\n\nResponse\n\nMetadata CSV export\n\nField filtering works almost the same for CSV (please note that using CSV on the /api/metadata endpoint is not supported), but not that field fransformations are not yet supported.\n\nFor endpoints that support CSV (our metadata endpoints like /api/dataElements /api/organisationUnits) you can either use the Accept header with value text/csv or you can use the extension .csv. Be aware that complex objects are not supported, and we only support id-object collections (so a list of UIDs will be returned).\n\nNome Options DescriÃ§Ã£o fields Same as metadata field filter (with the caveats mentioned above) Default filter is id,displayName skipHeader false/true Should the header (with column names) be included or not separator Default: . Column separator arraySeparator Default: ; If one of the field is a collection of id-objects this separator will separate all the UIDs\n\nExemplos\n\nGet all data elements including their group associations\n\nGet all org units including geometry (which will get ignored)\n\nExportaÃ§Ã£o de metadados\n\nThis section explains the metatada API which is available at /api/metadata. XML and JSON resource representations are supported.\n\nThe most common parameters are described below in the \"Export Parameter\" table. You can also apply this to all available types by using type:fields=<filter> and type:filter=<filter>. You can also enable/disable the export of certain types by setting type=true|false.\n\nExport Parameter Nome Options DescriÃ§Ã£o fields Same as metadata field filter Default field filter to apply for all types, default is :owner. filter Same as metadata object filter Default object filter to apply for all types, default is none. order Same as metadata order Default order to apply to all types, default is name if available, or created if not. translate false/true Enable translations. Be aware that this is turned off by default (in other endpoints this is on by default). locale <locale> Change from user locale, to your own custom locale. defaults INCLUDE/EXCLUDE Should auto-generated category object be included or not in the payload. If you are moving metadata between 2 non-synced instances, it might make sense to set this to EXCLUDE to ease the handling of these generated objects. skipSharing false/true Enabling this will strip the sharing properties from the exported objects. This includes user, publicAccess, userGroupAccesses, userAccesses, and externalAccess. download false/true Enabling this will add HTTP header Content-Disposition that specifies that the data should be handled as an attachment and will be offered by web browsers as a download.\n\nExemplos de exportaÃ§Ã£o de metadados\n\nExport all metadata. Be careful as the response might be very large depending on your metadata configuration:\n\nExportar todos os metadados ordenados por lastUpdated decrescente:\n\nExporte metadados apenas incluindo indicadores e grupos de indicadores:\n\nExportar id e displayName para todos os elementos de dados, ordenados por displayName:\n\nExporte elementos de dados e indicadores onde o nome comeÃ§a com \"ANC\":\n\nExportaÃ§Ã£o de metadados com dependÃªncias\n\nWhen you want to exchange metadata for a data set, program, category combo, dashboard, option set or data element group from one DHIS2 instance to another instance there are six dedicated endpoints available:\n\nEssas exportaÃ§Ãµes podem entÃ£o ser importadas usando / api / metadata.\n\nEsses endpoints tambÃ©m oferecem suporte aos seguintes parÃ¢metros:\n\nExport Parameter Nome Options DescriÃ§Ã£o skipSharing false/true Enabling this will strip the sharing properties from the exported objects. This includes user, publicAccess, userGroupAccesses, userAccesses, and externalAccess. download false/true Enabling this will add HTTP header Content-Disposition that specifies that the data should be handled as an attachment and will be offered by web browsers as a download.\n\nImportaÃ§Ã£o de metadados\n\nThis section explains the metadata import API. XML and JSON resource representations are supported. Metadata can be imported using a POST request.\n\nThe importer allows you to import metadata payloads which may include many different entities and any number of objects per entity. The metadata export generated by the metadata export API can be imported directly.\n\nThe metadata import endpoint support a variety of parameters, which are listed below.\n\nImport Parameter Nome Options (first is default) DescriÃ§Ã£o importMode COMMIT, VALIDATE Sets overall import mode, decides whether or not to only VALIDATE or also COMMIT the metadata, this has similar functionality as our old dryRun flag. identifier UID, CODE, AUTO Sets the identifier scheme to use for reference matching. AUTO means try UID first, then CODE. importReportMode ERRORS, FULL, DEBUG Sets the ImportReport mode, controls how much is reported back after the import is done. ERRORS only includes ObjectReports for object which has errors. FULL returns an ObjectReport for all objects imported, and DEBUG returns the same plus a name for the object (if available). preheatMode REFERENCE, ALL, NONE Sets the preheater mode, used to signal if preheating should be done for ALL (as it was before with preheatCache=true) or do a more intelligent scan of the objects to see what to preheat (now the default), setting this to NONE is not recommended. importStrategy CREATE_AND_UPDATE, CREATE, UPDATE, DELETE Sets import strategy, CREATE_AND_UPDATE will try and match on identifier, if it doesn't exist, it will create the object. atomicMode ALL, NONE Sets atomic mode, in the old importer we always did a best effort import, which means that even if some references did not exist, we would still import (i.e. missing data elements on a data element group import). Default for new importer is to not allow this, and similar reject any validation errors. Setting the NONE mode emulated the old behavior. flushMode AUTO, OBJECT Sets the flush mode, which controls when to flush the internal cache. It is strongly recommended to keep this to AUTO (which is the default). Only use OBJECT for debugging purposes, where you are seeing hibernate exceptions and want to pinpoint the exact place where the stack happens (hibernate will only throw when flushing, so it can be hard to know which object had issues). skipSharing false, true Skip sharing properties, does not merge sharing when doing updates, and does not add user group access when creating new objects. skipValidation false, true Skip validation for import. NOT RECOMMENDED. async false, true Asynchronous import, returns immediately with a Location header pointing to the location of the importReport. The payload also contains a json object of the job created. inclusionStrategy NON_NULL, ALWAYS, NON_EMPTY NON_NULL includes properties which are not null, ALWAYS include all properties, NON_EMPTY includes non empty properties (will not include strings of 0 length, collections of size 0, etc.) userOverrideMode NONE, CURRENT, SELECTED Allows you to override the user property of every object you are importing, the options are NONE (do nothing), CURRENT (use import user), SELECTED (select a specific user using overrideUser=X) overrideUser User ID If userOverrideMode is SELECTED, use this parameter to select the user you want override with.\n\nNOTE When updating objects, all property values will be overwritten even if the new values are null. Please use JSON Patch API in case you want do partial update to an object.\n\nAn example of a metadata payload to be imported looks like this. Note how each entity type have their own property with an array of objects:\n\nWhen posting this payload to the metadata endpoint, the response will contain information about the parameters used during the import and a summary per entity type including how many objects were created, updated, deleted and ignored:\n\nGeoJSON import\n\nThe GeoJSON import is used to attach geometry data to organisation units.\n\nFor a bulk import a GeoJSON file with a feature collection is expected. Each feature in the collection requires a reference to the organisation unit it should be linked to.\n\nBy default, the geometry from the file is stored as the geometry property of an organisation unit. To store additional geometries attributes of type GEOJSON can be created. When attributes are use all geometries from a file are stored for the same attribute which is provided with an additional parameter attributeId.\n\nGeoJSON Bulk Data Import\n\nImport Parameters Nome Modelo PadrÃ£o DescriÃ§Ã£o geoJsonId boolean true When true the id property of the GeoJSON features is expected to hold the organisation unit identifier geoJsonProperty String undefined If geoJsonId is false this parameter names the property in the GeoJSON feature's properties that holds the organisation unit identifier orgUnitProperty enum: [id, code, name] id The property of the organisation unit that is referred to by the identifiers used in the GeoJSON file attributeId String undefined When set the geometry is stored as value of the attribute referenced by ID dryRun boolean false When true the import is processed without actually updating the organisation units async boolean false When true the import is processed asnychronously\n\nUasge:\n\nThe post body is the GeoJSON file. Content type should be application/json or application/geo+json. The file may be .zip or .gzip compressed.\n\nFor example, a default file where id is used to refer to an organisation unit id has this structure:\n\nA file where a feature property is used to refer to the organisation unit code would have this structure:\n\nThe coordinates in a geometry may be pairs or triplets. If a third dimension is present it is stripped during the import.\n\nA geometry may also be null to effectively clear or delete the geometry for specific organisation units. There is a special bulk deletion API that is described in the next section.\n\nWhen run synchronously an import report is returned directly. The HTTP status code is always OK, the status in the message payload indicates if all rows were imported successfully. The import counts statistics contained in the report give further information:\n\nimported: number of organisation units that were successfully updated with a geometry that did not have one before for the updated property\n\nupdated: number of organisation units that were successfully updated with a geometry that did have value for the updated property already\n\nignored: number of organisation units that failed to update\n\ndeleted: number of organisation units that where successfully update with a empty geometry\n\nWhen the import is run asynchronous the request returns immediately with status OK and job configuration response that contains a relative reference to the task endpoint that allows to track the status of the asynchronous import. For example:\n\nThe summary that is returned directly for synchronous execution is available at\n\nonce the import is finished.\n\nGeoJSON Bulk Data Deletion\n\nTo clear or unset the geometry data for all organisation units use:\n\nTo clear or unset the geometry data for a specific GEOJSON attribute for all organisation units use:\n\nClearing is always synchronous and returns a similar report as the bulk import. It does not support any other parameters. No dry-run can be performed. Bulk clearing requires the F_PERFORM_MAINTENANCE authority.\n\nGeoJSON Single Data Import\n\nThe single import allows to update the geometry of a single organisation unit.\n\nThe post body only contains the GeoJSON geometry value, for example:\n\nSingle import only supports attributeId and dryRun parameters.\n\nGeoJSON Single Data Deletion\n\nTo clear the geometry GeoJSON data of an individual organisation unit use:\n\nSimilarly to clear a GEOJSON attribute value for an individual organisation unit use:\n\nClearing is always synchronous returns a similar report as single import. The dry-run parameter is supported as well. The performing user requires authority to modify the target organisation unit.\n\nEsquema\n\nA resource which can be used to introspect all available DXF 2 objects can be found on /api/schemas. For specific resources you can have a look at /api/schemas/<type>.\n\nPara obter todos os esquemas disponÃ­veis em XML:\n\nPara obter todos os esquemas disponÃ­veis em JSON:\n\nPara obter o esquema JSON para uma classe especÃ­fica:\n\nÃcones\n\nDHIS2 includes a collection of icons that can be used to give visual context to metadata. There are two different kind of icons: - Default icons: they are pre-installed in the application and are not possible to modify nor delete. - Custom icons: can be created, updated and deleted at will.\n\nBoth of them be accessed through the icons resource.\n\nThis endpoint returns a list of information about the available default and custom icons. By default key, description, keywords and href will be included in response. But fields parameter can be used to change this behaviour.\n\nIt's also possible to get a particular icon directly by filtering by its key, in the example below, the key is mosquito_outline.\n\nCustom icon operations\n\nA list of custom icons can be fetched retrieved certain request parameters\n\nRequest parameter Modelo Allowed values DescriÃ§Ã£o type Text DEFAULT,CUSTOM,ALL What type of icons should be retrieved. Default is ALL keys Text List of keys custom icons should be retrieved for keywords Text List of keywords custom icons should be retrieved for search Text Search for a given text across icon keys and keywords, and retrieve all icons that contain this text in their key or keywords. createdStartDate Date Starting point of created date createdEndDate Date End point of created date lastUpdatedStartDate Date Starting point of last updated date lastUpdatedEndDate Date End point of last updated date\n\nRequest parameters for pagination\n\nRequest parameter Modelo Allowed values DescriÃ§Ã£o page Integer Any positive integer Page number to return. Defaults to 1 if missing pageSize Integer Any positive integer Page size. Defaults to 50. paging Boolean true|false Indicates whether paging should be ignored and all rows should be returned. Defaults to true, meaning that by default all requests are paginated, unless paging=false\n\nRequest parameters for ordering\n\nRequest parameter Modelo Allowed values DescriÃ§Ã£o order Text created:desc Comma-separated list of property name and sort direction pairs in format propName:sortDirection. By default icons will be ordered based on key:asc\n\nRequest parameter to filter responses\n\nThe endpoints accept a fields parameter which controls which fields will be returned in the JSON response. fields parameter accepts a comma separated list of field names. If nothing is specified, default fields will be used and those are\n\nkey,keywords,description,fileResourceUid,createdByUserUid,href\n\nA custom icon resource can be downloaded by providing the icon key:\n\nCustom icons can be created, modified and deleted. To create a custom icon, use the resource below.\n\nIt expects a payload containing the icon key, description, list of keywords and the file resource uid to be linked to the data.\n\nOnly custom icons can be updated using below resource.\n\nWith the following payload, the icon's description and keywords would be updated.\n\nPlease notice that's also possible to just update one of the two. That means in case we would like to update the description while keeping the keywords, we would just need to provide the icon key and the descripton json field. Same would work the other way around, to update the keywords and leave the original description untouched.\n\nOnly custom icon can be deleted using below resource.\n\nRender type\n\nAlguns tipos de metadados tÃªm uma propriedade chamada renderType. O tipo de renderizaÃ§Ã£o Ã© um mapa entre um device e um renderingType. FormulÃ¡rios pode usar esta informaÃ§Ã£o como uma dica sobre como o objeto deve ser renderizado em um dispositivo especÃ­fico. Por exemplo, um dispositivo mÃ³vel pode querer renderizar um elemento de dados de forma diferente de um computador desktop.\n\nAtualmente, existem dois tipos diferentes de renderingTypes disponÃ­veis:\n\nRenderizaÃ§Ã£o de tipo de valor\n\nRenderizaÃ§Ã£o da seÃ§Ã£o do estÃ¡gio do programa\n\nExistem tambÃ©m 2 tipos de dispositivos disponÃ­veis:\n\nMÃVEL\n\nÃREA DE TRABALHO\n\nA tabela a seguir lista os metadados e os tipos de renderizaÃ§Ã£o disponÃ­veis. A renderizaÃ§Ã£o do tipo de valor tem restriÃ§Ãµes de adiÃ§Ã£o com base nos metadados configuraÃ§Ã£o, que serÃ¡ mostrada em uma segunda tabela.\n\nMetadata and RenderingType overview Metadata type Available RenderingTypes SeÃ§Ã£o de estÃ¡gio do programa * LISTING (default)\n\n* SEQUENTIAL\n\n* MATRIX Elemento de dados * DEFAULT\n\n* DROPDOWN\n\n* VERTICAL_RADIOBUTTONS\n\n* HORIZONTAL_RADIOBUTTONS\n\n* VERTICAL_CHECKBOXES\n\n* HORIZONTAL_CHECKBOXES\n\n* SHARED_HEADER_RADIOBUTTONS\n\n* ICONS_AS_BUTTONS\n\n* SPINNER\n\n* ICON\n\n* TOGGLE\n\n* VALUE\n\n* SLIDER\n\n* LINEAR_SCALE\n\n* AUTOCOMPLETE\n\n* QR_CODE\n\n* BAR_CODE\n\n* GS1_DATAMATRIX\n\nSince handling the default rendering of data elements and tracked entity attributes are depending on the value type of the object, there is also a DEFAULT type to tell the client it should be handled as normal. Program Stage Section is LISTING as default.\n\nRenderingTypes allowed based on value types Tipo de valor Is object an optionset? RenderingTypes allowed TRUE_ONLY NÃ£o DEFAULT, VERTICAL_RADIOBUTTONS, HORIZONTAL_RADIOBUTTONS, VERTICAL_CHECKBOXES, HORIZONTAL_CHECKBOXES, TOGGLE BOOLEAN NÃ£o - sim DEFAULT, DROPDOWN, VERTICAL_RADIOBUTTONS, HORIZONTAL_RADIOBUTTONS, VERTICAL_CHECKBOXES, HORIZONTAL_CHECKBOXES, SHARED_HEADER_RADIOBUTTONS, ICONS_AS_BUTTONS, SPINNER, ICON INTEGER NÃ£o DEFAULT, VALUE, SLIDER, LINEAR_SCALE, SPINNER TEXT NÃ£o DEFAULT, VALUE, AUTOCOMPLETE, QR_CODE, BAR_CODE, GS1_DATAMATRIX INTEGER_POSITIVE NÃ£o INTEGER_NEGATIVE NÃ£o INTEGER_ZERO_OR_POSITIVE NÃ£o NUMBER NÃ£o UNIT_INTERVAL NÃ£o PERCENTAGE NÃ£o\n\nA complete reference of the previous table can also be retrieved using the following endpoint:\n\nValue type rendering also has some additional properties that can be set, which is usually needed when rendering some of the specific types:\n\nrenderType object properties Property DescriÃ§Ã£o Modelo type The RenderingType of the object, as seen in the first table. This property is the same for both value type and program stage section, but is the only property available for program stage section. Enum (See list in the Metadata and Rendering Type table) min Only for value type rendering. Represents the minimum value this field can have. Inteiro max Only for value type rendering. Represents the maximum value this field can have. Inteiro step Only for value type rendering. Represents the size of the steps the value should increase, for example for SLIDER og LINEAR_SCALE Inteiro decimalPoints Only for value type rendering. Represents the number of decimal points the value should use. Inteiro\n\nThe renderingType can be set when creating or updating the metadata listed in the first table. An example payload for the rendering type for program stage section looks like this:\n\nPara elemento de dados e atributo de entidade rastreada:\n\nObject Style\n\nMost metadata have a property names \"style\". This property can be used by clients to represent the object in a certain way. The properties currently supported by style is as follows:\n\nStyle properties Property DescriÃ§Ã£o Modelo color A color, represented by a hexadecimal. String (#000000) icon An icon, represented by a icon-name. Corda\n\nCurrently, there is no official list or support for icon-libraries, so this is currently up to the client to provide. The following list shows all objects that support style:\n\nElemento de dados\n\nOpÃ§Ã£o de categoria de elemento de dados\n\nConjunto de dados\n\nIndicador\n\nOpÃ§Ã£o\n\nPrograma\n\nIndicador de programa\n\nSeÃ§Ã£o do programa\n\nEstÃ¡gio do Programa\n\nSeÃ§Ã£o de estÃ¡gio do programa\n\nRelacionamento (Tracker)\n\nAtributo de entidade rastreada\n\nTipo de entidade rastreada\n\nWhen creating or updating any of these objects, you can include the following payload to change the style:\n\nElementos de Dados\n\nMerge data elements\n\nCaution\n\nMerging DataElements should be carried out with the utmost care. Particular attention should be given to the merging of data values that have data element references involved in the merge. Knowing the potential side effects of a merge should be fully understood before performing the merge. The merging of DataElements has far-reaching effects. The information below will try to help show what's involved in a DataElement merge. A DataElement merge touches all the major parts of the system (metadata, data, tracker, analytics and audit).\n\nSystem performance may be impacted if the source DataElements are linked to large amounts of Data/Audit records particularly.\n\nThe data element merge endpoint allows you to merge a number of data elements (sources) into a target data element.\n\nAuthorisation\n\nThe main authority required to perform a data element merge is F_DATA_ELEMENT_MERGE.\n\nOther authorities required relate to the general sharing and access of data elements, F_DATAELEMENT_PUBLIC_ADD and F_DATAELEMENT_DELETE.\n\nRequest\n\nMerge data elements with a POST request:\n\nThe payload in JSON format looks like the following:\n\nThe JSON properties are described in the following table.\n\nMerge payload fields Campo Requerido Valor sources sim Array of identifiers of the data elements to merge (the source data elements) target sim Identifier of the data element to merge the sources into (the target data element) deleteSources NÃ£o Whether to delete the source data elements after the operation. Default is false. If true is chosen, then all source audit records will also be deleted. dataMergeStrategy sim How to handle merging of data values. Options are 'DISCARD' or 'LAST_UPDATED'. DISCARD will delete all source data values. LAST_UPDATED will use the data value which was last updated.\n\nThe merge operation will merge the source data elements into the target data element. One or many source data elements can be specified. Only one target should be specified.\n\nThe merge operation will transfer all source data element metadata associations to the target data element. The following metadata get updated:\n\nMetadados Property Action taken DataDimensionItem dataElement set to target EventVisualization dataElementValueDimension set to target ProgramStageDataElement dataElement set to target ProgramNotificationTemplate recipientDataElement set to target ProgramRuleVariable dataElement set to target ProgramRuleAction dataElement set to target TrackedEntityDataElementDimension dataElement set to target MinMaxDataElement dataElement set to target SMSCode dataElement set to target SMSCode dataElement set to target Predictor output set to target DataSetElement dataElement set to target DataElementOperand dataElement set to target ProgramStageDataElement dataElements remove sources, add target Section dataElements remove sources, add target DataElementGroup members remove sources, add target Evento eventDataValues remove sources, add target Indicador numerator replace source with target Indicador denominator replace source with target Predictor generator replace source with target Predictor sampleSkipTest replace source with target DataEntryForm htmlCode replace source with target ProgramIndicator expression replace source with target ProgramIndicator filter replace source with target DataValue dataElement\n\nDados Property Action taken Evento eventDataValues action based on merge strategy (DISCARD / LAST_UPDATED). DISCARD will delete all source event data values. LAST_UPDATED will use the event data value which was last updated, when more than one exists. DataValue dataElement action based on merge strategy (DISCARD / LAST_UPDATED). DISCARD will delete all source data values. LAST_UPDATED will use the data value which was last updated, when more than one exists. TrackedEntityDataValueChangeLog deleted if sources are being deleted, otherwise no action. DataValueAudit deleted if sources are being deleted, otherwise no action.\n\nValidation\n\nThe following constraints and error codes apply.\n\nConstraints and error codes Erro de cÃ³digo DescriÃ§Ã£o E1550 At least one source data element must be specified E1551 Target data element must be specified E1552 Target data element cannot be a source indicator E1553 Source/Target data element does not exist: {uid} E1554 All source ValueTypes must match target ValueType: ValueType. Other ValueTypes found: ValueType E1555 All source DataElementDomains must match target DataElementDomain: DataElementDomain. Other DataElementDomains found: DataElementDomain E1556 dataMergeStrategy field must be specified. With value DISCARD or LAST_UPDATED\n\nDatabase constraints\n\nThere are unique constraints in place that can prevent a successful merge. These constraints are set by DHIS2 in order to maintain a logical domain model.\n\nBelow are a list of the known database unique key constraints at the time of writing. For example, you can only have 1 data set element with the same dataset and data element.\n\nDatabase table unique key constraints Table Unique key constraint minmaxdataelement orgunit, dataelement, categoryoptioncombo programstagedataelement programstage, dataelement datasetelement dataset, dataelement\n\nResponse\n\nSuccess\n\nSample success response looks like:\n\nFailure\n\nSample error response looks like:\n\nAnother sample validation error response:\n\nA database constraint sample error response:\n\nIndicadores\n\nEsta seÃ§Ã£o descreve indicadores e expressÃµes de indicadores.\n\nIndicadores agregados\n\nTo retrieve indicators you can make a GET request to the indicators resource like this:\n\nIndicators represent expressions which can be calculated and presented as a result. The indicator expressions are split into a numerator and denominator. The numerators and denominators are mathematical expressions which can contain references to data elements, other indicators, constants and organisation unit groups. The variables will be substituted with data values when used e.g. in reports. Variables which are allowed in expressions are described in the following table.\n\nIndicator variables VariÃ¡vel Objecto DescriÃ§Ã£o #{<data-element-id>.<category-option-combo-id>.<attribute-option-combo-id>} Data element operand Refers to a combination of an aggregate data element and a category option combination. Both category and attribute option combo ids are optional, and a wildcard \"*\" symbol can be used to indicate any value. #{<dataelement-id>.<category-option-group-id>.<attribute-option-combo-id>} Category Option Group Refers to an aggregate data element and a category option group, containing multiple category option combinations. #{<data-element-id>} Aggregate data element Refers to the total value of an aggregate data element across all category option combinations. D{<program-id>.<data-element-id>} Program data element Refers to the value of a tracker data element within a program. A{<program-id>.<attribute-id>} Program tracked entity attribute Refers to the value of a tracked entity attribute within a program. I{<program-indicator-id>} Program indicator Refers to the value of a program indicator. R{<dataset-id>.<metric>} Reporting rate Refers to a reporting rate metric. The metric can be REPORTING_RATE, REPORTING_RATE_ON_TIME, ACTUAL_REPORTS, ACTUAL_REPORTS_ON_TIME, EXPECTED_REPORTS. C{<constant-id>} Constante Refers to a constant value. N{<indicator-id>} Indicador Refers to an existing Indicator. OUG{<orgunitgroup-id>} Organisation unit group Refers to the count of organisation units within an organisation unit group.\n\nWithin a Data element operand or an Aggregate data element, the following substitutions may be made:\n\nItem Valor DescriÃ§Ã£o data-element-id data-element-id An aggregate data element data-element-id deGroup:data-element-group-id All the aggregate data elements in a data element group category-option-combo-id category-option-combo-id A category option combination category-option-combo-id co:category-option-id All the category option combinations in a category option category-option-combo-id coGroup:category-option-group-id All the category option combinations in a category option group category-option-combo-id coGroup:co-group-id1&co-group-id2... All the category option combinations that are members of multiple category option groups\n\nThe syntax looks like this:\n\nUm exemplo correspondente tem a seguinte aparÃªncia:\n\nNote that for data element variables the category option combo identifier can be omitted. The variable will then represent the total for the data element, e.g. across all category option combos. Example:\n\nData element operands can include any of category option combination and attribute option combination, and use wildcards to indicate any value:\n\nAn example using a data element group:\n\nAn example using a category option, data element group, and a category option group:\n\nAn example using multiple category option groups:\n\nAn example using a program data element and a program attribute:\n\nAn example combining program indicators and aggregate indicators:\n\nAn example using a reporting rate:\n\nAnother reporting rate example using actual data set reports and expected reports:\n\nAn example using an existing indicator:\n\nExpressions can be any kind of valid mathematical expression, as an example:\n\nIndicadores de programa\n\nTo retrieve program indicators you can make a GET request to the program indicators resource like this:\n\nProgram indicators can contain information collected in a program. Indicators have an expression which can contain references to data elements, attributes, constants and program variables. Variables which are allowed in expressions are described in the following table.\n\nProgram indicator variables VariÃ¡vel DescriÃ§Ã£o #{<programstage-id>.<dataelement-id>} Refers to a combination of program stage and data element id. A{<attribute-id>} Refers to a tracked entity attribute. V{<variable-id>} Refers to a program variable. C{<constant-id>} Refers to a constant.\n\nThe syntax looks like this:\n\nUm exemplo correspondente tem a seguinte aparÃªncia:\n\nExpressÃµes\n\nExpressions are mathematical formulas which can contain references to data elements, constants and organisation unit groups. To validate and get the textual description of an expression, you can make a GET request to the expressions resource:\n\nThe response follows the standard JSON web message format. The status property indicates the outcome of the validation and will be \"OK\" if successful and \"ERROR\" if failed. The message property will be \"Valid\" if successful and provide a textual description of the reason why the validation failed if not. The description provides a textual description of the expression.\n\nMerge indicators\n\nThe indicator merge endpoint allows you to merge a number of indicators (sources) into a target indicator.\n\nAuthorisation\n\nThe authority F_INDICATOR_MERGE is required to perform indicator merges.\n\nRequest\n\nMerge indicators with a POST request:\n\nThe payload in JSON format looks like the following:\n\nThe JSON properties are described in the following table.\n\nMerge payload fields Campo Requerido Valor sources sim Array of identifiers of the indicators to merge (the source indicators) target sim Identifier of the indicator to merge the sources into (the target indicator) deleteSources NÃ£o Whether to delete the source indicators after the operation. Default is false\n\nThe merge operation will merge the source indicators into the target indicator. One or many source indicators can be specified. Only one target should be specified.\n\nThe merge operation will transfer all source indicator metadata associations to the target indicator. The following metadata get updated:\n\nMetadados Property Action taken IndicatorGroup members Source indicator removed, target indicator added Conjunto de Dados indicadores Source indicator removed, target indicator added DataDimensionalItem n/a Any linked data items with sources will be linked with the target Section indicadores Source indicator removed, target indicator added ConfiguraÃ§Ã£o infrastructuralIndicators (IndicatorGroup) Source indicator removed, target indicator added Indicador numerator / denominator Replace any source reference with the target reference DataEntryForm htmlCode Replace any source reference with the target reference Visualization sorting Replace any source reference with the target reference as Sorting dimension\n\nValidation\n\nThe following constraints and error codes apply.\n\nConstraints and error codes Erro de cÃ³digo DescriÃ§Ã£o E1540 At least one source indicator must be specified E1541 Target indicator must be specified E1542 Target indicator cannot be a source indicator E1543 Source/Target indicator does not exist: {uid}\n\nResponse\n\nSuccess\n\nSample success response looks like:\n\nSample error response looks like:\n\nIndicator Types\n\nMerge indicator types\n\nThe indicator type merge endpoint allows you to merge a number of indicator types into a target indicator type.\n\nAuthorisation\n\nThe authority F_INDICATOR_TYPE_MERGE is required to perform indicator type merges.\n\nRequest\n\nMerge indicator types with a POST request:\n\nThe payload in JSON format looks like the following:\n\nThe JSON properties are described in the following table.\n\nMerge payload fields Campo Requerido Valor sources sim Array of identifiers of the indicator types to merge (the source indicator types). target sim Identifier of the indicator type to merge the sources into (the target indicator type). deleteSources NÃ£o Whether to delete the source indicator types after the operation. Default is false.\n\nThe merge operation will merge the source indicator types into the target indicator type. One or many source indicator types can be specified. Only one target should be specified.\n\nThe merge operation will transfer all of the indicator metadata associations to the source indicator types over to the target indicator type.\n\nValidation\n\nThe following constraints and error codes apply.\n\nConstraints and error codes Erro de cÃ³digo DescriÃ§Ã£o E1530 At least one source indicator type must be specified E1531 Target indicator type must be specified E1532 Target indicator type cannot be a source indicator type E1533 Source/Target indicator type does not exist: {uid}\n\nResponse\n\nSuccess\n\nSample success response looks like:\n\nSample error response looks like:\n\nUnidades organizacionais\n\nThe organisationUnits resource follows the standard conventions as other metadata resources in DHIS2. This resource supports some additional query parameters.\n\nObtenha uma lista de unidades organizacionais\n\nPara obter uma lista de unidades de organizaÃ§Ã£o, vocÃª pode usar o seguinte recurso.\n\nOrganisation units query parameters Query parameter Options DescriÃ§Ã£o userOnly false | true Data capture organisation units associated with current user only. userDataViewOnly false | true Data view organisation units associated with current user only. userDataViewFallback false | true Data view organisation units associated with current user only with fallback to data capture organisation units. query string Query against the name, code and ID properties. level inteiro Organisation units at the given level in the hierarchy. maxLevel inteiro Organisation units at the given max level or levels higher up in the hierarchy. withinUserHierarchy false | true Limits search and retrieval to organisation units that are within the users data capture scope. withinUserSearchHierarchy false | true Limits search and retrieval to organisation units that are within the current users search scope. Note: \"withinUserHierarchy\", if true, takes higher precedence. memberCollection string For displaying count of members within a collection, refers to the name of the collection associated with organisation units. memberObject UID For displaying count of members within a collection, refers to the identifier of the object member of the collection.\n\nGet organisation unit with sub-hierarchy\n\nTo get an organisation unit including organisation units in its sub-hierarchy you can use the following resource.\n\nOrganisation unit parameters Query parameter Options DescriÃ§Ã£o includeChildren false | true Include immediate children of the specified organisation unit, i.e. the units at the immediate level below in the subhierarchy. includeDescendants false | true Include all children of the specified organisation unit, i.e. all units in the sub-hierarchy. includeAncestors false | true Include all parents of the specified organisation unit. level inteiro Include children of the specified organisation unit at the given level of the sub-hierarchy. This is relative to the organisation unit, starting on 1 for the level immediately below the org unit.\n\nGet organisation units by category option\n\nPurpose-built endpoint to retrieve associations between category options and organisation units. This endpoint is the preferred way to retrieve program organisation unit associations.\n\nresponses will have the following format:\n\nCategory options that are accessible by all organisation units are returned with an empty array ([]) of organisation units.\n\nGet organisation units by programs\n\nPurpose-built endpoint to retrieve associations between programs and organisation units. This endpoint is the preferred way to retrieve program organisation unit associations.\n\nresponses will have the following format:\n\nPrograms which are accessible by all organisation units are returned with an empty array ([]) of organisation units.\n\nSplit organisation unit\n\nThe organisation unit split endpoint allows you to split organisation units into a number of target organisation units.\n\nRequest\n\nSplit organisation units with a POST request:\n\nThe payload in JSON format looks like the following:\n\nThe JSON properties are described in the following table.\n\nSplit payload fields Campo Requerido Valor source sim Identifier of the organisation unit to split (the source organisation unit). targets sim Array of identifiers of the organisation units to split the source into (the target organisation units). primaryTarget NÃ£o Identifier of the organisation unit to transfer the aggregate data, events and tracked entities associated with the source over to. If not specified, the first target will be used. deleteSource NÃ£o Whether to delete the source organisation unit after the operation. Default is true.\n\nThe split operation will split the source org unit into the target org units. It is recommended to first create new target org units before performing the split, and at a minimum ensure that no aggregate data exists for the target org units. Any number of target org units can be specified.\n\nThe split operation will transfer all of the metadata associations of the source org unit over to the target org units. This includes data sets, programs, org unit groups, category options, users, visualizations, maps and event reports.\n\nThe operation will transfer all data records of the source org unit over to the org unit specified as the primary target, or if not specified, the first specified target org unit. This includes aggregate data values, data approval records, events, tracked entities and more.\n\nValidation\n\nThe following constraints and error codes apply.\n\nConstraints and error codes Erro de cÃ³digo DescriÃ§Ã£o E1510 Source org unit must be specified E1511 At least two target org units must be specified E1512 Source org unit cannot be a target org unit E1513 Primary target must be specified E1514 Primary target must be a target org unit E1515 Target org unit does not exist\n\nMerge organisation units\n\nThe organisation unit merge endpoint allows you to merge a number of organisation units into a target organisation unit.\n\nRequest\n\nMerge organisation units with a POST request:\n\nThe payload in JSON format looks like the following:\n\nThe JSON properties are described in the following table.\n\nMerge payload fields Campo Requerido Valor sources sim Array of identifiers of the organisation units to merge (the source organisation units). target sim Identifier of the organisation unit to merge the sources into (the target organisation unit). dataValueMergeStrategy NÃ£o Strategy for merging data values. Options: LAST_UPDATED (default), DISCARD. dataApprovalMergeStrategy NÃ£o Strategy for merging data approval records. Options: LAST_UPDATED (default), DISCARD. deleteSources NÃ£o Whether to delete the source organisation units after the operation. Default is true.\n\nThe merge operation will merge the source org units into the target org unit. It is recommended to first create a new target org unit before performing the merge, and at a minimum ensure that no aggregate data exists for the target org unit. Any number of source org units can be specified.\n\nThe merge operation will transfer all of the metadata associations of the source org units over to the target org unit. This includes data sets, programs, org unit groups, category options, users, visualizations, maps and event reports. The operation will also transfer all event and tracker data, such as events, enrollments, ownership history, program ownership and tracked entities, over to the target org unit.\n\nThe specified data value merge strategy defines how data values are handled. For strategy LAST_UPDATED, data values for all source org units are transferred over to the target org unit, and in situation where data values exist for the same parameters, the last updated or created data value will be used. This is done to avoid duplication of data. For strategy DISCARD, data values are not transferred over to the target org unit, and simply deleted. The specified data approval merge strategy defines how data approval records are handled, and follows the same logic as data values.\n\nValidation\n\nThe following constraints and error codes apply.\n\nConstraints and error codes Erro de cÃ³digo DescriÃ§Ã£o E1500 At least two source orgs unit must be specified E1501 Target org unit must be specified E1502 Target org unit cannot be a source org unit E1503 Source org unit does not exist\n\nConjuntos de dados\n\nThe dataSets resource follows the standard conventions as other metadata resources in DHIS2. This resource supports some additional query parameters.\n\nPara recuperar a versÃ£o de um conjunto de dados, vocÃª pode emitir uma solicitaÃ§Ã£o GET:\n\nTo bump (increase by one) the version of a data set you can issue a POST request:\n\nData set notification template\n\nThe dataset notification templates resource follows the standard conventions as other metadata resources in DHIS2.\n\nPara recuperar o modelo de notificaÃ§Ã£o de conjunto de dados, vocÃª pode emitir uma solicitaÃ§Ã£o GET:\n\nPara adicionar um modelo de notificaÃ§Ã£o de conjunto de dados, vocÃª pode emitir uma solicitaÃ§Ã£o POST:\n\nPara excluir o modelo de notificaÃ§Ã£o de conjunto de dados, vocÃª pode emitir uma solicitaÃ§Ã£o DELETE:\n\nA amostra de carga Ãºtil JSON Ã© fornecida abaixo:\n\nnotificationRecipient can be one of: - USER_GROUP for internal messages - ORGANISATION_UNIT_CONTACT for external messages\n\nNÃ­veis de unidade organizacional preenchidos\n\nThe filledOrganisationUnitLevels resource provides an ordered list of organisation unit levels, where generated levels are injected into the list to fill positions for which it does not exist a persisted level.\n\nTo set the organisation unit levels you can issue a POST request with a JSON payload and content type application/json looking like this:\n\nPreditores\n\nA predictor allows you to generate data values based on an expression. This can be used for example to generate targets, thresholds, or estimated values.\n\nTo retrieve predictors you can make a GET request to the predictors resource like this:\n\nCriaÃ§Ã£o de um preditor\n\nYou can create a predictor with a POST request to the predictors resource:\n\nUma amostra de carga tem a seguinte aparÃªncia:\n\nThe output element refers to the identifier of the data element for which to saved predicted data values. The generator element refers to the expression to use when calculating the predicted values.\n\nPredictor expressions\n\nA predictor always has a generator expression that describes how the predicted value is calculated. A predictor may also have a skip test expression returning a boolean value. When the skip test expression is present, it is evaluated in each of the sampled periods to tell whether values from that period should be skipped.\n\nThe following variables may be used in either a generator expression or a skip test expression:\n\nVariÃ¡vel Objecto DescriÃ§Ã£o #{} Aggregate data element Refers to the total value of an aggregate data element across all category option combinations. #{. Data element operand Refers to a combination of an aggregate data element and a category option combination. D{.} Program data element Refers to the value of a tracker data element within a program. A{.} Program tracked entity attribute Refers to the value of a tracked entity attribute within a program. I{} Program indicator Refers to the value of a program indicator. R{.} Reporting rate Refers to a reporting rate metric. The metric can be REPORTING_RATE, REPORTING_RATE_ON_TIME, ACTUAL_REPORTS, ACTUAL_REPORTS_ON_TIME, EXPECTED_REPORTS. C{} Constante Refers to a constant value. OUG{} Organisation unit group Refers to the count of organisation units within an organisation unit group. [days] Number of days The number of days in the current period.\n\nGerando valores previstos\n\nTo run all predictors (generating predicted values) you can make a POST request to the run resource:\n\nTo run a single predictor you can make a POST request to the run resource for a predictor:\n\nRegras do programa\n\nThis section is about sending and reading program rules, and explains the program rules data model. The program rules give functionality to configure dynamic behaviour in the programs in DHIS2.\n\nModelo de regra de programa\n\nThe program rules data model consists of programRuleVariables, programRules and programRuleActions. The programRule contains an expression - when this expression is true, the child programRuleActions is triggered. The programRuleVariables is used to address data elements, tracked entity data values and other data values needed to run the expressions. All programRules in a program share the same library of programRuleVariables, and one programRuleVariable can be used in several programRules' expressions.\n\nDetalhes do modelo de regra do programa\n\nThe following table gives a detailed overview over the programRule model.\n\nprogramRule nome descriÃ§Ã£o Compulsory âRegistro de Casos de MalÃ¡riaâ The program of which the programRule is executed in. Compulsory nome The name with which the program rule will be displayed to dhis2 configurators. Not visible to the end user of the program. Compulsory descriÃ§Ã£o The description of the program rule, can be used by configurators to describe the rule. Not visible to the end user of the program. Compulsory programStage If a programStage is set for a program rule, the rule will only be evaluated inside the specified program stage. optional doenÃ§a The expression that needs to be evaluated to true in order for the program rule to trigger its child actions. The expression is written using operators, function calls, hard coded values, constants and program rule variables. d2:hasValue('hemoglobin') && #{hemoglobin} <= 7 Compulsory priority The priority to run the rule in cases where the order of the rules matters. In most cases the rules does not depend on being run before or after other rules, and in these cases the priority can be omitted. If no priority is set, the rule will be run after any rules that has a priority defined. If a priority(integer) is set, the rule with the lowest priority will be run before rules with higher priority. optional\n\nDetalhes do modelo de aÃ§Ã£o da regra do programa\n\nThe following table gives a detailed overview over the programRuleAction model.\n\nprogramRuleAction nome descriÃ§Ã£o Compulsory programRule The programRule that is the parent of this action. Compulsory programRule- ActionType The type of action that is to be performed.\n\n* DISPLAYTEXT - Displays a text in a given widget.\n\n* DISPLAYKEYVALUEPAIR - Displays a key and value pair(like a program indicator) in a given widget.\n\n* HIDEFIELD - Hide a specified dataElement or trackedEntityAttribute.\n\n- content - if defined, the text in content will be displayed to the end user in the instance where a value is previously entered into a field that is now about to be hidden (and therefore blanked). If content is not defined, a standard message will be shown to the user in this instance.\n\n- dataElement - if defined, the HIDEFIELD action will hide this dataElement when the rule is effective.\n\n- trackedEntityDataValue - if defined, the HIDEFIELD action will hide this trackedEntityDataValue when the rule is effective.\n\n* HIDESECTION - Hide a specified section.\n\n- programStageSection - must be defined. This is the programStageSection that will be hidden in case the parent rule is effective.\n\n* ASSIGN - Assign a dataElement a value(help the user calculate something or fill in an obvious value somewhere)\n\n- content - if defined, the value in data is assigned to this variable. If content id defined, and thus a variable is assigned for use in other rules, it is important to also assign a programRule.priority to make sure the rule with an ASSIGN action runs before the rule that will in turn evaluate the assigned variable.\n\n- data - must be defined, data forms an expression that is evaluated and assigned to either a variable(#{myVariable}), a dataElement, or both.\n\n- dataElement - if defined, the value in data is assigned to this data element.\n\nEither the content or dataElement must be defined for the ASSIGN action to be effective.\n\n* SHOWWARNING - Show a warning to the user, not blocking the user from completing the event or registration.\n\n- content - if defined, content is a static part that is displayed at the end of the error message.\n\n- data - if defined, data forms an expression that is evaluated and added to the end of the warning message.\n\n- dataElement - if defined, the warning message is displayed next to this data element.\n\n- trackedEntityAttribute - if defined, the warning message is displayed next to this tracked entity attribute.\n\nEither dataElement or trackedEntityAttribute must be specified.\n\n* SHOWERROR - Show an error to the user, blocking the user from completing the event or registration.\n\n- content - if defined, content is a static part that is displayed in the start of the error message.\n\n- data - if defined, data forms an expression that is evaluated and added to the end of the error message.\n\n- dataElement - if defined, the error message is linked to this data element.\n\n- trackedEntityAttribute - if defined, the error message is linked to this tracked entity attribute.\n\nEither dataElement or trackedEntityAttribute must be specified.\n\n* WARNINGONCOMPLETE - Show a warning to the user on the \"Complete form\" dialog, but allowing the user to complete the event.\n\n- content - if defined, content is a static part that is displayed at the end of the error message.\n\n- data - if defined, data forms an expression that is evaluated and added to the end of the warning message.\n\n- dataElement - if defined, the warning message prefixed with the name/formName of the data element.\n\n* ERRORONCOMPLETE - Show an error to the user on in a modal window when the user tries to complete the event. The user is prevented from completing the event.\n\n- content - if defined, content is a static part that is displayed in the start of the error message.\n\n- data - if defined, data forms an expression that is evaluated and added to the end of the error message.\n\n- dataElement - if defined, the error message is linked to this data element.\n\n* CREATEEVENT - Create an event within the same enrollment.\n\n- content\n\n- data - if defined, contains data values to assign the created event. The format is <uid>:<data value>. Where several values is specified, these are separated with comma.\n\nAcMrnleqHqc:100,AqK1IHqCkEE:'Polyhydramnios' - programStage - must be defined, and designates the program stage that the rule shall create an event of.\n\n* SETMANDATORYFIELD - Set a field to be mandatory.\n\n- dataElement - if defined, this data element will be set to be mandatory in the data entry form.\n\n- trackedEntityAttribute - if defined, this tracked entity attribute will be set to mandatory in the registration form or profile.\n\n* SENDMESSAGE - To send message at completion of event/enrollment or at data value update.\n\n- messageTemplate - if defined, this template will be delivered either as SMS or EMAIL depending upon DeliveryChannel value in message template.\n\n* SCHEDULEMESSAGE - To schedule message at completion of event/enrollment or at data value update.\n\n- messageTemplate - if defined, this template will be delivered either as SMS or EMAIL depending upon DeliveryChannel value in message template.\n\n- Date to send message - Expression which is going to be used for evaluation of scheduled date. This expression should result in Date, any other resultant will be discarded and notification will not get scheduled. Compulsory location Used for actionType DISPLAYKEYVALUEPAIR and DISPLAYTEXT to designate which widget to display the text or keyvaluepair in. Compulsory for DISPLAYKEYVALUEPAIR and DISPLAYTEXT. See description content Used for user messages in the different actions. See the actionType overview for a detailed explanation for how it is used in each of the action types. Compulsory for SHOWWARNING, SHOWERROR, WARNINGONCOMPLETE, ERRORONCOMPLETE, DISPLAYTEXT and DISPLAYKEYVALUEPAIR. Optional for HIDEFIELD and ASSIGN. See description dados Used for expressions in the different actions. See the actionType overview for a detailed explanation for how it is used in each of the action types. Compulsory for ASSIGN. Optional for SHOWWARNING, SHOWERROR, WARNINGONCOMPLETE, ERRORONCOMPLETE, DISPLAYTEXT, CREATEEVENT and DISPLAYKEYVALUEPAIR See description dataElement Used for linking rule actions to dataElements. See the actionType overview for a detailed explanation for how it is used in each of the action types. Optional for SHOWWARNING, SHOWERROR, WARNINGONCOMPLETE, ERRORONCOMPLETE, ASSIGN and HIDEFIELD See description trackedEntity- Attribute Used for linking rule actions to trackedEntityAttributes. See the actionType overview for a detailed explanation for how it is used in each of the action types. Optional for SHOWWARNING, SHOWERROR and HIDEFIELD. See description option Used for linking rule actions to options. See the actionType overview for a detailed explanation for how it is used in each of the action types. Optional for HIDEOPTION See description optionGroup Used for linking rule actions to optionGroups. See the actionType overview for a detailed explanation for how it is used in each of the action types. Compulsory for SHOWOPTIONGROUP, HIDEOPTIONGROUP. See description programStage Only used for CREATEEVENT rule actions. Compulsory for CREATEEEVENT. See description programStage- Section Only used for HIDESECTION rule actions. Compulsory for HIDESECTION See description\n\nProgramRuleAction Validation\n\nThere are certain validations added to ProgramRuleAction model in 2.37. Main purpose was to keep user from creating erroneous ProgramRules in order to keep the database consistent. These validations depends on program rule action type. Each action type has its own respective validation.\n\nProgramRuleAction Validations nome validation check for id existence SENDMESSAGE Notification template id SCHEDULEMESSAGE Notification template id HIDESECTION ProgramStage section id HIDEPROGRAMSTAGE ProgramStage id HIDEFIELD DataElement or TrackedEntityAttribute id HIDEOPTION Option id HIDEOPTIONGROUP Option group id SHOWOPTIONGROUP Option group id SETMANDATORYFIELD DataElement or TrackedEntityAttribute id SHOWERROR Always valid SHOWWARNING Always valid DISPLAYTEXT DataElement or TrackedEntityAttribute id DISPLAYKEYVALUEPAIR ASSIGN DataElement or TrackedEntityAttribute id WARNINGONCOMPLETE DataElement or TrackedEntityAttribute id ERRORONCOMPLETE DataElement or TrackedEntityAttribute id\n\nApart from above validations, data field in program rule action which normally contains expression can also be evaluated using below api endpoint.\n\nDetalhes do modelo de variÃ¡vel de regra de programa\n\nThe following table gives a detailed overview over the programRuleVariable model.\n\nprogramRuleVariable nome descriÃ§Ã£o Compulsory nome the name for the programRuleVariable - this name is used in expressions. #{myVariable} > 5 Compulsory sourceType Defines how this variable is populated with data from the enrollment and events.\n\n* DATAELEMENT_NEWEST_EVENT_PROGRAM_STAGE - In tracker capture, gets the newest value that exists for a data element, within the events of a given program stage in the current enrollment. In event capture, gets the newest value among the 10 newest events on the organisation unit.\n\n* DATAELEMENT_NEWEST_EVENT_PROGRAM - In tracker capture, get the newest value that exists for a data element across the whole enrollment. In event capture, gets the newest value among the 10 newest events on the organisation unit.\n\n* DATAELEMENT_CURRENT_EVENT - Gets the value of the given data element in the current event only.\n\n* DATAELEMENT_PREVIOUS_EVENT - In tracker capture, gets the newest value that exists among events in the program that precedes the current event. In event capture, gets the newvest value among the 10 preceeding events registered on the organisation unit.\n\n* CALCULATED_VALUE - Used to reserve a variable name that will be assigned by a ASSIGN program rule action\n\n* TEI_ATTRIBUTE - Gets the value of a given tracked entity attribute"
    }
}
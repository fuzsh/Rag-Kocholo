{
    "id": "dbpedia_109_1",
    "rank": 69,
    "data": {
        "url": "https://www.screamingfrog.co.uk/seo-spider/user-guide/tabs/",
        "read_more_link": "",
        "language": "en",
        "title": "SEO Spider Tabs",
        "top_image": "https://www.screamingfrog.co.uk/wp-content/uploads/2021/09/screaming-frog-social.png",
        "meta_img": "https://www.screamingfrog.co.uk/wp-content/uploads/2021/09/screaming-frog-social.png",
        "images": [
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/logos/logo.png",
            "https://www.screamingfrog.co.uk/wp-content/uploads/2019/09/pagespeed-reporting.jpg",
            "https://www.screamingfrog.co.uk/wp-content/uploads/2020/05/duplicate-details-tab-left-hand-side.jpg",
            "https://www.screamingfrog.co.uk/wp-content/uploads/2020/05/duplicate-details-tab-right-hand-side.jpg",
            "https://www.screamingfrog.co.uk/wp-content/uploads/2015/11/serp-snippet-1.jpg",
            "https://www.screamingfrog.co.uk/wp-content/uploads/2017/06/screaming-frog-seo-spider-8.jpg",
            "https://www.screamingfrog.co.uk/wp-content/uploads/2017/06/screaming-frog-seo-spider-8.jpg",
            "https://www.screamingfrog.co.uk/wp-content/uploads/2020/04/View-source.png",
            "https://www.screamingfrog.co.uk/wp-content/uploads/2020/04/structured-data-details.png",
            "https://www.screamingfrog.co.uk/wp-content/uploads/2020/04/lighthouse-details-tab.png",
            "https://www.screamingfrog.co.uk/wp-content/uploads/2020/06/spelling-grammar-details-tab.jpg",
            "https://www.screamingfrog.co.uk/wp-content/uploads/2024/04/n-grams-lower-tab.png",
            "https://www.screamingfrog.co.uk/wp-content/uploads/2019/11/getting-started-right-issues-tab-new.png",
            "https://www.screamingfrog.co.uk/wp-content/uploads/2019/11/getting-started-issues-bulk-export-all.png",
            "https://www.screamingfrog.co.uk/wp-content/uploads/2023/07/segments-right-hand-tab.png",
            "https://www.screamingfrog.co.uk/wp-content/uploads/2020/05/right-hand-spelling-grammar.jpg",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/social/facebook.png",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/social/linkedin.png",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/social/twitter.png",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/social/youtube.png",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/social/rss.png",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/social/facebook.png",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/social/linkedin.png",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/social/twitter.png",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/social/youtube.png",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/social/rss.png",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/frogbg1.png",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/frogbg2.png",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/footer-fly.png",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/icons/operating-systems/windows.svg",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/icons/operating-systems/mac.svg",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/icons/operating-systems/linux.svg",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/icons/operating-systems/windows.svg",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/icons/operating-systems/mac.svg",
            "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/icons/operating-systems/linux.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2011-04-10T11:25:12+00:00",
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "https://www.screamingfrog.co.uk/wp-content/themes/screamingfrog/public/images/favicon.ico",
        "meta_site_name": "Screaming Frog",
        "canonical_link": "https://www.screamingfrog.co.uk/seo-spider/user-guide/tabs/",
        "text": "Internal\n\nThe Internal tab combines all data extracted from most other tabs, except the external, hreflang and structured data tabs. This means all data can be viewed comprehensively, and exported together for further analysis.\n\nURLs classed as ‘Internal’ are on the same subdomain as the start page of the crawl. URLs can be made to be internal, by using the ‘crawl all subdomains‘ configuration, list mode, or the CDNs feature.\n\nColumns\n\nThis tab includes the following columns.\n\nAddress – The URL address.\n\nContent – The content type of the URL.\n\nStatus Code – The HTTP response code.\n\nStatus – The HTTP header response.\n\nIndexability – Whether the URL is Indexable or Non-Indexable.\n\nIndexability Status – The reason why a URL is Non-Indexable. For example, if it’s canonicalised to another URL.\n\nTitle 1 – The (first) page title discovered on the page.\n\nTitle 1 Length – The character length of the page title.\n\nTitle 1 Pixel Width – The pixel width of the page title as described in our pixel width post.\n\nMeta Description 1 – The (first) meta description on the page.\n\nMeta Description Length 1 – The character length of the meta description.\n\nMeta Description Pixel Width – The pixel width of the meta description.\n\nMeta Keyword 1 – The meta keywords.\n\nMeta Keywords Length – The character length of the meta keywords.\n\nh1 – 1 – The first h1 (heading) on the page.\n\nh1 – Len-1 – The character length of the h1.\n\nh2 – 1 – The first h2 (heading) on the page.\n\nh2 – Len-1 – The character length of the h2.\n\nMeta Robots 1 – Meta robots directives found on the URL.\n\nX-Robots-Tag 1 – X-Robots-tag HTTP header directives for the URL.\n\nMeta Refresh 1 – Meta refresh data.\n\nCanonical Link Element – The canonical link element data.\n\nrel=“next” 1 – The SEO Spider collects these HTML link elements designed to indicate the relationship between URLs in a paginated series.\n\nrel=“prev” 1 – The SEO Spider collects these HTML link elements designed to indicate the relationship between URLs in a paginated series.\n\nHTTP rel=“next” 1 – The SEO Spider collects these HTTP link elements designed to indicate the relationship between URLs in a paginated series.\n\nHTTP rel=“prev” 1 – The SEO Spider collects these HTTP link elements designed to indicate the relationship between URLs in a paginated series.\n\nSize – The size of the resource, taken from the Content-Length HTTP header. If this field is not provided, the size is reported as zero. For HTML pages this is updated to the size of the (uncompressed) HTML. Upon export, size is in bytes, so please divide by 1,024 to convert to kilobytes.\n\nTransferred – The number of bytes that were actually transferred to load the resource, which might be less than the ‘size’ if compressed.\n\nWord Count – This is all ‘words’ inside the body tag, excluding HTML markup. The count is based upon the content area that can be adjusted under ‘Config > Content > Area’. By default, the nav and footer elements are excluded. You can include or exclude HTML elements, classes and IDs to calculate a refined word count. Our figures may not be exactly what performing this calculation manually would find, as the parser performs certain fix-ups on invalid HTML. Your rendering settings also affect what HTML is considered. Our definition of a word is taking the text and splitting it by spaces. No consideration is given to visibility of content (such as text inside a div set to hidden).\n\nText Ratio – Number of non-HTML characters found in the HTML body tag on a page (the text), divided by the total number of characters the HTML page is made up of, and displayed as a percentage.\n\nCrawl Depth – Depth of the page from the start page (number of ‘clicks’ away from the start page). Please note, redirects are counted as a level currently in our page depth calculations.\n\nFolder Depth – Depth of the URL based upon the number of subfolders (/sub-folder/) in the URL path. This is not an SEO metric to optimise, but can be useful for segmentation, and advanced table search.\n\nLink Score – A metric between 0-100, which calculates the relative value of a page based upon its internal links similar to Google’s own PageRank. For this column to populate, ‘crawl analysis‘ is required.\n\nInlinks – Number of internal hyperlinks to the URL. ‘Internal inlinks’ are links in anchor elements pointing to a given URL from the same subdomain that is being crawled.\n\nUnique Inlinks – Number of ‘unique’ internal inlinks to the URL. ‘Internal inlinks’ are links in anchor elements pointing to a given URL from the same subdomain that is being crawled. For example, if ‘page A’ links to ‘page B’ 3 times, this would be counted as 3 inlinks and 1 unique inlink to ‘page B’.\n\nUnique JS Inlinks – Number of ‘unique’ internal inlinks to the URL that are only in the rendered HTML after JavaScript execution. ‘Internal inlinks’ are links in anchor elements pointing to a given URL from the same subdomain that is being crawled. For example, if ‘page A’ links to ‘page B’ 3 times, this would be counted as 3 inlinks and 1 unique inlink to ‘page B’.\n\n% of Total – Percentage of unique internal inlinks (200 response HTML pages) to the URL from total internal HTML pages crawled. ‘Internal inlinks’ are links in anchor elements pointing to a given URI from the same subdomain that is being crawled.\n\nOutlinks – Number of internal outlinks from the URL. ‘Internal outlinks’ are links in anchor elements from a given URL to other URLs on the same subdomain that is being crawled.\n\nUnique Outlinks – Number of unique internal outlinks from the URL. ‘Internal outlinks’ are links in anchor elements from a given URL to other URLs on the same subdomain that is being crawled. For example, if ‘page A’ links to ‘page B’ on the same subdomain 3 times, this would be counted as 3 outlinks and 1 unique outlink to ‘page B’.\n\nUnique JS Outlinks – Number of unique internal outlinks from the URL that are only in the rendered HTML after JavaScript execution. ‘Internal outlinks’ are links in anchor elements from a given URL to other URLs on the same subdomain that is being crawled. For example, if ‘page A’ links to ‘page B’ on the same subdomain 3 times, this would be counted as 3 outlinks and 1 unique outlink to ‘page B’.\n\nExternal Outlinks – Number of external outlinks from the URL. ‘External outlinks’ are links in anchor elements from a given URL to another subdomain.\n\nUnique External Outlinks – Number of unique external outlinks from the URL. ‘External outlinks’ are links in anchor elements from a given URL to another subdomain. For example, if ‘page A’ links to ‘page B’ on a different subdomain 3 times, this would be counted as 3 external outlinks and 1 unique external outlink to ‘page B’.\n\nUnique External JS Outlinks – Number of unique external outlinks from the URL that are only in the rendered HTML after JavaScript execution. ‘External outlinks’ are links in anchor elements from a given URL to another subdomain. For example, if ‘page A’ links to ‘page B’ on a different subdomain 3 times, this would be counted as 3 external outlinks and 1 unique external outlink to ‘page B’.\n\nClosest Similarity Match – This shows the highest similarity percentage of a near duplicate URL. The SEO Spider will identify near duplicates with a 90% similarity match, which can be adjusted to find content with a lower similarity threshold. For example, if there were two near duplicate pages for a page with 99% and 90% similarity respectively, then 99% will be displayed here. To populate this column the ‘Enable Near Duplicates’ configuration must be selected via ‘Config > Content > Duplicates’, and post ‘Crawl Analysis’ must be performed. Only URLs with content over the selected similarity threshold will contain data, the others will remain blank. Thus by default, this column will only contain data for URLs with 90% or higher similarity, unless it has been adjusted via the ‘Config > Content > Duplicates’ and ‘Near Duplicate Similarity Threshold’ setting.\n\nNo. Near Duplicates – The number of near duplicate URLs discovered in a crawl that meet or exceed the ‘Near Duplicate Similarity Threshold’, which is a 90% match by default. This setting can be adjusted under ‘Config > Content > Duplicates’. To populate this column the ‘Enable Near Duplicates’ configuration must be selected via ‘Config > Content > Duplicates’, and post ‘Crawl Analysis’ must be performed.\n\nSpelling Errors – The total number of spelling errors discovered for a URL. For this column to be populated then ‘Enable Spell Check’ must be selected via ‘Config > Content > Spelling & Grammar’.\n\nGrammar Errors – The total number of grammar errors discovered for a URL. For this column to be populated then ‘Enable Grammar Check’ must be selected via ‘Config > Content > Spelling & Grammar’.\n\nLanguage – The language selected for spelling and grammar checks. This is based upon the HTML language attribute, but the language can also be set via ‘Config > Content > Spelling & Grammar’.\n\nHash – Hash value of the page using the MD5 algorithm. This is a duplicate content check for exact duplicate content only. If two hash values match, the pages are exactly the same in content. If there’s a single character difference, they will have unique hash values and not be detected as duplicate content. So this is not a check for near duplicate content. The exact duplicates can be seen under ‘URL > Duplicate’.\n\nResponse Time – Time in seconds to download the URL. More detailed information can be found in our FAQ.\n\nLast-Modified – Read from the Last-Modified header in the servers HTTP response. If there server does not provide this the value will be empty.\n\nRedirect URI – If the ‘address’ URL redirects, this column will include the redirect URL target. The status code above will display the type of redirect, 301, 302 etc.\n\nRedirect Type – One of: HTTP Redirect: triggered by an HTTP header, HSTS Policy: Turned around locally by the SEO Spider due to a previous HSTS header, JavaScript Redirect: triggered by execution of JavaScript (can only happen when using JavaScript rendering) or MetaRefresh Redirect: triggered by a meta refresh tag in the HTML.\n\nHTTP Version – This shows the HTTP version the crawl was under, which will be HTTP/1.1 by default. The SEO Spider currently only crawls using HTTP/2 in JavaScript rendering mode, if it’s enabled by the server.\n\nURL Encoded Address – The URL actually requested by the SEO Spider. All non ASCII characters percent encoded, see RFC 3986 for further details.\n\nTitle 2, meta description 2, h1-2, h2-2 etc – The SEO Spider will collect data from the first two elements it encounters in the source code. Hence, h1-2 is data from the second h1 heading on the page.\n\nFilters\n\nThis tab includes the following filters.\n\nHTML – HTML pages.\n\nJavaScript – Any JavaScript files.\n\nCSS – Any style sheets discovered.\n\nImages – Any images.\n\nPDF – Any portable document files.\n\nFlash – Any .swf files.\n\nOther – Any other file types, like docs etc.\n\nUnknown – Any URLs with an unknown content type. Either because it’s not been supplied, incorrect, or because the URL can’t be crawled. URLs blocked by robots.txt will also appear here, as their filetype is unknown for example.\n\nPage titles\n\nThe page title tab includes data related to page title elements of internal URLs in the crawl. The filters show common issues discovered for page titles.\n\nThe page title, often referred to as the ‘title tag’, ‘meta title’ or sometimes ‘SEO title’ is an HTML element in the head of a webpage that describes the purpose of the page to users and search engines. They are widely considered to be one of the strongest on-page ranking signals for a page.\n\nThe page title element should be placed in the head of the document and looks like this in HTML:\n\n<title>This Is A Page Title</title>\n\nColumns\n\nThis tab includes the following columns.\n\nAddress – The URL crawled.\n\nOccurrences – The number of page titles found on the page (the maximum the SEO Spider will find is 2).\n\nTitle 1/2 – The content of the page title elements.\n\nTitle 1/2 length – The character length of the page title(s).\n\nIndexability – Whether the URL is Indexable or Non-Indexable.\n\nIndexability Status – The reason why a URL is Non-Indexable. For example, if the URL is canonicalised to another URL, or has a ‘noindex’ etc.\n\nFilters\n\nThis tab includes the following columns.\n\nMissing – Any pages which have a missing page title element, the content is empty or has a whitespace. Page titles are read and used by both users and the search engines to understand the purpose of a page. So it’s critical that pages have concise, descriptive and unique page titles.\n\nDuplicate – Any pages which have duplicate page titles. It’s really important to have distinct and unique page titles for every page. If every page has the same page title, then it can make it more challenging for users and the search engines to understand one page from another.\n\nOver 60 characters – Any pages which have page titles over 60 characters in length. Characters over this limit might be truncated in Google’s search results and carry less weight in scoring.\n\nBelow 30 characters – Any pages which have page titles under 30 characters in length. This isn’t necessarily an issue, but you have more room to target additional keywords or communicate your USPs.\n\nOver X Pixels – Google snippet length is actually based upon pixels limits, rather than a character length. The SEO Spider tries to match the latest pixel truncation points in the SERPs, but it is an approximation and Google adjusts them frequently. This filter shows any pages which have page titles over X pixels in length.\n\nBelow X Pixels – Any pages which have page titles under X pixels in length. This isn’t necessarily a bad thing, but you have more room to target additional keywords or communicate your USPs.\n\nSame as h1 – Any page titles which match the h1 on the page exactly. This is not necessarily an issue, but may point to a potential opportunity to target alternative keywords, synonyms, or related key phrases.\n\nMultiple – Any pages which have multiple page titles. There should only be a single page title element for a page. Multiple page titles are often caused by multiple conflicting plugins or modules in CMS.\n\nOutside <head> – Pages with a title element that is outside of the head element in the HTML. The page title should be within the head element, or search engines may ignore it. Google will often still recognise the page title even outside of the head element, however this should not be relied upon.\n\nPlease see our Learn SEO guide on writing Page Titles.\n\nMeta description\n\nThe meta description tab includes data related to meta descriptions of internal URLs in the crawl. The filters show common issues discovered for meta descriptions.\n\nThe meta description is an HTML attribute in the head of a webpage that provides a summary of the page to users. The words in a description are not used in ranking by Google, but they can be shown in the search results to users, and therefore heavily influence click through rates.\n\nThe meta description should be placed in the head of the document and looks like this in HTML:\n\n<meta name=\"description\" content=\"This is a meta description.\"/>\n\nColumns\n\nThis tab includes the following columns.\n\nAddress – The URL crawled.\n\nOccurrences – The number of meta descriptions found on the page (the maximum we find is 2).\n\nMeta Description 1/2 – The meta description.\n\nMeta Description 1/2 length – The character length of the meta description.\n\nIndexability – Whether the URL is indexable or Non-Indexable.\n\nIndexability Status – The reason why a URL is Non-Indexable. For example, if the URL is canonicalised to another URL.\n\nFilters\n\nThis tab includes the following filters.\n\nMissing – Any pages which have a missing meta description, the content is empty or has a whitespace. This is a missed opportunity to communicate the benefits of your product or service and influence click through rates for important URLs.\n\nDuplicate – Any pages which have duplicate meta descriptions. It’s really important to have distinct and unique meta descriptions that communicate the benefits and purpose of each page. If they are duplicate or irrelevant, then they will be ignored by search engines.\n\nOver 155 characters – Any pages which have meta descriptions over 155 characters in length. Characters over this limit might be truncated in Google’s search results.\n\nBelow 70 characters – Any pages which have meta descriptions below 70 characters in length. This isn’t strictly an issue, but an opportunity. There is additional room to communicate benefits, USPs or call to actions.\n\nOver X Pixels – Google snippet length is actually based upon pixels limits, rather than a character length. The SEO Spider tries to match the latest pixel truncation points in the SERPs, but it is an approximation and Google adjusts them frequently. This filter shows any pages which have descriptions over X pixels in length and might be truncated in Google’s search results.\n\nBelow X Pixels – Any pages which have meta descriptions under X pixels in length. This isn’t strictly an issue, but an opportunity. There is additional room to communicate benefits, USPs or call to actions.\n\nMultiple – Any pages which have multiple meta descriptions. There should only be a single meta description for a page. Multiple meta descriptions are often caused by multiple conflicting plugins or modules in CMS.\n\nOutside <head> – Pages with a meta description that is outside of the head element in the HTML. The meta description should be within the head element, or search engines may ignore it.\n\nPlease see our Learn SEO guide on writing Meta Descriptions.\n\nMeta keywords\n\nThe meta keywords tab includes data related to meta keywords. The filters show common issues discovered for meta keywords.\n\nMeta keywords are widely ignored by search engines and they are not used as a signal in scoring for all major Western search engines. In particular Google does not consider it at all in their scoring of pages in ranking of their search results. Therefore we recommend ignoring it completely unless you are targeting alternative search engines.\n\nOther search engines such as Yandex or Baidu may still use them in ranking, but we recommend performing research to this status before taking the time to optimise them.\n\nThe meta keywords tag should be placed in the head of the document and looks like this in HTML\n\n:\n\n<meta name=\"keywords\" content=\"seo, seo agency, seo services\"/>\n\nColumns\n\nThis tab includes the following columns.\n\nAddress – The URL crawled.\n\nOccurrences – The number of meta keywords found on the page (the maximum we find is 2).\n\nMeta Keyword 1/2 – The meta keywords.\n\nMeta Keyword 1/2 length – The character length of the meta keywords.\n\nIndexability – Whether the URL is indexable or Non-Indexable.\n\nIndexability Status – The reason why a URL is Non-Indexable. For example, if it’s canonicalised to another URL.\n\nFilters\n\nThis tab includes the following filters.\n\nMissing – Any pages which have a missing meta keywords. If you’re targeting Google, Bing and Yahoo then this is fine as they do not use them in ranking. If you’re targeting Baidu or Yandex, then you may wish to consider including relevant target keywords.\n\nDuplicate – Any pages which have duplicate meta keywords. If you’re targeting Baidu or Yandex, then unique keywords relevant to the purpose of the page are recommended.\n\nMultiple – Any pages which have multiple meta keywords. There should only be a single tag on the page.\n\nh1\n\nThe h1 tab shows data related to the <h1> heading of a page. The filters show common issues discovered for <h1>s.\n\nThe <h1> to <h6> tags are used to define HTML headings. The <h1> is considered as the most important first main heading of a page, and <h6> as the least important.\n\nHeadings should ordered by size and importance and they help users and search engines understand the content on the page and sections. The <h1> should describe the main title and purpose of the page and are widely considered to be one of the stronger on-page ranking signals.\n\nThe <h1> element should be placed in the body of the document and looks like this in HTML:\n\n<h1>This Is An h1</h1>\n\nBy default, the SEO Spider will only extract and report on the first two <h1>’s discovered on a page. If you wish to extract all h1s, then we recommend using custom extraction.\n\nColumns\n\nThis tab includes the following columns.\n\nAddress – The URL crawled.\n\nOccurrences – The number of <h1>s found on the page. As outlined above, the maximum we find is 2.\n\nh1-1/2 – The content of the <h1>.\n\nh1-length-1/2 – The character length of the <h1>.\n\nIndexability – Whether the URL is indexable or Non-Indexable.\n\nIndexability Status – The reason why a URL is Non-Indexable. For example, if it’s canonicalised to another URL.\n\nFilters\n\nThis tab includes the following filters.\n\nMissing – Any pages which have a missing <h1>, the content is empty or has a whitespace. <h1>’s are read and used by both users and the search engines to understand the purpose of a page. So it’s critical that pages have concise, descriptive and unique headings.\n\nDuplicate – Any pages which have duplicate <h1>s. It’s important to have distinct, unique and useful pages. If every page has the same <h1>, then it can make it more challenging for users and the search engines to understand one page from another.\n\nOver 70 characters – Any pages which have <h1> over 70 characters in length. This is not strictly an issue, as there isn’t a character limit for headings. However, they should be concise and descriptive for users and search engines.\n\nMultiple – Any pages which have multiple <h1>. While this is not strictly an issue because HTML5 standards allow multiple <h1>s on a page, there are some problems with this modern approach in terms of usability. It’s advised to use heading rank (h1–h6) to convey document structure. The classic HTML4 standard defines there should only be a single <h1> per page, and this is still generally recommended for users and SEO.\n\nAlt Text in h1 – Pages which have image alt text within an h1. This can be because text within the image is considered as the main heading on the page, or due to inappropriate mark-up. Some CMS templates will automatically include an h1 around a logo across a website. While there are strong arguments that text rather than alt text should be used for headings, search engines may understand alt text within an h1 as part of the h1 and score accordingly.\n\nNon-sequential – Pages with an h1 that is not the first heading on the page. Heading elements should be in a logical sequentially-descending order. The purpose of heading elements is to convey the structure of the page and they should be in logical order from h1 to h6, which helps navigating the page and users that rely on assistive technologies.\n\nPlease see our Learn SEO guide on Heading Tags.\n\nh2\n\nThe h2 tab shows data related to the <h2> heading of a page. The filters show common issues discovered for <h2>s.\n\nThe <h1> to <h6> tags are used to define HTML headings. The <h2> is considered as the second important heading of a page and is generally sized and styled as the second largest heading.\n\nThe <h2> heading is often used to describe sections or topics within a document. They act as sign posts for the user, and can help search engines understand the page.\n\nThe <h2> element should be placed in the body of the document and looks like this in HTML:\n\n<h2>This Is An h2</h2>\n\nBy default, the SEO Spider will only extract and report on the first two h2’s discovered on a page. If you wish to extract all h2s, then we recommend using custom extraction.\n\nColumns\n\nThis tab includes the following columns.\n\nAddress – The URL crawled.\n\nOccurrences – The number of <h2>s found on the page. As outlined above, the maximum we find is 2.\n\nh2-1/2 – The content of the <h2>.\n\nh2-length-1/2 – The character length of the <h2>.\n\nIndexability – Whether the URL is indexable or Non-Indexable.\n\nIndexability Status – The reason why a URL is Non-Indexable. For example, if it’s canonicalised to another URL.\n\nFilters\n\nThis tab includes the following filters.\n\nMissing – Any pages which have a missing <h2>, the content is empty or has a whitespace. <h2>’s are read and used by both users and the search engines to understand the page and sections. Ideally most pages would have logical, descriptive <h2>s.\n\nDuplicate – Any pages which have duplicate <h2>s. It’s important to have distinct, unique and useful pages. If every page has the same <h2>, then it can make it more challenging for users and the search engines to understand one page from another.\n\nOver 70 characters – Any pages which have <h2> over 70 characters in length. This is not strictly an issue, as there isn’t a character limit for headings. However, they should be concise and descriptive for users and search engines.\n\nMultiple – Any pages which have multiple <h2>s. This is not an issue as HTML standards allow multiple <h2>’s when used in a logical hierachical heading structure. However, this filter can help you quickly scan to review if they are used appropriately.\n\nNon-sequential – Pages with an h2 that is not the second heading level after the h1 on the page. Heading elements should be in a logical sequentially-descending order. The purpose of heading elements is to convey the structure of the page and they should be in logical order from h1 to h6, which helps navigating the page and users that rely on assistive technologies.\n\nPlease see our Learn SEO guide on Heading Tags.\n\nImages\n\nThe images tab shows data related to any images discovered in a crawl. This includes both internal and external images, discovered by either <img src= tags, or <a href= tags. The filters show common issues discovered for images and their alt text.\n\nImage alt attributes (often referred to incorrectly as ‘alt tags’) can be viewed by clicking on an image and then the ‘Image Details’ tab at the bottom, which populates the lower window tab.\n\nAlt attributes should specify relevant and descriptive alternative text about the purpose of an image and appear in the source of the HTML like the below example.\n\n<img src=\"screamingfrog-logo.jpg\" alt=\"Screaming Frog\" />\n\nDecorative images should provide a null (empty) alt text (alt=””) so that they can be ignored by assistive technologies, such as screen readers, rather than not including an alt attribute at all.\n\n<img src=\"decorative-frog-space.jpg\" alt=\"\" />\n\nColumns\n\nThis tab includes the following columns.\n\nAddress – The URL crawled.\n\nContent – The content type of the image (jpeg, gif, png etc).\n\nSize – Size of the image in kilobytes. File size is in bytes in the export, so divide by 1,024 to convert to kilobytes.\n\nIndexability – Whether the URL is indexable or Non-Indexable.\n\nIndexability Status – The reason why a URL is Non-Indexable. For example, if it’s canonicalised to another URL.\n\nFilters\n\nThis tab includes the following filters.\n\nOver 100kb – Large images over 100kb in size. Page speed is extremely important for users and SEO and often large resources such as images are one of the most common issues that slow down web pages. This filter simply acts as a general rule of thumb to help identify images that are fairly large in file size and may take longer to load. These should be considered for optimisation, alongside opportunities identified in the PageSpeed tab which uses the PSI API and Lighthouse to audit speed. This can help identify images that haven’t been optimised in size, load offscreen, are unoptimised etc.\n\nMissing Alt Text – Images that have an alt attribute, but are missing alt text. Click the address (URL) of the image and then the ‘Image Details’ tab in the lower window pane to view which pages have the image on, and which pages are missing alt text of the said image. Images should have descriptive alternative text about it’s purpose, which helps the blind and visually impaired and the search engines understand it and it’s relevance to the web page. For decorative images a null (empty) alt text should be provided (alt=””) so that they can be ignored by assistive technologies, such as screen readers.\n\nMissing Alt Attribute – Images that are missing an alt attribute all together. Click the address (URL) of the image and then the ‘Image Details’ tab in the lower window pane to view which pages have the image on, and are missing alt attributes. All images should contain an alt attribute with descriptive text, or blank when it’s a decorative image.\n\nAlt Text Over 100 Characters – Images which have one instance of alt text over 100 characters in length. This is not strictly an issue, however image alt text should be concise and descriptive. It should not be used to stuff lots of keywords or paragraphs of text onto a page.\n\nBackground Images – CSS background and dynamically loaded images discovered across the website, which should be used for non-critical and decorative purposes. Background images are not typically indexed by Google and browsers do not provide alt attributes or text on background images to assistive technology. For this filter to populate, JavaScript rendering must be enabled, and crawl analysis needs to be performed.\n\nMissing Size Attributes – Image elements without dimensions (width and height size attributes) specified in the HTML. This can cause large layout shifts as the page loads and be frustrating experience for users. It is one of the major reasons that contributes to a high Cumulative Layout Shift (CLS).\n\nIncorrectly Sized Images – Images identified where their real dimensions (WxH) do not match the display dimensions when rendered. If there is an estimated 4kb file size difference or more, the image is flagged for potential optimisation. In particular, this can help identify oversized images, which can contribute to poor page load speed. It can also help identify smaller sized images, that are being stretched when rendered. For this filter to populate, JavaScript rendering must be enabled, and crawl analysis needs to be performed.\n\nFor more on optimising images, please read our guide on How To View Alt Text & Find Missing Alt Text and consider using the the PageSpeed Insights Integration. This has opportunities and diagnostics for ‘Properly Size Images’, ‘Defer Offscreen Images’, ‘Efficiently Encode Images’, ‘Serve Images in Next-Gen Formats’ and ‘Image Elements Do Not Have Explicit Width & Height’.\n\nCanonicals\n\nThe canonicals tab shows canonical link elements and HTTP canonicals discovered during a crawl. The filters show common issues discovered for canonicals.\n\nThe rel=”canonical” element helps specify a single preferred version of a page when it’s available via multiple URLs. It’s a hint to the search engines to help prevent duplicate content, by consolidating indexing and link properties to a single URL to use in ranking.\n\nThe canonical link element should be placed in the head of the document and looks like this in HTML:\n\n<link rel=\"canonical\" href=\"https://www.example.com/\" >\n\nYou can also use rel=”canonical” HTTP headers, which looks like this:\n\nLink: <http://www.example.com>; rel=\"canonical\"\n\nColumns\n\nThis tab includes the following columns.\n\nAddress – The URL crawled.\n\nOccurrences – The number of canonicals found (via both link element and HTTP).\n\nIndexability – Whether the URL is indexable or Non-Indexable.\n\nIndexability Status – The reason why a URL is Non-Indexable. For example, if it’s canonicalised to another URL.\n\nCanonical Link Element 1/2 etc – Canonical link element data on the URL. The SEO Spider will find all instances if there are multiple.\n\nHTTP Canonical 1/2 etc – Canonical issued via HTTP. The SEO Spider will find all instances if there are multiple.\n\nMeta Robots 1/2 etc – Meta robots found on the URL. The SEO Spider will find all instances if there are multiple.\n\nX-Robots-Tag 1/2 etc – X-Robots-tag data. The SEO Spider will find all instances if there are multiple.\n\nrel=“next” and rel=“prev” – The SEO Spider collects these HTML link elements designed to indicate the relationship between URLs in a paginated series.\n\nFilters\n\nThis tab includes the following filters.\n\nContains Canonical – The page has a canonical URL set (either via link element, HTTP header or both). This could be a self-referencing canonical URL where the page URL is the same as the canonical URL, or it could be ‘canonicalised’, where the canonical URL is different to the page URL.\n\nSelf Referencing – The URL has a canonical which is the same URL as the page URL crawled (hence, it’s self referencing). Ideally only canonical versions of URLs would be linked to internally, and every URL would have a self-referencing canonical to help avoid any potential duplicate content issues that can occur (even naturally on the web, such as tracking parameters on URLs, other websites incorrectly linking to a URL that resolves etc).\n\nCanonicalised – The page has a canonical URL that is different to itself. The URL is ‘canonicalised’ to another location. This means the search engines are being instructed to not index the page, and the indexing and linking properties should be consolidated to the target canonical URL. These URLs should be reviewed carefully. In a perfect world, a website wouldn’t need to canonicalise any URLs as only canonical versions would be linked to, but often they are required due to various circumstances outside of control, and to prevent duplicate content.\n\nMissing – There’s no canonical URL present either as a link element, or via HTTP header. If a page doesn’t indicate a canonical URL, Google will identify what they think is the best version or URL. This can lead to ranking unpredicatability, and hence generally all URLs should specify a canonical version.\n\nMultiple – There’s multiple canonicals set for a URL (either multiple link elements, HTTP header, or both combined). This can lead to unpredictability, as there should only be a single canonical URL set by a single implementation (link element, or HTTP header) for a page.\n\nMultiple Conflicting – Pages with multiple canonicals set for a URL that have different URLs specified (via either multiple link elements, HTTP header, or both combined). This can lead to unpredictability, as there should only be a single canonical URL set by a single implementation (link element, or HTTP header) for a page.\n\nNon-Indexable Canonical – The canonical URL is a non-indexable page. This will include canonicals which are blocked by robots.txt, no response, redirect (3XX), client error (4XX), server error (5XX) or are ‘noindex’. Canonical versions of URLs should always be indexable, ‘200’ response pages. Therefore, canonicals that go to non-indexable pages should be corrected to the resolving indexable versions.\n\nCanonical Is Relative – Pages that have a relative rather than absolute rel=”canonical” link tag. While the tag, like many HTML tags, accepts both relative and absolute URLs, it’s easy to make subtle mistakes with relative paths that could cause indexing-related issues.\n\nUnlinked – URLs that are only discoverable via rel=”canonical” and are not linked-to via hyperlinks on the website. This might be a sign of a problem with internal linking, or the URLs contained in the canonical.\n\nOutside <head> – Pages with a canonical link element that is outside of the head element in the HTML. The canonical link element should be within the head element, or search engines will ignore it.\n\nPlease see our Learn SEO guide on canonicals, and our ‘How to Audit Canoncials‘ tutorial.\n\nPagination\n\nThe pagination tab includes information on rel=”next” and rel=”prev” HTML link elements discovered in a crawl, which are used to indicate the relationship between component URLs in a paginated series. The filters show common issues discovered for pagination.\n\nWhile Google announced on the 21st of March 2019 that they have not used rel=”next” and rel=”prev” in indexing for a long time, other search engines such as Bing (which also powers Yahoo), still use it as a hint for discovery and understanding site structure.\n\nPagination attributes should be placed in the head of the document and looks like this in HTML:\n\n<link rel=\"prev\" href=\"https://www.example.com/seo/\"/>\n\n<link rel=\"next\" href=\"https://www.example.com/seo/page/2/\"/>\n\nColumns\n\nThis tab includes the following columns.\n\nAddress – The URL crawled.\n\nOccurrences – The number of canonicals found (via both link element and HTTP).\n\nIndexability – Whether the URL is indexable or Non-Indexable.\n\nIndexability Status – The reason why a URL is Non-Indexable. For example, if it’s canonicalised to another URL.\n\nrel=“next” – The SEO Spider collects these HTML link elements designed to indicate the relationship between URLs in a paginated series.\n\nrel=“prev” – The SEO Spider collects these HTML link elements designed to indicate the relationship between URLs in a paginated series.\n\nCanonical Link Element 1/2 etc – Canonical link element data on the URI. The SEO Spider will find all instances if there are multiple.\n\nHTTP Canonical 1/2 etc – Canonical issued via HTTP. The SEO Spider will find all instances if there are multiple.\n\nMeta Robots 1/2 etc – Meta robots found on the URI. The SEO Spider will find all instances if there are multiple.\n\nX-Robots-Tag 1/2 etc – X-Robots-tag data. The SEO Spider will find all instances if there are multiple.\n\nFilters\n\nThis tab includes the following filters.\n\nContains Pagination – The URL has a rel=”next” and/or rel=”prev” attribute, indicating it’s part of a paginated series.\n\nFirst Page – The URL only has a rel=“next” attribute, indicating it’s the first page in a paginated series. It’s easy and useful to scroll through these URLs and ensure they are accurately implemented on the parent page in the series.\n\nPaginated 2+ Pages – The URL has a rel=“prev” on it, indicating it’s not the first page, but a paginated page in a series. Again, it’s useful to scroll through these URLs and ensure only paginated pages appear under this filter.\n\nPagination URL Not In Anchor Tag – A URL contained in either, or both, the rel=”next” and rel=”prev” attributes of the page, are not found as a hyperlink in an HTML anchor element on the page itself. Paginated pages should be linked to with regular links to allow users to click and navigate to the next page in the series. They also allow Google to crawl from page to page, and PageRank to flow between pages in the series. Google’s own Webmaster Trends analyst John Mueller recommended proper HTML links for pagination as well in a Google Webmaster Central Hangout.\n\nNon-200 Pagination URL – The URLs contained in the rel=”next” and rel=”prev” attributes do not respond with a 200 ‘OK’ status code. This can include URLs blocked by robots.txt, no responses, 3XX (redirects), 4XX (client errors) or 5XX (server errors). Pagination URLs must be crawlable and indexable and therefore non-200 URLs are treated as errors, and ignored by the search engines. The non-200 pagination URLs can be exported in bulk via the ‘Reports > Pagination > Non-200 Pagination URLs’ export.\n\nUnlinked Pagination URL – The URL contained in the rel=”next” and rel=”prev” attributes are not linked to across the website. Pagination attributes may not pass PageRank like a traditional anchor element, so this might be a sign of a problem with internal linking, or the URLs contained in the pagination attribute. The unlinked pagination URLs can be exported in bulk via the ‘Reports > Pagination > Unlinked Pagination URLs’ export.\n\nNon-Indexable – The paginated URL is non-indexable. Generally they should all be indexable, unless there is a ‘view-all’ page set, or there are extra parameters on pagination URLs, and they require canonicalising to a single URL. One of the most common mistakes is canonicalising page 2+ paginated pages to the first page in a series. Google recommend against this implementation because the component pages don’t actually contain duplicate content. Another common mistake is using ‘noindex’, which can mean Google drops paginated URLs from the index completely and stops following outlinks from those pages, which can be a problem for the products on those pages. This filter will help identify these common set-up issues.\n\nMultiple Pagination URLs – There are multiple rel=”next” and rel=”prev” attributes on the page (when there shouldn’t be more than a single rel=”next” or rel=”prev” attribute). This may mean that they are ignored by the search engines.\n\nPagination Loop – This will show URLs that have rel=”next” and rel=”prev” attributes that loop back to a previously encountered URL. Again, this might mean that the expressed pagination series are simply ignored by the search engines.\n\nSequence Error – This shows URLs that have an error in the rel=”next” and rel=”prev” HTML link elements sequence. This check ensures that URLs contained within rel=”next” and rel=”prev” HTML link elements reciprocate and confirm their relationship in the series.\n\nFor more information on pagination, please read our guide on ‘How To Audit rel=”next” and rel=”prev” Pagination Attributes‘.\n\nDirectives\n\nThe directives tab shows data related to the meta robots tag, and the X-Robots-Tag in the HTTP Header. These robots directives can control how your content and URLs are displayed in search engines, such as Google.\n\nThe meta robots tag should be placed in the head of the document and an example of a ‘noindex’ meta tag looks like this in HTML:\n\n<meta name=\"robots\" content=\"noindex\"/>\n\nThe same directive can be issued in the HTTP header using the X-Robots-Tag, which looks like this:\n\nX-Robots-Tag: noindex\n\nColumns\n\nThis tab includes the following columns.\n\nAddress – The URL crawled.\n\nMeta Robots 1/2 etc – Meta robots directives found on the URL. The SEO Spider will find all instances if there are multiple.\n\nX-Robots-Tag 1/2 etc – X-Robots-tag HTTP header directives for the URL. The SEO Spider will find all instances if there are multiple.\n\nFilters\n\nThis tab includes the following filters.\n\nIndex – This allows the page to be indexed. It’s unnecessary, as search engines will index URLs without it.\n\nNoindex – This instructs the search engines not to index the page. The page will still be crawled (to see the directive), but it will then be dropped from the index. URLs with a ‘noindex’ should be inspected carefully.\n\nFollow – This instructs any links on the page to be followed for crawling. It’s unnecessary, as search engines will follow them by default.\n\nNofollow – This is a ‘hint’ which tells the search engines not to follow any links on the page for crawling. This is generally used by mistake in combination with ‘noindex’, when there is no need to include this directive. To crawl pages with a meta nofollow tag the configuration ‘Follow Internal Nofollow’ must be enabled under ‘Config > Spider’.\n\nNone – This does not mean there are no directives in place. It means the meta tag ‘none’ is being used, which is the equivalent to “noindex, nofollow”. These URLs should be reviewed carefully to ensure they are being correctly kept out of the search engines indexes.\n\nNoArchive – This instructs Google not to show a cached link for a page in the search results.\n\nNoSnippet – This instructs Google not to show a text snippet or video preview from being shown in the search results.\n\nMax-Snippet – This value allows you to limit the text snippet length for this page to [number] characters in Google. Special values include – 0 for no snippet, or -1 to allow any snippet length.\n\nMax-Image-Preview – This value can limit the size of any image associated with this page in Google. Setting values can be “none”, “standard”, or “large”.\n\nMax-Video-Preview – This value can limit any video preview associated with this page to [number] seconds in Google. You can also specify 0 to allow only a still image, or -1 to allow any preview length.\n\nNoODP – This is an old meta tag that used to instruct Google not to use the Open Directory Project for its snippets. This can be removed.\n\nNoYDIR – This is an old meta tag that used to instruct Google not to use the Yahoo Directory for its snippets. This can be removed.\n\nNoImageIndex – This tells Google not to show the page as the referring page for an image in the Image search results. This has the effect of preventing all images on this page from being indexed in this page.\n\nNoTranslate – This value tells Google that you don’t want them to provide a translation for this page.\n\nUnavailable_After – This allows you to specify the exact time and date you want Google to stop showing the page in their search results.\n\nRefresh – This redirects the user to a new URL after a certain amount of time. We recommend reviewing meta refresh data within the response codes tab.\n\nOutside <head> – Pages with a meta robots that is outside of the head element in the HTML. The meta robots should be within the head element, or search engines may ignore it. Google will typically still recognise meta robots such as a ‘noindex’ directive, even outside of the head element, however this should not be relied upon.\n\nIn this tab we also display columns for meta refresh and canonicals. However, we recommend reviewing meta refresh data within the response codes tab and relevant filter, and canonicals within the canonicals tab.\n\nhreflang\n\nThe hreflang tab includes details of hreflang annotations crawled by the SEO Spider, delivered by HTML link element, HTTP Header or XML Sitemap. The filters show common issues discovered for hreflang.\n\nHreflang is useful when you have multiple versions of a page for different languages or regions. It tells Google about these different variations and helps them show the most appropriate version of your page by language or region.\n\nHreflang link elements should be placed in the head of the document and looks like this in HTML:\n\n<link rel=\"alternate\" hreflang=\"en-gb\" href=\"https://www.example.com\" >\n\n<link rel=\"alternate\" hreflang=\"en-us\" href=\"https://www.example.com/us/\" >\n\n‘Store Hreflang‘ and ‘Crawl Hreflang‘ options need to be enabled (under ‘Config > Spider’) for this tab and respective filters to be populated. To extract hreflang annotations from XML Sitemaps during a regular crawl ‘Crawl Linked XML Sitemaps‘ must be selected as well.\n\nColumns\n\nThis tab includes the following columns.\n\nAddress – The URL crawled.\n\nTitle 1/2 etc – The page title element of the page.\n\nOccurrences – The number of hreflang discovered on a page.\n\nHTML hreflang 1/2 etc – The hreflang language and region code from any HTML link element on the page.\n\nHTML hreflang 1/2 URL etc – The hreflang URL from any HTML link element on the page.\n\nHTTP hreflang 1/2 etc – The hreflang language and region code from the HTTP Header.\n\nHTTP hreflang 1/2 URL etc – The hreflang URL from the HTTP Header.\n\nSitemap hreflang 1/2 etc – The hreflang language and region code from the XML Sitemap. Please note, this only populates when crawling the XML Sitemap in list mode.\n\nSitemap hreflang 1/2 URL etc – The hreflang URL from the XML Sitemap. Please note, this only populates when crawling the XML Sitemap in list mode.\n\nFilters\n\nThis tab includes the following filters.\n\nContains Hreflang – These are simply any URLs that have rel=”alternate” hreflang annotations from any implementation, whether link element, HTTP header or XML Sitemap.\n\nNon-200 Hreflang URLs – These are URLs contained within rel=”alternate” hreflang annotations that do not have a 200 response code, such as URLs blocked by robots.txt, no responses, 3XX (redirects), 4XX (client errors) or 5XX (server errors). Hreflang URLs must be crawlable and indexable and therefore non-200 URLs are treated as errors, and ignored by the search engines. The non-200 hreflang URLs can be seen in the lower window ‘URL Info’ pane with a ‘non-200’ confirmation status. They can be exported in bulk via the ‘Reports > Hreflang > Non-200 Hreflang URLs’ export.\n\nUnlinked Hreflang URLs – These are pages that contain one or more hreflang URLs that are only discoverable via its rel=”alternate” hreflang link annotations. Hreflang annotations do not pass PageRank like a traditional anchor tag, so this might be a sign of a problem with internal linking, or the URLs contained in the hreflang annotation. To find out exactly which hreflang URLs on these pages are unlinked, use the ‘Reports > Hreflang > Unlinked Hreflang URLs’ export.\n\nMissing Return Links – These are URLs with missing return links (or ‘return tags’ in Google Search Console) to them, from their alternate pages. Hreflang is reciprocal, so all alternate versions must confirm the relationship. When page X links to page Y using hreflang to specify it as it’s alternate page, page Y must have a return link. No return links means the hreflang annotations may be ignored or not interpreted correctly. The missing return links URLs can be seen in the lower window ‘URL Info’ pane with a ‘missing’ confirmation status. They can be exported in bulk via the ‘Reports > Hreflang > Missing Return Links’ export.\n\nInconsistent Language & Region Return Links – This filter includes URLs with inconsistent language and regional return links to them. This is where a return link has a different language or regional value than the URL is referencing itself. The inconsistent language return URLs can be seen in the lower window ‘URL Info’ pane with an ‘Inconsistent’ confirmation status. They can be exported in bulk via the ‘Reports > Hreflang > Inconsistent Language Return Links’ export.\n\nNon-Canonical Return Links – URLs with non canonical hreflang return links. Hreflang should only include canonical versions of URLs. So this filter picks up return links that go to URLs that are not the canonical versions. The non canonical return URLs can be seen in the lower window ‘URL Info’ pane with a ‘Non Canonical’ confirmation status. They can be exported in bulk via the ‘Reports > Hreflang > Non Canonical Return Links’ export.\n\nNoindex Return Links – Return links which have a ‘noindex’ meta tag. All pages within a set should be indexable, and hence any return URLs with ‘noindex’ may result in the hreflang relationship being ignored. The noindex return links URLs can be seen in the lower window ‘URL Info’ pane with a ‘noindex’ confirmation status. They can be exported in bulk via the ‘Reports > Hreflang > Noindex Return Links’ export.\n\nIncorrect Language & Region Codes – This simply verifies the language (in ISO 639-1 format) and optional regional (in ISO 3166-1 Alpha 2 format) code values are valid. Unsupported hreflang values can be viewed in the lower window ‘URL Info’ pane with an ‘invalid’ status.\n\nMultiple Entries – URLs with multiple entries to a language or regional code. For example, if page X links to page Y and Z using the same ‘en’ hreflang value annotation. This filter will also pick up multiple implementations, for example, if hreflang annotations were discovered as link elements and via HTTP header.\n\nMissing Self Reference – URLs missing their own self referencing rel=”alternate” hreflang annotation. It was previously a requirement to have a self-referencing hreflang, but Google has updated their guidelines to say this is optional. It is however good practice and often easier to include a self referencing attribute.\n\nNot Using Canonical – URLs not using the canonical URL on the page, in it’s own hreflang annotation. Hreflang should only include canonical versions of URLs.\n\nMissing X-Default – URLs missing an X-Default hreflang attribute. This is optional, and not necessarily an error or issue.\n\nMissing – URLs missing an hreflang attribute completely. These might be valid of course, if they aren’t multiple versions of a page.\n\nOutside <head> – Pages with an hreflang link element that is outside of the head element in the HTML. The hreflang link element should be within the head element, or search engines will ignore it.\n\nPlease note – The SEO Spider has a 500 hreflang annotation limit currently. If you have over this limit, they will not be reported. Over 500 hreflang annotations is unsual and might be on the extreme side for the majority of set-ups.\n\nFor more information on hreflang, please read our guide on ‘How to Audit Hreflang‘.\n\nStructured data\n\nThe Structured Data tab includes details of structured data and validation issues discovered from a crawl.\n\n‘JSON-LD’, ‘Microdata’, ‘RDFa’, ‘Schema.org Validation’ and ‘Google Rich Result Feature Validation’ configuration options need to be enabled (under ‘Config > Spider > Extraction’) for this tab and respective filters to be fully populated.\n\nColumns\n\nThis tab includes the following columns.\n\nAddress – The URL crawled.\n\nErrors – The total number of validation errors discovered for the URL.\n\nWarnings – The total number of validation warnings discovered for the URL.\n\nTotal Types – The total number of itemtypes discovered for the URL.\n\nUnique Types – The unique number of itemtypes discovered for the URL.\n\nType 1 – The first itemtype discovered for the URL.\n\nType 2 etc – The second itemtype discovered for the URL.\n\nFilters\n\nThis tab includes the following filters.\n\nContains Structured Data – These are simply any URLs that contain structured data. You can see the different types in columns in the upper window.\n\nMissing Structured Data – These are URLs that do not contain any structured data.\n\nValidation Errors – These are URLs that contain validation errors. The errors can be either Schema.org, Google rich result features, or both – depending on your configuration. Schema.org issues will always be classed as errors, rather than warnings. Google rich result feature validation will show errors for missing required properties or problems with the implementation of required properties. Google’s ‘required properties’ must be included and be valid for content to be eligible for display as a rich result.\n\nValidation Warnings – These are URLs that contain validation warnings for Google rich result features. These will always be for ‘recommended properties’, rather than required properties. Recommended properties can be included to add more information about content, which could provide a better user experience – but they are not essential to be eligible for rich snippets and hence why they are only a warning. There are no ‘warnings’ for Schema.org validation issues, however there is a warning for using the older data-vocabulary.org schema.\n\nParse Errors – These are URLs which have structured data that failed to parse correctly. This is often due to incorrect mark-up. If you’re using Google’s preferred format JSON-LD, then the JSON-LD Playground is an excellent tool to help debug parsing errors.\n\nMicrodata URLs – These are URLs that contain structured data in microdata format.\n\nJSON-LD URLs – These are URLs that contain structured data in JSON-LD format.\n\nRDFa URLs – These are URLs that contain structured data in RDFa format.\n\nStructured Data & Google Rich Snippet Feature Validation\n\nStructured Data validation includes checks against whether the types and properties exist according to Schema.org and will show ‘errors’ for any issues encountered.\n\nFor example, it checks to see whether https://schema.org/author exists for a property, or https://schema.org/Book exist as a type. It validates against main and pending Schema vocabulary from Schema.org latest version.\n\nThere might be a short time between a Schema.org vocabulary release, and it being updated in the SEO Spider.\n\nThe SEO Spider also performs validation against Google rich result features to check the presence of required and recommended properties and their values are accurate.\n\nThe full list of that the SEO Spider is able to validate against includes –\n\nArticle & AMP Article\n\nBook Actions\n\nBreadcrumb\n\nCarousel\n\nCourse list\n\nCOVID-19 announcements\n\nDataset\n\nEmployer Aggregate Rating\n\nEstimated Salary\n\nEvent\n\nFact Check\n\nFAQ\n\nHome Activities\n\nImage Metadata\n\nJob Posting\n\nLearning Video\n\nLocal Business\n\nLogo\n\nMath Solver\n\nMovie Carousel\n\nPractice Problem\n\nProduct\n\nQ&A\n\nRecipe\n\nReview Snippet\n\nSitelinks Searchbox\n\nSoftware App\n\nSpeakable\n\nSubscription and Paywalled Content\n\nVehicle Listing\n\nVideo\n\nThe list of Google rich result features that the SEO Spider doesn’t currently validate against is –\n\nWe currently support all Google features.\n\nFor more information on structured data validation, please read our guide on ‘How To Test & Validate Structured Data‘.\n\nSitemaps\n\nThe Sitemaps tab shows all URLs discovered in a crawl, which can then be filtered to show additional information related to XML Sitemaps.\n\nTo crawl XML Sitemaps in a regular crawl and for the filters to be populated, the ‘Crawl Linked XML Sitemaps‘ configuration needs to be enabled (under ‘Configuration > Spider’).\n\nA ‘Crawl Analysis‘ will also need to be performed at the end of the crawl to populate some of the filters.\n\nColumns\n\nThis tab includes the following columns.\n\nAddress – The URL crawled.\n\nContent – The content type of the URI.\n\nStatus Code – HTTP response code.\n\nStatus – The HTTP header response.\n\nIndexability – Whether the URL is indexable or Non-Indexable.\n\nIndexability Status – The reason why a URL is Non-Indexable. For example, if it’s canonicalised to another URL.\n\nFilters\n\nThis tab includes the following filters.\n\nURLs In Sitemap – All URLs that are in an XML Sitemap. This should contain indexable and canonical versions of important URLs.\n\nURLs Not In Sitemap – URLs that are not in an XML Sitemap, but were discovered in the crawl. This might be on purpose (as they are not important), or they might be missing, and the XML Sitemap needs to be updated to include them. This filter does not consider non-indexable URLs, it assumes they are correctly non-indexable, and therefore shouldn’t be flagged to be included.\n\nOrphan URLs – URLs that are only in an XML Sitemap, but were not discovered during the crawl. Or, URLs that are only discovered from URLs in the XML Sitemap, but were not found in the crawl. These might be accidentally included in the XML Sitemap, or they might be pages that you wish to be indexed, and should really be linked to internally.\n\nNon-Indexable URLs in Sitemap – URLs that are in an XML Sitemap, but are non-indexable, and hence should be removed, or their indexability needs to be fixed.\n\nURLs In Multiple Sitemaps – URLs that are in more than one XML Sitemap. This isn’t necessarily a problem, but generally a URL only needs to be in a single XML Sitemap.\n\nXML Sitemap With Over 50k URLs – This shows any XML Sitemap that has more than the permitted 50k URLs. If you have more URLs, you will have to break your list into multiple sitemaps and create a sitemap index file which lists them all.\n\nXML Sitemap With Over 50mb – This shows any XML Sitemap that is larger than the permitted 50mb file size. If the sitemap is over the 50MB (uncompressed) limit, you will have to break your list into multiple sitemaps.\n\nFor more information on XML Sitemaps, please read our guide on ‘How to Audit XML Sitemaps‘, as well as Sitemaps.org and Google Search Console help.\n\nPageSpeed\n\nThe PageSpeed tab includes data from PageSpeed Insights which uses Lighthouse for ‘lab data’ speed auditing, and is able to look up real-world data from the Chrome User Experience Report (CrUX, or ‘field data’).\n\nTo pull in PageSpeed data simply go to ‘Configuration > API Access > PageSpeed Insights’, insert a free PageSpeed API key, connect and run a crawl. Data will then start to be populated against crawled URLs.\n\nPlease read our PageSpeed Insights integration guide on how to set up a free API and configure the SEO Spider.\n\nColumns & Metrics\n\nThe following speed metrics, opportunities and diagnostics data can be configured to be collected via the PageSpeed Insights API integration.\n\nOverview Metrics\n\nTotal Size Savings\n\nTotal Time Savings\n\nTotal Requests\n\nTotal Page Size\n\nHTML Size\n\nHTML Count\n\nImage Size\n\nImage Count\n\nCSS Size\n\nCSS Count\n\nJavaScript Size\n\nJavaScript Count\n\nFont Size\n\nFont Count\n\nMedia Size\n\nMedia Count\n\nOther Size\n\nOther Count\n\nThird Party Size\n\nThird Party Count\n\nCrUX Metrics (‘Field Data’ in PageSpeed Insights)\n\nCore Web Vitals Assessment\n\nCrUX First Contentful Paint Time (sec)\n\nCrUX First Contentful Paint Category\n\nCrUX First Input Delay Time (sec)\n\nCrUX First Input Delay Category\n\nCrUX Largest Contentful Paint Time (sec)\n\nCrUX Largest Contentful Paint Category\n\nCrUX Cumulative Layout Shift\n\nCrUX Cumulative Layout Shift Category\n\nCrUX Interaction to Next Paint (ms)\n\nCrUX Interaction to Next Paint Category\n\nCrUX Time to First Byte (ms)\n\nCrUX Time to First Byte Category\n\nCrUX Origin Core Web Vitals Assessment\n\nCrUX Origin First Contentful Paint Time (sec)\n\nCrUX Origin First Contentful Paint Category\n\nCrUX Origin First Input Delay Time (sec)\n\nCrUX Origin First Input Delay Category\n\nCrUX Origin Largest Contentful Paint Time (sec)\n\nCrUX Origin Largest Contentful Paint Category\n\nCrUX Origin Cumulative Layout Shift\n\nCrUX Origin Cumulative Layout Shift Category\n\nCrUX Origin Interaction to Next Paint (ms)\n\nCrUX Origin Interaction to Next Paint Category\n\nCrUX Origin Time to First Byte (ms)\n\nCrUX Origin Time to First Byte Category\n\nLighthouse Metrics (‘Lab Data’ in PageSpeed Insights)\n\nPerformance Score\n\nTime to First Byte (ms)\n\nFirst Contentful Paint Time (sec)\n\nFirst Contentful Paint Score\n\nSpeed Index Time (sec)\n\nSpeed Index Score\n\nLargest Contentful Paint Time (sec)\n\nLargest Contentful Paint Score\n\nTime to Interactive (sec)\n\nTime to Interactive Score\n\nFirst Meaningful Paint Time (sec)\n\nFirst Meaningful Paint Score\n\nMax Potential First Input Delay (ms)\n\nMax Potential First Input Delay Score\n\nTotal Blocking Time (ms)\n\nTotal Blocking Time Score\n\nCumulative Layout Shift\n\nCumulative Layout Shift Score\n\nOpportunities\n\nEliminate Render-Blocking Resources Savings (ms)\n\nDefer Offscreen Images Savings (ms)\n\nDefer Offscreen Images Savings\n\nEfficiently Encode Images Savings (ms)\n\nEfficiently Encode Images Savings\n\nProperly Size Images Savings (ms)\n\nProperly Size Images Savings\n\nMinify CSS Savings (ms)\n\nMinify CSS Savings\n\nMinify JavaScript Savings (ms)\n\nMinify JavaScript Savings\n\nReduce Unused CSS Savings (ms)\n\nReduce Unused CSS Savings\n\nReduce Unused JavaScript Savings (ms)\n\nReduce Unused JavaScript Savings\n\nServe Images in Next-Gen Formats Savings (ms)\n\nServe Images in Next-Gen Formats Savings\n\nEnable Text Compression Savings (ms)\n\nEnable Text Compression Savings\n\nPreconnect to Required Origin Savings\n\nServer Response Times (TTFB) (ms)\n\nServer Response Times (TTFB) Category (ms)\n\nMultiple Redirects Savings (ms)\n\nPreload Key Requests Savings (ms)\n\nUse Video Format for Animated Images Savings (ms)\n\nUse Video Format for Animated Images Savings\n\nTotal Image Optimization Savings (ms)\n\nAvoid Serving Legacy JavaScript to Modern Browser Savings\n\nDiagnostics\n\nDOM Element Count\n\nJavaScript Execution Time (sec)\n\nJavaScript Execution Time Category\n\nEfficient Cache Policy Savings\n\nMinimize Main-Thread Work (sec)\n\nMinimize Main-Thread Work Category\n\nText Remains Visible During Webfont Load\n\nImage Elements Do Not Have Explicit Width & Height\n\nAvoid Large Layout Shifts\n\nYou can read more about the definition of each metric, opportunity or diagnostic according to Lighthouse.\n\nFilters\n\nThis tab includes the following filters.\n\nEliminate Render-Blocking Resources – This highlights all pages with resources that are blocking the first paint of the page, along with the potential savings.\n\nProperly Size Images – This highlights all pages with images that are not properly sized, along with the potential savings when they are resized appropriately.\n\nDefer Offscreen Images – This highlights all pages with images that are hidden or offscreen, along with the potential savings if they were lazy-loaded.\n\nMinify CSS – This highlights all pages with unminified CSS files, along with the potential savings when they are correctly minified.\n\nMinify JavaScript – This highlights all pages with unminified JavaScript files, along with the potential savings when they are correctly minified.\n\nReduce Unused CSS – This highlights all pages with unused CSS, along with the potential savings when they are removed of unnecessary bytes.\n\nReduce Unused JavaScript – This highlights all pages with unused JavaScript, along with the potential savings when they are removed of unnecessary bytes.\n\nEfficiently Encode Images – This highlights all pages with unoptimised images, along with the potential savings.\n\nServe Images in Next-Gen Formats – This highlights all pages with images that are in older image formats, along with the potential savings.\n\nEnable Text Compression – This highlights all pages with text based resources that are not compressed, along with the potential savings.\n\nPreconnect to Required Origin – This highlights all pages with key requests that aren’t yet prioritizing fetch requests with link rel=preconnect, along with the potential savings.\n\nReduce Server Response Times (TTFB) – This highlights all pages where the browser has had to wait for over 600ms for the server to respond to the main document request.\n\nAvoid Multiple Page Redirects – This highlights all pages which have resources that redirect, and the potential saving by using the direct URL.\n\nPreload Key Requests – This highlights all pages with resources that are third level of requests in your critical request chain as preload candidates.\n\nUse Video Format for Animated Images – This highlights all pages with animated GIFs, along with the potential savings of converting them into videos.\n\nAvoid Excessive DOM Size – This highlights all pages with a large DOM size over the recommended 1,500 total nodes.\n\nReduce JavaScript Execution Time – This highlights all pages with average or slow JavaScript execution time.\n\nServe Static Assets With An Efficient Cache Policy – This highlights all pages with resources that are not cached, along with the potential savings.\n\nMinimize Main-Thread Work – This highlights all pages with average or slow execution timing on the main thread.\n\nEnsure Text Remains Visible During Webfont Load – This highlights all pages with fonts that may flash or become invisible during page load.\n\nImage Elements Do Not Have Explicit Width & Height – This highlights all pages that have images without dimensions (width and height size attributes) specified in the HTML. This can be a big cause of poor CLS.\n\nAvoid Large Layout Shifts – This highlights all pages that have DOM elements contributing most to the CLS of the page and provides a contribution score of each to help prioritise.\n\nAvoid Serving Legacy JavaScript to Modern Browsers – This highlights all pages with legacy JavaScript. Polyfills and transforms enable legacy browsers to use new JavaScript features. However, many aren’t necessary for modern browsers. For your bundled JavaScript, adopt a modern script deployment strategy using module/nomodule feature detection to reduce the amount of code shipped to modern browsers, while retaining support for legacy browsers.\n\nPlease read the Lighthouse performance audits guide for more definitions and explanations of each of the opportunities and diagnostics above.\n\nThe speed opportunities, source pages and resource URLs that have potential savings can be exported in bulk via the ‘Reports > PageSpeed’ menu.\n\nThe ‘CSS Coverage Summary’ report highlights how much of each CSS file is unused across a crawl, and the potential savings that could be made by removing unused code that is loading across the site.\n\nThe ‘JavaScript Coverage Summary’ report highlights how much of each JS file is unused across a crawl, and the potential savings that could be made by removing unused code that is loading across the site.\n\nPageSpeed Insights API Status & Errors\n\nThe PSI Status column shows whether an API request for a URL has been a ‘success’ and shows data, or there has been an error and no data is displayed. An ‘error’ usually reflects the web interface, where you would see the same error and message.\n\nThe ‘PSI Error’ column displays the full message received from the PSI API to provide more information about the cause. Some errors are due to the Lighthouse audit itself failing, other errors can be due to the PSI API being unavailable when the request is made.\n\nPlease read our FAQ on PageSpeed Insights API Errors for more information.\n\nMobile\n\nThe Mobile tab includes data from Lighthouse via the PageSpeed Insights API, or running Lighthouse locally.\n\nTo audit mobile usability issues, then PSI must be connected via ‘Configuration > API Access > PageSpeed Insights’ and ‘Mobile Friendly’ metrics selected under the ‘Metrics’ tab.\n\nThe ‘Source’ can be set as either ‘Remote’ or ‘Local’. Remote means the data is collected via the PageSpeed Insights API, which requires a free PageSpeed API key. If ‘Local’ is selected, then Lighthouse will be run locally on the machine in headless Chrome.\n\nWith either option selected and PSI connected, data will then start to be populated against crawled URLs.\n\nPlease read our PageSpeed Insights integration guide on how to set up a free API and configure the SEO Spider.\n\nColumns\n\nThis tab includes the following columns.\n\nAddress – The URL crawled.\n\nPSI Status – Either a ‘success’ retrieving data, or an ‘error’.\n\nPSI Error – If there is an error, the error response.\n\nViewport – The Lighthouse performance score out of 100 for this audit.\n\nTap Targets – The Lighthouse performance score out of 100 for this audit.\n\nContent Width – The Lighthouse performance score out of 100 for this audit.\n\nFont Display – The Lighthouse performance score out of 100 for this audit.\n\nPlugins – The Lighthouse performance score out of 100 for this audit.\n\nMobile Alternate Link – Any desktop URLs that contain a rel=”alternate” link element to a mobile version.\n\nFilters\n\nThis tab includes the following filters.\n\nViewport Not Set – Pages without a viewport meta tag, or a viewport meta tag without a content attribute that includes the text width=. Setting the viewport meta tag allows the width and scaling to be sized correctly on all devices. Without this set, mobile devices will render pages at desktop screen widths and scale them down, making the text difficult to read.\n\nTarget Size – Pages with tap targets that are too small or there is not enough space around them, which means they are difficult to interact with on mobile devices. Tap targets (also known as ‘touch targets’) are buttons, links or form elements that users on touch devices can use. Insufficient size or spacing can also make it challenging for users with mobility impairments, or anyone experiencing difficulties controlling fine movement. Tap targets must be at least 24 by 24 CSS pixels in size.\n\nContent Not Sized Correctly – Pages with content that is smaller or larger than the viewport width, which means it may not render correctly on mobile devices. Lighthouse flags pages whose width isn’t equal to the width of the viewport. This is only available in ‘Local’ Lighthouse, it is no longer available via the ‘Remote’ PSI API as it has been deprecated.\n\nIllegible Font Size – Pages with small font sizes that can make it difficult to read for users on mobile devices. Lighthouse will flag pages that have font sizes smaller than 12px, which make up more than 40% of the page text.\n\nUnsupported Plugins – Pages with browser plugins such as Flash, Silverlight, or Java Applets that most mobile devices and browsers do not support and search engines cannot index.\n\nMobile Alternate Link – Pages that contain a rel=”alternate” link element to a mobile version. While this is an acceptable set up, it means serving different HTML to each device on separate URLs. This can often be less efficient than a responsive design approach.\n\nBulk exports of mobile issues including granular details from Lighthouse are available under the ‘Reports’ menu\n\nPageSpeed Insights API Status & Errors\n\nThe PSI Status column shows whether an API request for a URL has been a ‘success’ and shows data, or there has been an error and no data is displayed. An ‘error’ usually reflects the web interface, where you would see the same error and message.\n\nThe ‘PSI Error’ column displays the full message received from the PSI API to provide more information about the cause. Some errors are due to the Lighthouse audit itself failing, other errors can be due to the PSI API being unavailable when the request is made.\n\nPlease read our FAQ on PageSpeed Insights API Errors for more information.\n\nSearch Console\n\nThe Search Console tab includes data from the Search Analyitcs and URL Inspection APIs when the SEO Spider is integrated with Google Search Console under ‘Configuration > API Access > Google Search Console’.\n\nPlease read our Google Search Console integration guide for more details. When integrated, the following data is collected.\n\nColumns\n\nThis tab includes the following columns from Search Analytics by default.\n\nClicks\n\nImpressions\n\nCTR\n\nPosition\n\nYou can read more about the definition of each metric from Google.\n\nOptionally, you can choose to ‘Enable URL Inspection’ alongside Search Analytics data, which provides Google index status data for up to 2,000 URLs per property a day. This includes the following columns for the URL Inspection API.\n\nSummary – A top level verdict on whether the URL is indexed and eligible to display in the Google search results. ‘URL is on Google’ means the URL has been indexed, can appear in Google Search results, and no problems were found with any enhancements found in the page (rich results, mobile, AMP). ‘URL is on Google, but has Issues’ means it has been indexed and can appear in Google Search results, but there are some problems with mobile usability, AMP or Rich results that might mean it doesn’t appear in an optimal way. ‘URL is not on Google’ means it is not indexed by Google and won’t appear in the search results. This filter can include non-indexable URLs (such as those that are ‘noindex’) as well as Indexable URLs that are able to be indexed.\n\nCoverage – A short, descriptive reason for the status of the URL, explaining why the URL is or isn’t on Google.\n\nLast Crawl – The last time this page was crawled by Google, in your local time. All information shown in this tool is derived from this last crawled version.\n\nCrawled As – The user agent type used for the crawl (desktop or mobile).\n\nCrawl Allowed – Indicates whether your site allowed Google to crawl (visit) the page or blocked it with a robots.txt rule.\n\nPage Fetch – Whether or not Google could actually get the page from your server. If crawling is not allowed, this field will show a failure.\n\nIndexing Allowed – Whether or not your page explicitly disallowed indexing. If indexing is disallowed, the reason is explained, and the page won’t appear in Google Search results.\n\nUser-Declared Canonical – If your page explicitly declares a canonical URL, it will be shown here.\n\nGoogle-Selected Canonical – The page that Google selected as the canonical (authoritative) URL, when it found similar or duplicate pages on your site.\n\nMobile Usability – Whether the page is mobile friendly or not.\n\nMobile Usability Issues – If the ‘page is not mobile friendly’, this column will display a list of mobile usability errors.\n\nAMP Results – A verdict on whether the AMP URL is valid, invalid or has warnings. ‘Valid’ means the AMP URL is valid and indexed. ‘Invalid’ means the AMP URL has an error that will prevent it from being indexed. ‘Valid with warnings’ means the AMP URL can be indexed, but there are some issues that might prevent it from getting full features, or it uses tags or attributes that are deprecated, and might become invalid in the future.\n\nAMP Issues – If the URL has AMP issues, this column will display a list of AMP errors.\n\nRich Results – A verdict on whether Rich results found on the page are valid, invalid or has warnings. ‘Valid’ means rich results have been found and are eligible for search. ‘Invalid’ means one or more rich results on the page has an error that will prevent it from being eligible for search. ‘Valid with warnings’ means the rich results on the page are eligible for search, but there are some issues that might prevent it from getting full features.\n\nRich Results Types – A comma separated list of all rich result enhancements discovered on the page.\n\nRich Results Types Errors – A comma separated list of all rich result enhancements discovered with an error on the page. To export specific errors discovered, use the ‘Bulk Export > URL Inspection > Rich Results’ export.\n\nRich Results Warnings – A comma separated list of all rich result enhancements discovered with a warning on the page. To export specific warnings discovered, use the ‘Bulk Export > URL Inspection > Rich Results’ export.\n\nYou can read more about the the indexed URL results from Google.\n\nFilters\n\nThis tab includes the following filters.\n\nClicks Above 0 – This simply means the URL in question has 1 or more clicks.\n\nNo Search Analytics Data – This means that the Search Analytics API didn’t return any data for the URLs in the crawl. So the URLs either didn’t receive any impressions, or perhaps the URLs in the crawl are just different to those in GSC for some reason.\n\nNon-Indexable with Search Analytics Data – URLs that are classed as non-indexable, but have Google Search Analytics data.\n\nOrphan URLs – URLs that have been discovered via Google Search Analytics, rather than internal links during a crawl. This filter requires ‘Crawl New URLs Discovered In Google Search Console’ to be enabled in the ‘Search Analytics’ tab of the Google Search Console configuration (‘Configuration > API Access > Google Search Console > Search Analytics ‘) and post ‘crawl analysis‘ to be populated. Please see our guide on how to find orphan pages.\n\nURL Is Not on Google – The URL is not indexed by Google and won’t appear in the search results. This filter can include non-indexable URLs (such as those that are ‘noindex’) as well as Indexable URLs that are able to be indexed. It’s a catch all filter for anything not on Google according to the API.\n\nIndexable URL Not Indexed – Indexable URLs found in the crawl that are not indexed by Google and won’t appear in the search results. This can include URLs that are unknown to Google, or those that have been discovered but not indexed, and more.\n\nURL is on Google, But Has Issues – The URL has been indexed and can appear in Google Search results, but there are some problems with mobile usability, AMP or Rich results that might mean it doesn’t appear in an optimal way.\n\nUser-Declared Canonical Not Selected – Google has chosen to index a different URL to the one declared by the user in the HTML. Canonicals are hints, and sometimes Google does a great job of this, other times it’s less than ideal.\n\nPage Is Not Mobile Friendly – The page has issues on mobile devices.\n\nAMP URL Is Invalid – The AMP has an error that will prevent it from being indexed.\n\nRich Result Invalid – The URL has an error with one or more rich result enhancements that will prevent the rich result from showing in the Google search results. To export specific errors discovered, use the ‘Bulk Export > URL Inspection > Rich Results’ export.\n\nFor more on using the URL Inspection API, please read our guide on ‘How To Automate the URL Inspection API‘.\n\nLink Metrics\n\nThe Link Metrics tab includes data from Majestic, Ahrefs, and Moz when the SEO Spider is integrated with their APIs.\n\nTo pull in link metrics simply go to ‘Configuration > API Access’. After selecting a tool, you will need to generate and insert an API key. Once connected, run a crawl and data will be populated against URLs.\n\nPlease read the following guides for more details on setting up the API with each tool respectively:\n\nMajestic\n\nAhrefs\n\nMoz\n\nColumns & Metrics\n\nAddress – The URL address\n\nStatus Code – The HTTP Response Code\n\nTitle 1 – The (first) Page title discovered on the URL\n\nWhen Integrated the Link Data can be collected for the following metric groups:\n\nExact URL\n\nExact URL (HTTP + HTTPS)\n\nSubdomain\n\nDomain\n\nMajestic Metrics\n\nExternal Backlinks\n\nReferring Domains\n\nTrust Flow\n\nCitation Flow\n\nReferring IPs\n\nReferring Subnets\n\nIndexed URLs\n\nExternal Backlinks EDU\n\nExternal Backlinks GOV\n\nReferring Domains EDU\n\nReferring Domains GOV\n\nTrust Flow Topics\n\nAnchor Text\n\nYou can read more about the definition of each metric from Majestic.\n\nAhrefs Metrics\n\nBacklinks\n\nRefDomains\n\nURL Rating\n\nRefPages\n\nPages\n\nText\n\nImage\n\nSite Wide\n\nNot Site Wide\n\nNoFollow\n\nDoFollow\n\nRedirect\n\nCanonical\n\nGov\n\nEdu\n\nHTML Pages\n\nLinks Internal\n\nLinks External\n\nRef Class C\n\nRefips\n\nLinked Root Domains\n\nTwitter\n\nPinterest\n\nGPlus\n\nFacebook\n\nFacebook Likes\n\nFacebook Shares\n\nFacebook Comments\n\nFacebook Clicks\n\nFacebook Comments Box\n\nTotal Shares\n\nMedium Shares\n\nKeywords\n\nKeywords Top 3\n\nKeywords Top 10\n\nTraffic\n\nTraffic Top 3\n\nTraffic Top 10\n\nValue\n\nValue Top 3\n\nValue Top 10\n\nYou can read more about the definition of each metric from Ahrefs.\n\nMoz Metrics\n\nPage Authority\n\nMozRank\n\nMozRank External Equity\n\nMozRank Combined\n\nMozTrust\n\nTime Last Crawled (GMT)\n\nTotal Links (Internal or External)\n\nExternal Equity-Passing Links\n\nTotal Equity-Passing Links (internal or External)\n\nSubdomains Linking\n\nTotal Linking Root Domains\n\nTotal External Links\n\nSpam Score\n\nLinks to Subdomain\n\nRoot Domains Linking to Subdomain\n\nExternal Links to Subdomain\n\nDomain Authority\n\nRoot Domains Linking\n\nLinking C Blocks\n\nLinks to Root Domain\n\nExternal Links to Root Domain\n\nYou can read more about the definition of each metric from Moz.\n\nChange Detection\n\nThe Change Detection tab contains data and filters around changes between current and previous crawls.\n\nThis tab will only be available if you are in ‘Compare’ mode when performing a crawl comparison.\n\nIn ‘Compare’ mode, click on the compare configuration via ‘Config > Compare’ (or the ‘cog’ icon at the top) and select the elements and metrics you want to identify changes in.\n\nOnce the crawl comparison has been run, the ‘Change Detection’ tab will appear in the master view and in the Overview tab, containing filters for any elements and metrics selected with details of changes discovered.\n\nColumns\n\nThis tab includes the following columns for current and previous crawls.\n\nAddress – The URL address.\n\nIndexability – Whether the URL is Indexable or Non-Indexable.\n\nTitle 1 – The (first) page title discovered on the page.\n\nMeta Description 1 – The (first) meta description on the page.\n\nh1 – 1 – The first h1 (heading) on the page.\n\nWord Count – This is all ‘words’ inside the body tag, excluding HTML markup. The count is based upon the content area that can be adjusted under ‘Config > Content > Area’. By default, the nav and footer elements are excluded. You can include or exclude HTML elements, classes and IDs to calculate a refined word count. Our figures may not be exactly what performing this calculation manually would find, as the parser performs certain fix-ups on invalid HTML. Your rendering settings also affect what HTML is considered. Our definition of a word is taking the text and splitting it by spaces. No consideration is given to visibility of content (such as text inside a div set to hidden).\n\nCrawl Depth – Depth of the page from the start page (number of ‘clicks’ away from the start page). Please note, redirects are counted as a level currently in our page depth calculations.\n\nInlinks – Number of internal hyperlinks to the URL. ‘Internal inlinks’ are links in anchor elements pointing to a given URL from the same subdomain that is being crawled.\n\nUnique Inlinks – Number of ‘unique’ internal inlinks to the URL. ‘Internal inlinks’ are links in anchor elements pointing to a given URL from the same subdomain that is being crawled. For example, if ‘page A’ links to ‘page B’ 3 times, this would be counted as 3 inlinks and 1 unique inlink to ‘page B’.\n\nOutlinks – Number of internal outlinks from the URL. ‘Internal outlinks’ are links in anchor elements from a given URL to other URLs on the same subdomain that is being crawled.\n\nUnique Outlinks – Number of unique internal outlinks from the URL. ‘Internal outlinks’ are links in anchor elements from a given URL to other URLs on the same subdomain that is being crawled. For example, if ‘page A’ links to ‘page B’ on the same subdomain 3 times, this would be counted as 3 outlinks and 1 unique outlink to ‘page B’.\n\nExternal Outlinks – Number of external outlinks from the URL. ‘External outlinks’ are links in anchor elements from a given URL to another subdomain.\n\nUnique External Outlinks – Number of unique external outlinks from the URL. ‘External outlinks’ are links in anchor elements from a given URL to another subdomain. For example, if ‘page A’ links to ‘page B’ on a different subdomain 3 times, this would be counted as 3 external outlinks and 1 unique external outlink to ‘page B’.\n\nUnique Types – The unique number of structured data itemtypes discovered for the URL.\n\nFilters\n\nThis tab includes the following filters.\n\nIndexability – Pages that have changed indexability (Indexable or Non-Indexable).\n\nPage Titles – Pages that have changed page title elements.\n\nMeta Description – Pages that have changed meta descriptions.\n\nH1 – Pages that have changed h1.\n\nWord Count – Pages that have changed word count.\n\nCrawl Depth – Pages that have changed crawl depth.\n\nInlinks – Pages that have changed inlinks.\n\nUnique Inlinks – Pages that have changed unique inlinks.\n\nInternal Outlinks – Pages that have changed internal outlinks.\n\nUnique Internal Outlinks – Pages that have changed unique internal outlinks.\n\nExternal Outlinks – Pages that have changed external outlinks.\n\nUnique External Outlinks – Pages that have changed unique external outlinks.\n\nStructured Data Unique Types – Pages that have changed unique number of structured data itemtypes discovered.\n\nContent – Pages where the content has changed by more than 10% (or the configured similarity change under ‘Config > Compare’).\n\nFor more information on Change Detection, please read our tutorial on ‘How To Compare Crawls‘.\n\nSERP snippet\n\nIf you highlight a URL in the top window, this bottom window tab populates.\n\nThe SERP Snippet tab shows you how the URL may display in the Google search results. The truncation point (where Google shows an elipsis (…) and cuts off words) is calculated based upon pixel width, rather than number of characters. The SEO Spider uses the latest pixel width cut off point and counts the number of pixels used in page titles and meta descriptions for every character to show an emulated SERP snippet for greater accuracy.\n\nThe current limits are displayed under the page titles and meta description tabs and filters ‘Over X Pixels’ and in the ‘available’ pixels column below.\n\nGoogle changes the SERPs regularly and we have covered some of the changes in previous blog posts, here and here.\n\nGoogle don’t provide pixel width or character length recommendations, and hence the SERP snippet emulator in the SEO Spider is based upon our research in the SERPs. Google may use more characters than are displayed in scoring, however it is important to include key information in the visible SERP for users.\n\nThe SEO Spider’s SERP snippet emulator defaults to desktop, and both mobile and tablet pixel width truncation points are different. You can update the max description preferences under ‘Config > Spider > Preferences’ to a mobile or tablet length. You can switch ‘device’ type within the SERP snippet emulator to view how these appear different to desktop and our current estimated pixel lengths for mobile.\n\nEditing SERP Snippets\n\nYou can edit page titles and descriptions directly in the interface to view how the SERP snippet may appear in Google.\n\nThe SEO Spider will by default remember the edits you make to page titles and descriptions, unless you click the ‘reset title and description’ button. This allows you to make as many changes as you like using the emulator to perfect your SERP snippets, export (‘Reports > SERP Summary’) and send to a client or development team to make the changes to the live site.\n\nPlease note – The SEO Spider does not update your website, this will need to be performed independently.\n\nRendered page\n\nYou can view the rendered page the SEO Spider crawled in the ‘Rendered Page’ tab which populates when crawling in JavaScript rendering mode. This only populates the lower window pane when selecting URLs in the top window.\n\nThis feature is enabled by default when using JavaScript rendering functionality, and works alongside the configured user-agent, AJAX timeout and view port size.\n\nIn the left hand lower window, ‘blocked resources’ of the rendered page can also be viewed. The filter is set to ‘blocked resources’ by default, but this can also be changed to show ‘all resources’ used by the page.\n\nThe rendered screenshots are viewable within the ‘C:\\Users\\User Name\\.ScreamingFrogSEOSpider\\screenshots-XXXXXXXXXXXXXXX’ folder, and can be exported via the ‘bulk export > Screenshots’ top level menu, to save navigating, copying and pasting.\n\nIf you’re utilising JavaScript rendering mode, then please refer to our guide on How To Crawl JavaScript Websites.\n\nStructured Data Details\n\nYou can view the Structured data details of any highlighted URL providing your crawl is set to extract Structured Data. This only populates the lower windowpane when selecting URLs in the top window.\n\nTo enable structured data extraction, simply go to ‘Configuration > Spider > Extraction > JSON-LD/Microdata/RDFa & Schema.org Validation/Google Validation’.\n\nThe left-hand side of the tab shows property values alongside error and/or warning icons. Clicking one of these values will provide specific details on the validation errors/warnings in the right-hand window. The columns listed in this right-hand side window include:\n\nValidation Type – The structured data field with validation issues (Article, Person, Product etc).\n\nIssue Severity – Whether the issue value is recommended or required to validate.\n\nIssue – Details on the specific issue.\n\nFor more details please read our ‘How to Test & Validate Structured Data Guide’.\n\nLighthouse Details\n\nWhen integrated, you can view the page speed and mobile usability details of any highlighted URL. This will require a crawl to be connected to the PageSpeed Insights API.\n\nTo pull these metrics, simply to go ‘Configuration > API Access > PageSpeed Insights’, insert a free PageSpeed API key or choose to run Lighthouse locally, and then connect and run a crawl.\n\nWith data available, selecting a URL in the top window will provide more details in the lower window tab.\n\nThe left-hand window provides specific information on both the metrics extracted and available opportunities specific to the highlighted URL. Clicking an opportunity will display more information in the right-hand window. This consists of the following columns:\n\nThe Source Page – The URL chosen in the top window.\n\nURL – The linked resource which has opportunities available.\n\nSize (Bytes) – The current size of the listed resource.\n\nPotential Savings – The potential size savings by implementing highlighted opportunity.\n\nPlease see our PageSpeed Insights integration guide for full detail of available speed metrics and opportunities.\n\nSpelling & Grammar Details\n\nIf you highlight a URL in the top window, this lower window tab populates. This contains details on any spelling and grammar issues for the URL in question.\n\nThe spell check and / or grammar check must be enabled before the crawl for this tab to be populated.\n\nThe lower window ‘Spelling & Grammar Details’ tab shows the error, type (spelling or grammar), detail, and provides a suggestion to correct the issue. The right hand-side of the details tab also show a visual of the text from the page and errors identified.\n\nN-grams\n\nThe N-grams tab includes details of sequences of phrases and their frequency within HTML page content.\n\nTo enable this functionality, ‘Store HTML / Store Rendered HTML’ needs to be enabled under ‘Config > Spider > Extraction’. A URL or a selection of URLs can then be highlighted in the top window, and the n-grams tab will populate with aggregated n-gram data.\n\nThe left-hand side displays aggregated n-grams for all highlighted URLs. The filters can be adjusted to display a range of n-grams, from 1-gram to 6-gram.\n\nThe right-hand side displays the details of the URL the selected n-grams are on. In the example above, the 2-gram ‘broken links’ is highlighted and is shown to be on 3 pages.\n\nIssues\n\nThe issues tab updates in real-time to provide details of potential issues, warnings and opportunities discovered in a crawl. This data is based upon existing data from the overview tabs and filters, but only shows potential ‘issues’.\n\nThe data is classified with issue type, priority and has in-app issue descriptions and tips.\n\nIssue Name – The issue name, based upon the tab and filter.\n\nIssue Type – Whether it’s likely an ‘Issue’, an ‘Opportunity’ or a ‘Warning’.\n\nIssue Priority – ‘High’, ‘Medium’ or ‘Low’ based upon potential impact and may require more attention.\n\nURLs – The number of URLs with the issue.\n\n% of Total – Proportion of URLs with the issue from the total.\n\nEach issue has a ‘type’ and an estimated ‘priority’ based upon the potential impact.\n\nIssues are an error or issue that should ideally be fixed.\n\nOpportunities are ‘potential’ areas for optimisation and improvement.\n\nWarnings are not necessarily an issue, but should be checked – and potentially fixed.\n\nPriorities are based upon potential impact that may require more attention, rather than definitive action – from broadly accepted SEO best practice. They are not hard rules for what should be prioritised in your SEO strategy or to be ‘fixed’ in your SEO audit, as no tool can provide that as they lack context.\n\nHowever, they can help users spot potential issues more efficiently than manually filtering data.\n\nE.g – ‘Directives: Noindex’ will be classed as a ‘warning’, but with a ‘High’ priority as it could potentially have a big impact if URLs are incorrectly noindex.\n\nAll Issues can be exported in bulk via ‘Bulk Export > Issues > All’. This will export each issue discovered (including their ‘inlinks’ variants for things like broken links) as a separate spreadsheet in a folder (as a CSV, Excel and Sheets).\n\nIt’s important to understand that the issues tab does not substitute expertise and an SEO professional who has context of the business, SEO and nuances in prioritising what’s important.\n\nThe Issues tab acts as a guide to help provide direction to users who can make sense of the data and interpret it into appropriate prioritised actions relevant to each unique website and scenario.\n\nA simple export of ‘Issues’ data is in itself not an ‘SEO Audit’ that we’d recommend without expert guidance and prioritisation over what’s really important.\n\nSite Structure\n\nThe site structure tab updates in real-time to provide an aggregated directory tree view of the website. This helps visualise site architecture, and identify where issues are at a glance, such as indexability of different paths.\n\nThe top table updates in real-time to show the path, total number of URLs, Indexable and Non-Indexable URLs in each path of the website.\n\nPath – The URL path of the website crawled.\n\nURLs – The total number of unique children URLs found within the path.\n\nIndexable – The total number of unique Indexable children URLs found within the path.\n\nNon-Indexable – The total number of unique Non-Indexable children URLs found within the path.\n\nYou’re able to adjust the ‘view’ of the aggregated Site Structure, to also see ‘Indexability Status’, ‘Response Codes’ and ‘Crawl Depth’ of URLs in each path.\n\nThe lower table and graph show the number of URLs at crawl depths between 1-10+ in buckets based upon their response codes.\n\nDepth (Clicks from Start URL) – Depth of the page from the homepage or start page (number of ‘clicks’ away from the start page).\n\nNumber of URLs – Number of URLs encountered in the crawl that have a particular Depth.\n\n% of Total – Percentage of URLs in the crawl that have a particular Depth.\n\n‘Crawl Depth’ data for every URL can be found and exported from the ‘Crawl Depth’ column in the ‘Internal’ tab.\n\nSegments\n\nYou can segment a crawl to better identify and monitor issues and opportunities from different templates, page types, or areas of priority.\n\nThe segments configuration and right-hand tab is only available if you’re using database storage mode. If you’re not already using database storage mode, we highly recommend it.\n\nThis can be adjusted via ‘File > Settings > Storage Mode’ and has a number of benefits.\n\nThe segments tab updates in real-time to provide an aggregated view of segmented data and URLs. The tab will be blank, if segments have not been set-up. Segments can be set up by clicking the cog icon, or via ‘Config > Segments’.\n\nThe data shown for segments includes the following:\n\nSegment – The segment name. The order of this follows the order selected in the segments configuration unless sorted.\n\nURLs – The number of URLs within the segment.\n\n% Segmented – The proportion of URLs segmented from the total number of URLs.\n\nIndexable – The number of Indexable URLs in the segment.\n\nNon-Indexable – The number of Non-Indexable URLs in the segment.\n\nIssues – An error or issue that should ideally be fixed.\n\nWarnings – Not necessarily an issue, but should be checked – and potentially fixed.\n\nOpportunities – Potential areas for optimisation and improvement.\n\nHigh – Priority based upon potential impact and may require more attention.\n\nMedium – Priority based upon potential impact and may require some attention.\n\nLow – Priority based upon potential impact and may require less attention.\n\nThe Segments tab ‘view’ filter can be adjusted to better analyse issues, indexability status, reponse codes and crawl depth by segment.\n\nResponse Times\n\nThe response times tab updates in real-time to provide a top level view of URL response times during a crawl.\n\nResponse Times – A range of times in seconds to download the URL.\n\nNumber of URLs – Number of URLs encountered in the crawl in a particular Response Time range.\n\n% of Total – Percentage of URLs in the crawl in a particular Response Time range.\n\nResponse time is calculated from the time it takes to issue an HTTP request and get the full HTTP response back from the server. The figure displayed on the SEO Spider interface is in seconds. Please note that this figure may not be 100% reproducible as it depends very much on server load and client network activity at the time the request was made.\n\nThis figure does not include the time taken to download additional resources when in JavaScript rendering mode. Each resource appears separately in the user interface with its own individual response time.\n\nFor thorough PageSpeed analysis, we recommend the PageSpeed Insights API integration."
    }
}
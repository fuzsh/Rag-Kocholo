{
    "id": "dbpedia_8301_3",
    "rank": 94,
    "data": {
        "url": "https://blog.anynines.com/posts/get-to-know-apache-mesos-and-apache-spark/",
        "read_more_link": "",
        "language": "en",
        "title": "Get To Know Apache Mesos and Apache Spark",
        "top_image": "https://a9s-int-blog-cms.s3.amazonaws.com/small_get_to_know_apache_mesos_and_apache_spark_f2984c78bb.png",
        "meta_img": "https://a9s-int-blog-cms.s3.amazonaws.com/small_get_to_know_apache_mesos_and_apache_spark_f2984c78bb.png",
        "images": [
            "https://a9s-int-blog-cms.s3.amazonaws.com/Jens_Breuer_5f15282800.png",
            "https://a9s-int-blog-cms.s3.amazonaws.com/architecture3_3a82a14e25.png",
            "https://a9s-int-blog-cms.s3.amazonaws.com/architecture_example_648919e994.png",
            "https://a9s-int-blog-cms.s3.amazonaws.com/cluster_overview_0296bea07a.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Sven Schmidt"
        ],
        "publish_date": "2015-12-06T22:27:29+00:00",
        "summary": "",
        "meta_description": "Apache Mesos is a centralized, fault-tolerant cluster manager, designed for distributed computing environments – learn what it can do for you.",
        "meta_lang": "",
        "meta_favicon": "/images/favicon.ico",
        "meta_site_name": "anynines Blog",
        "canonical_link": "https://blog.anynines.com/posts/get-to-know-apache-mesos-and-apache-spark/",
        "text": "Table of Contents\n\nWhat is Apache Mesos?\n\nWhy is Mesos relevant?\n\nData Locality\n\nHow does Mesos work?\n\nMaster Daemon\n\nFrameworks\n\nScheduler\n\nExecutor\n\nWhat do I need Mesos for?\n\nHow to match resources to a task with Mesos?\n\nWhere can I learn more about Mesos?\n\nApache Spark\n\nWhat is Spark?\n\nWhy is Spark relevant?\n\nHow does Spark work?\n\nShow me a Spark Demo!\n\nLinks & Resources:\n\nWhat is Apache Mesos?\n\nBen Hindman, co-creator of Apache Mesos describes it like:\n\nApache Mesos is a centralized, fault-tolerant cluster manager, designed for distributed computing environments. Mesos is an open source project and was developed at the University of California at Berkeley. It provides resource management and isolation,scheduling of CPU & memory across the cluster. Mesos joins multiple physical resources into a single virtual one. In some ways, it is the opposite of classic virtualisation, where a single physical resource is split into multiple virtual resources. With Apache Mesos you can build/schedule cluster frameworks such as Apache Spark.\n\nWhy is Mesos relevant?\n\nThe clusters of commodity hardware, where you use a large number of already-available computing components for parallel computing are trendy nowadays. To handle such clusters you need a suitable framework. Although many cloud computing frameworks exist today, you have to choose the right one for you, since every framework has its pros and cons. In larger organizations, multiple cluster-frameworks are required.\n\nLet us look at legacy strategies to run multiple cluster compute frameworks:\n\nSplit your cluster and run one framework per sub-cluster.\n\nVirtualize and allocate a set of VMs to each framework.\n\nWith these strategies you face the following problems:\n\nsuboptimal server utilization\n\ninefficient data sharing\n\nData Locality\n\nData Locality simply answers the question : How expensive is it to access the needed data? An example of such access cost could be the elapsed time. You have probably already heard about that concept, because it is also used by routers to choose the best route in a network.\n\nCompute frameworks often divide workloads into jobs and tasks. Tasks usually are executed fastly, often multiple jobs per node can be run.\n\nJobs should be run where the data is, so you have a better ratio between time used for data transport vs. computation. Short job execution times enable higher cluster utilization.\n\nWhat we need is a unified, generic approach of sharing cluster resources such as CPU time and data across compute frameworks. This is what Mesos provides!\n\nHow does Mesos work?\n\nMesos consists of the following components:\n\nZooKeeper\n\nMesos masters\n\nMesos slaves\n\nFrameworks\n\nChronos, Marathon,…\n\nAurora, Hadoop, Jenkins, Spark, Torque\n\nMesos has also a master daemon that manages slave daemons running on each cluster node.\n\nMaster Daemon\n\nThe master controls resources (cpu, ram, …) across applications by making resource offers to applications. They can either take them by specifying tasks that can run on those resources or reject them. The master decides about resource offering to frameworks based on organizational policy such as fair sharing or strict priorities. If the policies don’t fit, you can add new policy strategies via plug-ins.\n\nFrameworks\n\nA Framework running on top of Mesos,consists of two components:\n\nScheduler\n\nExecutor\n\nScheduler\n\nThe scheduler registers with the master and receives resource offerings from the master. The Scheduler decides what to do with resources offered by the master within the framework.\n\nExecutor\n\nThe Executor is launched on slave nodes and runs framework tasks.\n\nExample Resource Offer\n\nStep 1:\n\nSlave 1 tells the master that it has 4 free CPUs and 4GB memory. The Mesos master invokes the allocation module which tells that framework 1 should be offered all available resources.\n\nStep 2:\n\nThe allocation module sends a resource offer to the framework describing what is available on slave 1 for it.\n\nStep 3:\n\nThe framework scheduler of framework 1 responds to the Mesos master and sends information about two tasks which should run on slave 1.\n\nStep 4:\n\nThe Mesos master sends the two tasks to Slave 1, which allocates appropriate resources to the executor, which launches the two tasks. As you can see, the tasks need only 3 CPUs and 3GB of memory. The 4th CPU and the other 1GB of RAM are now offered to Framework 2.\n\nWhat do I need Mesos for?\n\nProviding a “thin resource sharing layer that enables ﬁne-grained sharing across diverse cluster computing frameworks, by giving frameworks a common interface for accessing cluster resources.”\n\nMesos: A platform for fine-grained resource sharing in the data center\n\nHow to match resources to a task with Mesos?\n\nBe framework agnostic to adapt to different scheduling needs\n\nBe highly scalable\n\nScheduling must be HA and fault-tolerant\n\nAddresses large data warehouse scenarios, such as Facebook’s Hadoop data warehouse ( ~1200 nodes in 2010)\n\nMedian job length ~84s built of\n\nMap reduce tasks ~23s\n\nWhere can I learn more about Mesos?\n\nOn the Mesos website you can find a list of companies using Mesos:\n\nhttps://mesos.apache.org/documentation/latest/powered-by-mesos/\n\nAnd for projects built on Mesos you can visit:\n\nhttps://mesos.apache.org/documentation/latest/mesos-frameworks/\n\nApache Spark\n\nWhat is Spark?\n\nWhen you look at the official documentation of Apache Spark it says:\n\nhttps://spark.apache.org/docs/latest/\n\nSpark provides APIs/SDKs for:\n\nJava\n\nScala\n\nPython\n\nand supports these Tools:\n\nSpark SQL – SQL and structured data processing\n\nMLib – Machine learning library\n\nGraphX – Graph processing\n\nSpark Streaming – scalable, high-throughput, fault-tolerant stream processing of live data streams\n\nIt supports a much wider class of applications than MapReduce while maintaining its automatic fault-tolerance.\n\nWhy is Spark relevant?\n\nSpark is well designed for data analytics use cases:\n\nIterative algorithms\n\nE.g. machine learning algorithms and graph algorithms such as PageRank.\n\nInteractive data mining\n\nUser loads data into RAM across cluster and query it repeatedly.\n\nStreaming applications\n\nMaintain aggregate state over time.\n\nTo support these applications efficiently, Spark offers an abstraction called Resilient Distributed Datasets (RDDs). RDDs can be stored in memory between queries without requiring replication. RDDs can rebuild lost data by lineage, therefore it remembers how it was built from other datasets.\n\nhttps://spark.apache.org/research.html\n\nHow does Spark work?\n\nSpark runs as independent sets of processes on a cluster and is coordinated by the SparkContext in your main program (driver program). The SparkContext can connect to several types of cluster managers, which allocate resources across applications. The Cluster Manager can be a Spark standalone manager, Apache Mesos or Apache Hadoop YARN. Spark acquires executors on nodes in the cluster. The executor is a process, runs computations and stores data for your app. Then Spark sends your application code to the executors.\n\nEach application has its own executor, which lives as long as the app lives and runs tasks in multiple threads. This isolates one application from others. Each scheduler schedules its own tasks. When you have different apps, they have different executors and different JVMs.\n\nShow me a Spark Demo!\n\nHere you can find Spark examples:\n\nhttps://spark.apache.org/examples.html\n\nOn the official Spark website you can find a list of companies using Spark:\n\nhttps://cwiki.apache.org/confluence/display/SPARK/Powered+By+Spark"
    }
}
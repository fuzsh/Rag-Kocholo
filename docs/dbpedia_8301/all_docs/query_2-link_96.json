{
    "id": "dbpedia_8301_2",
    "rank": 96,
    "data": {
        "url": "https://towardsdatascience.com/operationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b",
        "read_more_link": "",
        "language": "en",
        "title": "Operationalization of ML Pipelines on Apache Mesos and Hadoop using Airflow",
        "top_image": "https://miro.medium.com/v2/resize:fit:416/1*huWWLy6PqZ16_dPt3TolTA.jpeg",
        "meta_img": "https://miro.medium.com/v2/resize:fit:416/1*huWWLy6PqZ16_dPt3TolTA.jpeg",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/resize:fill:88:88/1*58WJutqmflPTzJoa72smZQ.jpeg",
            "https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg",
            "https://miro.medium.com/v2/resize:fill:144:144/1*58WJutqmflPTzJoa72smZQ.jpeg",
            "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Mihail Vieru",
            "medium.com"
        ],
        "publish_date": "2020-05-19T23:38:54.310000+00:00",
        "summary": "",
        "meta_description": "An architecture for orchestrating machine learning pipelines in production on Apache Mesos and Hadoop using Airflow",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*VzTUkfeGymHP4Bvav-T-lA.png",
        "meta_site_name": "Medium",
        "canonical_link": "https://towardsdatascience.com/operationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b",
        "text": "This blog post originally appeared on the NEW YORKER Tech Blog.\n\nOne of the highest impact use cases for machine learning in retail companies is markdown pricing. As one of Europe’s largest fashion retailers, NEW YORKER sets an initial price for each item sold in our 1100 stores in 45 countries. The price is gradually marked down in a series of price reductions until the item is sold out. The goal is to predict when and how to mark down the price, so that the whole inventory is sold at the highest possible price. Thus, maximizing the company’s revenue.\n\nSolving this use case involves two tasks: creating a model which delivers accurate predictions, and operationalizing it into a production pipeline on our current infrastructure.\n\nIn this post we focus on the latter task, and present an architecture for orchestrating machine learning pipelines in production on Apache Mesos and Hadoop using Airflow. We start with the requirements for the machine learning model along with the ones posed by our current infrastructure and data engineering architecture. We then present an architecture which fulfills those, including containerization, pipeline orchestration with Airflow, performance considerations and the simplified DAG code. We conclude with a summary and an outlook on future work.\n\nRequirements\n\nLet’s start with the requirements of the model we wish to operationalize, along with the ones stemming from our current infrastructure and data engineering architecture.\n\nThe training data for the model are stored as Parquet files on HDFS in our Hadoop cluster. They are the result of extensive prior batch data processing with Spark, including data cleaning, aggregation and imputation where necessary. This legacy ETL is not orchestrated with Airflow yet and uses Luigi for the workflow management and Cron Jobs for the scheduling.\n\nThe model is written in Python and the machine learning framework employed should be up to the choice of our Data Scientists. Currently LightGBM is used. The training and inference are done on a per country basis — a characteristic which we can leverage later for parallelization. GPU support is important, as our Data Scientists are also considering deep learning approaches, which would profit from it.\n\nThe resulting predictions of the model need to be written to our ERP system’s persistence layer — a relational database. The predictions serve as a guide to our Pricing department, which checks them via the ERP’s UI for plausibility and applies the suggested price reductions where and when it deems them appropriate. As the predictions change very slowly, a refresh is required once at the beginning of each week.\n\nBesides the aforementioned Hadoop cluster, we employ a DC/OS cluster in our infrastructure. It provides us with container orchestration and resource scheduling based on Apache Mesos. GPUs can be used in containers via Marathon application and Metronome job definitions for long and short running applications. We have a containerized Airflow installation running on DC/OS for orchestrating most of our data pipelines.\n\nArchitecture\n\nFor fulfilling the requirements stated above, we have come up with the following architecture.\n\nContainerization\n\nWe’ve decided to containerize the model training and inference computation and use DC/OS to run it. This has two advantages. First, it preserves the independence of the machine learning framework chosen by our Data Scientists. Second, we can leverage existing GPUs on the DC/OS cluster to speed up future deep learning approaches.\n\nIf we would have chosen to use SparkML or parallelize another ML framework with Spark on Hadoop, we would have had to migrate the existing LightGBM model to this approach. We would have then essentially locked in the ML framework employed. Furthermore, to facilitate GPU usage, we would have needed to install GPUs on the Hadoop cluster or move them from the DC/OS cluster to it. A Hadoop version upgrade to 3.x from the present 2.x would have also been required. All this would have been non trivial and very time consuming.\n\nThe container itself has the following workflow:\n\nAt the beginning, it downloads the training data from HDFS. Then it trains the model on it and computes the predictions for a given country. Finally, the predictions are refreshed in the ERP’s underlying relational database.\n\nThe refreshing of the predictions is done in a single transaction, i.e. it deletes old predictions for all items for a single country and it writes new ones. We don’t use updates here, as the set of predictions can vary across time. In this way, the ERP’s users will always see predictions in the UI for the selected set of items with no downtime.\n\nWe’ve parameterized the container, so that it can calculate and refresh predictions not only for a single country but for a specified group of countries.\n\nPipeline Orchestration\n\nHaving a parameterized container for the model training and inference in place, we can now proceed to deploy and orchestrate multiple instances of it to form a proper pipeline with the help of Airflow:\n\nFor short-running applications, i.e. jobs, DC/OS provides Metronome as a task scheduler on top of Mesos. As our container is ephemeral in nature, we can employ Metronome to deploy it on the DC/OS cluster. We do this from Airflow using the MetronomeOperator, which we’ll introduce more in-depth shortly.\n\nWe start multiple instances of the MetronomeOperator for groups of countries so that all 45 countries are covered, thus effectively parallelizing the work.\n\nAs the legacy ETL — which processes the needed training data on Hadoop — is not orchestrated with Airflow yet, we employ a modified HDFS sensor in order to trigger the start of the MetronomeOperator instances. The original Airflow HDFS sensor was only functioning with Python 2. We’ve made some small changes so that it works with the Python 3 version that we use in our containerized Airflow installation.\n\nThe code for it can be found here:\n\nhttps://github.com/NewYorkerData/ny-public-airflow-operators\n\nWe schedule the pipeline to start simultaneously with the legacy ETL on Hadoop. The HDFS sensor will continuously poll for the success file and trigger the remaining operators in the DAG when the file is found.\n\nNote: As part of the production pipeline we also write the predictions to HDFS for future analysis. During the computation thereof we log all steps to Elasticsearch. We have Kibana dashboards in place and currently use elastalert to monitor and alert on anomalies, e.g. the number of predictions decreased by 10% from last run.\n\nBefore promoting changes to production, we use Neptune as part of the test pipeline to keep track and evaluate the impact of the various changes to the model itself.\n\nMetronome Airflow Operator\n\nNo Metronome operator existed for Airflow to the best of our knowledge at the inception of the project. So we created one.\n\nIt takes a Metronome job definition as a JSON parameter and works as depicted in the figure below:\n\nFirst, we get an authorization token from the IAM REST API to be able to deploy containers on the DC/OS cluster. Second, we check whether the job exists via the Metronome REST API. Depending on the response, we upsert the job definition. Then we start the job. Finally, we poll for the job status and, depending on the response, change the Airflow operator’s status to SUCCESSFUL or FAILED.\n\nThe code for the operator can be found here:\n\nhttps://github.com/NewYorkerData/ny-public-airflow-operators\n\nPerformance Considerations\n\nIn the Metronome job definition we can specify CPU, RAM and GPU usage for the container. Therefore we can scale vertically. Regarding the horizontal scaling, we do this by parallelizing the model training and inference for the groups of countries. The degree of parallelism is however not unbounded and is limited by the available cluster resources. In order not to use up all resources, we limit the number of concurrently running containers using the Airflow resource pool feature. The resource pool name is set in all MetronomeOperator instances, as we’ll see in the DAG code below. The size of the resource pool is thus equal to the degree of parallelism.\n\nImplementation as Airflow DAG\n\nBelow you can find the simplified version of the Airflow DAG:\n\ndefault_args = {\n\n\"owner\": \"ny-data-science\",\n\n\"start_date\": datetime(2020, 1, 1),\n\n\"provide_context\": True\n\n}dag = DAG(\n\n\"markdown-pricing-dag\",\n\nschedule_interval=\"0 8 * * 1\", # every Monday at 8am\n\ndagrun_timeout=timedelta(days=1),\n\ndefault_args=default_args,\n\nmax_active_runs=1,\n\ncatchup=True\n\n)# Example list of countries for the model training and inference\n\nCOUNTRY_LIST = [\"DEU\", \"AUT\", \"NLD\", \"FRA\", \"ESP\", \"ITA\"]# Job template we will use in the MetronomeOperator\n\nMETRONOME_JOB_TEMPLATE = \"\"\"\n\n{{\n\n\"id\": \"{}\",\n\n\"description\": \"Markdown Pricing ML Job\",\n\n\"run\": {{\n\n\"cpus\": 4,\n\n\"mem\": 32768,\n\n\"gpus\": 0,\n\n\"disk\": 0,\n\n\"ucr\": {{\n\n\"image\": {{\n\n\"id\": \"registry-dns-address:1234/markdown-pricing:latest-master\",\n\n\"forcePull\": true\n\n}}\n\n}},\n\n\"env\": {{\n\n\"COUNTRY_GROUP\": \"{}\",\n\n\"DB_USER\": \"{}\",\n\n\"DB_PASSWORD\": \"{}\"\n\n}}\n\n}}\n\n}}\n\n\"\"\"# Creates a Metronome job on DC/OS by instantiating the MetronomeOperator with the job template above\n\n# and setting the country group and other environment variables\n\ndef create_metronome_job_for_country_group(country_group, index):\n\nreturn MetronomeOperator(\n\ntask_id=f\"metronome_operator_{index}\",\n\nmetronome_job_json=\n\nMETRONOME_JOB_TEMPLATE.format(f\"markdown-job-{index}\",\n\ncountry_group,\n\nVariable.get(\"markdown_erp_db_user\"), Variable.get(\"markdown_erp_db_pwd\")),\n\ndcos_http_conn_id=\"dcos_master\",\n\ndcos_robot_user_name=Variable.get(\"robot_user_name_dcos\"),\n\ndcos_robot_user_pwd=Variable.get(\"robot_user_pwd_dcos\"),\n\ndag=dag,\n\npool=\"markdown_metronome_job_pool\",\n\nretries=3\n\n)# Get the resource pool size (in slots) for the MetronomeOperator instances from Airflow configuration\n\nmetronome_job_pool_size = get_pool_size(pool_name=\"markdown_metronome_job_pool\")# Split the country list into groups into N parts of approximately equal length for parallelization purposes.\n\n# N is here the size of the Metronome job pool.\n\n# Given the COUNTRY_LIST defined above and N = 3, the function will return: [[\"DEU\",\"AUT\"], [\"NLD\",\"FRA\"], [\"ESP\",\"ITA\"]]\n\ncountry_groups = split_country_list(COUNTRY_LIST, metronome_job_pool_size)# Iterates through the country groups and creates a Metronome job for each of those groups\n\nmetronome_countries_jobs = [create_metronome_job_for_country_group(country_group=country_group, index=index) for\n\nindex, country_group in enumerate(country_groups)]# HDFS sensor on the latest training data\n\ntraining_data_sensor = NYHDFSSensor(\n\ntask_id=\"training_data_sensor\",\n\nfilepaths=[f\"/data/production/markdown_training_data/{get_current_date()}/_SUCCESS\"],\n\nhdfs_conn_id=\"hdfs_conn_default\",\n\nretries=1440,\n\nretry_delay=timedelta(minutes=1),\n\ntimeout=0,\n\ndag=dag)# Create DAG\n\ntraining_data_sensor >> metronome_countries_jobs\n\nConclusion and Future Steps\n\nWe have shown an architecture which allows us to easily orchestrate machine learning pipelines in a mixed Mesos and Hadoop cluster environment using Airflow. We leverage Mesos’ container orchestration and resource scheduling to scale the model training and inference both horizontally and vertically, while being able to take advantage of available hardware acceleration provided by GPUs. We employ Airflow’s powerful features, such as sensors and dynamic DAGs, to manage the whole workflow effectively across the clusters.\n\nThere are still many improvement areas we can work on in the future.\n\nFor instance, migrating the legacy ETL on Hadoop from Luigi/Cron to Airflow will allow us to deprecate the HDFS sensor and simplify the DAG by using Airflow’s pipeline interdependency feature.\n\nWe’re also aware that some computation is common across all countries. This step could be extracted in a separate stage, on which all further stages would depend on, possibly improving the performance.\n\nFinally, we’re in the process of preparing the migration of our clusters from Mesos to Kubernetes. In the short-term we will need to adjust all our machine learning pipelines in Airflow. This should be in theory relatively easy to accomplish, e.g. use the KubernetesOperator instead of the MetronomeOperator in the DAGs. In the longer term, Kubernetes opens up a new world of possibilities in terms of machine learning operationalization approaches, such as Kubeflow."
    }
}
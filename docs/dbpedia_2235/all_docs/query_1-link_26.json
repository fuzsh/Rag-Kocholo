{
    "id": "dbpedia_2235_1",
    "rank": 26,
    "data": {
        "url": "https://arxiv.org/html/2310.05649v2",
        "read_more_link": "",
        "language": "en",
        "title": "Context, Composition, Automation, and Communication",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Context, Composition, Automation, and Communication - The C2AC Roadmap for Modeling and Simulation\n\nAdelinde Uhrmacher University of Rostock, Germany, adelinde.uhrmacher@uni-rostock.de Peter Frazier Cornell University, USA, pf98@cornell.edu Reiner Hähnle TU Darmstadt, Germany, haehnle@informatik.tu-darmstadt.de Franziska Klügl University of Örebro, Sweden, franziska.klugl@oru.se Fabian Lorig Malmö University, Sweden, fabian.lorig@mau.se Bertram Ludäscher University of Illinois Urbana-Champaign, USA, ludaesch@illinois.edu Laura Nenzi University of Trieste, Ital, lnenzi@units.it Cristina Ruiz-Martin Carleton University, Canada, cristinaruizmartin@sce.carleton.ca Bernhard Rumpe RWTH Aachen, Germany, rumpe@se-rwth.de Claudia Szabo University of Adelaide, Australia, claudia.szabo@adelaide.edu.au Gabriel A. Wainer Carleton University, Canada, gwainer@sce.carleton.ca Pia Wilsdorf University of Rostock, Germany, pia.wilsdorf@uni-rostock.de\n\nAbstract\n\nSimulation has become, in many application areas, a sine-qua-non. Most recently, COVID-19 has underlined the importance of simulation studies and limitations in current practices and methods. We identify four goals of methodological work for addressing these limitations. The first is to provide better support for capturing, representing, and evaluating the context of simulation studies, including research questions, assumptions, requirements, and activities contributing to a simulation study. In addition, the composition of simulation models and other simulation studies’ products must be supported beyond syntactical coherence, including aspects of semantics and purpose, enabling their effective reuse. A higher degree of automating simulation studies will contribute to more systematic, standardized simulation studies and their efficiency. Finally, it is essential to invest increased effort into effectively communicating results and the processes involved in simulation studies to enable their use in research and decision-making. These goals are not pursued independently of each other, but they will benefit from and sometimes even rely on advances in other subfields. In the present paper, we explore the basis and interdependencies evident in current research and practice and delineate future research directions based on these considerations.\n\n1 Introduction\n\nSimulation has become, in many areas, a sine qua non. Simulation, empirical evaluation, and analytical reasoning are regarded as the three pillars of science [1]. Simulation studies rely on soundly conducting and effectively intertwining steps of analyzing the system of interest, developing and refining the simulation model, executing diverse simulation experiments, and interpreting the (intermediate) results [2]. In this process, starting from the research question and the system of interest, inputs are selected and modified, assumptions and simplifications are revised in choosing a suitable abstraction for the model, requirements referring to outputs are specified and adapted, and data sources identified that might be used as input or to calibrate or validate the simulation model until a useful approximation has been achieved [3, 4] (Fig. 1). Thus, each step is empowered and constrained by the methods used, as well as the knowledge and experiences of the modeler. In addition to the modeler - possibly joined by data analysts, programmers, and visualization experts - domain experts and decision-makers might become involved at various points.\n\nRecently, the COVID-19 pandemic has underlined the importance of simulation studies [5]. Simulations were widely used during the pandemic to make forecasts [6] and support decisions made by governments [7, 8], hospitals [9], industry [10] and universities [11]. Simulations revealed some current limitations in conducting such studies [12, 13], including how quickly useful models can be developed, how the results can be interpreted, and how results and crucial aspects of simulation studies can be communicated to domain experts, decision-makers, and the general public [14].\n\nTo address these limitations in conducting and communicating simulation studies, further methodological research is needed:\n\n1.\n\nEnsure that simulation studies come with context. Context is crucial for helping modelers and domain experts to interpret results and reuse simulation products. It is equally important to explain simulation results to decision-makers confidently.\n\n2.\n\nImprove model composition and reuse. Model composition and reuse avoid building models from scratch. This saves time and, in addition, improves analysis quality because reuse is an important incentive for designing high-quality models.\n\n3.\n\nIncrease simulation automation. Central artifacts of a simulation study, e.g., simulation models and experiments, may be generated automatically. In addition, conducting and documenting the simulation study will benefit from intelligent guidance and support. Automation may save time for the modeler and contribute to the overall quality of simulation studies.\n\n4.\n\nFacilitate communication. This refers to communications between the modeler and domain expert and between the modeler and decision-makers. Problems in communication appear to be a central limiting factor in effectively using simulation for decisions. Better communication would also reduce the time required to produce impact.\n\nAny progress towards these goals relies largely on an unambiguous and accessible representation of the simulation study, its activities, sources, and products, and a goal-directed and situation-specific processing of this knowledge. Therefore, in the following section, we will first look at the current state of the art in terms of how simulation models (with a focus on discrete and stochastic models), simulation experiments, and behavioral requirements are specified. These considerations will be revisited when scrutinizing promising research avenues toward achieving the identified four goals. The present paper builds upon discussions during the Dagstuhl seminar “Computer Science Methods for Effective and Sustainable Simulation Studies (Dagstuhl Seminar 22401)” [15].\n\n2 State of the Art in Formal Approaches to Modeling and Simulation\n\nModeling means structuring and capturing knowledge about a given system in a suitably abstract manner. With the separation between the simulation model and executing the simulation model, the simulation model becomes explicit, accessible, and interpretable (possibly by different simulators) [16]. The value of this separation of concerns and an explicitly and formally specified simulation model is undisputed and reflected in the development of diverse formalisms [16, 17, 18], by pragmatically augmented and extended general modeling languages, such as UML and SysML [19, 20, 21, 22, 23, 24], and by the development of application-specific modeling languages [25, 26, 27, 20, 28] (see Table 1).\n\nThe credibility crisis in simulation [29] and the desire to promote the reproduction of computational results [30] strengthened the case for the accessibility of simulation models and code. In addition, they moved the explicit specification of simulation experiments into the focus of interest [31, 32, 33, 34], and motivated reporting and documentation guidelines for simulation experiments [35] as well as for entire simulation studies [36, 37, 38]. Increasingly, behavioral requirements, i.e., expectations referring to simulation model outputs, are expressed formally in temporal logic, or a domain-specific language to be checkable by (statistical) model checking [39, 40] or customized algorithms [41]. Many specification languages come with a formal semantics. In Table 1, we exemplarily summarize approaches used for simulation models, simulation experiments, and behavioral requirements, which will be discussed in greater detail in the following. It should be noted that the distinction between formalism and domain-specific language (DSL) is not that crisp. However, with DSLs, we refer to approaches and developments that consider the concrete syntax and questions referring to the realization as a usable and comfortable language right from the beginning, while formalisms concentrate more on puristic core concepts.\n\n2.1 Formalisms and Theoretical Approaches\n\nFormalisms allow an implementation-independent specification of simulation models and other entities of relevance in the simulation life cycle, such as simulation experiments or (behavioral) requirements (Table 1). Formalisms cater to broad applicability. Their classification happens typically at the level of an entire system class such as discrete stepwise, discrete event-based, continuous, or hybrid systems modeling [16]. We focus on modeling and simulating discrete-event (stochastic) systems in the following.\n\n2.1.1 Formalisms for Simulation Models\n\nSeveral formalisms were developed to describe simulation models of discrete event systems, including DEVS (Discrete EVent Systems Specification) [16], stochastic Petri nets [17], and processes [18].\n\nDEVS is a formalism for discrete event modeling that allows the modeler to define hierarchical modular models. The formalism has been inspired by systems theory [56], which emphasizes a clear boundary between a system and its environment via inputs and outputs. A system decides how to react to inputs in terms of its state changes and which events to produce, and a system may be composed of interacting sub-systems. This perception results in DEVS’s modular modeling approach of loosely coupled components that can be composed to form model hierarchies. Such a design facilitates model reuse and reduces the effort required for development and testing.\n\nIn contrast to DEVS, Petri nets interpret a system as a network of dependent entities and causally interrelated concurrent processes. Petri nets form directed, bipartite graphs, with places and transitions (forming the nodes) connected by directed edges. In contrast to DEVS, whose time model is continuous so that events can occur at arbitrary times, in the original formulation of Petri Nets, the update of a state (due to the firing of a transition) occurs without an explicit notion of time. However, extensions exist, such as stochastic Petri nets based on continuous time [17].\n\nLikewise, process algebras, such as the π𝜋\\piitalic_π-calculus, targeting the modeling and analysis of concurrent processes [57], originally lacked a notion of time and later were extended to describe dynamic systems as communicating, stochastic concurrent processes in continuous time [58]. In contrast to DEVS or Petri nets, the interaction structure of the π𝜋\\piitalic_π-calculus is not fixed but dynamic: new processes and new channels for letting processes interact are frequently generated.\n\nWhereas the syntax in which a model is written is quite different in stochastic Petri Nets and stochastic π𝜋\\piitalic_π-calculus, the processes (i.e., the semantics) in either case are Continuous Time Markov Chains (CTMCs). Thus, in contrast to DEVS, they are based on stochastic semantics. All three formalisms clearly separate model syntax, semantics, and implementation; the same model can be implemented on different platforms supporting reliability and correctness (see Section 2.2.1). An explicitly defined semantics enables verifying simulation algorithms, e.g., [59].\n\nSpecifications in the above formalisms may not be succinct or too limited for specific model classes. This has resulted in further extensions of the formalisms, e.g., the introduction of colored tokens in the case of (stochastic) Petri nets [60] and attributes in the (stochastic) π𝜋\\piitalic_π-calculus [61]. Similarly, in DEVS, we find various extensions, e.g., to capture variable model structures [62].\n\n2.1.2 Formalisms for Simulation Experiments\n\nA simulation can be interpreted as an experiment performed with a model and an experiment as ”the process of extracting data from a system by exerting it through its inputs” [63, p.4]. Therefore, in principle, the above formalisms can also be used to model this process and, thus, specify experiments. Already in the 1970s, Zeigler [43] emphasized the role of explicitly defining experiments conducted with a model by introducing the concept of experimental frame. An experimental frame is intended to specify the conditions under which a system is observed or experimented with [16]. Experimental frames consist of three model components: a generator that is responsible for generating input (traces), a transducer that analyzes the simulation outputs (e.g., conducting summary statistics), and the acceptor that decides whether the experimental conditions are met. DEVS has not been designed (nor have the other formalisms above) for specifying simulation experiments, such as sensitivity analysis or simulation-based optimization. The use of formalizing a simulation experiment as a dynamic system might be limited, but with experimental frames, crucial ingredients of simulation experiments, such as scanning the parameter space, monitoring the output, and properties that need to be checked, have been identified, and later work could build upon it [64, 65, 66].\n\nIn the quest for formalisms used for specifying simulation experiments, applying workflows for specifying and conducting simulation experiments requires further consideration [67, 34, 68]. Scientific workflows aim to accelerate scientific discovery in various ways, e.g., by providing workflow automation, scaling, abstraction, and provenance support [69] (see also Section 3). The most basic model treats a scientific workflow as a directed, acyclic graph (DAG) of computational tasks and their dependencies, i.e., a subsequent task can only be executed once all upstream tasks it depends on have been completed. More advanced computational models view workflows as process networks [70], synchronized by the data flow. Implementations of such process networks employ FIFO queues on input ports, resulting in a stream-based execution model [71, 72].\n\n2.1.3 Formalisms for Specifying Requirements\n\nThe calibration and validation of simulation models imply the execution of various (types) of simulation experiments [73]. Thereby, behavioral requirements play a central role. Often, they are defined in terms of data that are supposed to be replicated by the simulation or in terms of formally specified properties that the simulation output is expected to fulfill. The latter has received increasing attention during the last two decades [74]. A relevant and frequently used formalism to describe requirements relating to expected simulation results is the class of temporal logics [75], e.g., Linear-Time Temporal Logic (LTL) or Signal-Temporal Logic (STL) [76]. Temporal logics are modal logics with specific temporal operators that permit the specification of properties over time; for example, the always operator is a universal quantifier used to describe that a specification holds at all time instances, the eventually operator is used to describe that a specification holds at some point in the future. Describing subsequent events using the until operator is also possible. There are many extensions to specify more refined behavior. Signal Spatio-Temporal Logic (SSTL) [77] and the Spatio-Temporal Reach and Escape Logic (STREL) are extensions of STL with certain spatial operators and permit to describe complex emergent spatio-temporal behavior as the formation of patterns. STL-* [78] and Time-Frequency Logic (TFL) [79] extend STL with means to express oscillatory behavior. The Three-valued Spatio-Temporal Logic (TSTL) enriches SSTL with a three-valued semantics. In this logic, statements about spatiotemporal trajectories can be true, false, and unknown, accounting for the simulations’ intrinsic uncertainty and statistical analysis [80]. It should be noted that these behavioral requirements are only one type of requirement that simulation studies face [2, 4].\n\n2.2 Domain-specific Languages\n\nDomain-specific Languages (DSLs), in contrast to General-purpose Programming Languages (GPL), are designed for a specific application domain [81]. Similar to formalisms, they might be applied to simulation models and other artifacts of the modeling and simulation life cycle (Table 1). External and internal DSLs are distinguished. An external DSL is parsed independently of the host general-purpose language, so the model is explicitly accessible as a syntax tree. External DSLs have their own custom syntax and parser to process them. In contrast, internal DSLs are embedded within a GPL, i.e., they are a kind of API designed to exhibit a natural reading flow: the host language is used in a way that gives the feel of a specific language [81], are Turing-complete and, typically, more difficult to analyze than an external DSL.\n\n2.2.1 Domain-specific Languages for Modeling\n\nModeling languages aim for reuse, better understanding, and communicative abilities of the underlying model and, thus, better sustainability of the model and simulation results. Furthermore, modeling languages can be equipped with sophisticated static analysis techniques for specific properties, giving developers quick feedback and thus considerably increasing efficiency during development and simulation. DSLs for modeling map the formalisms discussed in Section 2.1 into specific languages, including a concrete syntax, to be executed by simulation algorithms according to the formalism’s semantics. They feature various convenient domain- and problem-specific modeling constructs to simplify modeling tasks.\n\nDSL-based simulations have the advantage that the design of DSLs promotes a clear separation of concern between the model and execution engine. This allows the modeler to focus on the model and, since the syntax and semantics of the DSL are given explicitly, to analyze the model for certain properties, e.g., by model checking techniques [75] or a structural comparison of data structures [82]. This is particularly the case for external domain-specific modeling languages.\n\nThe design of models based on DSLs stands in contrast to highly optimized general-purpose simulation programs, in which a model and its execution are encoded together. Due to a lack of separation of concern, the model is not easily accessible and reusable either by a human modeler or by another inference program, e.g., to analyze the model statically. In addition, the approach results in other problems, as the simulation engine has not been verified independently of the model. Both can threaten the credibility and validity of computational results [30].\n\nThe Unified Modeling Language (UML) [19, 21, 83, 20] has been designed as a standardized modeling language consisting of 14 different explicit modeling sublanguages for different aspects of software systems. UML’s class diagrams and object diagrams focus on structure, while Statecharts and activity diagrams focus on behavior. Semantics has been defined, e.g., in [84] to make UML precise [85], but due to the general use of UML, no generally accepted semantics exists. Typically, specific profiles of UML are used if a model is to be developed to improve the system understanding via simulation. These tools synthesize executable simulation code from UML models, typically connected with a core simulation framework. [86] discusses UML and, in a broader sense, MBSE approaches to simulation and their merits and drawbacks.\n\nThe Systems Modeling Language (SysML) is based on UML for Systems Engineering, and thus, both languages share many common modeling concepts. SysML has received widespread use, e.g., in mechanical engineering [87, 23, 22]. SysML provides additional diagrams to model distributed processes and components with a static structure. SysML also supports discrete event simulation as well as continuous systems simulation. Today, SysML is mainly used for higher-level systems definitions, including evaluating design alternatives, calculating what-if scenarios, and conducting requirements compliance analysis, including necessary quality assurances and similar engineering tasks. Many of these are based on [88] or refer to [89] simulation. Because of the increasing necessity to simulate engineered systems virtually before the first physical prototypes emerge, it can be assumed that simulation using DSLs, e.g., based on SysML [90], will become a major technique in engineering.\n\nThe syntax and semantics of a DSL reflect the primary modeling metaphor(s) and needs of the application domain or community. This becomes particularly evident if a DSL for modeling focuses on a particular application domain with a well-established modeling metaphor, such as studying gene-regulatory or biochemical systems. Consequently, the syntax of various DSLs for biochemical systems that have been developed over the last two decades builds on the reaction (or rule-) metaphor [91]. The semantics of these DSLs vary between continuous system semantics, transforming a set of reactions to a set of ODEs to be solved by numerical integration, or taking the stochasticity of the system into account by executing the model by stochastic simulation algorithms (SSA) [92] (and thus interpreting the model as a CTMC [27]), or, even considering the spatial heterogeneity, interpreting reactions as collisions between particles in space [93]. Whereas switching between spatial and non-spatial semantics requires additional information, e.g., about the diffusion constant or size of reactants, the switching between ODE-based and SSA execution (or a combination thereof) is supported to occur transparently to the simulation model by various simulation tools in the field. This includes established modeling and simulation tools such as COPASI [92] and research tools such as BioPepa [94]. BioPepa is a DSL derived from the process algebra Pepa [95] and has been equipped with different types of semantics to be executed as a set of ODEs or by SSAs, depending on the biochemical system being investigated.\n\nThe role of internal DSLs for modeling grows, particularly if the subject of modeling is a hardly constrained class of models that can easily be mapped, e.g., to an object-oriented GPL. This is the case in agent-based [96] and DEVS-based modeling for simulation [97, 98]. Internal or embedded DSLs allow us to use all the features of the host language, including inheritance and type systems, and to program, e.g., agents, as the modeler likes [99, 100].\n\nSelecting a suitable host language is a crucial first step in designing an internal DSL. The ease of realizing a modeling language as an internal DSL depends on how the programming paradigm of the host language and the offered features fit the requirements of the envisioned DSL and how widely used the host language is; one advantage of an internal DSL is not to learn a new language. For example the spread of the Java language in the late 90s, with its object-oriented programming paradigm and its convenient features such as simplicity, platform independence, type system, reflection, and support for distributed execution, has led to the development of various [101, 102], in particular agent-based [99, 100], modeling and simulation tools. Based on these tools, specialized internal DSLs can be created tailored to specific models’ sub-classes, e.g., for modeling continuous-time agent-based models with CTMC semantics (Fig. 2). In these cases where an object-oriented programming paradigm is adopted for modeling and simulation, the use of UML diagrams, in particular the class, sequence, state, and activity diagrams, is advocated, e.g., during development and for the documentation of agent-based simulation models [103], or for conceptual modeling of discrete event systems [104].\n\nNew programming languages or paradigms with compelling features (either for modeling or simulation) always spur interest in the modeling and simulation community. Active objects (AO) [106, 107] are such a programming paradigm. Its characteristic feature is that tasks are executed on objects, each with exclusive resource access to the object’s memory and processor. Consequently, no interleaving occurs at the statement level but only at the task level. Suspension and resumption of tasks are governed by guards that watch for time- or data-driven events. AO languages are designed to scale to thousands of objects [108]. They constitute a language paradigm that permits event-driven simulation at scale while abstracting away from low-level concurrency. Language features such as strong data encapsulation, modules, and type safety support modular rule design similar to Figure 2. Active objects have been used to simulate complex systems, for example, the safety mechanisms of railway operations [109], high-performance computing interconnection networks [110], or container frameworks [111]. The active object paradigm can be extended to the simulation of real-time [112, 113, 114] and hybrid [54] systems. An interesting aspect of AO languages is that their explicit synchronization enables advanced static analysis techniques, including deductive verification [115, 116], deadlock detection [117] or worst-case resource analysis [118]. Specifically, the AO language ABS [53] was designed with the capability of analysis in mind [119]. Some AO languages, for example, ABS [53], are independently executable from any host language, while others are conceived as libraries [120]. Nevertheless, AO languages can be classified as internal DSLs in the present setting because they build upon an object-oriented or object-based core language.\n\nThe appeal of a GPL as a host language for designing an internal DSL for modeling (and simulation) depends not only on features related to the ease of modeling and the execution efficiency but also on features that are related to the execution and analysis of simulation experiments, i.e., how rich the ecosystem is that a potential host language offers for conducting and analyzing a wide variety of simulation experiments. Thus, also due to Python’s widespread use, low threshold, and, in particular, libraries offered for data sciences, several Python-based modeling and simulation tools have been developed in the last decade [121], including implementations of Petri Nets [122] and DEVS-based simulation tools [123].\n\n2.2.2 DSLs for Simulation Experiments\n\nThe increasing awareness about the role of simulation experiments in developing simulation models and conducting simulation studies, on the one hand, and about the credibility crisis of simulation, on the other hand, pushed the development of internal and external DSLs for specifying simulation experiments. An example of an internal DSL is SESSL: the Simulation Experiment Specification via a Scala Layer [31]. It relies on bindings to simulation tools and experiment libraries to offer a wide range of simulation experiments [55], including parameter scans, sensitivity analysis, simulation-based optimization, bifurcation analysis, and statistical model checking. Another example is NLRX, a package embedded in R that supports the specification and execution of various experiments with NetLogo models [33].\n\nAlso, GPLs aimed at data sciences, such as Python, or computational science, such as Julia, increasingly support the specification of simulation experiments via libraries that offer experiment design, sensitivity analysis, and optimization methods [124, 125]. These libraries also show the fluent transition between internal DSLs and APIs [81]. The advantage of internal DSLs is their flexibility and the range of tools that ship with the GPL. If combined with a thorough design of the DSL, internal DSLs enable an executable and, at the same time, highly succinct and readable specification of simulation experiments and help to establish those as first-class objects of simulation studies.\n\nOne of the drawbacks of using an internal DSL for specifying simulation experiments is that automatically interpreting, adapting, and reusing these scripts requires significant effort [126]. This is the virtue of external DSLs. During parsing, the crucial parts of the simulation experiment specifications can be easily identified and accessed. SED-ML is an external DSL, an XML-based format in which simulation experiments can be encoded [32]. SED-ML is a community standard introduced to facilitate the reuse of simulation experiments across simulation tools (Sec. 4). In RASE (Reuse and Adapt framework for Simulation Experiments [49]), the simulation experiments are also encoded in a tool-independent format, namely, JSON [127]. In SED-ML and RASE, using ontologies referring to the methods used, e.g., specific simulation algorithms or optimization methods, is crucial (see Sec. 4).\n\nWhereas the above DSLs have been explicitly designed for specifying simulation experiments, workflow languages are also applicable in principle. The business process modeling community has embraced BPMN [50] as a control-flow-oriented language suitable for business workflow applications. The scientific workflow community, on the other hand, has different requirements due to the data-intensive and compute-intensive nature of computational science applications and thus has not embraced business workflow models and standards such as BPMN or BPEL [128]. Instead, specific dataflow-oriented languages and models have been developed, sometimes with specialized features to aid workflow design and comprehension (e.g., COMAD [129] for collection-oriented modeling and design, see also [130]). Subsequently, the Common Workflow Language (CWL) for computational data-analysis workflows was defined [131]. However, control structures play a role in simulation, and consequently, adaptations of BPEL have been successfully applied for specifying and conducting simulation experiments [34].\n\n2.2.3 DSLs for Requirements\n\nAnother area where DSLs are of importance is in specifying requirements. Formally specified behavioral requirements, e.g., in a logic-based language, can be tested automatically. This applies to deterministic models [132] as well as to stochastic models [39]. To specify behavioral requirements, such as desirable properties of simulation output, variants of temporal logics are used as a basis (see Section 2.1.3), as are custom-built languages for specifying properties or hypotheses of the simulation model [41, 51, 133]. The latter efforts aim at providing languages tailored to expressing requirements or hypotheses by a modeler in ”a natural manner.” Thus, the usability of the languages by users without a background in computer science propels research on these languages, even though they may forestall analysis capabilities.\n\nUsing a logic-based language (with formal syntax and model-theoretic semantics) can decrease the possibility of creating incompatible requirements and help to standardize their definition. In addition, established model-checking techniques permit one to automatically verify the satisfaction of expressions in a logic language, avoiding ad hoc creation of property test code or even manual inspection of simulations.\n\nIn a stochastic setting, probabilistic model checking is a well-established verification technique that can compute the probability that a property expressed in temporal logic may be satisfied by a given stochastic process. However, standard model checking techniques [134] are not feasible for large-scale stochastic systems. In this case, the standard procedure is to use Statistical Model Checking (SMC) [39, 40]. The underlying idea is to approximate the probability of satisfaction of a given formula statistically utilizing simulation, checking only a subset of the whole trajectory space, with usually a guarantee of asymptotic correctness. There are several approaches: qualitative SMC (based on hypothesis testing), quantitative SMC (based on confidence intervals), Bayesian SMC, and SMC for rare events. See [39, 40] for surveys on the topic. SMC is an efficient technique when the model is fully specified. Still, it is computationally too expensive to analyze a model with uncertain parameters if we want to study some parameters or input space of the model. A method to overcome this situation is smoothed model checking (smMC) [135]. Another consideration when using a logic-based approach in modeling and simulation is that formal methods can be fashioned to infer the requirements directly and automatically for trajectories. Mining logic specifications from data is a promising and challenging new line of research [136], which also circumvents the need for the modeler to specify the logic formulas.\n\nBehavioral requirements are only one type of requirement, although likely the most obvious one relating to modeling and simulation. Several UML sub-languages support abstractly capturing structural, behavioral, and interaction requirements. With its Object Constraint Language (OCL), UML provides this textual logic language OCL, which is roughly an executable subset of first-order logic with operations for container structures and associations that can be used to define properties such as requirements, invariants, pre- or postconditions. It is useful to provide mechanisms for underspecification [137] in the language in various forms to be able to capture known behavior but abstract away from unknown or irrelevant details. This allows iterative refinement during a development process [138] and non-deterministic and probabilistic simulations. In the context of software product lines, where variability management is considered a key aspect, requirements modeling is of central importance. It has motivated the development of a larger number of requirement modeling languages [139]. The potential of this perception and the developed languages still wait to be exploited for simulation studies, particularly in the context of model composition and reuse (see Section 4).\n\n2.2.4 The Role of Metamodeling in DSLs\n\nWhen developing an external DSL for simulation purposes, defining the language in its constituents is necessary [140]. There are three main approaches to developing a DSL: from scratch, through reuse and variant building of a previously given DSL [141], and via customization and adaptation of a more general modeling language, such as SysML. For textual languages, one often uses grammar to describe concrete and abstract syntax [142]. For diagrammatic languages, metamodeling [143, 144, 145, 146] is the best option to define abstract syntax as an essential core of a modeling language. Metamodeling became prominent with the Meta Object Facility (MOF) [147] and was first used as the syntactic foundation for the UML.\n\nMetamodeling can be used to define a language’s structure and various additional ingredients, such as internal data types, default values, predefined functions, simulation schedulers, etc. These forms of language definitions support a compact and human-readable specification of simulation models, simulation experiments, or requirements, while if an appropriate code generator is available, the specifications can be directly mapped to executable simulation models, simulation experiments, or algorithms that analyze the results. For instance, a metamodel specification may be used to map models in SysML to models for a specific DEVS simulator [148], or models may be mapped from BPNM to DEVS, and then from DEVS to executable Java code [149]. For that purpose, it helps if the source language, here SysML, was extended by DEVS-specific constructs to simplify the mapping [90].\n\nIn multi-formalism or multi-paradigm modeling, a complex system’s components are expressed through different formalisms, for example, Petri nets, Statecharts, and ordinary differential equations [150, 151]. The various formalisms are represented using an abstract syntax graph, i.e., a “model of formalism”. Via graph grammars, their metamodels can be transformed into a common formalism, and code can be generated for simulation execution and further analysis. With respect to simulation experiments, the model-driven architecture (incl. metamodels) [152] has been applied to the generation of experiment designs [153] and, more generally, for specifying and generating different types of simulation experiments in a back end-independent format, such as JSON [49].\n\nMore widespread use of model-driven engineering of simulation studies as a whole would increase the reusability and self-explainability of models, their requirements and assumptions, simulation experiments, and input and output data and allow further analysis techniques. For example, Zschaler and Polack propose a model-driven approach based on a family of DSLs [154]. As a central feature, they include a language for fitness-for-purpose argumentation, adapted from the Goal Structuring Notation (GSN) [155]. They argue that this combination of languages allows for building trustworthy and scientifically robust simulation models.\n\n3 Supporting context within simulation studies\n\nContext relates to any important information to conduct and interpret a simulation study. Collecting, revising, and representing suitable context information is also at the heart of conceptual modeling as defined in [156]. It should be noted that in contrast to other areas of computer science, such as software engineering, no agreed-upon definition exists for the conceptual model. However, its importance is undisputed in the modeling and simulation field [157, 158]. Definitions range from defining the conceptual model as an abstract description of the simulation model, e.g., exploiting qualitative modeling methods such as UML (see Section 2), to forming a conglomerate of all information possibly helpful in conducting and consequently, interpreting the results of a simulation study [159]. The conceptual model in [156] subsumes research questions; requirements and general project objectives regarding, e.g., visualization or simulation speed; model inputs, outputs, as well as the data used; scope, level of detail, assumptions, and simplifications; entities, equations (or rules) referring to the system and its dynamics to be modeled, and which modeling approach to use; and, finally, justifications for each design choice. In addition to the conceptual model, simulation experiments that have been performed with the model for calibration, validation, or analysis hold important information to interpret and reuse the results of a simulation study and consequently belong to its context as well. Similarly, previous versions of a simulation model and how they have been refined form valuable information about a simulation study [160]. Both emphasize a more process-oriented view of the context, i.e., how the different sources and (sub-)products are interrelated and have been used to generate the (final) results.\n\n3.1 State of the Art\n\nTo identify what is typically considered information important to interpret a simulation study, we will take a closer look at documentation standards to move from there to a process-oriented view of context and supporting methods that can be exploited for representing and processing this information, such as provenance standards and workflows.\n\n3.1.1 Reporting Guidelines for Simulation Studies.\n\nThe wish to document information about a simulation study that helps to reproduce, interpret, and reuse its results has led to various reporting guidelines. These take the particular demands of the type of simulation model and experiments being executed into account, e.g., systems dynamics [36], agent-based models [161, 162], or finite element methods [163]. In addition, they may build on sources and conventions of the application field, such as ontologies [164, 35]. Independently of the type of simulation model, specific context information about a simulation study, such as research questions, assumptions, data used, and simulation experiments, is an intrinsic part of its documentation. Documentation guidelines — examples are TRACE (TRAnsparent and Comprehensive model Evaluation) [37] and STRESS (Strengthening the Reporting of Empirical Simulation Studies) [38]—aim at documenting all of the essential steps, sources, and products of a modeling and simulation life cycle [2]. The checklist of TRACE, e.g., comprises problem formulation, model description, data evaluation, conceptual model evaluation, implementation, verification, model output verification, model analysis, and model corroboration.\n\n3.1.2 Provenance and Provenance Standards.\n\nReporting guidelines concern provenance, i.e., providing “information about entities, activities, and people involved in producing a piece of data or thing” [165]. However, reporting guidelines typically refer to the final results of simulation studies, not the processes by which those have been generated, including variations of the different artifacts or unsuccessful attempts. Provenance opens up a specific view on context, i.e., focusing on the production process of entities in which activities put sources and (intermediate) products of the simulation study into relation to each other by being used and generated by activities. Adopting provenance standards [165] allows modeling these processes qualitatively [166]. Thereby, entities, such as research question, simulation model, simulation experiment (specification), simulation data, parameter, requirement (referring to output behavior), qualitative model, and assumption, are related by activities such as creating a simulation model, refining a simulation model, re-implementing a simulation model, calibrating, analyzing, and validating a simulation model [127].\n\nVariations of simulation models are of interest not only to document how a valid simulation model is finally developed based on a successive refinement [160] but also in relating simulation models across simulation studies and thereby forming families of simulation models [167]. Thus, the provenance of simulation models might be partly cast as a variability management problem (see Section 4.2.4), and thus open challenges approached by adopting methods from this area of software engineering.\n\n3.1.3 Workflows for Knowledge-intensive Processes.\n\nWorkflows are an approach closely related to capturing provenance. Workflows and workflow systems have a long history, e.g., in databases and business process modeling. Already in the 1980s and increasingly in the 1990s and 2000s, the specific requirements of scientific data management led to the development of scientific workflow systems [168, 169]. Since workflow systems provide a controlled execution environment, they often support capturing provenance information at various levels of granularity [170]. Often, the very fine-grained capturing of provenance requires post-processing of the collected provenance information, e.g., user-specific filtering, to provide abstractions on demand, as shown for simulation studies in [171].\n\nA downside of workflow systems is that they often treat tasks as black boxes whose semantics are opaque to users. As a result, user-oriented workflow design and meaningful provenance capture are often challenging, as the required information is unavailable. Workflows have been applied to specify (see section 2) and execute individual simulation experiments since workflows enable flexible reuse of repetitive processes. However, applying traditional workflows to entire simulation studies is challenging, as model refinement phases are intertwined with model analysis, calibration, and validation activities. To support these knowledge-intensive processes, which are driven by the user’s expertise, declarative workflows may offer the required flexibility [172]. In the study [173], an artifact-based workflow approach is applied, specifying declaratively the life cycle of central artifacts, such as the conceptual model (with a focus on formally defined requirements), the simulation model, and the simulation experiment and their interdependencies, and exploiting inference mechanisms based on the defined constraints to guide and support the user in conducting the simulation study.\n\n4 Composition and Reuse\n\nSimulation models are often composed of separate sub-models to cope with the complexity of a system to be modeled. Consequently, a simulation model consists of a set of interacting components, each of which ideally has been designed for reuse. Similarly, other artifacts, such as simulation experiments, can be composed and reused. Generally, a composition-based design facilitates and, in some cases, enables reuse. Reuse in the context of modeling and simulation can include everything from “code scavenging” to the reuse of model components, up to the reuse of an entire model [188, 189, 190]. Reusing existing simulation artifacts promises to reduce development time and cost [191, 192, 189] and helps proliferate knowledge across a wider user community. Simulation model reuse requires methodologies for abstraction, retrieval, selection, integration, and execution [193, 189].\n\n4.1 State of the Art\n\nDuring the COVID-19 crisis, when simulation models needed to be developed quickly to inform policymaking, the lack of respectively the benefits of reusing model components and composing models became apparent. Popper et al. [194] describe how they reused a generic agent-based model of the Austrian population developed before the pandemic [195] and that large parts of their COVID-19 model could build on independently validated components which the final model benefitted from. As also proposed in [196], they exploited the layered architecture of their agent-based modeling and simulation framework. The challenge remains in understanding what is needed to achieve this on larger scales, including methodological approaches and initiatives that can be advanced.\n\n4.1.1 Model Composition and Reuse at Different Levels\n\nThe Covid-19 example mentioned above highlights the need for fast model development times, which can be best achieved through the composition and reuse of existing models or components appropriately within the given context. The benefits of composition (and reuse) of software are well known and have also been exploited for the design of modeling and simulation frameworks [197, 198, 97, 199] and the reuse, automatic generation, and execution of various simulation experiments [127]. The composition and reuse of simulation models have a different quality, as a simulation model encapsulates a specific relation to the system to be studied, partly reflected in its context that needs consideration if the composed model shall work as intended, i.e., reliably answering the current questions about the system of interest. This is the reason why model composition and reuse have been identified as a central challenge of modeling and simulation [188, 158].\n\nThe Level of Conceptual Interoperability Model (LCIM) [200] describes seven levels of model interoperability to characterize which level of interoperability has been ensured in the current reuse (or composition) of models. Interoperability can refer to letting simulators interact (thus, the simulation algorithm and model are treated as a non-separable unit) or composing simulation models. In the latter case, the composed model is executed by a simulation engine. Level 1 refers to technical interoperability. Most formalisms or languages for modeling support some form of composition [16, 58, 18]. The first interesting level is the level of syntactic interoperability [192]. At this level, a more elaborate definition of interfaces and the integration of type systems becomes crucial. Interfaces kept separately from concrete model implementations guarantee that the coupling of components is syntactically correct and supports successive refinement and compatibility analysis [201, 198].\n\nIf the ontologies of the application domain are accessed to specify the components’ interfaces, we move to the next, the semantic level. According to [202], exchanging content is what the semantic level is about. Further up, the pragmatic, dynamic, and conceptual levels are distinguished, requiring increasing levels of information that the components have about each other’s context to correctly interpret the meaning (Section 3). However, most efforts that support composition do so at lower levels. This usually implies ensuring that input and output ports are correctly connected and that data flows in the correct format without an in-depth understanding of the assumptions and constraints of each of the connected components. In addition, as the composition may involve different modeling paradigms (multi-formalism modeling) and even integrate discrete or continuous components, suitable means for model transformations [203] or synchronization schemes for simulation are required [204]. Semantic and higher-level composability is significantly more difficult to achieve, demanding knowledge and alignment of model assumptions, constraints, and a common understanding of the simulation context (see section LABEL:context).\n\n4.1.2 Reporting Guidelines and Formats for Reuse and Composition\n\nBesides access to the source code of a model, which often requires the use of a specific simulation framework, the successful reuse of a model or some of its components within a given context also requires a comprehensive and explicit description of the model’s structure, underlying assumptions, configuration, and other information that might be relevant for potential future users. Likewise, reproducing experiments necessitates information on the data and model that have been used, how the experiments were conducted, and how the outputs were conducted. Common guidelines and formats facilitate the sharing of this information.\n\nExamples of guidelines for describing models and simulation experiments include MIRIAM (Minimal Information Requested In the Annotation of biochemical Models) [205] and MIASE (Minimum Information About a Simulation Experiment) [35]. What characterizes many of these guidelines (also those that aim to capture entire simulation studies, see Section 3.1.1) is that they were proposed by a community, meaning that a larger group of researchers developed them and signaled their commitment. There is no direct link between verbal description and the model’s components and code. Still, when applied consistently, documentation guidelines enable a high level of interoperability (and reuse) as they make assumptions, constraints, and simplifications of the model explicit. However, this heavily relies on the author’s rigor when describing the model.\n\nThe above reporting guidelines result in documents in the form of structured text with little to no formalization. The following formats are aimed at automatic reuse by different simulation tools. CellML is an XML-based model description language that originated in biology but can be used for different types of mathematical models [206]. Its goal is to facilitate the storing and exchange of models and the reuse of model components independent of the software that has been used for model building. The language can be used for describing both the structure of the model, i.e., its components and how they are connected, as well as metadata for the annotation of the model, i.e., purpose, authorship, and references, but also for the reproduction of simulations and the visualization of outputs. A closely related language with a similar purpose is SMBL, the Systems Biology Markup Language, [207]. It is possible to translate SMBL to CellML, and vice versa [208]. Both formats enforce or at least encourage using annotations and ontologies, e.g., to uniquely identify variables and parameters, which supports semantically meaningful reuse and composition of simulation models [164]. Thereby, they also address computational challenges of model composition [209]. Standardization efforts, such as SBML and CellML, have been facilitated by the momentum in systems biology in the early 2000s, existing ontologies in the application domain, and, last but not least, the structural similarity of simulation models being developed, i.e., species reaction systems often expressed as ODEs. To support other types of models, the standard core of SBML is extended by specialized packages, e.g., to support spatial models [210].\n\nLikewise, facilitating the exchangeability and reproducibility of simulation experiments requires the specification and description of the experiments using a common interchange format or language. This might include the data and the model that has been used, potential modifications that need to be applied to the model before experimentation, and how the output data should be analyzed. The Simulation Experiment Description Markup Language (SED-ML, see also Section 2.2.2) is an example of a format that can be used to specify simulation setups [32, 211]. In practice, it is typically not used by modelers but to export and import simulation experiments between different tools or frameworks. SED-ML offers a semantic annotation of elements using ontologies of the application domain and simulation methods.\n\nEven though markup and exchange formats and documentation guidelines pursue different approaches, they can be connected. SED-ML, for instance, enables encoding information required by the MIASE guidelines. Yet, little work exists on the automatic conversion between exchange formats and documentation guidelines, nor assessing the consistency between both.\n\n4.1.3 Model Repositories\n\nJanssen et al. [193] discuss different types of model sharing that exist for making model code available, i.e., archives (e.g., open science model libraries), web-based version control repositories (e.g., GitHub and SourceForge), journals, personal or organizational storages (e.g., Dropbox or institutional websites), and distinct framework repositories. General code repositories such as GitHub or SourceForge host a large number of models. A simple search of the term ‘simulation model’ on GitHub returned nearly 2,000 hits. Several challenges exist with general-purpose code repositories. The discovery and selection of existing models are nearly impossible without prior knowledge of a specific model. The discovery relies heavily on appropriate keywords being used within the model metadata, which benefits from ontologies of the application domain and ontologies specific to the methods being used [212]. For the selection, additional information becomes crucial.\n\nTo address this, purpose-built model repositories exist. The NetLogo Model Library contains hundreds of NetLogo models that contain code, instructions on how to run and modify them. Inclusion within the library requires the submission to a central database where the model is checked and released for wider community use. ComSES.Net, Network for Computational Modeling in Social and Ecological Sciences, is an open community of researchers, educators, and professionals focused on improving the development and reuse of agent-based and computational models to study social and ecological systems. The community develops and maintains the CoMSES Model Library. Upon submission, authors can request their model to be peer-reviewed for structural completeness and to fulfill the ComSES community standards. The library contains over 7,500 publications of models, their metadata, and citations. Around 2,500 models are currently stored in the BioModels repository [213], most of which are specified in SBML with metadata that include references to established ontologies. More than 1000 of these models have been curated. This implies in Biomodels that a model has been independently tested and checked, whether the simulation results stated in the publication could be reproduced based on simulation experiments. Publishing artifacts, such as simulation models, in repositories is increasingly accompanied by some assessment of quality, which requires a separate review. This development aligns with an ACM initiative that introduces separate reviewing processes for the artifacts associated with ACM publications [214]. Five badges can be assigned to the publication after the artifact is reviewed, including Results Reproduced, Artifacts Available, and Artifacts Reusable.\n\n4.2 Future Research Directions\n\nModel composition and reuse have the potential to enable efficient model development. However, thorough documentation, proper revalidation, sharing platforms, and incentive structures are only some community efforts required to enable reuse. In this chapter, we discuss different existing approaches and methods that facilitate specific aspects of composition and reuse. To systematically address existing shortcomings in model reuse, further advances in community engagement and documentation formats are required, as well as new mechanisms for composing models.\n\n4.2.1 Community and User Engagement.\n\nTo be able to reuse simulation models or their components and simulation experiments, they must be made available in the first place. A study on the availability of agent-based models indicates an upward trend regarding the share of publications that make their models available. However, model availability is still generally low (under 20% in 2018 [193]). Nevertheless, the availability of simulation artifacts alone does not promote reuse. There is a need for empirical studies focused on practitioners and researchers to understand the process of model reuse better and identify requirements. Numerous empirical studies exist on reusing software components [215, 216]. Similar empirical studies in the realm of simulation models would allow a more in-depth understanding of the barriers to model reuse. It could guide the design and implementation of solutions. These could include specific DSLs to capture research questions or model assumptions or to motivate the development of application-specific ontologies.\n\nNew methodological developments to further composition or reuse without well-functioning tools broadly supported within the community will not advance model reuse. There is a need for community support in contributing to, testing, and trialing various supporting tools. Drawing again from the software engineering community, where tool demonstrations and competitions are commonplace, there is a need for more tool-focused papers within major conferences and journals.\n\n4.2.2 Standards and Formalizations.\n\nAs discussed above, there is a need for standardized languages or formats to specify models and means to specify their context unambiguously to understand various aspects of a model and how to reuse it. However, for any new language to be successful in its facilitation of reuse, requires broad community buy-in. This can be achieved through joint development of new languages across the community, ensuring that everyone contributes to their continuous development.\n\nFor example, in the area of agent-based modeling and simulation, one might build on the existing momentum of ODD, perhaps through extending ODD to various specialized domains, such as building templates for specifying models of agents with decision capabilities based on ODD+D [217]. As stated in [162], there is a need to complement the ODD documentation guidelines with more formal (and succinct) approaches. Transformation methods must be designed, allowing for easy translation from existing specifications to new, more formal languages or protocols.\n\nThese considerations apply to the documentation or specification of the simulation model and aspects of a simulation model’s context (Section 3). For requirements, different formal approaches mostly based on temporal logic exist (see Section 2); what is missing are means to facilitate their adoption by a community, be this in terms of languages with a suitable expressiveness, or even a repository of domain-specific requirements, e.g., in the form of stylized facts [218], that can be reused to check a simulation model’s validity automatically within a particular domain.\n\n4.2.3 New Mechanisms for Composing Simulation Models\n\nExisting composition mechanisms, such as import/export, inheritance, refinement, delegation, etc., appear insufficient to compose simulation models from given constituents in a semantically valid manner. One reason is that the composition of simulation models is an intrinsically parallel way of composition: two sub-models do not execute independently of each other, but they interfere. For example, one model might rely on the constant concentration of a certain species (and accordingly, the parameters have been calibrated), and this model shall now be extended by being composed with another sub-model that produces this species. How can these relationships be captured? One necessary ingredient for the composition of subsystems is a suitable notion of scope that lets one define the boundary of a subsystem, including the quantities it may depend on and the ones that it might possibly change.\n\nExchanging information via output and input events might prove cumbersome or insufficient to model certain situations. For example, to capture upward and downward causation of multi-level systems, the upper levels might directly access the states of models at the lower levels and vice versa, to change their states accordingly [219]. These interactions form a kind of value coupling, i.e., different variables in different sub-components have the same value during simulation [220], so interfaces must be enriched by other interaction means between simulation models. In many areas, the composition does not happen as a black box composition (via traditional interfaces) but can occur as a fusion [221] or merging [222] of simulation models, in which also the internals of simulation models are accessed (Fig. 4). Invariants describe what does not change during the execution of a sub-model or a composition of sub-models. They are a way to define constraints that contribute to a valid composition and fusion of models. If we interpret invariants as properties expressed on simulation results, e.g., in terms of temporal logic, behavioral requirements could be rechecked whether they hold for the composed model as they did for each component, e.g., [126].\n\n4.2.4 Representation and Evaluation of Variability\n\nHussain et al. [223] distinguish model reuse, whether individual components are reused, if the model is developed as a composition of existing ones, or if an entire simulation model is reused, and how many adaptations (variations) are introduced. These variations of simulation models can also be observed if various models have been generated over time, reflecting increasing knowledge about a system of interest, its mechanisms and behavior, and different research questions [167]. In software engineering, systematic management of variability (and commonality) is well-established as a design approach known as Software Product Lines (SPL) [185, 224]. Differences between related model variants are represented abstractly as features, which may be parameterized. A set of features and parameter values then characterizes one concrete model, called a product in SPL terminology. Variability modeling techniques are fairly agnostic with respect to the underlying implementation paradigm [225] and, thus, might be used in connection with various simulation models. Variability modeling also seems a promising approach to represent and reason about model variants that are stored in model repositories: to keep track of different versions that evolve over time, to interpret similarities and differences within models [226], to record incompatibility between model features or to state specific features as being required, see also Section 3.2.3. In any case, domain knowledge is needed to define and interpret commonalities and differences.\n\n5 Automation\n\nAutomation of the modeling and simulation life cycle promises to increase complex simulation studies’ efficiency, quality, and reproducibility. To this end, automation also bears the potential to “close the loop” such that model adaptations and new experiments can be iteratively derived from previous results and studies [227]. The problem of automating feedback loops is also a central challenge in digital twins. A digital twin needs to mirror the state of its physical counterpart reliably. Hence, based on data obtained by monitoring the physical system, the model must be updated, and new simulation experiments must be executed automatically.\n\nA variety of approaches for automation have been explored in the field of modeling and simulation or may be transferred from related research fields. However, automation is challenging because most knowledge rests as implicit assumptions in the modeler’s or domain expert’s mind, and the cognitive processes behind modeling and simulation are poorly understood.\n\n5.1 State of the Art\n\nWhen discussing automation of modeling and simulation studies, the various tasks of a simulation study have to be considered. These include conceptual modeling, building the simulation model, specifying and executing simulation experiments, data analysis, and visualization. Alongside these tasks, automation should guarantee the simulation study’s reproducibility, reusability, and credibility.\n\n5.1.1 Conceptual Modeling and Model Building\n\nMachine learning approaches can automatically construct conceptual models from verbal descriptions. Named entity recognition, association rule learning, link prediction, ontology mapping, and process discovery are just some of the many techniques for rule, text, and graph mining discussed in the context of conceptual modeling [228]. So far, for instance, a semi-automatic approach has been developed for generating conceptual model diagrams from verbal narratives about agent-based models based on pattern-based rules and grammar about the concepts and relationships [229]. The automatic extraction may be supported by knowledge graphs that connect knowledge of an entire domain from diverse sources and allow for semantic querying [230]. The CovidGraph, for instance, interrelates publications, patents, and clinical trials with biomedical ontologies [231].\n\nFor the automatic construction of simulation models, formal transformations between domain-independent, concept-level models (also known as metamodels) and executable models in domain-specific modeling languages were developed [148]. In between the high-level conceptual model and the implementation-level simulation model, various additional layers of abstraction may need to be generated to cater to the needs of the different stakeholders. Here, techniques from process mining may come into play to produce models with differing complexity [232]. In addition to accommodating different views of the simulated problem, there has also been an interest in learning model abstractions to speed up simulations [233].\n\nTo take a “shortcut” from verbal narratives to executable code of simulation models [234], there have been first attempts to use Generative Pre-trained Transformer language models for model building (see the GPT family of models [235]). These types of natural language models have the capability to generate and organize semantic concepts [236].\n\nAnother major class of approaches aims to generate simulation models that can accurately capture some (measured) time series data [237, 238]. This includes discovering the underlying nonlinear differential equations and their parametrizations using symbolic regression, which is based on genetic programming principles [239]. However, symbolic regression is computationally expensive and prone to overfitting. To overcome these challenges, sparse regression has been used for identifying nonlinear dynamics (SINDy) [240]. This approach is based on the assumption that only a few terms define the dynamics of a system. Sparse identification has also been tailored for biochemical reaction networks by introducing a library of candidate components that may be involved in a reaction system [241]. In contrast to SINDy, this approach considers system components not only individually but also as couplings between them. In addition, the formulated regression problem can be solved by a non-negative least squares algorithm. Methods for sparse Bayesian inference can additionally provide uncertainty estimates [242]. To effectively recommend models that achieve the desired behavior, the automatic retrieval and incorporation of context information from literature was investigated [243].\n\n5.1.2 Simulation Experiments and Model Execution\n\nWith respect to simulation experiments, various approaches have focused on their unambiguous design, generation, and reuse. Consequently, languages for specifying efficient experiment designs based on hypotheses [244], logics for checking temporal and spatial properties [47, 77, 48], and metamodels for making the ingredients of different types of simulation experiments explicit [49] were developed and applied in generating simulation experiments automatically [127] (see Section 2). Also, frameworks exist that provide general guidance for the experimentation process, e.g., the SAFE simulation automation framework for experiments guides its users through the initialization of model parameters, the configuration of parallel simulation execution, the processing of output data, and the visualization of the results [245].\n\nTo lend further support, assistance for simulation experiments has been tailored to the specific type of simulation experiment at hand. In particular, which methods and parametrization to use, e.g., variance-based analysis versus partial rank correlation coefficients [246] in sensitivity analysis or batch mean versus moving window in steady-state estimation [247], has been addressed. With such specialized guidance for setting up these analyses and means for executing the experiments automatically, problems regarding the validity and reproducibility of a model can be identified or even avoided. In the study [13], for example, the importance of sensitivity and uncertainty analysis was demonstrated for applying and interpreting a COVID-19 model. The model of the pandemic was shown to be highly sensitive with respect to several of the intervention, disease, and geographic parameters: Uncertainty in these input parameters amplified the uncertainty in the model output by 300 %. Providing such information automatically, in addition to the simulation result itself, is crucial for the decision-makers to interpret adequately, e.g., the number of available ICU beds predicted by the model.\n\nFurthermore, generating and executing simulation experiments automatically may support model-building decisions and drive the progress of an entire simulation study. In the approach of sensitivity-driven simulation development, e.g., the model is refined or reduced depending on the outcome of sensitivity analysis [248]. However, clear guidelines for when to conduct which analysis may not exist. In addition, the question of which specific method to select cannot easily be answered. For example, in the context of optimization, choosing the right method proved difficult as the response surface of the objective function would have to be known a priori [249]. Gradient-based optimization methods, e.g., assume smoothness of the response surface. However, this is not the case for many simulation optimization problems. To deal with non-smooth response surfaces, novel approaches for automatic differentiation over discontinuous functions with smooth interpretation can be employed [250].\n\nThere is a growing pool of machine learning approaches with the goal of choosing which method to apply to solve a problem in an automated way. Work in this area includes an automated selection of methods and hyperparameters in integer programming solvers, e.g., via optimized algorithm portfolios [251], synthetic problem solvers for composing algorithms for various subtasks of simulation experiments [247], premise and strategy selection in automated theorem proving [252], and automated selection of the architectures and hyperparameters in deep neural networks [253].\n\nSimilarly, approaches for adaptively selecting the most efficient simulation algorithm [254], adaptive methods for an evenly distributed Pareto set in multiobjective optimization [255], or adaptive parallelization of simulations in heterogeneous hardware environments [256] have been investigated. When dealing with limited resources, approaches for test prioritization could filter out the experiments or simulation runs that are most critical, e.g., in testing the validity of a simulation model [257]. In addition, a metamodel-based approach has been applied to automate the implementation and deployment of distributed simulations in High-Level Architecture (HLA) compliant cloud services [258].\n\n5.1.3 Data Analysis and Visualization\n\nAnother research target is the automatic analysis, interpretation, and visualization of simulation data and data used as input for calibration or validation. Various supervised and unsupervised machine learning methods can be combined in a knowledge discovery process for simulation data [259]. These, in addition to advances in clustering and classification of time series data [260], as well as detection of oscillations [261] or outliers [262], will be crucial for providing automatic support in the data analysis, visualization, and interpretation phase, and to go beyond manual validation. With the increasing push for sustainability and efficiency of computing, specifically in simulation studies (“green simulations”), parts of simulation outputs may be stored and reused for answering new questions [263].\n\n5.1.4 Reproducibility, Reusability, and Credibility\n\nEnsuring the reproducibility, reusability, and credibility of models and associated artifacts are ongoing challenges (see Sections 3 and 4). Accordingly, approaches for recording provenance traces of entire simulation studies in a non-intrusive manner and presenting aggregated views on provenance are of high relevance [264, 171]. To capture provenance and, therefore, to document a simulation study automatically, approaches such as system wrappers, application reporting, operating system observation, or log file parsing [265] have been explored. They allow observing modelers in their usual working space, e.g., specific IDEs (integrated development environments), consoles, or libraries.\n\nTo correctly capture the provenance traces and to understand their meaning, methods for detecting, interpreting, and visualizing the differences between model versions are required [266]. The variability in model versions may, e.g., refer to different parameter settings, level of detail, the goals and hypotheses addressed, and the choice of modeling formalism. Approaches to managing the variability of models (particularly over time) are discussed in Section 4.2.4.\n\nProvenance traces of previous simulation studies may be used to construct workflow models that can be applied in future simulation studies to execute suitable next actions automatically. Here, process mining techniques may be employed to generate data science workflows from code [267] automatically and to provide case-based support [leake2008].\n\n5.2 Future Research Directions\n\nOpen challenges in the automation of simulation studies include the necessity for semantically annotated knowledge as a prerequisite for automation, the effective application of diverse machine learning methods, the imposition of constraints on automation (particularly human involvement), and the demonstration of the benefits of newly developed automation methods for modelers and other stakeholders in simulation studies.\n\n5.2.1 Semantically Annotated Knowledge\n\nAutomating simulation studies requires explicit and formally specified or annotated knowledge about all modeling and simulation life cycle phases. This includes knowledge about the goals and intentions behind the activities of the modeler (e.g., calibration, validation, or prediction), specific hypotheses regarding the model behavior under certain circumstances, and knowledge of established methodologies of a domain.\n\nWhen applying machine learning approaches for automation, knowledge is required, e.g., for annotating the training, test, and validation data, for selecting relevant features, and for interpreting the results. Also, when applying rule-based inference systems, explicit and semantically unambiguous knowledge is the foundation for automatic reasoning about simulation studies.\n\nThe approaches presented in Section 2, including model-based approaches, ontologies, DSLs, and temporal logics, will be essential for unambiguously and machine-accessibly representing the various knowledge. In addition, provenance graphs and other documentation standards discussed in Section 3 will be crucial, as they contain valuable information about how the different artifacts of a simulation study are related. Moreover, open-source software and open repositories need to be facilitated to enable automatic retrieval and exploitation of the explicitly specified knowledge.\n\nThe overall body of knowledge about modeling and simulation studies will be growing by leveraging the various formal and open methods. This knowledge and the methods for automation may be shared and exploited within but also across application domains and simulation approaches (e.g., finite element analysis), as simulation studies often share important characteristics [49].\n\nWhen reusing information within and across domains and approaches, beyond formally specifying the information needed, there is the additional challenge of dealing with the different terminologies used. Without controlled, structured vocabularies (ontologies) that provide clear semantics to the expressions, more general support will remain elusive.\n\nEfforts towards collecting and consolidating that knowledge need to be truly community-driven. This includes regular opportunities for consultation and exchange via simulation working groups and the collaborative development of tools via hackathons. To facilitate these processes, they may be integrated with existing (domain-specific) organizations, such as COmputational Modeling in BIology NEtwork (COMBINE) or the Open Modeling Foundation (OMF).\n\n5.2.2 Intelligent Modeling and Simulation Life Cycle\n\nSo far, modeling and simulation studies automation has merely targeted individual steps of the modeling and simulation (M&S) life cycle, e.g., conceptual modeling, model building, experiment specification, experiment execution, output analysis, etc. Future efforts thus should not only be directed toward improving the automation of these steps but also toward combining and integrating existing and newly developed approaches to support modeling and simulation studies as a whole. Fig. 5 shows the tasks of the M&S life cycle and lists methodologies used for enhancing the degree of automation in these tasks. In addition, the various sources that provide context information for automation are depicted. The developed approaches, frameworks, and tools combined should be able to make (semi-)automatic, intelligent decisions during the modeling and simulation life cycle.\n\nQuestions that are still not answered automatically include, for example, when and how to refine a model further or reuse and compose existing model components (see Section 4) or how to generate a model from scratch when no data is available for model fitting. Not only simulation models but also simulation experiments, requirements, and even assumptions may be automatically generated depending on the context provided. In addition, automation may assist in deciding which type of simulation experiment to conduct using what method and parameterization or when to collect more data and what kind. These decisions might be based on state-of-the-art machine learning methods. A considerable challenge in this regard is acquiring data for training, testing, and validating the learning models. To be suitable input data for machine learning, existing simulation studies need to be semantically annotated with context. Ontologies may, e.g., provide context information about the type and role of simulation experiments, as well as the methods applied.\n\nHowever, this manual annotation and data collection entails a substantial workload overhead for the modelers, and the potential payoff in terms of development time may not sufficiently justify these efforts. One solution to circumvent the manual effort could be the generation of synthetic data (i.e., synthetic simulation studies) to train machine learning models. Either way, the challenge of dealing with incorrect, inconsistent, or incomplete knowledge when building learning models needs to be addressed.\n\n5.2.3 Large Language Models for Simulation\n\nFollowing the release of ChatGPT on 30 November 2022, new developments in natural language generation have changed how we think and work, including in science and engineering. ChatGPT and other large language models (LLMs) are based on transformer architectures and pre-trained on massive data sets, which may further be fine-tuned to specific applications [268].\n\nThe use of LLMs to support simulation development and to automate the entire M&S life cycle is becoming a topic of growing interest. For instance, [269] identified four main M&S tasks where text generation via LLMs may be effectively applied. The first one is explaining the simulation models’ narrative to be understandable for all stakeholders, which will also facilitate participatory modeling (see section 6). The second one is focused on summarizing simulation outputs and conveying the main differences between what-if scenarios. The third task is generating textual reports to aid the interpretation of simulation visualizations. Lastly, LLMs may assist in finding and explaining simulation errors and offering guidance to resolve them, thereby assisting in verifying and validating simulation models.\n\nRecently, LLMs have also been trained to generate code in various programming languages [270]. For simulation models, the automatic generation of fully functional and executable simulation models has been discussed [234]. However, generating simulation experiments, requirements, or assumptions in domain-specific languages also needs to be investigated. In software engineering, there has been a trend towards no-code and low-code development for several years, intending to minimize the amount of manual coding required [271]. Certainly, LLM-powered tools will soon also drastically change how we develop and analyze simulation models and implement modeling and simulation tools.\n\n5.2.4 Human in the Loop\n\nMost simulations are designed to be interpreted by humans. Keeping humans in the loop during the modeling and simulation life cycle via approaches like visual modeling or interactive, exploratory analysis of results is, therefore, one of the primary goals of providing automatic support for simulation studies. Different levels of information need to be conveyed to the decision makers, domain experts, and modelers (see also Section 6). Even if substantial parts of a simulation study are conducted automatically, humans should retain control over the intelligent M&S process. This includes establishing trust in the automation, e.g., based on methods for explainable AI. Furthermore, parts or sequences of the M&S life cycle may require user-specific or project-specific workflows instead of a one-fits-all approach. Thus, approaches for tailoring the M&S workflow, e.g., via preference learning, will be more than welcome as long as some ground rules, such as “do not use the same data for calibration and validation of the model”, are obeyed. These approaches should ideally learn “as you go”, with as few customizations by the user as possible. However, a compromise will have to be made that trades off the user overhead of manually entering additional information against increased support when conducting a simulation study.\n\n5.2.5 Evaluation\n\nIn modeling and simulation research, new software is typically evaluated with respect to its runtime performance and the number of steps required using benchmark models [272]. Moreover, illustrative case studies from diverse application areas can be used for demonstrating new methods and tools, such as in [273]. Nevertheless, there is still a need for suitable benchmarks and measures for assessing the gain in productivity and the reduction of error in simulation studies by automatic, intelligent support. In particular, there is a need for both quantitative and qualitative metrics for how efficient (with respect to time and other resources), effective (in terms of results and information gain), and how accurate (without technical or methodological errors) the automation is. Realistic case studies with representative user groups are required to evaluate the superiority of automatic support compared to the fully manual or randomized case. Depending on what task is evaluated (e.g., automatic model generation, algorithm selection, or output interpretation), different measures and different study designs may be required.\n\nFor users, including modelers and stakeholders, well-designed studies are crucial in providing the necessary argument for the widespread adoption of the developed automation methods and tools in their daily practice. For researchers, on the other hand, a thorough—and if possible quantitative—evaluation will improve understanding of the developed method and their impact and guide future research directions. In the field of visual computing, quantitative methods, and user studies became a separate research field, which is reflected in the numerous projects and research centers, as well as recommendations on the topic (e.g., [274, 275]). There, publishers increasingly demanding explicit reporting of user studies (e.g., the journal “Visual Computing for Industry, Biomedicine, and Art” expects documentation via the STROBE guidelines for observational studies [276]). Similar initiatives will have to be pursued in modeling and simulation.\n\n6 Communication and Stakeholder Understanding\n\nEffective communication with stakeholders (decision-makers and others with a stake in the outcome of the simulation study) is a prerequisite for successfully influencing decisions with simulation [277]. Effective communication includes working with stakeholders to understand their beliefs and preconceptions, the underlying goals of the simulation study, and the broader organizational context in which decisions based on simulation will be made [277, 278]. Effective communication also includes explaining simulation study conclusions in clear language tailored toward stakeholders’ viewpoints. Although the simulation analyst may be confident in the simulation’s accuracy because of deep knowledge of the simulation implementation, stakeholders often lack this basis for confidence. The simulation analyst may try to convey this knowledge by explaining how the simulation works, but this faces obstacles. First, many decision makers (e.g., public officials and leaders in industry) lack training in computer-based simulation and other quantitative prediction methods, which makes it difficult for them to judge the accuracy of a simulation based on detailed technical explanations. Second, decision-makers tend to be busy, which makes even the technically trained among them unwilling to invest the time needed to parse detailed explanations. Third, simulation models are unable to represent every detail of the real world and often have input parameters that are hard to estimate accurately. Thus, even a complete understanding of how a simulator works may be insufficient to give high confidence in the accuracy of its predictions.\n\nOther forms of communication and analysis, better tailored to the stakeholders and their situation, may be required to build a basis for confidence in a simulation’s accuracy and to inform a decision properly. For example, imagine an agent-based simulation model that predicts the effect of public health interventions (e.g., COVID-19 vaccinations) on health outcomes. The simulation might predict that allocating fewer vaccines to older individuals and more toward younger ones decreases mortality. Stakeholders may find this counter-intuitive because they think (correctly) that older individuals have a higher mortality risk when infected with SARS-CoV-2. A simple, clear explanation would be, “Young people are more social, and vaccinating them prevents the fast spread of the virus, which indirectly reduces infections in older people.”.\n\n6.1 State of the Art\n\nSuccessful simulation analysts must communicate directly and extensively with stakeholders to understand their viewpoints and explain simulation outputs. While partial automated support exists for these tasks (described below, including visualization, utility, and prior elicitation methods for learning stakeholder goals and beliefs), many communication tasks are not sufficiently supported by current computer science methods.\n\n6.1.1 Best Practices from Simulation Practitioners\n\nExperienced simulation practitioners have written about their experience using simulations to inform stakeholders. The studies [277, 279] argue for the importance of understanding stakeholders: How they will potentially use simulation results, how they define success, their background, and the broader power structure and organizational context in which they operate. Both also point to the danger of being asked to perform a simulation study that “justifies” the correctness of a decision in hindsight. This phenomenon also arises using evidence in public health policy [280]. The paper [277] argues for the importance of delivering results in a timely manner that aligns with deadlines when decisions must be made. Sturrock [279] points out that stakeholders, who often have substantial domain expertise, can be valuable partners in validating a simulation model. He also advises simulation analysts presenting results to avoid excessive detail, to avoid overemphasizing the accuracy of their output data, and to contextualize information from the simulation by explaining how it relates to stakeholders’ needs. In the context of healthcare, [280] argues for the value of an iterative approach where simulation analysts closely collaborate with stakeholders to build a simulation model, reminiscent of co-design approaches to public policy [281]. This helps create a shared understanding among stakeholders of how the simulation works and the reasoning behind its design. This is highly related to the concept of Participatory Modelling (see Subsection 6.1.5 below).\n\n6.1.2 Communicating about Scientific Evidence with Policymakers\n\nA line of research reviewed in [282, 283, 284] studies how scientific evidence and models influence policy decisions, with much of the literature focusing on public health. They find a significant gap between policy decisions and scientific evidence that could support these decisions. Indeed, [283] goes as far as to argue that the primary value of evidence tools to public health policymakers is actually not that it often leads to better decisions but that its use can signal to others that the policy maker is making “good” decisions.\n\nFactors that support the use of evidence for influencing policy decisions include decision makers’ perceptions of evidence quality [282], a culture of using evidence to make decisions [282], timeliness and relevance of the evidence [284], the need to account for the practical context in which policy decisions are made [282], and the strength of the relationship and level of collaboration between policymakers and researchers [284]. While much of this literature focuses on the influence of evidence reviews, some work [285] specifically considers the influence of quantitative models. This work points again to the importance of decision-makers perceptions of evidence quality, which can be supported by peer review, transparency, and the value of user support for models.\n\nScience Communication [286, 287] is a related endeavor that includes methods for creating scientific awareness, understanding, literacy, and culture among stakeholders, decision-makers, and the general public. For more than 50 years, Science Communication has been seen as a research field with tools and techniques mostly drawn from social and behavioral sciences [288]. As many challenges are shared, relevant principles and techniques can be borrowed to communicate simulation studies to decision-makers and stakeholders. A good example is research towards tools that summarize scientific articles in understandable language (e.g., [289] for biomedical research articles).\n\n6.1.3 Learning Stakeholder Preferences and Beliefs\n\nAs argued above, the successful use of simulation to support decisions requires the simulation analyst to know how the simulation results will be used to support decision making [277, 278]. The most common approach for learning about decision criteria is to talk with a decision-maker. However, with increasing size and complexity of models, detailed walk-troughs become infeasible with respect to timeliness with which decisions need to be taken, as for example argued by [290] for defense applications. Similar experiences are stated by [291] even for cost-effective and innovative ways to test new ideas or prototypes. The report discusses decision-makers need rather information from quick-term analyses while experts are not equipped to provide those.\n\nDifferent approaches have been suggested to systematically determine the relevant decision criteria to be taken into account when developing simulators to support decision-makers: Multi-attribute utility theory models how humans make decisions when multiple outcomes matter. Utility elicitation and preference learning methods [292] estimate such utility functions from stakeholder feedback. Closely related to utility elicitation methods, prior elicitation methods [293] estimate these probability distributions from decision-maker feedback.\n\nThere is a large and closely related literature on multi-criteria decision-making and multi-attribute utility theory [294]. In this literature, the focus is on supporting one or a group of decision-makers in coming to a decision. In the context of a simulation study, this includes lowering the cognitive effort required to identify the most preferred option among those that have been simulated. Much of this literature can be understood as helping decision-makers explore a Pareto frontier – the set of non-dominated outcome vectors – with high-dimensional outcome vectors.\n\nIn conjunction with multi-criteria decision-making, there is literature on multi-objective optimization combining simulation and multi-objective Bayesian optimization. Here, there are two lines of literature: one estimates the Pareto frontier without modeling the human decision-maker [295, 296], and another models the decision-makers utility function to help focus simulation effort on the parts of the Pareto frontier that are most important to a final decision [297, 298]. There is also a line of literature on preferential Bayesian optimization [299] which works directly with pairwise comparison feedback from a decision maker over potential decisions. Algorithms developed seek to minimize the number of pairwise comparisons needed to help the decision-maker find a good decision.\n\nExisting automated methods for understanding stakeholder preferences and beliefs have several shortcomings that could be improved via future work in the context of simulation studies. First, they assume access to stakeholders is sufficient to support collecting time-intensive pairwise comparisons. This may be unrealistic for some stakeholders, especially prominent politicians or business leaders. Second, they assume that decision-makers believe the simulation results; this may not be the case also, due to a lack of trust [300]. Third, they assume that the criteria used to make a decision are fully captured by the presented predicted outputs. Stakeholder consideration over non-included criteria may be missed.\n\n6.1.4 Visualization\n\nModel visualization provides access to the actual model with visual representations and presentations of the (static) model itself, i.e., its structure and logic. These are often based on conceptual diagrams [301], such as causal loop diagrams and stock/flow diagrams in System Dynamics [302], and UML diagrams in Agent-Based Simulation [303]. When using formalisms and DSLs for modeling (see Section 2.2), a higher abstraction level can be automatically derived and, possibly, graphically represented, e.g., focusing on the network structure as in Petri nets or reaction-based models, or on the model hierarchy as in DEVS. Hierarchical, modular, composite modeling approaches (see Section 4) inherently provide abstraction levels that allow one to zoom in and out on demand. For visually exploring these models, various methods of graph [304] or tree visualizations [305] are available.\n\nSimulation-run visualization is essential in revealing insights into the model’s dynamics. There are basically two ways: model state and simulation output can be animated during a simulation run, and output data can be aggregated, analyzed, and presented after (first) simulation runs are finished. St-Aubin et al. [306] survey visualization support of 50 simulation platforms – mostly for discrete event and agent-based simulation. Hereby, they not only distinguish between visualization of graphs, 2D, 3D, but also survey logging methods and integration of analytics. Motivated by the large amount of data generated by simulation, increasingly advanced methods for accessing and visualizing these data have been developed and successfully applied, e.g., [307, 308, 309]. This interactive visualization, integrated with data mining algorithms, happens under the umbrella of exploratory data analysis and visual analytics.\n\nVisualization of simulation data has often been restricted to analyzing the generated data. However, this perspective foregoes the particular chances that the combination of visualization and simulation offers, i.e., to integrate visualization more deeply into the data-generating process of simulation experiments. In the paper [310], important concepts for effective integration of visual analytics and conducting simulation experiments are presented, e.g., for optimization and simulation experiment automation [311]. By more deeply integrating visualization into modeling and simulation studies, visualization becomes an additional tool, not merely in conducting simulation experiments with models, but also in the model development cycle, as in [312].\n\nThe conceptualization and implementation of these visual methods imply significant effort and knowledge in the area of visualization. Therefore, although important for the acceptance and use of a simulation tool, more elaborate visualizations and GUIs are mostly found in commercial rather than academic research simulation tools [306]. To make matters worse, problem- and stakeholder-specific visualization solutions are required to communicate simulation results effectively. Even when similar systems are simulated, different visual analytics solutions are needed for different purposes: For example, to inspect the spreading of COVID-19 infections due to contacts between individuals along with associated metadata [313] or to support public health officials in planning and ensuring the availability of resources (such as hospital beds) under different spreading scenarios [314].\n\n6.1.5 Participatory Modelling\n\nDirectly involving stakeholders and decision makers not merely in formulating requirements and using the "
    }
}
OSS vs. proprietary software is not the argument that should be made in my opinion. They are not mutually exclusive. For instance a small company desiring a low initial cost of ownership may be very happy to spend a few more man hours working with an OSS product. As the number of users and supported infrastructure grows, a larger company may see significant benefits (such as a reduction in administrative overhead) by being able to automate and manage servers, workstations and software at an enterprise level with the click of a button. Not to say that OSS cannot also perform that function, but many are not there yet. This might be because companies such as Microsoft have money to spend on focus groups to help determine the optimal design of their user interface, or the features which users desire.

MS as a single unit is only an illusion. For instance a support call under an SLA is actually routed to a call center, which could actually be a third party vendor. This is no different than hiring a consulting firm who specializes in a particular OSS, with an SLA.

Everyone would like to tell you that they have the most secure software. Certainly more eyes looking at code helps to find bugs and improve the design. However, with OSS _you_ can audit the source code if you desire. More than audit you can make changes where necessary. Certainly many people would judge this as impractical, but the ability is there. This further enables corporate software design policies to accept only maintainable code, which is less likely to have bugs that are often introduced with spaghetti like code.(and of course you have no idea what pre compiled code looks like in source form)

Many OSS projects have no product lifecycle announcements and may just disappear. But this is not unlike the .com’s. But unlike the .com’s, the code is there for you to develop if you desire. Further it may even be possible to find old OSS for a specific problem, even if it hasn’t been developed in a long long time.

This is certainly not to say the there is one correct application or operating system or that OSS is superior. You need the right tool for the right project. Unixish OS’s aren’t the best for everything, but neither is Windows. Sometimes you don’t get to choose, such as when a client or application requires something specific.

In my experience people would like to simplify things and form stereotypes. Free or low cost seems to imply a toy or a lack of professionalism. The same thing applies to security. People would like to have a construct that enables them to say that a product is provably secure. This is not possible. With an unlimited number of possibilities (possible input), we can only prove that software will react in a way which we have tested for. Thus a well designed OSS product is no more secure than a commercial product. (except that external input and review _might_ be from a larger or more diverse group). Thus the people using the software, wheather OSS or not, face the same security and development challenges.

Last but not least is the end user. They are possibly the weakest link in the chain, but often the source of profit for a company. Not only must users be trained how to use software (and OSS may have a different look or feel), but the proper procedures must also be in place to ensure that it is configured correctly and used per a company policy. It doesn’t matter how great your product, OSS or otherwise, is if no one can use it.

You haven’t really said anything untrue in this post. To be perfectly accurate in describing the relationship between Microsoft products and directly competing OSS products, it would probably be best phrased similarly to this:

Often, the knowledge entry barrier for deploying software at all is slightly higher for open source software than it is for professional proprietary software. The total amount of expertise required to deploy software correctly (meaning: stably, securely, and perhaps even functionally) tends to be the same, or perhaps even greater for professional proprietary software. As a result, the minimum effort and expertise required for deployment of the open source solution will typically render a more functional, stable, and secure product than the minimum required effort and expertise for depoyment of professional proprietary software of equivalent purpose.

Understand that there is a great deal more software available for the Linux platform than for the Windows platform. That software runs a broad range of functionality, from PostreSQL to a set of flat-file database scripts in Perl that someone whipped up in an afternoon. Microsoft’s stand-alone database software, by way of comparison, runs a much smaller range of functionality, between SQL Server and Access. SQL Server, by some measures, is more functionally complete than PostgreSQL, and by some measures is less functionally complete than PostgreSQL. Access, by any measure, is likely to prove to be more functionally complete than the hypothetical set of Perl scripts.

There are a couple of modifying points that need to be brought up in reference to those ranges of availability. One is that an amateur can muddle his way through setting up SQL Server without really learning a thing, but anyone managing to set up PostgreSQL either needs to come to the task with some competence already in hand or will learn something from the experience. That having been done, no production environment should ever trust either database for long under those conditions because of the hidden frailties and vulnerabilities that will exist in inexpertly deployed software of that complexity. PostgreSQL will tend to be more stable and secure under those conditions, but wearing a leather jacket as opposed to running around shirtless when many of your enemies are carrying swords isn’t really much protection. It takes experience, expertise, and a great deal of knowledge to maintain either DBMS as a secure, stable environment. Of those who have that level of expertise in both of them, the vast majority (I have no statistics to back this up: only my own experience and estimations) would prefer to choose PostgreSQL over SQL Server because of the platform(s) on which it will run, the eventual effort expended, and the long-run stability and extensibility of the software, to say nothing of the fact that whereas an amateur might need to contract outside help to understand the system, PostgreSQL is 100% free to the true expert.

That having been said, you also should realize something else about the way OSS development happens. Specifically, OSS is an extension of the Unix culture, where the preference is to create small tools that do one job and do it well, rather than create large tools that do many jobs and probably screw many of them up when they are put to a given purpose that their creators didn’t plan for. These small tools in Unix development are designed to be easily made to interact with other tools, so that meta-tools can be created whose job it is to basically tie several disparate tools together to achieve larger tasks. In proprietary software design, because tools are made large and incorporate a multitude of smaller functions, if you want a “large tool” effect that does something different than the task for which the large tool you already have was designed (even if it already has all the functionality you need, in terms of discrete internal capabilities), you have to buy another, separate, large tool that redundantly incorporates many of the same capabilities in its design. This, by the way, is part of the reason that Unixy OSes tend to run much leaner and faster, with less resource usage than Windows systems.

That being the case, I’m sure there are several graphical front-ends floating around for PostgreSQL, developed separately from PostgreSQL itself but designed to interoperate with it, that add the sort of functionality normally thought to exist only with MS SQL Server, and not with PostgreSQL. Some additional knowledge and/or effort might be required to get to the point of having the graphical, point-and-click interface from the point of thinking of “PostgreSQL”, if you aren’t just using RHDB from the beginning, but that is really the result of OSS “suffering” sort of a plague of freedoms: you have choices, options, and freedoms in choosing your software that greatly outnumber the individual pockets in your brain where you store decision options. Every time you start thinking about how you want to do something, you can just use the first tool you think of, but you can also research the matter and discover that there are (pretty much always) better ways to do it for your purposes. I think that it is in large part the fact that Windows platform administrators tend to fail to realize this wealth of options on the Linux platform that causes them to cling to the Windows platform much of the time. The Windows platform generally supplies exactly one option, and you force that option to fit your task. Doing similarly with OSS tends to be more work, because nobody ever set out to make every single tool cover every single forseeable need. Rather, each tool does one thing, and does it well, and by combining tools in the right way you should be able to achieve the Right Bundle of Tools for any given job in a far more effective fashion than with the equivalent situation in working with a Windows environment.

There are really two comparisons being made, generally, when comparing the two sides of this issue. One is the Unix vs. Windows comparison, and the other is the OSS vs. Commercial Proprietary Software comparison. There is a great deal of overlap between Unix and OSS, and a great deal of overlap between Windows and commercial proprietary software, so they tend to get confused, but there are times that it is useful to separate them in discussion. What I refer to in these debates as my preferred choice is generally the Unix plus OSS solution. When I do so, however, I try to do so in discussing the matter in terms of tendencies. I do not mean to portray any of this in terms of absolutes. It is possible for someone to create the bloated equivalent of Access as an OSS project for Unix (though highly unlikely) just as it is possible for someone to create the lean “one tool one job” equivalent of one of the several Perl scripts I hypothesized as a proprietary tool for Windows (though, again, highly unlikely, due largely to its probable unmaketability in a Windows environment). The tendencies to which I refer generally apply on the one hand just as much to Unix as to OSS, and on the other just as much to Windows as to commercial proprietary software, but those tendencies become very strong tendencies, often to the point that they can be treated for many purposes as certainties, when the Unix plus OSS combination of circumstances is set against the Windows plus commercial proprietary software combinatino of circumstances.

In summation: You’re right. It’s all just tendencies. I even carefully consider relevant Windows platform solutions using commercial proprietary software just as much as the Unix solutions using OSS whenever a new situation arises in which I must make a recommendation to a client. The more I learn about what is out there (and, really, I’ll never stop learning unless I choose to: the options are seemingly endless) in the OSS world, the less attractive the Windows platform options tend to become.

As little as a year or two ago, I’d agree with your estimation of relative ease of use. While it’s still true of many Linux distributions that the average Windows user would find the transition more difficult than moving up to a newer version of Windows, it is also true of some Linux distributions that it is actually easier to install the OS, even for the congenitally clueless, than it is to install a Windows OS. For examples, I refer you to MEPIS and SimplyMEPIS (two variations on the same LiveCD distribution, which includes an extremely simple GUI-intensive means of installing to the hard drive) and, more importantly, Novell’s SuSE Linux. The newest SuSE, in particular, is a very good Linux distribution for installation to the hard drive on a system being used by a complete newbie to the world of open source software. It is reasonably stable (in Linux terms, which means “exhibiting an unprecedented level of rock-solid stability” in Windows terms), extremely intuitive to anyone used to the Windows GUI environment, an full of tools that make life easy for the unskilled end user.

I’m going to a literary club meeting this evening with a compartment of my soft-side briefcase full of SimplyMEPIS CDs. I’m going to enlighten a bunch of stubborn, clueless Windows users to the vast benefits to be had from the world of open source software. I wouldn’t even attempt this if I didn’t think they’d be likely to suffer less difficulty with SimplyMEPIS than they do with (any version of) Windows.

The problem with getting Linux to the desktop has migrated from ease of use, which is basically solved in many distributions, to mindshare. Users are resistant to change, simply because it’s change. Many users are likely to require an epiphanic revelation to clue in to the fact that there is another option they might actually like more than what they’re already using. People who are used to only learning something new about their computers when something goes drastically, aggravatingly wrong are understandably resistant to learning new things about their computers ? even when what they have to learn this time might be fun, rather than aggravating. It takes more than seeing to believe, sometimes. The experience of the last decade of working with Windows-based systems has conditioned most users to consider the process of getting their hands “dirty” with new software environments to be painful, and something to be avoided at any cost.

Linux is changing that. More and more end users are discovering that Linux can be much easier to learn and use than Windows.

I pretty much caught the tail end of the bad old days of the high expertise entry barrier for learning Linux. While ease of use has been a hallmark of Unixlike systems for years, ease of initial learning for Linux is a very recent development. I had to learn things the hard way. I had a little help in making the transition to learning the shell because of SuSE 8.0, but I would have been better off with a Linux mentor and Slackware, truth be told.

As I said, though, things have changed. With SuSE 9.x, the new Debian installer, and LiveCD distributions like MEPIS, SimplyMEPIS, and Knoppix available, the entire Linux experience has become so easy to move into that it’s like falling down. All you really need to do is stop trying to stay upright, and gravity takes care of the rest. Luckily, unlike falling down, using Linux doesn’t hurt. That’s reserved for Windows.

You might consider giving the more user-friendly Linux distributions another look. In general, what Linux suffers from these days isn’t a lack of user-friendliness, but rather a lingering reputation of user-friendliness that isn’t really applicable any longer.

For instructing new Linux users, I recommend starting with SimplyMEPIS, moving from there to SuSE, and then eventually to Debian (which, because of the “apt” package management utility and extensive software package archives, is the most easily-maintained Linux distribution available). A newbie should move to the next step when comfortable with the preceding distribution.

Check out the book _Point_And_Click_Linux_ by Robin Miller. It uses MEPIS as its example distribution and explains how Linux can be a rewarding experience for the most GUI-centric end user with little or no interest in understanding how the OS actually works. If you happen to be in Florida, there’s going to be a presentation and book signing for the book, featuring the author of course, at a Barnes & Noble near here (in Brandon, I think ? certainly in the greater Tampa Bay area, at least).

In any case, I suspect that if you provided a system pre-installed with the right Linux setup on it, many users would hardly notice the difference. If they did, they’d probably find Linux to be vastly superior. Considering that most Windows end users just buy a computer with Windows preinstalled, throw it away when they get a new one, and call a vaguely geeky friend for help if something goes wrong, I don’t see how Linux could fail to live up to (and greatly surpass) these low expectations of functionality.

For the record: in general, I agree with all you say. I simply think that some Linux distributions are rather more ready for prime time than you do, apparently.

Heh. I’m flattered by all the kudos. Thank you.

1) For a business/production environment, your choice of server depends largely on how much support you want to contract out and how much you want to do in-house.

If you want to maintain a support contract with the vendor, I believe that RHEL would be the obvious choice for best vendor support within the US. For international use, SuSE might be a better alternative. There are, as well, a number of distributions that are of particular value in Asia, such as Turbolinux, though I’m afraid I don’t know as much about the various distros that focus on an Asian market.

If you intend to handle all of your support for servers in-house, with a knowledgeable staff, Debian is probably the distribution to go with. This is in large part because of native apt support and the fact that it’s what I call a “lean” distro. It is, in fact, probably the best production server distribution on the market, aside from the lack of direct corporate backing. The fact that it falls into that class I term “lean” (as opposed to “kitchen sink”) refers to the fact that it is easily installed with a bare-bones, minimal set of software packages, and is extremely easy to configure for whatever use to which you desire to put it. That ease of software configuration comes from the native apt support and the extremely well-populated apt archives. It also enjoys one of the best stability reputations of any Linux distribution, and is probably available across more hardware platforms than any other distribution as well.

Another option that might be worth considering is OpenBSD. While I don’t really have any significant experience with any of the BSDs, they are all Unices on roughly the same level as Linux in most regards. OpenBSD has a better security reputation, as far as I’m aware, than any other OS that runs on x86 hardware. You may decide that the difference in security capabilities is insignificant in comparison to other issues, as I have (so far) in my own work, but if you are inclined to consider security to the exclusion of all else you might want to look into OpenBSD.

There are also various SELinux implementations, which are security enhancements to Linux that were developed in concert with governmental standards. I have not as yet researched the matter enough to be able to give you much advice in this area, except that it is probably worth looking into.

In any case, as I deal primarily with businesses that handle day to day IT administration entirely in-house and outsource to the consultancy I work for when outside expertise is needed, the option that I’ve been working with is Debian. No vendor support contracts are in place in most instances where I consult, with the occasional exception of a mission-critical application, and as such the ease of administration and impressive stability of Debian GNU/Linux wins hands down. These characteristics are not applicable only because of the apt system and the fact that it’s a lean distro (though those are significant factors): they also apply because of the dedicated, and systematic, application of quality control and testing by the Debian maintainer teams.

2) Generally speaking, I would recommend matching your workstation solution to your server solution. Different distributions are interoperable, of course, in ways that even different Windows versions are not. By standardizing on a single distro, however, you can ensure that company-wide system maintenance procedures can be standardized as well, greatly reducing the amount of man-hours put into the maintenance of heterogenous systems. When all of your systems use apt, the same set of server-side scripts can be used to monitor and implement package upgrades for both workstations and servers. The same is true of YUM and YaST-based central management (these being the core package management solutions comparable to apt in RHEL and SuSE, respectively).

Also, of course, if you have an enterprise support contract with Red Hat, you may as well make sure that it covers both servers and workstations. If you’re going to do your system administration in-house for either the servers or the workstations, you should (probably) do it for the other as well.

On the other hand, for a stand-alone workstation, or in a sufficiently small environment, other issues arise. The first such issue is whether it will be implemented by the person that will use it, and whether that person is familiar with Linux. In other words: Is this a tutorial project, as well as a workstation? If the answer to that question is “yes”, I would probably recommend SuSE (or, perhaps, Mandrake, though I have some reservations about Mandrake; suffice to say that I wouldn’t recommend Mandrake, but I understand why some would). SuSE has become about the most easily implemented distro (discounting LiveCD distributions) for new migrants from Windows, particularly as an enterprise workstation class distribution.

For someone already somewhat familiar with Linux, I would be more inclined to suggest Debian. The reason for this, again, comes back to the fact that it is a lean distro with extreme ease of system administration (so long as you’re not afraid of the command line, where the most powerful administration tools are available). SuSE (like RHEL, Fedora, Mandrake, MEPIS, and any other kitchen sink distro) suffers from the problem of being implemented in a manner that the designers of the distribution think is “good”. Debian, by that same measure, enjoys the benefit of being implemented the way the user thinks is best. The downside of that, of course, is that you have to know what you want and have some idea of how to implement it. Such things are slightly more obvious to the beginner in a GUI-intensive default environment like that of SuSE or RHEL. On the other hand, if you don’t want your end-users running amuck with system administration, and don’t want a bunch of time-wasters installed on the system, that GUI-intensive user friendliness might be something you don’t need as much. Ease of use need not include extraneous bells and whistles, after all. Once Debian is set up, it can be as easy as (or easier than) any kitchen sink distro to use for day-to-day work, because all Linuxes draw on pretty much the same exact range of software.

If you’re just building a single, stand-alone, performance-intensive “dream machine”, there are many who would recommend Gentoo. I tend to feel that this is entirely the wrong choice for a production environment workstation (or server) because of the vastly greater amounts of time spent in getting that optimized system. The BSDs tend to be less “friendly” to the end-user than the various Linuxes, and is thus not typically as good a workstation option except in extremely stable environments where your users all follow very strict practices that won’t interfere with system administration.

3 & 4) I’m afraid I haven’t any experience with running Linux emulation in Windows. The most commonly recommended implementation of it that I’ve come across, though, is Cygwin. I recommend looking into that for possible future use of Linux from within Windows.

Another option is simply to set up a dual-boot system. That is typically extremely easy to arrange with any major (modern) Linux distro. You would, in short, choose which OS you want running when you boot up the machine. I don’t know if this is an option for you. I don’t mean to insult you if you’re already aware of this and have already discarded it, nor do I mean to insult you if you haven’t heard of the possibility before. I’m just trying to cover all the bases. In any case, it is an option.

Personally, aside from a laptop, I would have very little reason to run a dual-boot system. I do have to have access to a Windows system here because of my work (which often involves supporting Windows-using clients and prototyping Windows solutions), but I simply run more than one computer, managed through a KVM switch so that I can just toggle between computers with the same monitor, keyboard, and trackball. I currently have three systems thusly connected, one Windows and two Debian, at my desk.

By the way, remote administration of Linux and BSD servers is extremely easy, even from a Windows machine, through the use of SSH (Secure SHell). There is a Windows client application for SSH called PuTTY that might prove invaluable for you if you must administrate *nix boxen in a primarily Windows environment. It provides access to the shell on the server as though you actually had direct access to it through a keyboard and monitor connected to that server, but accessible from your Windows workstation. SSH can also, of course, be used from another Unix workstation. SSH is by far a better remote administration tool than some other, older options like RSH and Telnet, as SSH encrypts communications between server and client systems and uses an authentication procedure when going through session initiation. Use of SSH allows a server to be actively maintained and configured easily while running “headless” (without I/O peripherals), allowing complete functionality as though you were accessing it directly.

I’m beginning to sound like a broken record, I fear.

Other than as a means to access mission-critical applications that don’t run on Windows, or as a tool for learning the ropes of Linux, I generally wouldn’t recommend running Linux (or Linux emulation) inside a Windows environment. As I heard one wit put it, “Now you can have all the security and stability of Windows with the user friendliness of Unix!” Without knowing more about the specific use to which you’d intend to put such a thing, though, I don’t have much to offer in terms of alternatives if VMWare isn’t working out for you. As I said, I’ve not used it myself, and have no experience with it.

One of the most important pieces of advice for anyone new to Linux who wants to learn more about it is to join a LUG (Linux User Group). You can surely find one reasonably near you, if you want to attend meetings, but more important than the meetings is simply access to the mailing lists of LUGs. Any LUG with a couple years under its belt that is keeping active is certain to have two or three people monitoring it that know far more about any given question you might have than I ever will. On a given list, you’ll probably have a couple of people that are experts with multimedia Linux systems, a couple that are experts with Windows and Linux interoperation, a couple that are experts with wireless networking implementations, and so on. LUGs constitute a rich resource whose value is beyond measure. Just be humble, at least to begin with, and willing to learn. Most LUG members are friendly, helpful people. You can probably get good advice from the list about what distros to use for various purposes, too, though you might want to be wary of touching off a minor “holy war” between devotees of various distributions. Linux geeks (myself included) tend to be extremely attached to their favorite toys. I, for instance, have done my best to be impartial in considering the benefits of various distributions, though my knee-jerk reaction is always to recommend Debian first, foremost, and only. When I realized (lo these many moons ago) that I could actually upgrade my OS kernel with no more work than a single command at the shell, there was no turning back for me. That kind of power with that level of ease sold me once and for all on Debian, for my own purposes.

Thanks again for the kind words.
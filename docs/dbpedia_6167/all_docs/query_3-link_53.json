{
    "id": "dbpedia_6167_3",
    "rank": 53,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10920684/",
        "read_more_link": "",
        "language": "en",
        "title": "Peer-review of teaching materials in Canadian and Australian universities: A content analysis",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-jehp.gif",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/corrauth.gif",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/corrauth.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Roghayeh Gandomkar",
            "Azadeh Rooholamini"
        ],
        "publish_date": "2023-08-29T00:00:00",
        "summary": "",
        "meta_description": "Peer-review of teaching materials (PRTM) has been considered a rigorous method to evaluate teaching performance to overcome the student evaluation's psychometric limitations and capture the complexity and multidimensionality of teaching. The current study ...",
        "meta_lang": "en",
        "meta_favicon": "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico",
        "meta_site_name": "PubMed Central (PMC)",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10920684/",
        "text": "Introduction\n\nTeaching effectiveness is a major criterion used for formative or summative decision-making when evaluating faculty performance.[1] Student evaluation of teaching has remained the primary and, in many cases, the only tool to measure teaching effectiveness for the last decades, notwithstanding questioning its validity as a legitimate measure.[2] Other measures of teaching effectiveness, such as peer ratings, are also fallible in one or more ways, though.[3] Recently, there has been a trend toward triangulating evidence to measure teaching effectiveness to overcome psychometric limitations and capture the complexity and multidimensionality of teaching.[4,5,6,7]\n\nThere has been some progress toward the recognition of teaching since Boyer (1990) redefined scholarship to include teaching.[6] If teaching performance is to be rewarded as a scholarship, it should be subjected to the same rigorous peer-review process applied to other forms of scholarship such as research.[8]\n\nPeer-review of teaching (PRT) is defined as informed peer judgment about faculty teaching performance and is used to foster improvement or make personnel decisions.[9] Typically, the peer is a qualified colleague by expertise or training to serve as a knowledgeable judge of evidence accuracy. The notion of informed judgment implies a systematic evaluation based on appropriate criteria and thoughtful processes.[10]\n\nPRT has two forms: peer observation of teaching (POT) and peer review of teaching materials (PRTM). POT covers those aspects of teaching performance that peers are better qualified to evaluate than students.[11,12,13] PRTM involves rating the quality of the teaching materials such as course syllabus, instructional plans, texts, reading assignments, handouts, homework, and tests or projects generally through the teaching or course portfolios containing self-reflection of teachers themselves.[14,15,16] The two forms of PRT are the most complementary source of evidence for student evaluation of teaching. Although POT is conducted more commonly, PRTM is less subjective and more efficient and reliable than observations.[17]\n\nThe literature about PRT has mainly been focused on POT practices.[12,18,19,20,21,22,23,24] The PRTM format has been operationalized through teaching portfolio; first in Canadian and then in Australian higher education. Afterward, it was expanded to the United States and the United Kingdom academic context.[25] Although PRTM is advocated and its methods and effects were tested,[26,27,28,29,30] there has been little attention on how it has been used systematically in academic settings.[31] To help overcome this gap in knowledge, we aimed this study to identify the extent to which Canadian and Australian universities applied elements of PRTM in their faculty evaluation system. We focused on these two countries because they are pioneers in designing and using PRTM programs in the academic setting. The Canadian and Australian context for better accountability and improved quality in teaching has promoted and supported excellence in teaching and the scholarship of teaching in their higher education institutions.[31,32,33,34,35]\n\nTo evaluate the data related to the PRTM programs of the selected universities, we used a Chism (2007) systematic approach including PRTM program major design elements as a predetermined framework: the purpose of PRT, identifying the faculty members involved, logistics of the review, criteria, and evidence for peer-review, standards to be used in judgment, instruments for performing the reviews, and preparation of reviewers.[9]\n\nAlthough her framework contains all elements of the PRTM, it does not propose the details of each element. Therefore, another aim of this study was to analyze and categorize the details of PRTM programs in the selected universities to expand the Chism elements. This study takes the next step by providing empirical evidence of the use of those elements and how they interact as a framework to help guide the practical implementation and evaluation of PRTM in health profession education.\n\nResults\n\nWebsites of 46 (27 Canadian and 19 Australian) universities were searched. A total of 21 universities provided information on PRTM on their websites and were included in the results. Fourteen universities were in Canada. The main features of PRTM programs were organized under the seven major design elements: purpose, identifying the faculty members involved, logistics, criteria and evidence, standards, instruments, and preparation of reviewers.\n\nshows the major design elements and their categories and subcategories with frequencies.\n\nTable 1\n\nMajor design elementsCategoriesSubcategoriesNo. Universities (%)PurposeJust summative11 (52.38%)Both summative and formative10 (47.62%)Identifying the faculty members involvedWho the reviewers will be? (NM=13)Just external to the university3 (14.28%)Both external and internal to the university5 (23.81%)Who selects reviewers? (NM=13)Candidate2 (9.52%)Administrators/teaching, and learning center6 (28.57%)Criteria for selecting reviewers (NM=13)Experienced in the peer review of teaching4 (19.04%)Qualified to evaluate the candidate’s academic duties3 (14.28%)Disciplinary background6 (28.57%)Teaching and learning expertise5 (23.81%)Status (teaching award/excellence in teaching)2 (9.52%)level/rank (a certain level of seniority)4 (19.04%)Number of reviewers (NM=13)Two or more8 (38.09%)Candidate (NM=11)Only continuing academic staff members4 (19.04%)All faculty members6 (58.57%)LogisticsReview intervals (NM=12)Annually or each semester4 (19.04%)Between 1 and 3 years3 (14.28%)> 3 years2 (9.52%)Participation (NM=16)Mandatory2 (9.52%)Voluntary1 (4.76%)Both2 (9.52%)Criteria and evidence (NM=1)Teaching PhilosophyDescribing teaching goals, strategies, and evaluation methods20 (95.23%)Teaching activities (n=20)Number of courses and course outline9 (42.85%)Course, curriculum, or teaching materials9 (42.85%)Teaching methods6 (28.57%)Grading and assessment5 (23.81%)Guest lecturing and invited teaching activities3 (14.28%)Supervision, mentorship, and student advising14 (66.66%)Teaching effectiveness (n=19)Teacher evaluation17 (80.95%)Course evaluation10 (47.62%)Student development, success, and progress7 (33.33%)Formal recognition of teaching accomplishment12 (57.14%)Educational leadership (n=18)Committee services12 (57.14%)Peer mentoring, coaching, and consultation5 (23.81%)Coordinating or running workshops, and seminars6 (58.57%)Educational scholarship (n=17)Publications or presentations13 (61.90%)Research grants5 (23.81%)Peer-review or editorial activities3 (14.28%)Course and curriculum development9 (42.85%)Innovations in teaching and learning9 (42.85%)Professional development (n=14)Efforts made to improve teaching activities (scholarly teaching)13 (61.90%)Participation in POT as an observer6 (58.57%)Participating in seminars and workshops9 (42.85%)Participating in the educational scholarship1 (4.76%)Standards (NM=19)Completeness of documentation1 (4.76%)Quantity and quality of evidence1 (4.76%)Instruments (NM=4)Rubrics5 (23.81%)Rating scale2 (9.52%)Checklists5 (23.81%)Comments5 (23.81%)Preparation of reviewers (NM=10)Workshop7 (33.33%)Guidelines4 (19.04%)\n\nAll universities provided information about the purpose of the PRTM. Universities applied PRTM as a summative evaluation (n = 11) or with the purpose of both formative and summative (n = 10). Over half of the universities did not publish details regarding the faculty member involved; reviewers' characteristics and their selection (n = 13) and candidates (n = 11). Of that provided information, five recruited a mixture of external and internal reviewers to the university, six assigned the reviewers by a responsible body, and all used more than one criteria for selecting the reviewers and employed two or more reviewers. Selection based on the disciplinary background was the most applied criterion (n = 6). More than half of the universities that provided information implemented the PRTM for all faculty members. There was no report on the logistics of the PRTM by most universities (review intervals, 12; participation, 16) [ ].\n\nAlmost all universities (n = 20) defined the criteria for review in terms of teaching philosophy (n = 20), teaching activities (n = 20), teaching effectiveness (n = 19), educational leadership (n = 18), teaching scholarship (n = 17), and professional development (n = 14). An analysis of the types of evidence used within each criterion revealed various data sources and types of information. Interestingly, supervising and advising were the most used evidence for teaching activities (n = 14). Seventeen out of 19 universities reported evidence related to teacher evaluation including students' surveys, POT, and feedback from alumni for teaching effectiveness. Providing education committee services, publications, presentations, and scholarly teaching were among the common evidence for educational leadership (n = 12), educational scholarship (n = 13), and professional development (n = 13), respectively [ ].\n\nOnly two universities proposed standards for judgment as the completeness of documentation and quantity and quality of evidence, one for each university. Universities used a variety of instruments for performing reviews such as rubrics, checklists, rating scales, and written comments. Finally, universities provided workshops (n = 7) and guidelines (n = 4) for the preparation of reviewers [ ].\n\nDiscussion\n\nAlthough the POT component of PRT has been addressed adequately in higher education literature, there has been less emphasis on the PRTM constituent. We conducted a content analysis of PRTM practices presented on the website of 21 Canadian and Australian universities. Employing content analysis to web-based data is a practical and feasible research endeavor[37] that has been used in health professions education rarely.[38]\n\nTo our knowledge, this is the first study that utilized a well-documented framework to depict the major design elements of PRTM. We found that universities documented adequately the purpose of the PRTM program, and the criteria, evidence, and instruments applied for review. Meanwhile, other design elements of the program referring to process, procedure, logistics, and support aspects were not sufficiently addressed. Thus, between half to two-thirds of the universities did not provide information regarding the identification of the reviewers and candidates, preparation of reviewers, and logistics (how often and when) of the PRTM. Lastly, while universities utilized the indicators for performance measurement in the evaluation instruments, only two universities reported standards for judgment explicitly.\n\nAll universities in our study had specified their PRTM goals because of its effect on the other elements such as what information should be collected, and how that information should be used. Interestingly, none of the universities applied PRTM with only formative. The reason may be that only formative evaluations had universities that used the teaching observation (POT) component to provide information for developing teaching practice and were excluded from our study.[39] In our framework, the faculty members involved were categorized for these elements; who will serve as reviewers, who selects reviewers on what basis, how many reviewers will be needed, and who will be reviewed. Of those universities that provided information, all used more than one criterion for selecting the reviewers and employed two or more reviewers. Also, most of them used a mixture of reviewers as internal and external to the university. The reason may be to have a panel of reviewers that can reflect different perspectives collectively.[40]\n\nIn our framework, the criteria were categorized as teaching philosophy, teaching activities, teaching effectiveness, educational leadership, educational scholarship, and professional development. As we analyzed, most universities provided a complete report of these criteria and subcategorized them as evidence that revealed various data sources and types of information. These findings showed and confirmed PRT as a comprehensive approach for teaching evaluation that uses multiple sources of evidence.[4,7,41]\n\nFor education as a scholarship, most universities reported evidence based on the criteria for the scholarship. Furthermore, like Kanade et al.,[42] (2018) using Glassick's criteria including clear goals, adequate preparation, appropriate methods, significant results, effective presentation, and reflective critique will be a helpful framework for defining teaching criteria.\n\nWe are not aware of the methods that universities have used to identify criteria. Based on the literature, the value of consensus approaches including nominal or Delphi group for developing and validating criteria is discussed.[43,44]\n\nCriteria, evidence, and standards are three essential building blocks at the heart of every PRT system. Because they are the basis of developing tools to evaluate teaching.[9,45] Standards as the basis for evaluating good teaching are not explicitly reported in most university documentation. Arreola (2007) cautions that standards of performance be identified (e.g., “the syllabus contains the following items”) so that reviewers are rating the same thing, and labels on the rating scale are related to the criteria to be evaluated.[46]\n\nMost universities reported various tools for evaluating evidence and rating teaching portfolios based on criteria. However, universities did not describe the methodology of designing their tools both for evaluating evidence and rating portfolios.[45] This matter could be a line of inquiry for further research. For instance, Van der et al. (2008) suggested using a chain model of the assessment process to develop and validate a design for teacher portfolio assessment. According to their study, two links were verified: the link between content standards and portfolio format and the link between content standards and raters' scoring.[47]\n\nThese findings mirror the eight-step model for the design of faculty evaluation systems proposed by Arreola (1999) and applied in academic settings, which consider the definition of performance aspects as the main focus.[48,49] Our findings are also aligned with the results of a systematic scoping review conducted on papers reporting the use of portfolios of medical educators.[50] Hong et al. (2021) found the components of portfolios of medical educators as one of the main focuses of the reviewed papers. They categorized the components as teaching and scholarship, educational research products, leadership and administration, curriculum development, assessment of learners, formal recognition, professional development, and general, which was comparable with the criteria and evidence category and its subcategories in our study.[50]\n\nThe major design elements of PRTM with associated categories and subcategories offered in the current study provide a practical framework to design and implement a comprehensive and detailed PRTM system in the academic setting. Further studies are recommended to investigate the applicability of the framework.\n\nLimitations\n\nWe used data provided on the websites of the universities, which may not depict the full picture of their PRTM practices. Triangulating these findings with other sources of data such as surveying the universities' responsible bodies may broaden our understanding of PRTM practices. We investigated the PTRM practices of two (pioneering) countries which may limit the generalizability of our findings.\n\nImplications for practice\n\nBased on the findings from our analysis of the PRTM practices in Canadian and Australian universities, we offered a practical framework to assist and guide institutions to effectively design, develop, and even evaluate peer-review programs. In addition, documentation of this framework could be an essential part of a teaching portfolio design that can constitute the evidence in the portfolio."
    }
}
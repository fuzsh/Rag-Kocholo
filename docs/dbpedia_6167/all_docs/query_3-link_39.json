{
    "id": "dbpedia_6167_3",
    "rank": 39,
    "data": {
        "url": "https://journals.lww.com/jehp/fulltext/2023/12220/peer_review_of_teaching_materials_in_canadian_and.430.aspx",
        "read_more_link": "",
        "language": "en",
        "title": "Journal of Education and Health Promotion",
        "top_image": "https://images.journals.lww.com/jehp/SocialThumb.01679914-202312220-00430.T1.jpeg",
        "meta_img": "https://images.journals.lww.com/jehp/SocialThumb.01679914-202312220-00430.T1.jpeg",
        "images": [
            "https://cdn.wolterskluwer.io/wk-logos/1.0.x/favicon.png?rev=43",
            "https://journals.lww.com/_layouts/15/images/spcommon.png?rev=43",
            "https://images.journals.lww.com/jehp/ArticleViewerPreview.01679914-202312220-00430.T1.jpeg",
            "https://images.journals.lww.com/jehp/XLargeThumb.01679914-202407290-00000.CV.jpeg",
            "https://journals.lww.com/_layouts/1033/IMAGES/OAKS.Journals/CAPrivacyPolicy.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "ractices in Canadian and Australian universities in their faculty evaluation system.\r\r\nMATERIALS AND METHODS: \r\r\nThis is a qualitative content analysis study in which all websites of Canadian and Australian universities (n = 46) were searched based on the experts› opinion. Data related to PRTM were extracted and analyzed employing an integrative content analysis, incorporating both inductive and deductive elements iteratively. Data were coded and then organized into subcategories and categories using a predetermined framework including the major design elements of a PRTM system. The number of universities for each subcategory was calculated.\r\r\nRESULTS: \r\r\nA total of 21 universities provided information on PRTM on their websites. The main features of PRTM programs were organized under the seven major design elements. Universities applied PRTM mostly (n = 11) as a summative evaluation. Between half to two-thirds of the universities did not provide information regarding the identification of the reviewers and candidates, preparation of reviewers, and logistics (how often and when) of the PRTM. Almost all universities (n = 20) defined the criteria for review in terms of teaching philosophy (n = 20), teaching activities (n = 20), teaching effectiveness (n = 19), educational leadership (n = 18), teaching scholarship (n = 17), and professional development (n = 14).\r\r\nCONCLUSION: \r\r\nThe major design elements of PRTM, categories and subcategories offered in the current study provide a practical framework to design and implement a comprehensive and detailed PRTM system in the academic setting....",
        "meta_lang": "en",
        "meta_favicon": "https://cdn.wolterskluwer.io/wk-logos/1.0.x/favicon.png",
        "meta_site_name": "LWW",
        "canonical_link": "https://journals.lww.com/jehp/fulltext/2023/12220/peer_review_of_teaching_materials_in_canadian_and.430.aspx",
        "text": "Introduction\n\nTeaching effectiveness is a major criterion used for formative or summative decision-making when evaluating faculty performance.[1] Student evaluation of teaching has remained the primary and, in many cases, the only tool to measure teaching effectiveness for the last decades, notwithstanding questioning its validity as a legitimate measure.[2] Other measures of teaching effectiveness, such as peer ratings, are also fallible in one or more ways, though.[3] Recently, there has been a trend toward triangulating evidence to measure teaching effectiveness to overcome psychometric limitations and capture the complexity and multidimensionality of teaching.[4-7]\n\nThere has been some progress toward the recognition of teaching since Boyer (1990) redefined scholarship to include teaching.[6] If teaching performance is to be rewarded as a scholarship, it should be subjected to the same rigorous peer-review process applied to other forms of scholarship such as research.[8]\n\nPeer-review of teaching (PRT) is defined as informed peer judgment about faculty teaching performance and is used to foster improvement or make personnel decisions.[9] Typically, the peer is a qualified colleague by expertise or training to serve as a knowledgeable judge of evidence accuracy. The notion of informed judgment implies a systematic evaluation based on appropriate criteria and thoughtful processes.[10]\n\nPRT has two forms: peer observation of teaching (POT) and peer review of teaching materials (PRTM). POT covers those aspects of teaching performance that peers are better qualified to evaluate than students.[11-13] PRTM involves rating the quality of the teaching materials such as course syllabus, instructional plans, texts, reading assignments, handouts, homework, and tests or projects generally through the teaching or course portfolios containing self-reflection of teachers themselves.[14-16] The two forms of PRT are the most complementary source of evidence for student evaluation of teaching. Although POT is conducted more commonly, PRTM is less subjective and more efficient and reliable than observations.[17]\n\nThe literature about PRT has mainly been focused on POT practices.[12,18-24] The PRTM format has been operationalized through teaching portfolio; first in Canadian and then in Australian higher education. Afterward, it was expanded to the United States and the United Kingdom academic context.[25] Although PRTM is advocated and its methods and effects were tested,[26-30] there has been little attention on how it has been used systematically in academic settings.[31] To help overcome this gap in knowledge, we aimed this study to identify the extent to which Canadian and Australian universities applied elements of PRTM in their faculty evaluation system. We focused on these two countries because they are pioneers in designing and using PRTM programs in the academic setting. The Canadian and Australian context for better accountability and improved quality in teaching has promoted and supported excellence in teaching and the scholarship of teaching in their higher education institutions.[31-35]\n\nTo evaluate the data related to the PRTM programs of the selected universities, we used a Chism (2007) systematic approach including PRTM program major design elements as a predetermined framework: the purpose of PRT, identifying the faculty members involved, logistics of the review, criteria, and evidence for peer-review, standards to be used in judgment, instruments for performing the reviews, and preparation of reviewers.[9]\n\nAlthough her framework contains all elements of the PRTM, it does not propose the details of each element. Therefore, another aim of this study was to analyze and categorize the details of PRTM programs in the selected universities to expand the Chism elements. This study takes the next step by providing empirical evidence of the use of those elements and how they interact as a framework to help guide the practical implementation and evaluation of PRTM in health profession education.\n\nMaterials and Methods\n\nStudy design and setting\n\nWe followed two steps to address the two aims of the study. First, a qualitative content analysis methodology was considered to analyze the data related to PRTM programs in Canadian and Australian universities based on the experts' opinions and produce a practical framework. Second, the frequency of the framework's components was identified.\n\nData collections\n\nWebsite data were used for this study. We performed a content search on the all websites of Canadian and Australian universities through URL links available on the national/regional rank listing on the Academic Ranking of World Universities website. The search was conducted between June 2020 and September 2020 and updated in June 2022 using the following terms: peer-review of teaching, PRT, peer evaluation, peer assessment, peer development, teaching portfolio, academic portfolio, educator portfolio, faculty portfolio, and a teaching dossier. If the initial search failed to yield any results, a supplementary search was carried out with broader terms: promotion, rank, and tenure. We finally searched the “teaching and learning center” and its equivalent terms on the website but we found no record. We limited the information retrieval process to the university home page and all pages within drill-downs, which means moving from general to detailed data such as policies and guidelines. Universities that provided information on PRTM on their websites were included and universities that used only the POT component were excluded from further analysis.\n\nData analysis\n\nData from websites were extracted and entered into a Microsoft OneNote@2016 considering a separate sheet for each university. We developed a coding scheme based on the major design elements of a PRTM system outlined by Chism (2007) with modifications as a guide for data categorization. For step 1 of the study, we employed an integrative approach to qualitative content analysis, incorporating both inductive and deductive elements iteratively.[36] To gain a deeper understanding of the data, all retrieved data were read several times and the meaning units were recognized inductively and assigned appropriate codes. The codes were abstracted and allocated into subcategories based on similarities and differences. Subcategories were then assigned deductively under the elements of Chism's framework. The inductive and deductive processes were performed iteratively and elements were revised if it was necessary. Data analysis was conducted by one of the authors. However, over data analysis, frequent meetings between authors were conducted to review codes and refine the coding framework. Disagreements were resolved through ongoing discussions. For step 2, the number of universities for each subcategory was calculated using descriptive analysis of frequency and percentage.\n\nResults\n\nWebsites of 46 (27 Canadian and 19 Australian) universities were searched. A total of 21 universities provided information on PRTM on their websites and were included in the results. Fourteen universities were in Canada. The main features of PRTM programs were organized under the seven major design elements: purpose, identifying the faculty members involved, logistics, criteria and evidence, standards, instruments, and preparation of reviewers.\n\nTable 1 shows the major design elements and their categories and subcategories with frequencies.\n\nAll universities provided information about the purpose of the PRTM. Universities applied PRTM as a summative evaluation (n = 11) or with the purpose of both formative and summative (n = 10). Over half of the universities did not publish details regarding the faculty member involved; reviewers' characteristics and their selection (n = 13) and candidates (n = 11). Of that provided information, five recruited a mixture of external and internal reviewers to the university, six assigned the reviewers by a responsible body, and all used more than one criteria for selecting the reviewers and employed two or more reviewers. Selection based on the disciplinary background was the most applied criterion (n = 6). More than half of the universities that provided information implemented the PRTM for all faculty members. There was no report on the logistics of the PRTM by most universities (review intervals, 12; participation, 16) [Table 1].\n\nAlmost all universities (n = 20) defined the criteria for review in terms of teaching philosophy (n = 20), teaching activities (n = 20), teaching effectiveness (n = 19), educational leadership (n = 18), teaching scholarship (n = 17), and professional development (n = 14). An analysis of the types of evidence used within each criterion revealed various data sources and types of information. Interestingly, supervising and advising were the most used evidence for teaching activities (n = 14). Seventeen out of 19 universities reported evidence related to teacher evaluation including students' surveys, POT, and feedback from alumni for teaching effectiveness. Providing education committee services, publications, presentations, and scholarly teaching were among the common evidence for educational leadership (n = 12), educational scholarship (n = 13), and professional development (n = 13), respectively [Table 1].\n\nOnly two universities proposed standards for judgment as the completeness of documentation and quantity and quality of evidence, one for each university. Universities used a variety of instruments for performing reviews such as rubrics, checklists, rating scales, and written comments. Finally, universities provided workshops (n = 7) and guidelines (n = 4) for the preparation of reviewers [Table 1].\n\nDiscussion\n\nAlthough the POT component of PRT has been addressed adequately in higher education literature, there has been less emphasis on the PRTM constituent. We conducted a content analysis of PRTM practices presented on the website of 21 Canadian and Australian universities. Employing content analysis to web-based data is a practical and feasible research endeavor[37] that has been used in health professions education rarely.[38]\n\nTo our knowledge, this is the first study that utilized a well-documented framework to depict the major design elements of PRTM. We found that universities documented adequately the purpose of the PRTM program, and the criteria, evidence, and instruments applied for review. Meanwhile, other design elements of the program referring to process, procedure, logistics, and support aspects were not sufficiently addressed. Thus, between half to two-thirds of the universities did not provide information regarding the identification of the reviewers and candidates, preparation of reviewers, and logistics (how often and when) of the PRTM. Lastly, while universities utilized the indicators for performance measurement in the evaluation instruments, only two universities reported standards for judgment explicitly.\n\nAll universities in our study had specified their PRTM goals because of its effect on the other elements such as what information should be collected, and how that information should be used. Interestingly, none of the universities applied PRTM with only formative. The reason may be that only formative evaluations had universities that used the teaching observation (POT) component to provide information for developing teaching practice and were excluded from our study.[39] In our framework, the faculty members involved were categorized for these elements; who will serve as reviewers, who selects reviewers on what basis, how many reviewers will be needed, and who will be reviewed. Of those universities that provided information, all used more than one criterion for selecting the reviewers and employed two or more reviewers. Also, most of them used a mixture of reviewers as internal and external to the university. The reason may be to have a panel of reviewers that can reflect different perspectives collectively.[40]\n\nIn our framework, the criteria were categorized as teaching philosophy, teaching activities, teaching effectiveness, educational leadership, educational scholarship, and professional development. As we analyzed, most universities provided a complete report of these criteria and subcategorized them as evidence that revealed various data sources and types of information. These findings showed and confirmed PRT as a comprehensive approach for teaching evaluation that uses multiple sources of evidence.[4,7,41]\n\nFor education as a scholarship, most universities reported evidence based on the criteria for the scholarship. Furthermore, like Kanade et al.,[42] (2018) using Glassick's criteria including clear goals, adequate preparation, appropriate methods, significant results, effective presentation, and reﬂective critique will be a helpful framework for defining teaching criteria.\n\nWe are not aware of the methods that universities have used to identify criteria. Based on the literature, the value of consensus approaches including nominal or Delphi group for developing and validating criteria is discussed.[43,44]\n\nCriteria, evidence, and standards are three essential building blocks at the heart of every PRT system. Because they are the basis of developing tools to evaluate teaching.[9,45] Standards as the basis for evaluating good teaching are not explicitly reported in most university documentation. Arreola (2007) cautions that standards of performance be identified (e.g., “the syllabus contains the following items”) so that reviewers are rating the same thing, and labels on the rating scale are related to the criteria to be evaluated.[46]\n\nMost universities reported various tools for evaluating evidence and rating teaching portfolios based on criteria. However, universities did not describe the methodology of designing their tools both for evaluating evidence and rating portfolios.[45] This matter could be a line of inquiry for further research. For instance, Van der et al. (2008) suggested using a chain model of the assessment process to develop and validate a design for teacher portfolio assessment. According to their study, two links were verified: the link between content standards and portfolio format and the link between content standards and raters' scoring.[47]\n\nThese findings mirror the eight-step model for the design of faculty evaluation systems proposed by Arreola (1999) and applied in academic settings, which consider the definition of performance aspects as the main focus.[48,49] Our findings are also aligned with the results of a systematic scoping review conducted on papers reporting the use of portfolios of medical educators.[50] Hong et al. (2021) found the components of portfolios of medical educators as one of the main focuses of the reviewed papers. They categorized the components as teaching and scholarship, educational research products, leadership and administration, curriculum development, assessment of learners, formal recognition, professional development, and general, which was comparable with the criteria and evidence category and its subcategories in our study.[50]\n\nThe major design elements of PRTM with associated categories and subcategories offered in the current study provide a practical framework to design and implement a comprehensive and detailed PRTM system in the academic setting. Further studies are recommended to investigate the applicability of the framework.\n\nLimitations\n\nWe used data provided on the websites of the universities, which may not depict the full picture of their PRTM practices. Triangulating these findings with other sources of data such as surveying the universities' responsible bodies may broaden our understanding of PRTM practices. We investigated the PTRM practices of two (pioneering) countries which may limit the generalizability of our findings.\n\nImplications for practice\n\nBased on the findings from our analysis of the PRTM practices in Canadian and Australian universities, we offered a practical framework to assist and guide institutions to effectively design, develop, and even evaluate peer-review programs. In addition, documentation of this framework could be an essential part of a teaching portfolio design that can constitute the evidence in the portfolio.\n\nConclusions\n\nDespite the robustness of the PTRM to evaluate teaching performance, the investigation of its implementation in academic settings received little attention. We conducted a content analysis of PRTM practices presented on the website of 21 Canadian and Australian universities utilizing a well-documented framework. Although some major design elements of PRTM were not addressed on the websites adequately, categories and subcategories offered in the current study provide a framework to design and implement a comprehensive and detailed PRTM system in the academic setting. Further studies are suggested to investigate the applicability of the framework.\n\nFinancial support and sponsorship\n\nThis work was funded by the National Agency for Strategic Research in Medical Education. Tehran. Iran. Under Grant number 970559. Please contact to corresponding author for detailed information on major elements of PRTM applied by universities.\n\nConflicts of interest\n\nThere are no conflicts of interest."
    }
}
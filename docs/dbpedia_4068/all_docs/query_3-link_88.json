{
    "id": "dbpedia_4068_3",
    "rank": 88,
    "data": {
        "url": "https://discuss.pytorch.org/t/imbalanced-positive-negative-edges-graph-link-prediction/84032",
        "read_more_link": "",
        "language": "en",
        "title": "Imbalanced positive/negative edges - graph link prediction",
        "top_image": "https://discuss.pytorch.org/uploads/default/original/2X/1/15a7e2573aeb9e6ba8995f824d3b63171a433041.png",
        "meta_img": "https://discuss.pytorch.org/uploads/default/original/2X/1/15a7e2573aeb9e6ba8995f824d3b63171a433041.png",
        "images": [
            "https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/a/f08c70/48.png",
            "https://discuss.pytorch.org/uploads/default/optimized/3X/2/5/25fbcfee0b8df389f79b21fe20e6448f8dcd0e01_2_690x180.png",
            "https://discuss.pytorch.org/uploads/default/original/3X/3/e/3ead050569e5cf01a36402a86a014cc172f63f61.png",
            "https://discuss.pytorch.org/uploads/default/optimized/3X/b/6/b6b483dd1c22e8c91434be0b005f808437ff73ca_2_690x284.png",
            "https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/a/f08c70/48.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "dz_k (dz k)"
        ],
        "publish_date": "2020-06-03T10:23:16+00:00",
        "summary": "",
        "meta_description": "I am using a graph autoencoder to perform link prediction on a graph. The issue is that the number of negative (absent) edges is about 100 times the number of positive (existing) edges. To deal with the imbalance of data&hellip;",
        "meta_lang": "en",
        "meta_favicon": "https://discuss.pytorch.org/uploads/default/optimized/2X/b/bb2eeaba4e9f7e4a5944a0d83f52c4f2bf1b6a85_2_32x32.png",
        "meta_site_name": "PyTorch Forums",
        "canonical_link": "https://discuss.pytorch.org/t/imbalanced-positive-negative-edges-graph-link-prediction/84032",
        "text": "import pickle as pkl import networkx as nx import numpy as np import scipy.sparse as sp import torch from sklearn.metrics import roc_auc_score, average_precision_score from sklearn.metrics import confusion_matrix from sklearn.metrics import f1_score def sparse_to_tuple(sparse_mx): if not sp.isspmatrix_coo(sparse_mx): sparse_mx = sparse_mx.tocoo() coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose() values = sparse_mx.data shape = sparse_mx.shape return coords, values, shape def mask_test_edges(adj): # Function to build test set with 10% positive links # Remove diagonal elements adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape) adj.eliminate_zeros() # Check that diag is zero: # assert np.diag(adj.todense()).sum() == 0 adj_triu = sp.triu(adj) adj_tuple = sparse_to_tuple(adj_triu) edges = adj_tuple[0] edges_all = sparse_to_tuple(adj)[0] num_test = int(np.floor(edges.shape[0] / 10.)) num_val = int(np.floor(edges.shape[0] / 20.)) all_edge_idx = list(range(edges.shape[0])) np.random.shuffle(all_edge_idx) val_edge_idx = all_edge_idx[:num_val] test_edge_idx = all_edge_idx[num_val:(num_val + num_test)] test_edges = edges[test_edge_idx] val_edges = edges[val_edge_idx] train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0) def ismember(a, b, tol=5): rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1) return np.any(rows_close) test_edges_false = [] while len(test_edges_false) < len(test_edges): idx_i = np.random.randint(0, adj.shape[0]) idx_j = np.random.randint(0, adj.shape[0]) if idx_i == idx_j: continue if ismember([idx_i, idx_j], edges_all): continue if test_edges_false: if ismember([idx_j, idx_i], np.array(test_edges_false)): continue if ismember([idx_i, idx_j], np.array(test_edges_false)): continue test_edges_false.append([idx_i, idx_j]) val_edges_false = [] while len(val_edges_false) < len(val_edges): idx_i = np.random.randint(0, adj.shape[0]) idx_j = np.random.randint(0, adj.shape[0]) if idx_i == idx_j: continue if ismember([idx_i, idx_j], train_edges): continue if ismember([idx_j, idx_i], train_edges): continue if ismember([idx_i, idx_j], val_edges): continue if ismember([idx_j, idx_i], val_edges): continue if val_edges_false: if ismember([idx_j, idx_i], np.array(val_edges_false)): continue if ismember([idx_i, idx_j], np.array(val_edges_false)): continue val_edges_false.append([idx_i, idx_j]) data = np.ones(train_edges.shape[0]) # Re-build adj matrix adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape) adj_train = adj_train + adj_train.T # NOTE: these edge lists only contain single direction of edge! return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false def preprocess_graph(adj): adj = sp.coo_matrix(adj) adj_ = adj + sp.eye(adj.shape[0]) rowsum = np.array(adj_.sum(1)) degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten()) adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo() return sparse_mx_to_torch_sparse_tensor(adj_normalized) def sparse_mx_to_torch_sparse_tensor(sparse_mx): \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\" sparse_mx = sparse_mx.tocoo().astype(np.float64) indices = torch.from_numpy( np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)) values = torch.from_numpy(sparse_mx.data) shape = torch.Size(sparse_mx.shape) return torch.sparse.DoubleTensor(indices, values, shape) def get_roc_score(emb, adj_orig, edges_pos, edges_neg): def sigmoid(x): x = np.float128(x) return 1 / (1 + np.exp(-x)) # Predict on test set of edges adj_rec = np.dot(emb, emb.T) preds = [] pos = [] for e in edges_pos: preds.append(sigmoid(adj_rec[e[0], e[1]])) pos.append(adj_orig[e[0], e[1]]) preds_neg = [] neg = [] for e in edges_neg: preds_neg.append(sigmoid(adj_rec[e[0], e[1]])) neg.append(adj_orig[e[0], e[1]]) preds_all = np.hstack([preds, preds_neg]) labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds))]) p = np.array([1 if p>0.5 else 0 for p in preds_all]).reshape((-1,1)) y = labels_all.reshape((-1,1)) roc_score = roc_auc_score(labels_all, preds_all) ap_score = average_precision_score(labels_all, preds_all) c = confusion_matrix(y, p, normalize='true') # print(\"confusion matrix: \", c) tn, fp, fn, tp = c.ravel() # print(\"tn: \", tn, \"fp: \", fp, \"fn: \", fn, \"tp: \", tp) balanced_acc = (tp+tn)/2 f1_sco = f1_score(y, p, average='weighted') return roc_score, ap_score, balanced_acc, f1_sco"
    }
}
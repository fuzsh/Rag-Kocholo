{
    "id": "correct_subsidiary_00116_0",
    "rank": 7,
    "data": {
        "url": "https://www.cisco.com/c/en/us/td/docs/voice_ip_comm/cucm/srnd/collab10/collab10/netstruc.html",
        "read_more_link": "",
        "language": "en",
        "title": "Cisco Collaboration System 10.x Solution Reference Network Designs (SRND)",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.cisco.com/etc/designs/cdc/fw/i/ic_clear_gray.png",
            "https://www.cisco.com/etc/designs/cdc/fw/i/ic_clear_gray.png",
            "https://www.cisco.com/c/dam/en/us/td/i/000001-100000/75001-80000/77001-78000/77290.ps/_jcr_content/renditions/77290.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/200001-300000/250001-260000/253001-254000/253921.eps/_jcr_content/renditions/253921.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/200001-300000/270001-280000/271001-272000/271569.eps/_jcr_content/renditions/271569.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/200001-300000/270001-280000/271001-272000/271570.eps/_jcr_content/renditions/271570.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/200001-300000/250001-260000/253001-254000/253919.eps/_jcr_content/renditions/253919.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/200001-300000/250001-260000/253001-254000/253920.eps/_jcr_content/renditions/253920.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/caut.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/100001-200000/110001-120000/114001-115000/114469.ps/_jcr_content/renditions/114469.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/200001-300000/290001-300000/292001-293000/292585.eps/_jcr_content/renditions/292585.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/100001-200000/190001-200000/191001-192000/191938.eps/_jcr_content/renditions/191938.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/100001-200000/190001-200000/191001-192000/191939.eps/_jcr_content/renditions/191939.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/100001-200000/150001-160000/153001-154000/153371.ps/_jcr_content/renditions/153371.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/000001-100000/75001-80000/77001-78000/77295.ps/_jcr_content/renditions/77295.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/000001-100000/75001-80000/77001-78000/77296.ps/_jcr_content/renditions/77296.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/200001-300000/250001-260000/253001-254000/253922.eps/_jcr_content/renditions/253922.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/000001-100000/75001-80000/77001-78000/77298.ps/_jcr_content/renditions/77298.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/000001-100000/75001-80000/77001-78000/77291.ps/_jcr_content/renditions/77291.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/000001-100000/75001-80000/77001-78000/77292.ps/_jcr_content/renditions/77292.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/100001-200000/110001-120000/114001-115000/114470.ps/_jcr_content/renditions/114470.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/200001-300000/280001-290000/284001-285000/284266.eps/_jcr_content/renditions/284266.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/200001-300000/280001-290000/284001-285000/284267.eps/_jcr_content/renditions/284267.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/300001-400000/340001-350000/348001-349000/348679.eps/_jcr_content/renditions/348679.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/200001-300000/280001-290000/284001-285000/284268.eps/_jcr_content/renditions/284268.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/cdc/i/Feedback_OceanBlue.png",
            "https://www.cisco.com/etc/designs/cdc/fw/i/icon_lock_small.png",
            "https://cisco.112.2o7.net/b/ss/cisco-mobile/5/12345"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2015-06-10T12:11:25",
        "summary": "",
        "meta_description": "Network Infrastructure chapter of the Cisco Collaboration System Release (CSR) 10.x SRND.",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "Cisco",
        "canonical_link": "https://www.cisco.com/c/en/us/td/docs/voice_ip_comm/cucm/srnd/collab10/collab10/netstruc.html",
        "text": "LAN Infrastructure\n\nCampus LAN infrastructure design is extremely important for proper Unified Communications operation on a converged network. Proper LAN infrastructure design requires following basic configuration and design best practices for deploying a highly available network. Further, proper LAN infrastructure design requires deploying end-to-end QoS on the network. The following sections discuss these requirements:\n\nLAN Design for High Availability\n\nLAN Quality of Service (QoS)\n\nLAN Design for High Availability\n\nProperly designing a LAN requires building a robust and redundant network from the top down. By structuring the LAN as a layered model (see Figure 3-1) and developing the LAN infrastructure one step of the model at a time, you can build a highly available, fault tolerant, and redundant network. Once these layers have been designed correctly, you can add network services such as DHCP and TFTP to provide additional network functionality. The following sections examine the infrastructure layers and network services:\n\nCampus Access Layer\n\nCampus Distribution Layer\n\nCampus Core Layer\n\nNetwork Services\n\nFor more information on campus design, refer to the Design Zone for Campus at\n\nhttp://www.cisco.com/go/designzone\n\nCampus Access Layer\n\nThe access layer of the Campus LAN includes the portion of the network from the desktop port(s) to the wiring closet switch. Access layer switches have traditionally been configured as Layer 2 devices with Layer 2 uplinks to the distribution layer. The Layer 2 and spanning tree recommendations for Layer 2 access designs are well documented and are discussed briefly below. For newer Cisco Catalyst switches supporting Layer 3 protocols, new routed access designs are possible and offer improvements in convergence times and design simplicity. Routed access designs are discussed in the section on Routed Access Layer Designs.\n\nLayer 2 Access Design Recommendations\n\nProper access layer design starts with assigning a single IP subnet per virtual LAN (VLAN). Typically, a VLAN should not span multiple wiring closet switches; that is, a VLAN should have presence in one and only one access layer switch (see Figure 3-2). This practice eliminates topological loops at Layer 2, thus avoiding temporary flow interruptions due to Spanning Tree convergence. However, with the introduction of standards-based IEEE 802.1w Rapid Spanning Tree Protocol (RSTP) and 802.1s Multiple Instance Spanning Tree Protocol (MISTP), Spanning Tree can converge at much higher rates. More importantly, confining a VLAN to a single access layer switch also serves to limit the size of the broadcast domain. There is the potential for large numbers of devices within a single VLAN or broadcast domain to generate large amounts of broadcast traffic periodically, which can be problematic. A good rule of thumb is to limit the number of devices per VLAN to about 512, which is equivalent to two Class C subnets (that is, a 23-bit subnet masked Class C address). For more information on the campus access layer, refer to the documentation on available at http://www.cisco.com/en/US/products/hw/switches/index.html.\n\nNote The recommendation to limit the number of devices in a single Unified Communications VLAN to approximately 512 is not solely due to the need to control the amount of VLAN broadcast traffic. Installing Unified CM in a VLAN with an IP subnet containing more than 1024 devices can cause the Unified CM server ARP cache to fill up quickly, which can seriously affect communications between the Unified CM server and other Unified Communications endpoints.\n\nFigure 3-2 Access Layer Switches and VLANs for Voice and Data\n\nWhen you deploy voice, Cisco recommends that you enable two VLANs at the access layer: a native VLAN for data traffic (VLANs 10, 11, 30, 31, and 32 in Figure 3-2) and a voice VLAN under Cisco IOS or Auxiliary VLAN under CatOS for voice traffic (represented by VVIDs 110, 111, 310, 311, and 312 in Figure 3-2).\n\nSeparate voice and data VLANs are recommended for the following reasons:\n\nAddress space conservation and voice device protection from external networks\n\nPrivate addressing of phones on the voice or auxiliary VLAN ensures address conservation and ensures that phones are not accessible directly through public networks. PCs and servers are typically addressed with publicly routed subnet addresses; however, voice endpoints may be addressed using RFC 1918 private subnet addresses.\n\nQoS trust boundary extension to voice devices\n\nQoS trust boundaries can be extended to voice devices without extending these trust boundaries and, in turn, QoS features to PCs and other data devices.\n\nProtection from malicious network attacks\n\nVLAN access control, 802.1Q, and 802.1p tagging can provide protection for voice devices from malicious internal and external network attacks such as worms, denial of service (DoS) attacks, and attempts by data devices to gain access to priority queues through packet tagging.\n\nEase of management and configuration\n\nSeparate VLANs for voice and data devices at the access layer provide ease of management and simplified QoS configuration.\n\nTo provide high-quality voice and to take advantage of the full voice feature set, access layer switches should provide support for:\n\n802.1Q trunking and 802.1p for proper treatment of Layer 2 CoS packet marking on ports with phones connected\n\nMultiple egress queues to provide priority queuing of RTP voice packet streams\n\nThe ability to classify or reclassify traffic and establish a network trust boundary\n\nInline power capability (Although inline power capability is not mandatory, it is highly recommended for the access layer switches.)\n\nLayer 3 awareness and the ability to implement QoS access control lists (These features are recommended if you are using certain Unified Communications endpoints such as a PC running a softphone application that cannot benefit from an extended trust boundary.)\n\nSpanning Tree Protocol (STP)\n\nTo minimize convergence times and maximize fault tolerance at Layer 2, enable the following STP features:\n\nPortFast\n\nEnable PortFast on all access ports. The phones, PCs, or servers connected to these ports do not forward bridge protocol data units (BPDUs) that could affect STP operation. PortFast ensures that the phone or PC, when connected to the port, is able to begin receiving and transmitting traffic immediately without having to wait for STP to converge.\n\nRoot guard or BPDU guard\n\nEnable root guard or BPDU guard on all access ports to prevent the introduction of a rogue switch that might attempt to become the Spanning Tree root, thereby causing STP re-convergence events and potentially interrupting network traffic flows. Ports that are set to errdisable state by BPDU guard must either be re-enabled manually or the switch must be configured to re-enable ports automatically from the errdisable state after a configured period of time.\n\nUplinkFast and BackboneFast\n\nEnable these features where appropriate to ensure that, when changes occur on the Layer 2 network, STP converges as rapidly as possible to provide high availability. When using Cisco stackable switches, enable Cross-Stack UplinkFast (CSUF) to provide fast failover and convergence if a switch in the stack fails.\n\nUniDirectional Link Detection (UDLD)\n\nEnable this feature to reduce convergence and downtime on the network when link failures or misbehaviors occur, thus ensuring minimal interruption of network service. UDLD detects, and takes out of service, links where traffic is flowing in only one direction. This feature prevents defective links from being mistakenly considered as part of the network topology by the Spanning Tree and routing protocols.\n\nNote With the introduction of RSTP 802.1w , features such as PortFast and UplinkFast are not required because these mechanisms are built in to this standard. If RSTP has been enabled on the Catalyst switch, these commands are not necessary.\n\nRouted Access Layer Designs\n\nFor campus designs requiring simplified configuration, common end-to-end troubleshooting tools, and the fastest convergence, a hierarchical design using Layer 3 switching in the access layer (routed access) in combination with Layer 3 switching at the distribution layer provides the fastest restoration of voice and data traffic flows.\n\nMigrating the L2/L3 Boundary to the Access Layer\n\nIn the typical hierarchical campus design, the distribution layer uses a combination of Layer 2, Layer 3, and Layer 4 protocols and services to provide for optimal convergence, scalability, security, and manageability. In the most common distribution layer configurations, the access switch is configured as a Layer 2 switch that forwards traffic on high-speed trunk ports to the distribution switches. The distribution switches are configured to support both Layer 2 switching on their downstream access switch trunks and Layer 3 switching on their upstream ports toward the core of the network, as shown in Figure 3-3.\n\nFigure 3-3 Traditional Campus Design — Layer 2 Access with Layer 3 Distribution\n\nThe purpose of the distribution switch in this design is to provide boundary functions between the bridged Layer 2 portion of the campus and the routed Layer 3 portion, including support for the default gateway, Layer 3 policy control, and all the multicast services required.\n\nAn alternative configuration to the traditional distribution layer model illustrated in Figure 3-3 is one in which the access switch acts as a full Layer 3 routing node (providing both Layer 2 and Layer 3 switching) and the access-to-distribution Layer 2 uplink trunks are replaced with Layer 3 point-to-point routed links. This alternative configuration, in which the Layer 2/3 demarcation is moved from the distribution switch to the access switch (as shown in Figure 3-4), appears to be a major change to the design but is actually just an extension of the current best-practice design.\n\nFigure 3-4 Routed Access Campus Design — Layer 3 Access with Layer 3 Distribution\n\nIn both the traditional Layer 2 and the Layer 3 routed access designs, each access switch is configured with unique voice and data VLANs. In the Layer 3 design, the default gateway and root bridge for these VLANs is simply moved from the distribution switch to the access switch. Addressing for all end stations and for the default gateway remains the same. VLAN and specific port configurations remain unchanged on the access switch. Router interface configuration, access lists, \"ip helper,\" and any other configuration for each VLAN remain identical but are configured on the VLAN Switched Virtual Interface (SVI) defined on the access switch instead of on the distribution switches.\n\nThere are several notable configuration changes associated with the move of the Layer 3 interface down to the access switch. It is no longer necessary to configure a Hot Standby Router Protocol (HSRP) or Gateway Load Balancing Protocol (GLBP) virtual gateway address as the \"router\" interfaces because all the VLANs are now local. Similarly, with a single multicast router, for each VLAN it is not necessary to perform any of the traditional multicast tuning such as tuning PIM query intervals or ensuring that the designated router is synchronized with the active HSRP gateway.\n\nRouted Access Convergence\n\nThe many potential advantages of using a Layer 3 access design include the following:\n\nImproved convergence\n\nSimplified multicast configuration\n\nDynamic traffic load balancing\n\nSingle control plane\n\nSingle set of troubleshooting tools (for example, ping and traceroute)\n\nOf these advantages, perhaps the most significant is the improvement in network convergence times possible when using a routed access design configured with Enhanced Interior Gateway Routing Protocol (EIGRP) or Open Shortest Path First (OSPF) as the routing protocol. Comparing the convergence times for an optimal Layer 2 access design (either with a spanning tree loop or without a loop) against that of the Layer 3 access design, you can obtain a four-fold improvement in convergence times, from 800 to 900 msec for the Layer 2 design to less than 200 msec for the Layer 3 access design.\n\nFor more information on routed access designs, refer to the document on High Availability Campus Network Design – Routed Access Layer using EIGRP or OSPF, available at\n\nhttp://www.cisco.com/application/pdf/en/us/guest/netsol/ns432/c649/ccmigration_09186a0080811468.pdf\n\nCampus Distribution Layer\n\nThe distribution layer of the Campus LAN includes the portion of the network from the wiring closet switches to the next-hop switch. For more information on the campus distribution layer switches, refer to the product documentation available at\n\nhttp://www.cisco.com/en/US/products/hw/switches/index.html\n\nAt the distribution layer, it is important to provide redundancy to ensure high availability, including redundant links between the distribution layer switches (or routers) and the access layer switches. To avoid creating topological loops at Layer 2, use Layer 3 links for the connections between redundant Distribution switches when possible.\n\nFirst-Hop Redundancy Protocols\n\nIn the campus hierarchical model, where the distribution switches are the L2/L3 boundary, they also act as the default gateway for the entire L2 domain that they support. Some form of redundancy is required because this environment can be large and a considerable outage could occur if the device acting as the default gateway fails.\n\nGateway Load Balancing Protocol (GLBP), Hot Standby Router Protocol (HSRP), and Virtual Router Redundancy Protocol (VRRP) are all first-hop redundancy protocols. Cisco initially developed HSRP to address the need for default gateway redundancy. The Internet Engineering Task Force (IETF) subsequently ratified Virtual Router Redundancy Protocol (VRRP) as the standards-based method of providing default gateway redundancy. More recently, Cisco developed GLBP to overcome some the limitations inherent in both HSRP and VRRP.\n\nHSRP and VRRP with Cisco enhancements both provide a robust method of backing up the default gateway, and they can provide failover in less than one second to the redundant distribution switch when tuned properly.\n\nGateway Load Balancing Protocol (GLBP)\n\nLike HSRP and VRRP, Cisco's Gateway Load Balancing Protocol (GLBP) protects data traffic from a failed router or circuit, while also allowing packet load sharing between a group of redundant routers. When HSRP or VRRP are used to provide default gateway redundancy, the backup members of the peer relationship are idle, waiting for a failure event to occur for them to take over and actively forward traffic.\n\nBefore the development of GLBP, methods to utilize uplinks more efficiently were difficult to implement and manage. In one technique, the HSRP and STP/RSTP root alternated between distribution node peers, with the even VLANs homed on one peer and the odd VLANs homed on the alternate. Another technique used multiple HSRP groups on a single interface and used DHCP to alternate between the multiple default gateways. These techniques worked but were not optimal from a configuration, maintenance, or management perspective.\n\nGLBP is configured and functions like HSRP. For HSRP, a single virtual MAC address is given to the endpoints when they use Address Resolution Protocol (ARP) to learn the physical MAC address of their default gateways (see Figure 3-5).\n\nFigure 3-5 HSRP Uses One Virtual MAC Address\n\nTwo virtual MAC addresses exist with GLBP, one for each GLBP peer (see Figure 3-6). When an endpoint uses ARP to determine its default gateway, the virtual MAC addresses are checked in a round-robin basis. Failover and convergence work just like with HSRP. The backup peer assumes the virtual MAC address of the device that has failed, and begins forwarding traffic for its failed peer.\n\nFigure 3-6 GLBP Uses Two Virtual MAC Addresses, One for Each GLBP Peer\n\nThe end result is that a more equal utilization of the uplinks is achieved with minimal configuration. As a side effect, a convergence event on the uplink or on the primary distribution node affects only half as many hosts, giving a convergence event an average of 50 percent less impact.\n\nFor more information on HSRP, VRRP, and GLBP, refer to the Campus Network for High Availability Design Guide, available at\n\nhttp://www.cisco.com/application/pdf/en/us/guest/netsol/ns431/c649/ccmigration_09186a008093b876.pdf\n\nRouting Protocols\n\nConfigure Layer 3 routing protocols such as OSPF and EIGRP at the distribution layer to ensure fast convergence, load balancing, and fault tolerance. Use parameters such as routing protocol timers, path or link costs, and address summaries to optimize and control convergence times as well as to distribute traffic across multiple paths and devices. Cisco also recommends using the passive-interface command to prevent routing neighbor adjacencies via the access layer. These adjacencies are typically unnecessary, and they create extra CPU overhead and increased memory utilization because the routing protocol keeps track of them. By using the passive-interface command on all interfaces facing the access layer, you prevent routing updates from being sent out on these interfaces and, therefore, neighbor adjacencies are not formed.\n\nCampus Core Layer\n\nThe core layer of the Campus LAN includes the portion of the network from the distribution routers or Layer 3 switches to one or more high-end core Layer 3 switches or routers. Layer 3-capable Catalyst switches at the core layer can provide connectivity between numerous campus distribution layers. For more details on the campus core layer switches, refer to the documentation on available at http://www.cisco.com/en/US/products/hw/switches/index.html.\n\nAt the core layer, it is again very important to provide the following types of redundancy to ensure high availability:\n\nRedundant link or cable paths\n\nRedundancy here ensures that traffic can be rerouted around downed or malfunctioning links.\n\nRedundant devices\n\nRedundancy here ensures that, in the event of a device failure, another device in the network can continue performing tasks that the failed device was doing.\n\nRedundant device sub-systems\n\nThis type of redundancy ensures that multiple power supplies and modules are available within a device so that the device can continue to function in the event that one of these components fails.\n\nThe Cisco Catalyst switches with Virtual Switching System (VSS) is a method to ensure redundancy in all of these areas by pooling together two Catalyst supervisor engines to act as one. For more information regarding VSS, refer to the product documentation available at\n\nhttp://www.cisco.com/en/US/products/ps9336/index.html\n\nRouting protocols at the core layer should again be configured and optimized for path redundancy and fast convergence. There should be no STP in the core because network connectivity should be routed at Layer 3. Finally, each link between the core and distribution devices should belong to its own VLAN or subnet and be configured using a 30-bit subnet mask.\n\nData Center and Server Farm\n\nTypically, Cisco Unified Communications Manager (Unified CM) cluster servers, including media resource servers, reside in a firewall-secured data center or server farm environment. In addition, centralized gateways and centralized hardware media resources such as conference bridges, DSP or transcoder farms, and media termination points may be located in the data center or server farm. The placement of firewalls in relation to Cisco Unified Communications Manager (Unified CM) cluster servers and media resources can affect how you design and implement security in your network. For design guidance on firewall placement in relation to Unified Communications systems and media resources, see Firewalls.\n\nBecause these servers and resources are critical to voice networks, Cisco recommends distributing all Unified CM cluster servers, centralized voice gateways, and centralized hardware resources between multiple physical switches and, if possible, multiple physical locations within the campus. This distribution of resources ensures that, given a hardware failure (such as a switch or switch line card failure), at least some servers in the cluster will still be available to provide telephony services. In addition, some gateways and hardware resources will still be available to provide access to the PSTN and to provide auxiliary services. Besides being physically distributed, these servers, gateways, and hardware resources should be distributed among separate VLANs or subnets so that, if a broadcast storm or denial of service attack occurs on a particular VLAN, not all voice connectivity and services will be disrupted.\n\nPower over Ethernet (PoE)\n\nPoE (or inline power) is 48 Volt DC power provided over standard Ethernet unshielded twisted-pair (UTP) cable. Instead of using wall power, IP phones and other inline powered devices (PDs) such as the Aironet Wireless Access Points can receive power provided by inline power-capable Catalyst Ethernet switches or other inline power source equipment (PSE). Inline power is enabled by default on all inline power-capable Catalyst switches.\n\nDeploying inline power-capable switches with uninterruptible power supplies (UPS) ensures that IP phones continue to receive power during power failure situations. Provided the rest of the telephony network is available during these periods of power failure, then IP phones should be able to continue making and receiving calls. You should deploy inline power-capable switches at the campus access layer within wiring closets to provide inline-powered Ethernet ports for IP phones, thus eliminating the need for wall power.\n\nCaution The use of power injectors or power patch panels to deliver PoE can damage some devices because power is always applied to the Ethernet pairs. PoE switch ports automatically detect the presence of a device that requires PoE before enabling it on a port-by-port basis.\n\nIn addition to Cisco PoE inline power, Cisco now supports the IEEE 802.3af PoE and the IEEE 802.3at Enhanced PoE standards. For information on which Cisco Unified IP Phones support the 802.3af and 802.3at standards, refer to the product documentation for your particular phone models.\n\nEnergy Conservation for IP Phones\n\nCisco EnergyWise Technology provides intelligent management of energy usage for devices on the IP network, including Unified Communications endpoints that use Power over Ethernet (PoE). Cisco EnergyWise architecture can turn power on and off to devices connected with PoE on EnergyWise enabled switches, based on a configurable schedule. For more information on EnergyWise, refer to the documentation at\n\nhttp://www.cisco.com/en/US/products/ps10195/index.html\n\nWhen the PoE switch powers off IP phones for EnergyWise conservation, the phones are completely powered down. EnergyWise shuts down inline power on the ports that connect to IP phones and does so by a schedule or by commands from network management tools. When power is disabled, no verification occurs to determine whether a phone has an active call. The power is turned off and any active call is torn down. The IP phone loses registration from Cisco Unified Communications Manager and no calls can be made to or from the phone. There is no mechanism on the phone to power it on, therefore emergency calling will not be available on that phone.\n\nThe IP phone can be restarted only when the switch powers it on again. After power is restored, the IP phones will reboot and undergo a recovery process that includes requesting a new IP address, downloading a configuration file, applying any new configuration parameters, downloading new firmware or locales, and registering with Cisco Unified CM.\n\nThe EnergyWise schedule is configured and managed on the Cisco Network Infrastructure. It does not require any configuration on the IP phone or on Cisco Unified CM. However, power consumption on the phone can also be managed by a device profile configured on Unified CM. The energy saving options provided by Unified CM include the following:\n\nPower Save Plus Mode\n\nPower Save Mode\n\nPower Save Plus Mode\n\nIn Power Save Plus mode, the phone on and off times and the idle timeout periods can be configured on the IP phones. The Cisco IP Phones' EnergyWise Power Save Plus configuration options specify the schedule for the IP phones to sleep (power down) and wake (power up). This mode requires an EnergyWise enabled network. If EnergyWise is enabled, then the sleep and wake times, as well as other parameters, can be used to control power to the phones. The Power Save Plus parameters are configured in the product-specific device profile in Cisco Unified CM Administration and sent to the IP phones as part of the phone configuration XML file.\n\nDuring the configured power off period in this power saving mode, the IP phone sends a request to the switch asking for a wake-up at a specified time. If the switch is EnergyWise enabled, it accepts the request and reduces the power to the phone port, putting the phone to sleep. The sleep mode reduces the power consumption of the phone to 1 watt or less. The phone is not completely powered off in this case. When the phone is sleeping, the PoE switch provides minimal power that illuminates the Select key on the phone. A user can wake up the IP phone by using the Select button. The IP phone does not go into sleep mode if a call is active on the phone. Audio and visual alerts can optionally be configured to warn users before a phone enters the Power Save Plus mode. While the phone is in sleep mode, it is not registered to Cisco Unified CM and cannot receive any inbound calls. Use the Forward Unregistered setting in the phone's device configuration profile to specify how to treat any inbound calls to the phone's number.\n\nNote The Cisco EnergyWise Power Save Plus mode is supported in Unified CM 8.6 and later releases, and it requires phone firmware version 9.(2)1 or later. It is available on the Cisco Unified IP Phone 6900, 8900, and 9900 Series.\n\nPower Save Mode\n\nIn Power Save mode, the backlight on the screen is not lit when the phone is not in use. The phone stays registered to Cisco Unified CM in this mode and can receive inbound calls and make outbound calls. Cisco Unified CM Administration has product-specific configuration options to turn off the display at a designated time on some days and all day on other days. The phone remains in Power Save mode for the scheduled duration or until the user lifts the handset or presses any button. An EnergyWise enabled network is not required for the Power Save mode. Idle times can be scheduled so that the display remains on until the timeout and then turns off automatically. The phone is still powered on in this mode and can receive inbound calls.\n\nThe Power Save mode can be used together with the Power Save Plus mode. Using both significantly reduces the total power consumption by Cisco Unified IP Phones.\n\nFor information on configuring these modes, refer to the administration guides for the Cisco Unified IP Phones, available at the following locations:\n\nCisco Unified IP Phones 9900 Series\n\nhttp://www.cisco.com/en/US/products/ps10453/prod_maintenance_guides_list.html\n\nCisco Unified IP Phones 8900 Series\n\nhttp://www.cisco.com/en/US/products/ps10451/prod_maintenance_guides_list.html\n\nCisco Unified IP Phones 6900 Series\n\nhttp://www.cisco.com/en/US/products/ps10326/prod_maintenance_guides_list.html\n\nLAN Quality of Service (QoS)\n\nUntil recently, quality of service was not an issue in the enterprise campus due to the asynchronous nature of data traffic and the ability of network devices to tolerate buffer overflow and packet loss. However, with new applications such as voice and video, which are sensitive to packet loss and delay, buffers and not bandwidth are the key QoS issue in the enterprise campus.\n\nFigure 3-7 illustrates the typical oversubscription that occurs in LAN infrastructures.\n\nFigure 3-7 Data Traffic Oversubscription in the LAN\n\nThis oversubscription, coupled with individual traffic volumes and the cumulative effects of multiple independent traffic sources, can result in the egress interface buffers becoming full instantaneously, thus causing additional packets to drop when they attempt to enter the egress buffer. The fact that campus switches use hardware-based buffers, which compared to the interface speed are much smaller than those found on WAN interfaces in routers, merely increases the potential for even short-lived traffic bursts to cause buffer overflow and dropped packets.\n\nApplications such as file sharing (both peer-to-peer and server-based), remote networked storage, network-based backup software, and emails with large attachments, can create conditions where network congestion occurs more frequently and/or for longer durations. Some of the negative effects of recent worm attacks have been an overwhelming volume of network traffic (both unicast and broadcast-storm based), increasing network congestion. If no buffer management policy is in place, loss, delay, and jitter performance of the LAN may be affected for all traffic.\n\nAnother situation to consider is the effect of failures of redundant network elements, which cause topology changes. For example, if a distribution switch fails, all traffic flows will be reestablished through the remaining distribution switch. Prior to the failure, the load balancing design shared the load between two switches, but after the failure all flows are concentrated in a single switch, potentially causing egress buffer conditions that normally would not be present.\n\nFor applications such as voice, this packet loss and delay results in severe voice quality degradation. Therefore, QoS tools are required to manage these buffers and to minimize packet loss, delay, and delay variation (jitter).\n\nThe following types of QoS tools are needed from end to end on the network to manage traffic and ensure voice quality:\n\nTraffic classification\n\nClassification involves the marking of packets with a specific priority denoting a requirement for class of service (CoS) from the network. The point at which these packet markings are trusted or not trusted is considered the trust boundary. Trust is typically extended to voice devices (phones) and not to data devices (PCs).\n\nQueuing or scheduling\n\nInterface queuing or scheduling involves assigning packets to one of several queues based on classification for expedited treatment throughout the network.\n\nBandwidth provisioning\n\nProvisioning involves accurately calculating the required bandwidth for all applications plus element overhead.\n\nThe following sections discuss the use of these QoS mechanisms in a campus environment:\n\nTraffic Classification\n\nInterface Queuing\n\nBandwidth Provisioning\n\nImpairments to IP Communications if QoS is Not Employed\n\nTraffic Classification\n\nIt has always been an integral part of the Cisco network design architecture to classify or mark traffic as close to the edge of the network as possible. Traffic classification is an entrance criterion for access into the various queuing schemes used within the campus switches and WAN interfaces. Cisco IP Phones mark voice control signaling and voice RTP streams at the source, and they adhere to the values presented in Table 3-3 . As such, the IP phone can and should classify traffic flows.\n\nTable 3-3 lists the traffic classification requirements for the LAN infrastructure.\n\nTable 3-3 Traffic Classification Guidelines for Various Types of Network Traffic\n\nApplication\n\nLayer-3 Classification\n\nLayer-2 Classification\n\nType of Service (ToS)\n\nIP Precedence (IPP)\n\nPer-Hop Behavior (PHB)\n\nDifferentiated Services Code Point (DSCP)\n\nClass of Service (CoS)\n\nRouting\n\n6\n\nCS6\n\n48\n\n6\n\nVoice Real-Time Transport Protocol (RTP)\n\n5\n\nEF\n\n46\n\n5\n\nVideoconferencing\n\n4\n\nAF41\n\n34\n\n4\n\nIP video\n\n4\n\nAF41\n\n34\n\n4\n\nImmersive video\n\nReal-Time Interactive\n\n4\n\nCS4\n\n32\n\n4\n\nStreaming video\n\n3\n\nAF31\n\n26\n\n3\n\nCall signaling\n\n3\n\nCS3\n\n24\n\n3\n\nTransactional data\n\n2\n\nAF21\n\n18\n\n2\n\nNetwork management\n\n2\n\nCS2\n\n16\n\n2\n\nScavenger\n\n1\n\nCS1\n\n8\n\n1\n\nBest effort\n\n0\n\n0\n\n0\n\n0\n\nFor more information about traffic classification, refer to the Enterprise QoS Solution Reference Network Design (SRND), available at\n\nhttp://www.cisco.com/go/designzone\n\nTraffic Classification for Video Telephony\n\nThe main classes of interest for IP Video Telephony are:\n\nVoice\n\nVoice is classified as CoS 5 (IP Precedence 5, PHB EF, or DSCP 46).\n\nVideoconferencing\n\nVideoconferencing is classified as CoS 4 (IP Precedence 4, PHB AF41, or DSCP 34).\n\nCall signaling\n\nCall signaling for voice and videoconferencing is now classified as CoS 3 (IP Precedence 3, PHB CS3, or DSCP 24) but was previously classified as PHB AF31 or DSCP 26.\n\nCisco highly recommends these classifications as best practices in a Cisco Unified Communications network.\n\nQoS Marking Differences Between Video Calls and Voice-Only Calls\n\nThe voice component of a call can be classified in one of two ways, depending on the type of call in progress. A voice-only telephone call would have its media classified as CoS 5 (IP Precedence 5 or PHB EF), while the voice channel of a video conference would have its media classified as CoS 4 (IP Precedence 4 or PHB AF41). All the Cisco IP Video Telephony products adhere to the Cisco Corporate QoS Baseline standard, which requires that the audio and video channels of a video call both be marked as CoS 4 (IP Precedence 4 or PHB AF41). The reasons for this recommendation include, but are not limited to, the following:\n\nTo preserve lip-sync between the audio and video channels\n\nTo provide separate classes for audio-only calls and video calls\n\nThe signaling class is applicable to all voice signaling protocols (such as SCCP, MGCP, and so on) as well as video signaling protocols (such as SCCP, H.225, RAS, CAST, and so on).\n\nGiven the recommended classes, the first step is to decide where the packets will be classified (that is, which device will be the first to mark the traffic with its QoS classification). There are essentially two places to mark or classify traffic:\n\nOn the originating endpoint — the classification is then trusted by the upstream switches and routers\n\nOn the switches and/or routers — because the endpoint is either not capable of classifying its own packets or is not trustworthy to classify them correctly\n\nQoS Enforcement Using a Trusted Relay Point (TRP)\n\nA Trusted Relay Point (TRP) can be used to enforce and/or re-mark the DSCP values of media flows from endpoints. This feature allows QoS to be enforced for media from endpoints such as softphones, where the media QoS values might have been modified locally.\n\nA TRP is a media resource based upon the existing Cisco IOS media termination point (MTP) function.\n\nEndpoints can be configured to \"Use Trusted Relay Point,\" which will invoke a TRP for all calls.\n\nFor QoS enforcement, the TRP uses the configured QoS values for media in Unified CM's Service Parameters to re-mark and enforce the QoS values in media streams from the endpoint.\n\nTRP functionality is supported by Cisco IOS MTPs and transcoding resources. (Use Unified CM to check \"Enable TRP\" on the MTP or transcoding resource to activate TRP functionality.)\n\nInterface Queuing\n\nAfter packets have been marked with the appropriate tag at Layer 2 (CoS) and Layer 3 (DSCP or PHB), it is important to configure the network to schedule or queue traffic based on this classification, so as to provide each class of traffic with the service it needs from the network. By enabling QoS on campus switches, you can configure all voice traffic to use separate queues, thus virtually eliminating the possibility of dropped voice packets when an interface buffer fills instantaneously.\n\nAlthough network management tools may show that the campus network is not congested, QoS tools are still required to guarantee voice quality. Network management tools show only the average congestion over a sample time span. While useful, this average does not show the congestion peaks on a campus interface.\n\nTransmit interface buffers within a campus tend to congest in small, finite intervals as a result of the bursty nature of network traffic. When this congestion occurs, any packets destined for that transmit interface are dropped. The only way to prevent dropped voice traffic is to configure multiple queues on campus switches. For this reason, Cisco recommends always using a switch that has at least two output queues on each port and the ability to send packets to these queues based on QoS Layer 2 and/or Layer 3 classification. The majority of Cisco Catalyst Switches support two or more output queues per port. For more information on Cisco Catalyst Switch interface queuing capabilities, refer to the documentation at http://www.cisco.com/en/US/products/hw/switches/index.html\n\nBandwidth Provisioning\n\nIn the campus LAN, bandwidth provisioning recommendations can be summarized by the motto, Over provision and under subscribe. This motto implies careful planning of the LAN infrastructure so that the available bandwidth is always considerably higher than the load and there is no steady-state congestion over the LAN links.\n\nThe addition of voice traffic onto a converged network does not represent a significant increase in overall network traffic load; the bandwidth provisioning is still driven by the demands of the data traffic requirements. The design goal is to avoid extensive data traffic congestion on any link that will be traversed by telephony signaling or media flows. Contrasting the bandwidth requirements of a single G.711 voice call (approximately 86 kbps) to the raw bandwidth of a FastEthernet link (100 Mbps) indicates that voice is not a source of traffic that causes network congestion in the LAN, but rather it is a traffic flow to be protected from LAN network congestion.\n\nImpairments to IP Communications if QoS is Not Employed\n\nIf QoS is not deployed, packet drops and excessive delay and jitter can occur, leading to impairments of the telephony services. When media packets are subjected to drops, delay, and jitter, the user-perceivable effects include clicking sound, harsh-sounding voice, extended periods of silence, and echo.\n\nWhen signaling packets are subjected to the same conditions, user-perceivable impairments include unresponsiveness to user input (such as delay to dial tone), continued ringing upon answer, and double dialing of digits due to the user's belief that the first attempt was not effective (thus requiring hang-up and redial). More extreme cases can include endpoint re-initialization, call termination, and the spurious activation of SRST functionality at branch offices (leading to interruption of gateway calls).\n\nThese effects apply to all deployment models. However, single-site (campus) deployments tend to be less likely to experience the conditions caused by sustained link interruptions because the larger quantity of bandwidth typically deployed in LAN environments (minimum links of 100 Mbps) allows for some residual bandwidth to be available for the IP Communications system.\n\nIn any WAN-based deployment model, traffic congestion is more likely to produce sustained and/or more frequent link interruptions because the available bandwidth is much less than in a LAN (typically less than 2 Mbps), so the link is more easily saturated. The effects of link interruptions can impact the user experience, whether or not the voice media traverses the packet network, because signaling traffic between endpoints and the Unified CM servers can also be delayed or dropped.\n\nQoS Design Considerations for Virtual Unified Communications with Cisco UCS Servers\n\nUnified Communications applications such as Cisco Unified Communications Manager (Unified CM) run as virtual machines on top of the VMware Hypervisor. These Unified Communications virtual machines are connected to a virtual software switch rather than a hardware-based Ethernet. The following types of virtual software switches are available:\n\nVMware vSphere Standard Switch\n\nAvailable with all VMware vSphere editions and independent of the type of VMware licensing scheme. The vSphere Standard Switch exists only on the host on which it is configured.\n\nVMware vSphere Distributed Switch\n\nAvailable only with the Enterprise Plus Edition of VMware vSphere. The vSphere Distributed Switch acts as a single switch across all associated hosts on a datacenter and helps simplify manageability of the software virtual switch.\n\nCisco Nexus 1000V Switch\n\nCisco has a software switch called the Nexus 1000 Virtual (1000V) Switch. The Cisco Nexus 1000V requires the Enterprise Plus Edition of VMware vSphere. It is a distributed virtual switch visible to multiple VMware hosts and virtual machines. The Cisco Nexus 1000V Series provides policy-based virtual machine connectivity, mobile virtual machine security, enhanced QoS, and network policy.\n\nFrom the point of view of virtual connectivity, each virtual machine can connect to any one of the above virtual switches residing on a blade server. When using Cisco UCS B-Series blade servers, the blade servers physically connect to the rest of the network through a Fabric Extender in the UCS chassis to a UCS Fabric Interconnect Switch (for example, Cisco UCS 6100 or 6200 Series). The UCS Fabric Interconnect Switch is where the physical wiring connects to a customer's Ethernet LAN and FC SAN.\n\nFrom the point of view of traffic flow, traffic from the virtual machines first goes to the software virtual switch (for example, vSphere Standard Switch, vSphere Distributed Switch, or Cisco Nexus 1000V Switch). The virtual switch then sends the traffic to the physical UCS Fabric Interconnect Switch through its blade server's Network Adapter and Fabric Extender. The UCS Fabric Interconnect Switch carries both the IP and fibre channel SAN traffic via Fibre Channel over Ethernet (FCoE) on a single wire. The UCS Fabric Interconnect Switch sends IP traffic to an IP switch (for example, Cisco Catalyst or Nexus Series Switch), and it sends SAN traffic to a Fibre Channel SAN Switch (for example, Cisco MDS Series Switch).\n\nCongestion Scenario\n\nIn a deployment with Cisco UCS B-Series blades servers and with Cisco Collaboration applications only, network congestion or an oversubscription scenario is unlikely because the UCS Fabric Interconnect Switch provides a high-capacity switching fabric, and the usable bandwidth per server blade far exceeds the maximum traffic requirements of a typical Collaboration application.\n\nHowever, there might be scenarios where congestion could arise. For example, with a large number of B-Series blade servers and chassis, a large number of applications, and/or third-party applications requiring high network bandwidth, there is a potential for congestion on the different network elements of the UCS B-Series system (adapters, IO modules, Fabric Interconnects). In addition, FCoE traffic is sharing the same network elements as IP traffic, therefore applications performing a high amount of storage transfer would increase the utilization on the network elements and contribute to this potential congestion.\n\nTo address this potential congestion, QoS should be implemented.\n\nQoS Implementation with Cisco UCS B-Series\n\nCisco UCS Fabric Interconnect Switches and adapters such as the Cisco VIC adapter perform QoS based on Layer 2 CoS values. Traffic types are classified by CoS value into QoS system classes that determine, for example, the minimum amount of bandwidth guaranteed and the packet drop policy to be used for each class. However, Cisco Collaboration applications perform QoS marking at Layer 3 only, not at the Layer 2. Hence the need for mapping the L3 values used by the applications to the L2 CoS values used by the Cisco UCS elements.\n\nThe VMware vSphere Standard Switch, vSphere Distributed Switch, Cisco UCS Fabric Interconnect switches, and other UCS network elements do not have the ability to perform this mapping between L3 and L2 values. Use the Cisco Nexus 1000V, which like the traditional Cisco switches, can perform this mapping. For example, the Nexus 1000V can map PHB EF (real-time media traffic) to CoS 5 and PHB CS3 (voice/video signaling traffic) to CoS 3.\n\nNote Fibre Channel over Ethernet (FCoE) traffic has a reserved QoS system class that should not be used by any other type of traffic. By default, this system class has a CoS value of 3, which is the same value assigned to the system class used by voice and video signaling traffic in the example above. To prevent voice and video signaling traffic from using the FCoE system class, assign a different CoS value to the FCoE system class (2 or 4, for instance).\n\nNote The decision to use the Nexus 1000V will vary on a case-by-case basis, depending on the available bandwidth for Unified Communications applications within the UCS architecture. If there is a possibility that a congestion scenario will arise, then the Nexus 1000V switch should be deployed.\n\nIf the Nexus 1000V is not deployed, it is still possible to provide some QoS, but it would not be an optimal solution. For example, you could create multiple virtual switches and assign a different CoS value for the uplink ports of each of those switches. For example, virtual switch 1 would have uplink ports configured with a CoS value of 1, virtual switch 2 would have uplink ports configured with a CoS value of 2, and so forth. Then the application virtual machines would be assigned to a virtual switch, depending on the desired QoS system class. The downside to this approach is that all traffic types from a virtual machine will have the same CoS value. For example, with a Unified CM virtual machine, real-time media traffic such as MoH traffic, signaling traffic, and non-voice traffic (for example, backups, CDRs, logs, Web traffic, and so forth) would share the same CoS value.\n\nQoS Design Considerations for Video\n\nCisco recommends using different DSCP markings for different video applications. Unified CM 9. x provides support for different DSCP markings for immersive video traffic and videoconferencing (IP video telephony) traffic. By default, Unified CM 9. x has preconfigured the recommended DSCP values for TelePresence (immersive video) calls at CS4 and video (IP video telephony) calls at AF41. Figure 3-8 depicts the different video applications in a converged environment using the recommended DSCP values.\n\nFigure 3-8 Recommended QoS Traffic Markings in a Converged Network\n\nCalculating Overhead for QoS\n\nUnlike voice, real-time IP video traffic in general is a somewhat bursty, variable bit rate stream. Therefore video, unlike voice, does not have clear formulas for calculating network overhead because video packet sizes and rates vary proportionally to the degree of motion within the video image itself. From a network administrator's point of view, bandwidth is always provisioned at Layer 2, but the variability in the packet sizes and the variety of Layer 2 media that the packets may traverse from end-to-end make it difficult to calculate the real bandwidth that should be provisioned at Layer 2. However, the conservative rule that has been thoroughly tested and widely used is to over-provision video bandwidth by 20%. This accommodates the 10% burst and the network overhead from Layer 2 to Layer 4.\n\nNetwork Services\n\nThe deployment of an IP Communications system requires the coordinated design of a well structured, highly available, and resilient network infrastructure as well as an integrated set of network services including Domain Name System (DNS), Dynamic Host Configuration Protocol (DHCP), Trivial File Transfer Protocol (TFTP), and Network Time Protocol (NTP).\n\nDomain Name System (DNS)\n\nDNS enables the mapping of host names and network services to IP addresses within a network or networks. DNS server(s) deployed within a network provide a database that maps network services to hostnames and, in turn, hostnames to IP addresses. Devices on the network can query the DNS server and receive IP addresses for other devices in the network, thereby facilitating communication between network devices.\n\nA complete collaboration solution relies on DNS in order to function correctly for a number of services and thus requires a highly available DNS structure in place. For basic IP telephony deployments where reliance on DNS is not desired, Unified CM can be configured to support and ensure communication between Unified CM(s), gateways, and endpoint devices using IP addresses rather than hostnames.\n\nDeploying Unified CM without DNS\n\nFor basic IP telephony deployments where DNS is not desired, Cisco recommends that you configure Unified CM(s), gateways, and endpoint devices to use IP addresses rather than hostnames. This should be done during installation of the Unified CM cluster. During installation of the publisher and subscriber nodes, Cisco recommends that you do not select the option to enable DNS. After the initial installation of the publisher node in a Unified CM cluster, the publisher will be referenced in the server table by the hostname you provided for the system. Before installation and configuration of any subsequent subscriber nodes or the definition of any endpoints, you should change this server entry to the IP address of the publisher node rather than the hostname. Each subscriber node added to the cluster should be defined in this same server table by IP address and not by hostname. Each subscriber node should be added to this server table one device at a time, and there should be no definitions for non-existent subscriber nodes at any time other than for the new subscriber node being installed.\n\nDeploying Unified CM with DNS\n\nYou should always deploy DNS servers in a geographically redundant fashion so that a single DNS server failure will not prevent network communications between IP telephony devices. By providing DNS server redundancy in the event of a single DNS server failure, you ensure that devices relying on DNS to communicate on the network can still receive hostname-to-IP-address mappings from a backup or secondary DNS server.\n\nUnified CM can use DNS to:\n\nProvide simplified system management\n\nResolve fully qualified domain names to IP addresses for trunk destinations\n\nResolve fully qualified domain names to IP addresses for SIP route patterns based on domain name\n\nResolve service (SRV) records to host names and then to IP addresses for SIP trunk destinations\n\nProvide certificate-based security\n\nCollaboration clients use DNS for:\n\nSingle Sign-On (SSO)\n\nJabber deployments requiring user registration auto-discovery\n\nCertificate-based security for secure signaling and media\n\nWhen DNS is used, Cisco recommends defining each Unified CM cluster as a member of a valid sub-domain within the larger organizational DNS domain, defining the DNS domain on each Cisco Unified CM server, and defining the primary and secondary DNS server addresses on each Unified CM server.\n\nTable 3-4 shows an example of how DNS server could use A records (Hostname-to-IP-address resolution), Cname records (aliases), and SRV records (service records for redundancy, load balancing, and service discovery) in a Unified CM environment.\n\nTable 3-4 Example Use of DNS with Unified CM\n\nFor Jabber clients, refer to the Cisco Jabber DNS Configuration Guide, available at\n\nhttp://www.cisco.com/web/products/voice/jabber.html\n\nDynamic Host Configuration Protocol (DHCP)\n\nDHCP is used by hosts on the network to obtain initial configuration information, including IP address, subnet mask, default gateway, and TFTP server address. DHCP eases the administrative burden of manually configuring each host with an IP address and other configuration information. DHCP also provides automatic reconfiguration of network configuration when devices are moved between subnets. The configuration information is provided by a DHCP server located in the network, which responds to DHCP requests from DHCP-capable clients.\n\nYou should configure IP Communications endpoints to use DHCP to simplify deployment of these devices. Any RFC 2131 compliant DHCP server can be used to provide configuration information to IP Communications network devices. When deploying IP telephony devices in an existing data-only network, all you have to do is add DHCP voice scopes to an existing DHCP server for these new voice devices. Because IP telephony devices are configured to use and rely on a DHCP server for IP configuration information, you must deploy DHCP servers in a redundant fashion. At least two DHCP servers should be deployed within the telephony network such that, if one of the servers fails, the other can continue to answer DHCP client requests. You should also ensure that DHCP server(s) are configured with enough IP subnet addresses to handle all DHCP-reliant clients within the network.\n\nDHCP Option 150\n\nIP telephony endpoints can be configured to rely on DHCP Option 150 to identify the source of telephony configuration information, available from a server running the Trivial File Transfer Protocol (TFTP).\n\nIn the simplest configuration, where a single TFTP server is offering service to all deployed endpoints, Option 150 is delivered as a single IP address pointing to the system's designated TFTP server. The DHCP scope can also deliver two IP addresses under Option 150, for deployments where there are two TFTP servers within the same cluster. The phone would use the second address if it fails to contact the primary TFTP server, thus providing redundancy. To achieve both redundancy and load sharing between the TFTP servers, you can configure Option 150 to provide the two TFTP server addresses in reverse order for half of the DHCP scopes.\n\nNote If the primary TFTP server is available but is not able to grant the requested file to the phone (for example, because the requesting phone is not configured on that cluster), the phone will not attempt to contact the secondary TFTP server.\n\nCisco highly recommends using a direct IP address (that is, not relying on a DNS service) for Option 150 because doing so eliminates dependencies on DNS service availability during the phone boot-up and registration process.\n\nNote Even though IP phones support a maximum of two TFTP servers under Option 150, you could configure a Unified CM cluster with more than two TFTP servers. For instance, if a Unified CM system is clustered over a WAN at three separate sites, three TFTP servers could be deployed (one at each site). Phones within each site could then be granted a DHCP scope containing that site's TFTP server within Option 150. This configuration would bring the TFTP service closer to the endpoints, thus reducing latency and ensuring failure isolation between the sites (one site's failure would not affect TFTP service at another site).\n\nPhone DHCP Operation Following a Power Recycle\n\nIf a phone is powered down and comes back up while the DHCP server is still offline, it will attempt to use DHCP to obtain IP addressing information (as normal). In the absence of a response from a DHCP server, the phone will re-use the previously received DHCP information to register with Unified CM.\n\nDHCP Lease Times\n\nConfigure DHCP lease times as appropriate for the network environment. Given a fairly static network in which PCs and telephony devices remain in the same place for long periods of time, Cisco recommends longer DHCP lease times (for example, one week). Shorter lease times require more frequent renewal of the DHCP configuration and increase the amount of DHCP traffic on the network. Conversely, networks that incorporate large numbers of mobile devices, such as laptops and wireless telephony devices, should be configured with shorter DHCP lease times (for example, one day) to prevent depletion of DHCP-managed subnet addresses. Mobile devices typically use IP addresses for short increments of time and then might not request a DHCP renewal or new address for a long period of time. Longer lease times will tie up these IP addresses and prevent them from being reassigned even when they are no longer being used.\n\nCisco Unified IP Phones adhere to the conditions of the DHCP lease duration as specified in the DHCP server's scope configuration. Once half the lease time has expired since the last successful DHCP server acknowledgment, the IP phone will request a lease renewal. This DHCP client Request, once acknowledged by the DHCP server, will allow the IP phone to retain use of the IP scope (that is, the IP address, default gateway, subnet mask, DNS server (optional), and TFTP server (optional)) for another lease period. If the DHCP server becomes unavailable, an IP phone will not be able to renew its DHCP lease, and as soon as the lease expires, it will relinquish its IP configuration and will thus become unregistered from Unified CM until a DHCP server can grant it another valid scope.\n\nIn centralized call processing deployments, if a remote site is configured to use a centralized DHCP server (through the use of a DHCP relay agent such as the IP Helper Address in Cisco IOS) and if connectivity to the central site is severed, IP phones within the branch will not be able to renew their DHCP scope leases. In this situation, branch IP phones are at risk of seeing their DHCP lease expire, thus losing the use of their IP address, which would lead to service interruption. Given the fact that phones attempt to renew their leases at half the lease time, DHCP lease expiration can occur as soon as half the lease time since the DHCP server became unreachable. For example, if the lease time of a DHCP scope is set to 4 days and a WAN failure causes the DHCP server to be unavailable to the phones in a branch, those phones will be unable to renew their leases at half the lease time (in this case, 2 days). The IP phones could stop functioning as early as 2 days after the WAN failure, unless the WAN comes back up and the DHCP server is available before that time. If the WAN connectivity failure persists, all phones see their DHCP scope expire after a maximum of 4 days from the WAN failure.\n\nThis situation can be mitigated by one of the following methods:\n\nSet the DHCP scope lease to a long duration (for example, 8 days or more).\n\nThis method would give the system administrator a minimum of half the lease time to remedy any DHCP reachability problem. Long lease durations also have the effect of reducing the frequency of network traffic associated with lease renewals.\n\nConfigure co-located DHCP server functionality (for example, run a DHCP server function on the branch's Cisco IOS router).\n\nThis approach is immune to WAN connectivity interruption. One effect of such an approach is to decentralize the management of IP addresses, requiring incremental configuration efforts in each branch. (See DHCP Network Deployments, for more information.)\n\nNote The term co-located refers to two or more devices in the same physical location, with no WAN or MAN connection between them.\n\nDHCP Network Deployments\n\nThere are two options for deploying DHCP functionality within an IP telephony network:\n\nCentralized DHCP Server\n\nTypically, for a single-site campus IP telephony deployment, the DHCP server should be installed at a central location within the campus. As mentioned previously, redundant DHCP servers should be deployed. If the IP telephony deployment also incorporates remote branch telephony sites, as in a centralized multisite Unified CM deployment, a centralized server can be used to provide DHCP service to devices in the remote sites. This type of deployment requires that you configure the ip helper-address on the branch router interface. Keep in mind that, if redundant DHCP servers are deployed at the central site, both servers' IP addresses must be configured as ip helper-address. Also note that, if branch-side telephony devices rely on a centralized DHCP server and the WAN link between the two sites fails, devices at the branch site will be unable to send DHCP requests or receive DHCP responses.\n\nNote By default, service dhcp is enabled on the Cisco IOS device and does not appear in the configuration. Do not disable this service on the branch router because doing so will disable the DHCP relay agent on the device, and the ip helper-address configuration command will not work.\n\nCentralized DHCP Server and Remote Site Cisco IOS DHCP Server\n\nWhen configuring DHCP for use in a centralized multisite Unified CM deployment, you can use a centralized DHCP server to provide DHCP service to centrally located devices. Remote devices could receive DHCP service from a locally installed server or from the Cisco IOS router at the remote site. This type of deployment ensures that DHCP services are available to remote telephony devices even during WAN failures. Example 3-1 lists the basic Cisco IOS DHCP server configuration commands.\n\nExample 3-1 Cisco IOS DHCP Server Configuration Commands\n\n! Activate DHCP Service on the IOS Device\n\nservice dhcp\n\n! Specify any IP Address or IP Address Range to be excluded from the DHCP pool\n\nip dhcp excluded-address <ip-address>|<ip-address-low> <ip-address-high>\n\n! Specify the name of this specific DHCP pool, the subnet and mask for this\n\n! pool, the default gateway and up to four TFTP\n\nip dhcp pool <dhcp-pool name>\n\nnetwork <ip-subnet> <mask>\n\ndefault-router <default-gateway-ip>\n\noption 150 ip <tftp-server-ip-1>...\n\n! Note: IP phones use only the first two addresses supplied in the option 150\n\n! field even if more than two are configured.\n\nUnified CM DHCP Sever (Standalone versus Co-Resident DHCP)\n\nTypically DHCP servers are dedicated machine(s) in most network infrastructures, and they run in conjunction with the DNS and/or the Windows Internet Naming Service (WINS) services used by that network. In some instances, given a small Unified CM deployment with no more than 1000 devices registering to the cluster, you may run the DHCP server on a Unified CM server to support those devices. However, to avoid possible resource contention such as CPU contention with other critical services running on Unified CM, Cisco recommends moving the DHCP Server functionality to a dedicated server. If more than 1000 devices are registered to the cluster, DHCP must not be run on a Unified CM server but instead must be run on a dedicated or standalone server(s).\n\nNote The term co-resident refers to two or more services or applications running on the same server or virtual machine.\n\nTrivial File Transfer Protocol (TFTP)\n\nWithin a Cisco Unified CM system, endpoints such as IP phones rely on a TFTP-based process to acquire configuration files, software images, and other endpoint-specific information. The Cisco TFTP service is a file serving system that can run on one or more Unified CM servers. It builds configuration files and serves firmware files, ringer files, device configuration files, and so forth, to endpoints.\n\nThe TFTP file systems can hold several file types, such as the following:\n\nPhone configuration files\n\nPhone firmware files\n\nCertificate Trust List (CTL) files\n\nIdentity Trust List (ITL) files\n\nTone localization files\n\nUser interface (UI) localization and dictionary files\n\nRinger files\n\nSoftkey files\n\nDial plan files for SIP phones\n\nThe TFTP server manages and serves two types of files, those that are not modifiable (for example, firmware files for phones) and those that can be modified (for example, configuration files).\n\nA typical configuration file contains a prioritized list of Unified CMs for a device (for example, an SCCP or SIP phone), the TCP ports on which the device connects to those Unified CMs, and an executable load identifier. Configuration files for selected devices contain locale information and URLs for the messages, directories, services, and information buttons on the phone.\n\nWhen a device's configuration changes, the TFTP server rebuilds the configuration files by pulling the relevant information from the Unified CM database. The new file(s) is then downloaded to the phone once the phone has been reset. As an example, if a single phone's configuration file is modified (for example, during Extension Mobility login or logout), only that file is rebuilt and downloaded to the phone. However, if the configuration details of a device pool are changed (for example, if the primary Unified CM server is changed), then all devices in that device pool need to have their configuration files rebuilt and downloaded. For device pools that contain large numbers of devices, this file rebuilding process can impact server performance.\n\nNote The TFTP server can perform a local database read from the database on its co-resident subscriber server. Local database read not only provides benefits such as the preservation of user-facing features when the publisher in unavailable, but also allows multiple TFTP servers to be distributed by means of clustering over the WAN. (The same latency rules for clustering over the WAN apply to TFTP servers as apply to servers with registered phones.) This configuration brings the TFTP service closer to the endpoints, thus reducing latency and ensuring failure isolation between the sites.\n\nWhen a device requests a configuration file from the TFTP server, the TFTP server searches for the configuration file in its internal caches, the disk, and then alternate Cisco file servers (if specified). If the TFTP server finds the configuration file, it sends it to the device. If the configuration file provides Unified CM names, the device resolves the name by using DNS and opens a connection to the Unified CM. If the device does not receive an IP address or name, it uses the TFTP server name or IP address to attempt a registration connection. If the TFTP server cannot find the configuration file, it sends a \"file not found\" message to the device.\n\nA device that requests a configuration file while the TFTP server is rebuilding configuration files or while it is processing the maximum number of requests, will receive a message from the TFTP server that causes the device to request the configuration file later. The Maximum Serving Count service parameter, which can be configured, specifies the maximum number of requests that can be concurrently handled by the TFTP server. (Default value = 500 requests.) Use the default value if the TFTP service is run along with other Cisco CallManager services on the same server. For a dedicated TFTP server, use the following suggested values for the Maximum Serving Count: 1500 for a single-processor system or 3000 for a dual-processor system.\n\nThe Cisco Unified IP Phones 8900 Series and 9900 Series request their TFTP configuration files over the HTTP protocol (port 6970), which is much faster than TFTP.\n\nAn Example of TFTP in Operation\n\nEvery time an endpoint reboots, the endpoint will request a configuration file (via TFTP) whose name is based on the requesting endpoint's MAC address. (For a Cisco Unified IP Phone 7961 with MAC address ABCDEF123456, the file name would be SEPABCDEF123456.cnf.xml.) The received configuration file includes the version of software that the phone must run and a list of Cisco Unified CM servers with which the phone should register. The endpoint might also download, via TFTP, ringer files, softkey templates, and other miscellaneous files to acquire the necessary configuration information before becoming operational.\n\nIf the configuration file includes software file(s) version numbers that are different than those the phone is currently using, the phone will also download the new software file(s) from the TFTP server to upgrade itself. The number of files an endpoint must download to upgrade its software varies based on the type of endpoint and the differences between the phone's current software and the new software.\n\nTFTP File Transfer Times\n\nEach time an endpoint requests a file, there is a new TFTP transfer session. For centralized call processing deployments, the time to complete each of these transfers will affect the time it takes for an endpoint to start and become operational as well as the time it takes for an endpoint to upgrade during a scheduled maintenance. While TFTP transfer times are not the only factor that can affect these end states, they are a significant component.\n\nThe time to complete each file transfer via TFTP is predictable as a function of the file size, the percentage of TFTP packets that must be retransmitted, and the network latency or round-trip time.\n\nAt first glance, network bandwidth might seem to be missing from the previous statement, but it is actually included via the percentage of TFTP packets that must be retransmitted. This is because, if there is not enough network bandwidth to support the file transfer(s), then packets will be dropped by the network interface queuing algorithms and will have to be retransmitted.\n\nTFTP operates on top of the User Datagram Protocol (UDP). Unlike Transmission Control Protocol (TCP), UDP is not a reliable protocol, which means that UDP does not inherently have the ability to detect packet loss. Obviously, detecting packet loss in a file transfer is important, so RFC 1350 defines TFTP as a lock-step protocol. In other words, a TFTP sender will send one packet and wait for a response before sending the next packet (see Figure 3-9).\n\nFigure 3-9 Example of TFTP Packet Transmission Sequence\n\nIf a response is not received in the timeout period (4 seconds by default), the sender will resend the data packet or acknowledgment. When a packet has been sent five times without a response, the TFTP session fails. Because the timeout period is always the same and not adaptive like a TCP timeout, packet loss can significantly increase the amount of time a transfer session takes to complete.\n\nBecause the delay between each data packet is, at a minimum, equal to the network round-trip time, network latency also is a factor in the maximum throughput that a TFTP session can achieve.\n\nIn Figure 3-10, the round-trip time has been increased to 40 ms and one packet has been lost in transit. While the error rate is high at 12%, it is easy to see the effect of latency and packet loss on TFTP because the time to complete the session increased from 30 ms (in Figure 3-9) to 4160 ms (in Figure 3-10).\n\nFigure 3-10 Effect of Packet Loss on TFTP Session Completion Time\n\nUse the following formula to calculate how long a TFTP file transfer will take to complete:\n\nFileTransferTime = FileSize ∗ [(RTT + ERR ∗ Timeout) / 512000]\n\nWhere:\n\nFileTransferTime is in seconds.\n\nFileSize is in bytes.\n\nRTT is the round-trip time in milliseconds.\n\nERR is the error rate, or percentage of packets that are lost.\n\nTimeout is in milliseconds.\n\n512000 = (TFTP packet size) ∗ (1000 millisecond per seconds) =\n\n(512 bytes) ∗ (1000 millisecond per seconds)\n\nCisco Unified IP Phone Firmware Releases 7. x have a 10-minute timeout when downloading new files. If the transfer is not completed within this time, the phone will discard the download even if the transfer completes successfully later. If you experience this problem, Cisco recommends that you use a local TFTP server to upgrade phones to the 8. x firmware releases, which have a timeout value of 61 minutes.\n\nBecause network latency and packet loss have such an effect on TFTP transfer times, a local TFTP Server can be advantageous. This local TFTP server may be a Unified CM subscriber in a deployment with cluster over the WAN or an alternative local TFTP \"Load Server\" running on a Cisco Integrated Services Router (ISR), for example. Newer endpoints (which have larger firmware files) can be configured with a Load Server address, which allows the endpoint to download the relatively small configuration files from the central TFTP server but use a local TFTP Server (which is not part of the Unified CM cluster) to download the larger software files. For details on which Cisco Unified IP Phones support an alternative local TFTP Load Server, refer to the product documentation for your particular phone models (available at http://www.cisco.com).\n\nNote The exact process each phone goes through on startup and the size of the files downloaded will depend on the phone model, the signaling type configured for the phone (SCCP, MGCP, or SIP) and the previous state of the phone. While there are differences in which files are requested, the general process each phone follows is the same, and in all cases the TFTP server is used to request and deliver the appropriate files. The general recommendations for TFTP server deployment do not change based on the protocol and/or phone models deployed.\n\nTFTP Server Redundancy\n\nOption 150 allows up to two IP addresses to be returned to phones as part of the DHCP scope. The phone tries the first address in the list, and it tries the subsequent address only if it cannot establish communications with the first TFTP server. This address list provides a redundancy mechanism that enables phones to obtain TFTP services from another server even if their primary TFTP server has failed.\n\nTFTP Load Sharing\n\nCisco recommends that you grant different ordered lists of TFTP servers to different subnets to allow for load balancing. For example:\n\nIn subnet 10.1.1.0/24: Option 150: TFTP1_Primary, TFTP1_Secondary\n\nIn subnet 10.1.2.0/24: Option 150: TFTP1_Secondary, TFTP1_Primary\n\nUnder normal operations, a phone in subnet 10.1.1.0/24 will request TFTP services from TFTP1_Primary, while a phone in subnet 10.1.2.0/24 will request TFTP services from TFTP1_Secondary. If TFTP1_Primary fails, then phones from both subnets will request TFTP services from TFTP1_Secondary.\n\nLoad balancing avoids having a single TFTP server hot-spot, where all phones from multiple clusters rely on the same server for service. TFTP load balancing is especially important when phone software loads are transferred, such as during a Unified CM upgrade, because more files of larger size are being transferred, thus imposing a bigger load on the TFTP server.\n\nCentralized TFTP Services\n\nIn multi-cluster systems, it is possible to have a single subnet or VLAN containing phones from multiple clusters. In this situation, the TFTP servers whose addresses are provided to all phones in the subnet or VLAN must answer the file transfer requests made by each phone, regardless of which cluster contains the phone. In a centralized TFTP deployment, a set of TFTP servers associated with one of the clusters must provide TFTP services to all the phones in the multi-cluster system.\n\nIn order to provide this single point of file access, each cluster's TFTP server must be able to serve files via the central proxy TFTP server. This proxy arrangement is accomplished by configuring a set of possible redirect locations in the central TFTP server, pointing to each of the other clusters’ TFTP servers. This configuration uses a HOST redirect statement in the Alternate File Locations on the centralized TFTP server, one for each of the other clusters. Each of the redundant TFTP servers in the centralized cluster should point to one of the redundant servers in each of the child clusters. It is not necessary to point the centralized server to both redundant servers in the child clusters because the redistribution of files within each individual cluster and the failover mechanisms of the phones between the redundant servers in the central cluster provide for a very high degree of fault tolerance.\n\nFigure 3-11 shows an example of the operation of this process. A request from a phone registered to Cluster 3 is directed to the centralized TFTP server configured in Cluster 1 (C1_TFTP_Primary). This server will in turn query each of the configured alternate TFTP servers until one responds with a copy of the file initially requested by the phone. Requests to the centralized secondary TFTP server (C1_TFTP_Secondary) will be sent by proxy to the other clusters’ secondary TFTP servers until either the requested file is found or all servers report that the requested file does not exist.\n\nFigure 3-11 Centralized TFTP Servers\n\nNote Cisco does not recommend enabling auto-registration on the centralized TFTP node cluster. If auto-registration is enabled on the centralized TFTP cluster and any of the alternate cluster TFTP nodes are down, phones provisioned on those alternate TFTP clusters will get auto-registered to the centralized TFTP cluster rather than registering to their home cluster.\n\nNetwork Time Protocol (NTP)\n\nNTP allows network devices to synchronize their clocks to a network time server or network-capable clock. NTP is critical for ensuring that all devices in a network have the same time. When troubleshooting or managing a telephony network, it is crucial to synchronize the time stamps within all error and security logs, traces, and system reports on devices throughout the network. This synchronization enables administrators to recreate network activities and behaviors based on a common timeline. Billing records and call detail records (CDRs) also require accurate synchronized time.\n\nUnified CM NTP Time Synchronization\n\nTime synchronization is especially critical on Unified CM servers. In addition to ensuring that CDR records are accurate and that log files are synchronized, having an accurate time source is necessary for any future IPSec features to be enabled within the cluster and for communications with any external entity.\n\nUnified CM automatically synchronizes the NTP time of all subscribers in the cluster to the publisher. During installation, each subscriber is automatically configured to point to an NTP server running on the publisher. The publisher considers itself to be a master server and provides time for the cluster based on its internal hardware clock unless it is configured to synchronize from an external server. Cisco highly recommends configuring the publisher to point to a Stratum-1, Stratum-2, or Stratum-3 NTP server to ensure that the cluster time is synchronized with an external time source.\n\nCisco recommends synchronizing Unified CM with a Cisco IOS or Linux-based NTP server. Using Windows Time Services as an NTP server is not recommended or supported because Windows Time Services often use Simple Network Time Protocol (SNTP), and Linux-based Unified CM cannot successfully synchronize with SNTP.\n\nThe external NTP server specified for the primary node should be NTP v4 (version 4) to avoid potential compatibility, accuracy, and network jitter problems. External NTP servers must be NTP v4 if IPv6 addressing is used.\n\nCisco IOS and CatOS NTP Time Synchronization\n\nTime synchronization is also important for other devices within the network. Cisco IOS routers and Catalyst switches should be configured to synchronize their time with the rest of the network devices via NTP. This is critical for ensuring that debug, syslog, and console log messages are time-stamped appropriately. Troubleshooting telephony network issues is simplified when a clear timeline can be drawn for events that occur on devices throughout the network.\n\nWAN Infrastructure\n\nProper WAN infrastructure design is also extremely important for normal Unified Communications operation on a converged network. Proper infrastructure design requires following basic configuration and design best practices for deploying a WAN that is as highly available as possible and that provides guaranteed throughput. Furthermore, proper WAN infrastructure design requires deploying end-to-end QoS on all WAN links. The following sections discuss these requirements:\n\nWAN Design and Configuration\n\nWAN Quality of Service (QoS)\n\nBandwidth Provisioning\n\nWAN Design and Configuration\n\nProperly designing a WAN requires building fault-tolerant network links and planning for the possibility that these links might become unavailable. By carefully choosing WAN topologies, provisioning the required bandwidth, and approaching the WAN infrastructure as another layer in the network topology, you can build a fault-tolerant and redundant network. The following sections examine the required infrastructure layers and network services:\n\nDeployment Considerations\n\nGuaranteed Bandwidth\n\nBest-Effort Bandwidth\n\nDeployment Considerations\n\nWAN deployments for voice networks may use a hub-and-spoke, fully meshed, or partially meshed topology. A hub-and-spoke topology consists of a central hub site and multiple remote spoke sites connected into the central hub site. In this scenario, each remote or spoke site is one WAN-link hop away from the central or hub site and two WAN-link hops away from all other spoke sites. A meshed topology may contain multiple WAN links and any number of hops between the sites. In this scenario there may be many different paths to the same site or there may be different links used for communication with some sites compared to other sites. The simplest example is three sites, each with a WAN link to the other two sites, forming a triangle. In that case there are two potential paths between each site to each other site.\n\nFor more information about centralized and distributed multisite deployment models as well as Multiprotocol Label Switching (MPLS) implications for these deployment models, see the chapter on Collaboration Deployment Models.\n\nWAN links should, when possible, be made redundant to provide higher levels of fault tolerance. Redundant WAN links provided by different service providers or located in different physical ingress/egress points within the network can ensure backup bandwidth and connectivity in the event that a single link fails. In non-failure scenarios, these redundant links may be used to provide additional bandwidth and offer load balancing of traffic on a per-flow basis over multiple paths and equipment within the WAN.\n\nVoice and data should remain converged at the WAN, just as they are converged at the LAN. QoS provisioning and queuing mechanisms are typically available in a WAN environment to ensure that voice and data can interoperate on the same WAN links. Attempts to separate and forward voice and data over different links can be problematic in many instances because the failure of one link typically forces all traffic over a single link, thus diminishing throughput for each type of traffic and in most cases reducing the quality of voice. Furthermore, maintaining separate network links or devices makes troubleshooting and management difficult at best.\n\nBecause of the potential for WAN links to fail or to become oversubscribed, Cisco recommends deploying non-centralized resources as appropriate at sites on the other side of the WAN. Specifically, media resources, DHCP servers, voice gateways, and call processing applications such as Survivable Remote Site Telephony (SRST) and Cisco Unified Communications Manager Express (Unified CME) should be deployed at non-central sites when and if appropriate, depending on the site size and how critical these functions are to that site. Keep in mind that de-centralizing voice applications and devices can increase the complexity of network deployments, the complexity of managing these resources throughout the enterprise, and the overall cost of a the network solution; however, these factors can be mitigated by the fact that the resources will be available during a WAN link failure.\n\nWhen deploying voice in a WAN environment, Cisco recommends that you use the lower-bandwidth G.729 codec for any voice calls that will traverse WAN links because this practice will provide bandwidth savings on these lower-speed links. Furthermore, media resources such as MoH should be configured to use multicast transport mechanism when possible because this practice will provide additional bandwidth savings.\n\nWhere calls are made over best-effort networks with no QoS guarantees for voice, consider using Internet Low Bit Rate Codec (iLBC), which enables graceful speech quality degradation and good error resilience characteristics in networks where frames can get lost. See Table 3-7 for details of bandwidth consumption based on codec type and sample size.\n\nDelay in IP Voice Networks\n\nRecommendation G.114 of the International Telecommunication Union (ITU) states that the one-way delay in a voice network should be less than or equal to 150 milliseconds. It is important to keep this in mind when implementing low-speed WAN links within a network. Topologies, technologies, and physical distance should be considered for WAN links so that one-way delay is kept at or below this 150-millisecond recommendation. Implementing a VoIP network where the one-way delay exceeds 150 milliseconds introduces issues not only with the quality of the voice call but also with call setup and media cut-through times because several call signaling messages need to be exchanged between each device and the call processing application in order to establish the call.\n\nGuaranteed Bandwidth\n\nBecause voice is typically deemed a critical network application, it is imperative that bearer and signaling voice traffic always reaches its destination. For this reason, it is important to choose a WAN topology and link type that can provide guaranteed dedicated bandwidth. The following WAN link technologies can provide guaranteed dedicated bandwidth:\n\nLeased Lines\n\nFrame Relay\n\nAsynchronous Transfer Mode (ATM)\n\nATM/Frame-Relay Service Interworking\n\nMultiprotocol Label Switching (MPLS)\n\nCisco Voice and Video Enabled IP Security VPN (IPSec V3PN)\n\nThese link technologies, when deployed in a dedicated fashion or when deployed in a private network, can provide guaranteed traffic throughput. All of these WAN link technologies can be provisioned at specific speeds or bandwidth sizes. In addition, these link technologies have built-in mechanisms that help guarantee throughput of network traffic even at low link speeds. Features such as traffic shaping, fragmentation and packet interleaving, and committed information rates (CIR) can help ensure that packets are not dropped in the WAN, that all packets are given access at regular intervals to the WAN link, and that enough bandwidth is available for all network traffic attempting to traverse these links.\n\nDynamic Multipoint VPN (DMVPN)\n\nSpoke-to-spoke DMVPN networks can provide benefits for Cisco Unified Communications compared with hub-and-spoke topologies. Spoke-to-spoke tunnels can provide a reduction in end-to-end latency by reducing the number of WAN hops and decryption/encryption stages. In addition, DMVPN offers a simplified means of configuring the equivalent of a full mesh of point-to-point tunnels without the associated administrative and operational overhead. The use of spoke-to-spoke tunnels also reduces traffic at the hub, thus providing bandwidth and router processing capacity savings. Spoke-to-spoke DMVPN networks, however, are sensitive to the delay variation (jitter) caused during the transition of RTP packets routing from the spoke-hub-spoke path to the spoke-to-spoke path. This variation in delay during the DMVPN path transition occurs very early in the call and is generally unnoticeable, although a single momentary audio distortion might be heard if the latency difference is above 100 ms.\n\nFor information on the deployment of multisite DMVPN WANs with centralized call processing, refer to the Cisco Unified Communications Voice over Spoke-to-Spoke DMVPN Test Results and Recommendations, available at http://www.cisco.com/go/designzone.\n\nBest-Effort Bandwidth\n\nThere are some WAN topologies that are unable to provide guaranteed dedicated bandwidth to ensure that network traffic will reach its destination, even when that traffic is critical. These topologies are extremely problematic for voice traffic, not only because they provide no mechanisms to provision guaranteed network throughput, but also because they provide no traffic shaping, packet fragmentation and interleaving, queuing mechanisms, or end-to-end QoS to ensure that critical traffic such as voice will be given preferential treatment.\n\nThe following WAN network topologies and link types are examples of this kind of best-effort bandwidth technology:\n\nThe Internet\n\nDSL\n\nCable\n\nSatellite\n\nWireless\n\nIn most cases, none of these link types can provide the guaranteed network connectivity and bandwidth required for critical voice and voice applications. However, these technologies might be suitable for personal or telecommuter-type network deployments. At times, these topologies can provide highly available network connectivity and adequate network throughput; but at other times, these topologies can become unavailable for extended periods of time, can be throttled to speeds that render network throughput unacceptable for real-time applications such as voice, or can cause extensive packet losses and require repeated retransmissions. In other words, these links and topologies are unable to provide guaranteed bandwidth, and when traffic is sent on these links, it is sent best-effort with no guarantee that it will reach its destination. For this reason, Cisco recommends that you do not use best-effort WAN topologies for voice-enabled networks that require enterprise-class voice services and quality.\n\nNote There are some new QoS mechanisms for DSL and cable technologies that can provide guaranteed bandwidth; however, these mechanisms are not typically deployed by many service providers. For any service that offers QoS guarantees over networks that are typically based on best-effort, it is important to review and understand the bandwidth and QoS guarantees offered in the service provider's service level agreement (SLA).\n\nNote Upstream and downstream QoS mechanisms are now supported for wireless networks. For more information on QoS for Voice over Wireless LANs, refer to the Voice over Wireless LAN Design Guide, available at http://www.cisco.com/en/US/solutions/ns340/ns414/ns742/ns818/landing_wireless_uc.html.\n\nWAN Quality of Service (QoS)\n\nBefore placing voice and video traffic on a network, it is important to ensure that there is adequate bandwidth for all required applications. Once this bandwidth has been provisioned, voice priority queuing must be performed on all interfaces. This queuing is required to reduce jitter and possible packet loss if a burst of traffic oversubscribes a buffer. This queuing requirement is similar to the one for the LAN infrastructure.\n\nNext, the WAN typically requires additional mechanisms such as traffic shaping to ensure that WAN links are not sent more traffic than they can handle, which could cause dropped packets.\n\nFinally, link efficiency techniques can be applied to WAN paths. For example, link fragmentation and interleaving (LFI) can be used to prevent small voice packets from being queued behind large data packets, which could lead to unacceptable delays on low-speed links.\n\nThe goal of these QoS mechanisms is to ensure reliable, high-quality voice by reducing delay, packet loss, and jitter for the voice traffic. Table 3-5 lists the QoS features and tools required for the WAN infrastructure to achieve this goal.\n\nTable 3-5 QoS Features and Tools Required to Support Unified Communications for Each WAN Technology and Link Speed\n\nWAN Technology\n\nLink Speed: 56 kbps to 768 kbps\n\nLink Speed: Greater than 768 kbps\n\nLeased Lines\n\nMultilink Point-to-Point Protocol (MLP)\n\nMLP Link Fragmentation and Interleaving (LFI)\n\nLow Latency Queuing (LLQ)\n\nOptional: Compressed Real-Time Transport Protocol (cRTP)\n\nLLQ\n\nFrame Relay (FR)\n\nTraffic Shaping\n\nLFI (FRF.12)\n\nLLQ\n\nOptional: cRTP\n\nOptional: Voice-Adaptive Traffic Shaping (VATS)\n\nOptional: Voice-Adaptive Fragmentation (VAF)\n\nTraffic Shaping\n\nLLQ\n\nOptional: VATS\n\nAsynchronous Transfer Mode (ATM)\n\nTX-ring buffer changes\n\nMLP over ATM\n\nMLP LFI\n\nLLQ\n\nOptional: cRTP (requires MLP)\n\nTX-ring buffer changes\n\nLLQ\n\nFrame Relay and ATM Service Inter-Working (SIW)\n\nTX-ring buffer changes\n\nMLP over ATM and FR\n\nMLP LFI\n\nLLQ\n\nOptional: cRTP (requires MLP)\n\nTX-ring buffer changes\n\nMLP over ATM and FR\n\nLLQ\n\nMultiprotocol Label Switching (MPLS)\n\nSame as above, according to the interface technology\n\nClass-based marking is generally required to re-mark flows according to service provider specifications\n\nSame as above, according to the interface technology\n\nClass-based marking is generally required to re-mark flows according to service provider specifications\n\nThe following sections highlight some of the most important features and techniques to consider when designing a WAN to support both voice and data traffic:\n\nTraffic Prioritization\n\nLink Efficiency Techniques\n\nTraffic Shaping\n\nTraffic Prioritization\n\nIn choosing from among the many available prioritization schemes, the major factors to consider include the type of traffic involved and the type of media on the WAN. For multi-service traffic over an IP WAN, Cisco recommends low-latency queuing (LLQ) for all links. This method supports up to 64 traffic classes, with the ability to specify, for example, priority queuing behavior for voice and interactive video, minimum bandwidth class-based weighted fair queuing for voice control traffic, additional minimum bandwidth weighted fair queues for mission critical data, and a default best-effort queue for all other traffic types.\n\nFigure 3-12 shows an example prioritization scheme.\n\nFigure 3-12 Optimized Queuing for VoIP over the WAN\n\nCisco recommends the following prioritization criteria for LLQ:\n\nThe criterion for voice to be placed into a priority queue is the differentiated services code point (DSCP) value of 46, or a per-hop behavior (PHB) value of EF.\n\nThe criterion for video conferencing traffic to be placed into a priority queue is a DSCP value of 34, or a PHB value of AF41. However, due to the larger packet sizes of video traffic, these packets should be placed in the priority queue only on WAN links that are faster than 768 Kbps. Link speeds below this value require packet fragmentation, but packets placed in the priority queue are not fragmented, thus smaller voice packets could be queued behind larger video packets. For links speeds of 768 Kbps or lower, video conferencing traffic should be placed in a separate class-based weighted fair queue (CBWFQ).\n\nNote One-way video traffic, such as the traffic generated by streaming video applications for services such as video-on-demand or live video feeds, should always use a CBWFQ scheme because that type of traffic has a much higher delay tolerance than two-way video conferencing traffic\n\nAs the WAN links become congested, it is possible to starve the voice control signaling protocols, thereby eliminating the ability of the IP phones to complete calls across the IP WAN. Therefore, voice control protocols, such as H.323, MGCP, and Skinny Client Control Protocol (SCCP), require their own class-based weighted fair queue. The entrance criterion for this queue is a DSCP value of 24 or a PHB value of CS3.\n\nNote Cisco has transitioned the marking of voice control protocols from DSCP 26 (PHB AF31) to DSCP 24 (PHB CS3). However, some products still mark signaling traffic as DSCP 26 (PHB AF31); therefore, Cisco recommends that you reserve both AF31 and CS3 for call signaling.\n\nIn some cases, certain data traffic might require better than best-effort treatment. This traffic is referred to as mission-critical data, and it is placed into one or more queues that have the required amount of bandwidth. The queuing scheme within this class is first-in-first-out (FIFO) with a minimum allocated bandwidth. Traffic in this class that exceeds the configured bandwidth limit is placed in the default queue. The entrance criterion for this queue could be a Transmission Control Protocol (TCP) port number, a Layer 3 address, or a DSCP/PHB value.\n\nAll remaining enterprise traffic can be placed in a default queue for best-effort treatment. If you specify the keyword fair, the queuing algorithm will be weighted fair queuing (WFQ).\n\nScavenger Class\n\nThe Scavenger class is intended to provide less than best-effort services to certain applications. Applications assigned to this class have little or no contribution to the organizational objectives of the enterprise and are typically entertainment oriented in nature. Assigning Scavenger traffic to a minimal bandwidth queue forces it to be squelched to virtually nothing during periods of congestion, but it allows it to be available if bandwidth is not being used for business purposes, such as might occur during off-peak hours.\n\nScavenger traffic should be marked as DSCP CS1.\n\nScavenger traffic should be assigned the lowest configurable queuing service. For instance, in Cisco IOS, this means assigning a CBWFQ of 1% to Scavenger class.\n\nLink Efficiency Techniques\n\nThe following link efficiency techniques improve the quality and efficiency of low-speed WAN links.\n\nCompressed Real-Time Transport Protocol (cRTP)\n\nYou can increase link efficiency by using Compressed Real-Time Transport Protocol (cRTP). This protocol compresses a 40-byte IP, User Datagram Protocol (UDP), and RTP header into approximately two to four bytes. cRTP operates on a per-hop basis. Use cRTP on a particular link only if that link meets all of the following conditions:\n\nVoice traffic represents more than 33% of the load on the specific link.\n\nThe link uses a low bit-rate codec (such as G.729).\n\nNo other real-time application (such as video conferencing) is using the same link.\n\nIf the link fails to meet any one of the preceding conditions, then cRTP is not effective and you should not use it on that link. Another important parameter to consider before using cRTP is router CPU utilization, which is adversely affected by compression and decompression operations.\n\ncRTP on ATM and Frame Relay Service Inter-Working (SIW) links requires the use of Multilink Point-to-Point Protocol (MLP).\n\nNote that cRTP compression occurs as the final step before a packet leaves the egress interface; that is, after LLQ class-based queueing has occurred. Beginning in Cisco IOS Release 12.(2)2T and later, cRTP provides a feedback mechanism to the LLQ class-based queueing mechanism that allows the bandwidth in the voice class to be configured based on the compressed packet value. With Cisco IOS releases prior to 12.(2)2T, this mechanism is not in place, so the LLQ is unaware of the compressed bandwidth and, therefore, the voice class bandwidth has to be provisioned as if no compression is taking place. Table 3-6 shows an example of the difference in voice class bandwidth configuration given a 512-kbps link with G.729 codec and a requirement for 10 calls.\n\nNote that Table 3-6 assumes 24 kbps for non-cRTP G.729 calls and 10 kbps for cRTP G.729 calls. These bandwidth numbers are based on voice payload and IP/UDP/RTP headers only. They do not take into consideration Layer 2 header bandwidth. However, actual bandwidth provisioning should also include Layer 2 header bandwidth based on the type WAN link used.\n\nTable 3-6 LLQ Voice Class Bandwidth Requirements for 10 Calls with 512 kbps Link Bandwidth and G.729 Codec\n\nIt should also be noted that, beginning in Cisco IOS Release 12.2(13)T, cRTP can be configured as part of the voice class with the Class-Based cRTP feature. This option allows cRTP to be specified within a class, attached to an interface via a service policy. This new feature provides compression statistics and bandwidth status via the show policy interface command, which can be very helpful in determining the offered rate on an interface service policy class given the fact that cRTP is compressing the IP/RTP headers.\n\nFor additional recommendations about using cRTP with a Voice and Video Enabled IPSec VPN (V3PN), refer to the V3PN documentation available at\n\nhttp://www.cisco.com/en/US/solutions/ns340/ns414/ns742/ns817/landing_voice_video.html\n\nLink Fragmentation and In"
    }
}
{
    "id": "dbpedia_4988_3",
    "rank": 96,
    "data": {
        "url": "https://open.nytimes.com/how-does-this-article-make-you-feel-4684e5e9c47",
        "read_more_link": "",
        "language": "en",
        "title": "How Does This Article Make You Feel?",
        "top_image": "https://miro.medium.com/v2/da:true/resize:fit:1200/1*_zZRtba4ht_1Fl5FVCXW8Q.gif",
        "meta_img": "https://miro.medium.com/v2/da:true/resize:fit:1200/1*_zZRtba4ht_1Fl5FVCXW8Q.gif",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/resize:fill:88:88/1*8WVHZscv6J1CKqEXySmYiQ.jpeg",
            "https://miro.medium.com/v2/resize:fill:48:48/1*QUlYX1snC5csKT-E6Bmxvw.jpeg",
            "https://miro.medium.com/v2/resize:fill:144:144/1*8WVHZscv6J1CKqEXySmYiQ.jpeg",
            "https://miro.medium.com/v2/resize:fill:64:64/1*QUlYX1snC5csKT-E6Bmxvw.jpeg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Alexander Spangher",
            "medium.com"
        ],
        "publish_date": "2018-10-31T18:01:57.987000+00:00",
        "summary": "",
        "meta_description": "The New York Times Data and Insights team used crowdsourcing and machine learning to predict the emotional resonance of Times articles to help serve better ads for readers.",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*QUlYX1snC5csKT-E6Bmxvw.jpeg",
        "meta_site_name": "Medium",
        "canonical_link": "https://open.nytimes.com/how-does-this-article-make-you-feel-4684e5e9c47",
        "text": "Last year, the Advertising team at The New York Times asked a question: could we accurately predict the emotions that are evoked by Times articles? If so, we could empower advertisers to place ads more relevant to the context in which they are shown. To explore this idea, The Times’s Data Science team launched Project Feels, a project to understand and predict the emotional impact of Times articles.\n\nIn a nutshell, we built prediction algorithms with large amounts of data collected via crowdsourcing. Our predictions made sense qualitatively, and we ran successful experiments demonstrating that readers’ emotional response positively correlated with engagement on articles. This approach, called perspective targeting, was one of the first data products launched by nytDEMO, a new initiative aimed at helping advertisers place the right marketer stories with the right articles.\n\nTo be clear: this is an advertising project and was done without coordination with the newsroom; its findings will never impact our news report or other editorial decisions.\n\nData Collection\n\nTo learn to predict emotions from articles, we first needed the right data. We surveyed over 1,200 readers who participated voluntarily to create our initial dataset. This was the first time The New York Times ever systematically crowdsourced data for machine learning.\n\nWe asked respondents how they felt while reading a series of articles and asked them to choose from a number of different emotion categories (which were learned from earlier experiments), as well as a No Emotion category.\n\nAccording to a study from the Pew Research Center, crowdsourced respondents broadly match the type of user we were interested in studying for this project, as they are largely web-experienced, younger, educated and speak English.\n\nData Cleaning\n\nWe know that in online paid surveys, most respondents give thoughtful replies, but some respondents speed through surveys and give meaningless answers. We hit on several steps to limit this behavior, including setting a hard limit on the number of tasks each respondent could complete per batch — this required setting up an external API to record each respondent’s quota.\n\nThis both improved the diversity of respondents and limited the influence of any single indiscriminate respondent; indiscriminate respondents tend to complete 1,000 or more tasks in little or no time, as we learned from earlier runs.\n\nAnother way we guarded against indiscriminate respondents was monitoring for consistency of responses: genuine disagreement often occurs on specific types of articles — highly controversial or political pieces, for instance — and follows specific emotional patterns. For example, an article about a politically charged tweet might provoke either hope or hate for different people. A respondent is likely well-meaning if they follow these patterns of disagreement, but a small number of our respondents were clearly outliers in their responses.\n\nAfter collecting the data, we identified and cut out bad data. Researchers typically use a small set of pre-labeled examples, or a gold-set, to detect indiscriminate respondents. Since we did not have a gold-set available, we used statistical techniques to measure task-completion time and disagreement, as described above.\n\nModeling\n\nActive Learning\n\nWe uploaded our articles in multiple batches, which offered us an excellent opportunity to be smart about which articles we chose.\n\nIn essence, asking respondents questions about difficult articles gave us more information than if we had asked about straightforward articles. Selecting trickier pieces can help a machine learning algorithm achieve high predictive performance with limited data. Identifying these articles in successive batches is called active learning, which is the process of defining what is hard and identifying articles that fit this description.\n\nWe tried to build the most informative dataset, which meant labeling the articles that are the most difficult to classify. To do this, we needed to define what difficult means to us.\n\nA good way to assess difficulty is to see how close our models’ predictions are to random guesses.\n\nTo understand this, let’s imagine that you need to determine whether a certain article evokes hope. You know that 50 percent of all articles evoke hope, but you are still unsure after reading this particular article. The safe choice is to guess and say that the article is 50 percent likely to evoke hope, because according to the Law of Large Numbers, this guess will be right more often than not.\n\nThis means that when our model gives a near-average score for an emotion, it too is confused: the article’s text, X, does not help it with its prediction. Just as in the example above, the model (like the human) defaults to the “safe” prediction. (For reference: these are articles with a-posteriori expectations, or p(y | x), close to their a-priori expectation, or p(y)).\n\nTo train our model to do this, we first collected data on a batch of randomly selected articles, then learned initial predictive models and used these initial models to assess the likelihood an untagged article would contain an emotion. We did this iteratively by sampling new sets of articles for scoring, then updating our models and rescoring.\n\nProcessing the data this way significantly boosted our performance. We continued sampling batches in this fashion until our models stopped improving in accuracy from additional data.\n\nModels Explored: Deep vs. “Shallow”\n\nModeling in data science often involves iteratively testing different model types, starting with simpler models and interpreting their specific shortcomings to determine which complex models to try.\n\nWe explored three categories of models for this project:\n\nLinear methods are typically used when simple, interpretable models are needed. They assume no structure exists in the data: linear models do not capture feature-interactions or feature-outcome-nonlinearity well (I’ll get more into what that means below). In our case, we used these for sanity checks, baselines and active learning.\n\nTree-based ensemble methods are used when higher accuracy is needed. These methods are “catch-all” in that they are used when structure in the data is unknown: they can capture all possible feature-interactions and feature-outcome-nonlinearities.\n\nDeep methods are used in cases where ensemble methods can be improved by incorporating prior knowledge about structure in the data. For instance, we know that sentences have structure — words that appear in succession often impact each other’s meaning, so local interactions between words are probably more important than distant ones. I’ll discuss this more below.\n\nLinear Methods\n\nLinear models were the first models we tried for their simplicity and interpretability. The model took word-frequency as input, but did not observe the order of words. For example, the headline, “Trump Tweets ‘Trump!’” would have been processed as, “‘Tweets’=1, ‘Trump’=2,… ‘Sleeps’=0.” This was like a person reading the words of a headline out of order, assessing the emotion of each word and then adding these all up. Clearly, there was a lot to miss.\n\nThough these models performed below our targets, we were able to examine words that the linear model said were most positively associated with each emotion. So, the word “scientist” is more likely to be present in articles that cause interest, while the word “couple” is likely to be present in articles that cause love.\n\nTree-Based Ensemble Methods\n\nTo use models that could better assess context, we next turned to tree-based ensemble methods. Using the same input style (i.e. “‘Tweets’=1, ‘Trump’: 2,… ‘Sleeps’=0”), tree-based ensembles are able to consider all the words at once before assessing emotion, which can capture structure and show how words affect each other.\n\nFor example, the data we collected shows the word “nuclear” alone can cause fear, but it causes hope in the presence of “Negotiate.”The left panel in the visual below illustrates this statement, whereas the right panel illustrates another aspect of trees. Unlike a linear model where, say, if the word “attack” causes fear, then “attack” used 10 times causes more fear, tree-based models can capture nonlinearity.\n\nNonlinearity between features and outcomes occurs all the time in real life. For instance, if an article uses the word “Economy” just once, it might be a jobs-report article. These tend to cause hope. (Note: The current models were focused on U.S.-based respondents and thus captured U.S.-based emotional reactions to news.)\n\nEnsemble models out-performed our linear model, leading us to believe that careful modeling of interactions and nonlinear effects could improve accuracy further.\n\nAt this point, we had enough evidence to believe that deep learning methods would work.\n\nDeep Learning Methods\n\nDeep learning, or neural networks with many layers, are often used to model specific complicated input types such as word sequences or pixels in an image. Various assumptions about the way these features interact — for example, proximal words might interact more than distal — can be encoded in the network structure.\n\nDeep learning methods typically require large training datasets to perform well (on the order of millions of examples). The size of our dataset was too small for many of the architectures we experimented with. We used pretrained layers to enhance training.\n\nOverall, we deployed the max-scoring classifier for each emotion, measured using area-under-curve (AUC) as our accuracy metric because of the unbalanced nature of the labels. We found that generally, deep architectures gave us performance increases for predicting many emotions, but did not outperform ensemble methods on all of them.\n\nAll of our accuracy tests were calculated based on articles published after the articles used to train the model. This helped us evaluate how our models would perform on news events that emerged after training.\n\nThis was important to us because it prevented our models from overfitting on topics. For instance, if during a specific time window, every article about trade was about trade wars, a model would learn that “trade” always leads to fear without considering that in new contexts “trade” might evoke hope. We wanted our models to focus on language patterns that signaled emotions, not topics.\n\nPerformance\n\nQualitative Results\n\nThis is what our work looks like in practice. Below is a sample of articles that our models identified as having high levels of certain emotions (the articles were not included in our training set). The results look accurate, but we monitor on an ongoing basis to ensure that these results are logical.\n\nAn Elderly Couple Named Harvey and Irma Offered a Respite From the Summer’s Storms (Love)\n\nHeartburn Drugs Tied to Stomach Cancer Risk (Fear)\n\nCity of the Future? Humans, Not Technology, Are the Challenge in Toronto (Interest)\n\nQuantitative Results\n\nTo evaluate how well these models could differentiate articles, we partnered with The Times’s Advertising and Marketing Services department to perform an ad-effectiveness campaign.\n\nAds were screened against top-scoring articles in each emotional category (and a baseline) in a controlled experiment over a period of two days. We tracked performance across a wide variety of metrics. Our goal was to see whether ads on emotion-tagged articles performed better or worse than our control, and whether emotions performed differently.\n\nAcross the board, articles that were top in emotional categories, such as love, sadness and fear, performed significantly better than articles that were not. We saw significant differentiation between articles tagged with emotions, showing that readers’ emotional response to articles is useful for predicting advertising engagement."
    }
}
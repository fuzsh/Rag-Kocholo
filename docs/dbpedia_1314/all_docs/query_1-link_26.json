{
    "id": "dbpedia_1314_1",
    "rank": 26,
    "data": {
        "url": "https://www.science.gov/topicpages/a/auditory%2Breaction%2Btime.html",
        "read_more_link": "",
        "language": "en",
        "title": "auditory reaction time: Topics by Science.gov",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.science.gov/scigov/desktop/en/images/SciGov_logo.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "A comparative study of simple auditory reaction time in blind (congenitally) and sighted subjects.\n\nPubMed\n\nGandhi, Pritesh Hariprasad; Gokhale, Pradnya A; Mehta, H B; Shah, C J\n\n2013-07-01\n\nReaction time is the time interval between the application of a stimulus and the appearance of appropriate voluntary response by a subject. It involves stimulus processing, decision making, and response programming. Reaction time study has been popular due to their implication in sports physiology. Reaction time has been widely studied as its practical implications may be of great consequence e.g., a slower than normal reaction time while driving can have grave results. To study simple auditory reaction time in congenitally blind subjects and in age sex matched sighted subjects. To compare the simple auditory reaction time between congenitally blind subjects and healthy control subjects. STUDY HAD BEEN CARRIED OUT IN TWO GROUPS: The 1(st) of 50 congenitally blind subjects and 2(nd) group comprises of 50 healthy controls. It was carried out on Multiple Choice Reaction Time Apparatus, Inco Ambala Ltd. (AccuracyÂ±0.001 s) in a sitting position at Government Medical College and Hospital, Bhavnagar and at a Blind School, PNR campus, Bhavnagar, Gujarat, India. Simple auditory reaction time response with four different type of sound (horn, bell, ring, and whistle) was recorded in both groups. According to our study, there is no significant different in reaction time between congenital blind and normal healthy persons. Blind individuals commonly utilize tactual and auditory cues for information and orientation and they reliance on touch and audition, together with more practice in using these modalities to guide behavior, is often reflected in better performance of blind relative to sighted participants in tactile or auditory discrimination tasks, but there is not any difference in reaction time between congenitally blind and sighted people.\n\nInfluence of sleep deprivation and auditory intensity on reaction time and response force.\n\nPubMed\n\nWÅodarczyk, Dariusz; JaÅkowski, Piotr; Nowik, Agnieszka\n\n2002-06-01\n\nArousal and activation are two variables supposed to underlie change in response force. This study was undertaken to explain these roles, specifically, for strong auditory stimuli and sleep deficit. Loud auditory stimuli can evoke phasic overarousal whereas sleep deficit leads to general underarousal. Moreover, Van der Molen and Keuss (1979, 1981) showed that paradoxically long reaction times occurred with extremely strong auditory stimuli when the task was difficult, e.g., choice reaction or Simon paradigm. It was argued that this paradoxical behavior related to reaction time is due to active disconnecting of the coupling between arousal and activation to prevent false responses. If so, we predicted that for extremely loud stimuli and for difficult tasks, the lengthening of reaction time should be associated with reduction of response force. The effects of loudness and sleep deficit on response time and force were investigated in three different tasks: simple response, choice response, and Simon paradigm. According to our expectation, we found a detrimental effect of sleep deficit on reaction time and on response force. In contrast to Van der Molen and Keuss, we found no increase in reaction time for loud stimuli (up to 110 dB) even on the Simon task.\n\nReaction time and accuracy in individuals with aphasia during auditory vigilance tasks.\n\nPubMed\n\nLaures, Jacqueline S\n\n2005-11-01\n\nResearch indicates that attentional deficits exist in aphasic individuals. However, relatively little is known about auditory vigilance performance in individuals with aphasia. The current study explores reaction time (RT) and accuracy in 10 aphasic participants and 10 nonbrain-damaged controls during linguistic and nonlinguistic auditory vigilance tasks. Findings indicate that the aphasic group was less accurate during both tasks than the control group, but was not slower in their accurate responses. Further examination of the data revealed variability in the aphasic participants' RT contributing to the lower accuracy scores.\n\nStudy of Auditory, Visual Reaction Time and Glycemic Control (HBA1C) in Chronic Type II Diabetes Mellitus.\n\nPubMed\n\nM, Muhil; Sembian, Umapathy; Babitha; N, Ethiya; K, Muthuselvi\n\n2014-09-01\n\nDiabetes mellitus is a disease of insulin deficiencyleads to micro and macro vascular disorder. Neuropathy is one of the major complication of chronic uncontrolled Diabetes affecting the Reaction time. To study the correlation between the glycosylated HbA1C and Auditory, visual Reaction time in chronic Type II diabetes (40-60y) of on oral hypoglycemic drugs of>10 y duration in two groups (n-100 in each group , both Males & females) and compared within the study groups and also with the age matched control group (100). HbA1C-Glycosylated HbA1C was measured by Particle enhanced immunoturbidimetric test method. Auditory and visual reaction time (ART, VRT) were measured by PC 1000 Reaction timer for control & study groups i.e. Group-I - Chronic Type II DM for >10 y with HbA1c < 7.0, and Group II - chronic Type-IIDM for >10 y with HbA1c > 7.0 ie impaired glycemic control. Exclusion Criteria- Subjects with Auditory and visual disturbances, alcoholism and smoking. Statistical Analysis - One-way ANOVA. Using SPSS 21 software. Both the groups had prolonged ART and VRT than controls. Among the study group, G-II (DM with HbA1C >7) had increased Auditory & Visual Reaction time than Group I which is statistically significant p-value <0.05. Impairment of sensory motor function of peripheral nervous system is more in chronic diabetic with less glycemic control ie., HbA1C>7 who have shown increased Auditory and Visual Reaction time than chronic DM with HbA1C<7.Severity of Peripheral neuropathy in Type II Diabetics could be due to elevated HbA1C.\n\nEEG alpha spindles and prolonged brake reaction times during auditory distraction in an on-road driving study.\n\nPubMed\n\nSonnleitner, Andreas; Treder, Matthias Sebastian; Simon, Michael; Willmann, Sven; Ewald, Arne; Buchner, Axel; Schrauf, Michael\n\n2014-01-01\n\nDriver distraction is responsible for a substantial number of traffic accidents. This paper describes the impact of an auditory secondary task on drivers' mental states during a primary driving task. N=20 participants performed the test procedure in a car following task with repeated forced braking on a non-public test track. Performance measures (provoked reaction time to brake lights) and brain activity (EEG alpha spindles) were analyzed to describe distracted drivers. Further, a classification approach was used to investigate whether alpha spindles can predict drivers' mental states. Results show that reaction times and alpha spindle rate increased with time-on-task. Moreover, brake reaction times and alpha spindle rate were significantly higher while driving with auditory secondary task opposed to driving only. In single-trial classification, a combination of spindle parameters yielded a median classification error of about 8% in discriminating the distracted from the alert driving. Reduced driving performance (i.e., prolonged brake reaction times) during increased cognitive load is assumed to be indicated by EEG alpha spindles, enabling the quantification of driver distraction in experiments on public roads without verbally assessing the drivers' mental states. Copyright Â© 2013 Elsevier Ltd. All rights reserved.\n\nEffect of red bull energy drink on auditory reaction time and maximal voluntary contraction.\n\nPubMed\n\nGoel, Vartika; Manjunatha, S; Pai, Kirtana M\n\n2014-01-01\n\nThe use of \"Energy Drinks\" (ED) is increasing in India. Students specially use these drinks to rejuvenate after strenuous exercises or as a stimulant during exam times. The most common ingredient in EDs is caffeine and a popular ED available and commonly used is Red Bull, containing 80 mg of caffeine in 250 ml bottle. The primary aim of this study was to investigate the effects of Red Bull energy drink on Auditory reaction time and Maximal voluntary contraction. A homogeneous group containing twenty medical students (10 males, 10 females) participated in a crossover study in which they were randomized to supplement with Red Bull (2 mg/kg body weight of caffeine) or isoenergetic isovolumetric noncaffeinated control drink (a combination of Appy Fizz, Cranberry juice and soda) separated by 7 days. Maximal voluntary contraction (MVC) was recorded as the highest of the 3 values of maximal isometric force generated from the dominant hand using hand grip dynamometer (Biopac systems). Auditory reaction time (ART) was the average of 10 values of the time interval between the click sound and response by pressing the push button using hand held switch (Biopac systems). The energy and control drinks after one hour of consumption significantly reduced the Auditory reaction time in males (ED 232 Â± 59 Vs 204 Â± 34 s and Control 223 Â± 57 Vs 210 Â± 51 s; p < 0.05) as well as in females (ED 227 Â± 56 Vs 214 Â± 48 s and Control 224 Â± 45 Vs 215 Â± 36 s; p < 0.05) but had no effect on MVC in either sex (males ED 381 Â± 37 Vs 371 Â± 36 and Control 375 Â± 61 Vs 363 Â± 36 Newton, females ED 227 Â± 23 Vs 227 Â± 32 and Control 234 Â± 46 Vs 228 Â± 37 Newton). When compared across the gender groups, there was no significant difference between males and females in the effects of any of the drinks on the ART but there was an overall significantly lower MVC in females compared to males. Both energy drink and the control drink significantly improve the reaction time but may not have any effect\n\nDevelopment of a test for recording both visual and auditory reaction times, potentially useful for future studies in patients on opioids therapy\n\nPubMed Central\n\nMiceli, Luca; Bednarova, Rym; Rizzardo, Alessandro; Samogin, Valentina; Della Rocca, Giorgio\n\n2015-01-01\n\nObjective Italian Road Law limits driving while undergoing treatment with certain kinds of medication. Here, we report the results of a test, run as a smartphone application (app), assessing auditory and visual reflexes in a sample of 300 drivers. The scope of the test is to provide both the police force and medication-taking drivers with a tool that can evaluate the individualâs capacity to drive safely. Methods The test is run as an app for Apple iOS and Android mobile operating systems and facilitates four different reaction times to be assessed: simple visual and auditory reaction times and complex visual and auditory reaction times. Reference deciles were created for the test results obtained from a sample of 300 Italian subjects. Results lying within the first three deciles were considered as incompatible with safe driving capabilities. Results Performance is both age-related (r>0.5) and sex-related (female reaction times were significantly slower than those recorded for male subjects, P<0.05). Only 21% of the subjects were able to perform all four tests correctly. Conclusion We developed and fine-tuned a test called Safedrive that measures visual and auditory reaction times through a smartphone mobile device; the scope of the test is two-fold: to provide a clinical tool for the assessment of the driving capacity of individuals taking pain relief medication; to promote the sense of social responsibility in drivers who are on medication and provide these individuals with a means of testing their own capacity to drive safely. PMID:25709406\n\nDevelopment of a test for recording both visual and auditory reaction times, potentially useful for future studies in patients on opioids therapy.\n\nPubMed\n\nMiceli, Luca; Bednarova, Rym; Rizzardo, Alessandro; Samogin, Valentina; Della Rocca, Giorgio\n\n2015-01-01\n\nItalian Road Law limits driving while undergoing treatment with certain kinds of medication. Here, we report the results of a test, run as a smartphone application (app), assessing auditory and visual reflexes in a sample of 300 drivers. The scope of the test is to provide both the police force and medication-taking drivers with a tool that can evaluate the individual's capacity to drive safely. The test is run as an app for Apple iOS and Android mobile operating systems and facilitates four different reaction times to be assessed: simple visual and auditory reaction times and complex visual and auditory reaction times. Reference deciles were created for the test results obtained from a sample of 300 Italian subjects. Results lying within the first three deciles were considered as incompatible with safe driving capabilities. Performance is both age-related (r>0.5) and sex-related (female reaction times were significantly slower than those recorded for male subjects, P<0.05). Only 21% of the subjects were able to perform all four tests correctly. We developed and fine-tuned a test called Safedrive that measures visual and auditory reaction times through a smartphone mobile device; the scope of the test is two-fold: to provide a clinical tool for the assessment of the driving capacity of individuals taking pain relief medication; to promote the sense of social responsibility in drivers who are on medication and provide these individuals with a means of testing their own capacity to drive safely.\n\nThe Visual and Auditory Reaction Time of Adolescents with Respect to Their Academic Achievements\n\nERIC Educational Resources Information Center\n\nTaskin, Cengiz\n\n2016-01-01\n\nThe aim of this study was to examine in visual and auditory reaction time of adolescents with respect to their academic achievement level. Five hundred adolescent children from the Turkey, (age=15.24Â±0.78 years; height=168.80Â±4.89 cm; weight=65.24Â±4.30 kg) for two hundred fifty male and (age=15.28Â±0.74; height=160.40Â±5.77 cm; weight=55.32Â±4.13 kg)â¦\n\nEffect of age, gender and body mass index on visual and auditory reaction times in Indian population.\n\nPubMed\n\nNikam, Lalita H; Gadkari, Jayshree V\n\n2012-01-01\n\nThe effect of Age. Gender and Body Mass Index (BMI) on the Visual (VRT) and Auditory reaction time (ART) was studied in 30 males and 30 females in the age group of 18-20 years along with 30 males and 30 females in the age group of 65-75 years. Statistical analysis of the data by one-way ANOVA and post-hoc by Tukey-HSD test showed that BMI, VRT and ART were significantly higher in old than young individuals. Females had higher BMI and longer reaction times than males. There was significant positive correlation between BMI and reaction times (VRT and ART) in both males and females by Pearson correlation analysis. Older individuals should be more careful and vigilant about the injuries and falls due to increased reaction time. Longer reaction times and higher BMI in females could be attributed to fluid and salt retention due to female sex hormones affecting sensorimotor co-ordination.\n\nEffect of Eight Weekly Aerobic Training Program on Auditory Reaction Time and MaxVO[subscript 2] in Visual Impairments\n\nERIC Educational Resources Information Center\n\nTaskin, Cengiz\n\n2016-01-01\n\nThe aim of study was to examine the effect of eight weekly aerobic exercises on auditory reaction time and MaxVO[subscript 2] in visual impairments. Forty visual impairment children that have blind 3 classification from the Turkey, experimental group; (age = 15.60 Â± 1.10 years; height = 164.15 Â± 4.88 cm; weight = 66.60 Â± 4.77 kg) for twentyâ¦\n\nEffect of dual task activity on reaction time in males and females.\n\nPubMed\n\nKaur, Manjinder; Nagpal, Sangeeta; Singh, Harpreet; Suhalka, M L\n\n2014-01-01\n\nThe present study was designed to compare the auditory and visual reaction time on an Audiovisual Reaction Time Machine with the concomitant use of mobile phones in 52 women and 30 men in the age group of 18-40 years. Males showed significantly (p < 0.05) shorter reaction times, both auditory and visual, than females both during single task and multi task performance. But the percentage increase from their respective baseline auditory reaction times, was more in men than women during multitasking, in hand held (24.38% & 18.70% respectively) and hands free modes (36.40% & 18.40% respectively) of the use of cell phone. VRT increased non significantly during multitasking in both the groups. However, the multitasking per se has detrimental effect on the reaction times in both the groups studied. Hence, it should best be avoided in crucial and high attention demanding tasks like driving.\n\nVariation of Reaction Time in Different Phases of Menstrual Cycle\n\nPubMed Central\n\nKumar, Sunil; Mufti, Mehak; Kisan, Ravikiran\n\n2013-01-01\n\nObjective: To evaluate the influence of menstrual cycle on auditory and visual reaction times. Method: This study was conducted on thirty, healthy, regularly menstruating female subjects who were in the age group of 18-25 years. Influence of different phases of menstrual cycle on Auditory Reaction Time (ART) and Visual Reaction Time (VRT) was evaluated by using a portable audiovisual reaction time apparatus. Result: The statistical tests which were used were ANOVA and Students âtâ test, which showed that there were significant increases in ART and VRT during luteal phase, as compared to those in follicular phase. Conclusion: Changes in ART and VRT during different phases of menstrual cycle could be due to changes in the levels of female sex hormones, which in turn may lead to salt and water retention. PMID:24086851\n\nReaction time and anticipatory skill of athletes in open and closed skill-dominated sport.\n\nPubMed\n\nNuri, Leila; Shadmehr, Azadeh; Ghotbi, Nastaran; Attarbashi Moghadam, Behrouz\n\n2013-01-01\n\nIn sports, reaction time and anticipatory skill are critical aspects of perceptual abilities. To date, no study has compared reaction time and anticipatory skill of athletes from open and closed skill-dominated sport. Accordingly, the present study investigated whether a difference exists in sensory-cognitive skills between these two different sport domains. Eleven volleyball players and 11 sprinters participated in this experiment. Reaction time and anticipatory skill of both groups were recorded by a custom-made software called SART (speed anticipation and reaction time test). This software consists of six sensory-cognitive tests that evaluate visual choice reaction time, visual complex choice reaction time, auditory choice reaction time, auditory complex choice reaction time, and anticipatory skill of the high speed and low speed of the ball. For each variable, an independent t-test was performed. Results suggested that sprinters were better in both auditory reaction times (P<0.001 for both tests) and volleyball players were better in both anticipatory skill tests (P = 0.007 and P = 0.04 for anticipatory skill of the high speed and low speed of the ball, respectively). However, no significant differences were found in both visual choice reaction time tests (P > 0.05 for both visual reaction time tests). It is concluded that athletes have greater sensory-cognitive skills related to their specific sport domain either open or closed.\n\nPhasic heart rate responses and cardiac cycle time in auditory choice reaction time.\n\nPubMed\n\nvan der Molen, M W; Somsen, R J; Orlebeke, J F\n\n1983-01-01\n\nThis study investigated the cardiovascular-behavioral interaction under short and long stimulus interval conditions. In addition, the cardiovascular-behavioral interaction was studied as affected by cardiac cycle duration. Fourteen subjects performed a choice reaction time (RT) task employing a mixed speed-accuracy tradeoff design in which reactions were paced to coincide with a signal that occurs randomly at either 200 or 500 msec after the reaction stimulus. The preparatory interval between a warning stimulus and a lead-reaction stimulus complex was also varied (2 vs. 4.5 sec). Anticipatory deceleration occurred within the 4.5 sec interval but not in the 2 sec interval. The depth of anticipatory deceleration did not discriminate between fast and slow reactions; but an earlier shift from deceleration to acceleration was associated with fast reactions. The effect of stimulus timing relative to the R-wave of the electrocardiogram was also analysed. Meaningful stimuli tended to produce cardiac slowing as previously described in the literature. Early occurring stimuli prolong the cycle of their occurrence more than late occurring stimuli. The later prolong the subsequent cycle. Cardiac cycle time effects were absent for unattended stimuli. The results of anticipatory deceleration suggested that the depth of deceleration was regulated by time-uncertainty and speed-accuracy criterion.\n\nAuditory Proprioceptive Integration: Effects of Real-Time Kinematic Auditory Feedback on Knee Proprioception\n\nPubMed Central\n\nGhai, Shashank; Schmitz, Gerd; Hwang, Tong-Hun; Effenberg, Alfred O.\n\n2018-01-01\n\nThe purpose of the study was to assess the influence of real-time auditory feedback on knee proprioception. Thirty healthy participants were randomly allocated to control (n = 15), and experimental group I (15). The participants performed an active knee-repositioning task using their dominant leg, with/without additional real-time auditory feedback where the frequency was mapped in a convergent manner to two different target angles (40 and 75Â°). Statistical analysis revealed significant enhancement in knee re-positioning accuracy for the constant and absolute error with real-time auditory feedback, within and across the groups. Besides this convergent condition, we established a second divergent condition. Here, a step-wise transposition of frequency was performed to explore whether a systematic tuning between auditory-proprioceptive repositioning exists. No significant effects were identified in this divergent auditory feedback condition. An additional experimental group II (n = 20) was further included. Here, we investigated the influence of a larger magnitude and directional change of step-wise transposition of the frequency. In a first step, results confirm the findings of experiment I. Moreover, significant effects on knee auditory-proprioception repositioning were evident when divergent auditory feedback was applied. During the step-wise transposition participants showed systematic modulation of knee movements in the opposite direction of transposition. We confirm that knee re-positioning accuracy can be enhanced with concurrent application of real-time auditory feedback and that knee re-positioning can modulated in a goal-directed manner with step-wise transposition of frequency. Clinical implications are discussed with respect to joint position sense in rehabilitation settings. PMID:29568259\n\nAuditory-motor entrainment and phonological skills: precise auditory timing hypothesis (PATH).\n\nPubMed\n\nTierney, Adam; Kraus, Nina\n\n2014-01-01\n\nPhonological skills are enhanced by music training, but the mechanisms enabling this cross-domain enhancement remain unknown. To explain this cross-domain transfer, we propose a precise auditory timing hypothesis (PATH) whereby entrainment practice is the core mechanism underlying enhanced phonological abilities in musicians. Both rhythmic synchronization and language skills such as consonant discrimination, detection of word and phrase boundaries, and conversational turn-taking rely on the perception of extremely fine-grained timing details in sound. Auditory-motor timing is an acoustic feature which meets all five of the pre-conditions necessary for cross-domain enhancement to occur (Patel, 2011, 2012, 2014). There is overlap between the neural networks that process timing in the context of both music and language. Entrainment to music demands more precise timing sensitivity than does language processing. Moreover, auditory-motor timing integration captures the emotion of the trainee, is repeatedly practiced, and demands focused attention. The PATH predicts that musical training emphasizing entrainment will be particularly effective in enhancing phonological skills.\n\nAffective Evaluations of and Reactions to Exterior and Interior Vehicle Auditory Quality\n\nNASA Astrophysics Data System (ADS)\n\nVÃ¤stfjÃ¤ll, D.; Gulbol, M.-A.; Kleiner, M.; GÃ¤rling, T.\n\n2002-08-01\n\nAffective reactions to and evaluations of auditory stimuli are fundamental components of human perception. In three experiments, participants rated their affective reactions (how pleasant I feel) and preferences for these affective reactions (how much I like the way I feel) as well as affective evaluations (how pleasant the sound is) to interior and exterior binaurally recorded vehicle sounds varying in physical properties. Consistent with previous research, it was found that the orthogonal affect dimensions of valence (unpleasant-pleasant) and arousal or activation (deactivation-activation) discriminated between affective reactions induced by the different qualities of the sounds. Moreover, preference for affective reactions was related to both valence and activation. Affective evaluations (powerful-powerless/passive-active and unpleasant-pleasant) correlated significantly with affective reactions to the same sounds in both within-subjects and between-subjects designs. Standard sound quality metrics derived from the sounds correlated, however, poorly with the affective ratings of interior sounds and only moderately with affective ratings of exterior sounds. Taken together, the results suggest that affect is an important component in product auditory quality optimization.\n\nAuditory-visual object recognition time suggests specific processing for animal sounds.\n\nPubMed\n\nSuied, Clara; Viaud-Delmon, Isabelle\n\n2009-01-01\n\nRecognizing an object requires binding together several cues, which may be distributed across different sensory modalities, and ignoring competing information originating from other objects. In addition, knowledge of the semantic category of an object is fundamental to determine how we should react to it. Here we investigate the role of semantic categories in the processing of auditory-visual objects. We used an auditory-visual object-recognition task (go/no-go paradigm). We compared recognition times for two categories: a biologically relevant one (animals) and a non-biologically relevant one (means of transport). Participants were asked to react as fast as possible to target objects, presented in the visual and/or the auditory modality, and to withhold their response for distractor objects. A first main finding was that, when participants were presented with unimodal or bimodal congruent stimuli (an image and a sound from the same object), similar reaction times were observed for all object categories. Thus, there was no advantage in the speed of recognition for biologically relevant compared to non-biologically relevant objects. A second finding was that, in the presence of a biologically relevant auditory distractor, the processing of a target object was slowed down, whether or not it was itself biologically relevant. It seems impossible to effectively ignore an animal sound, even when it is irrelevant to the task. These results suggest a specific and mandatory processing of animal sounds, possibly due to phylogenetic memory and consistent with the idea that hearing is particularly efficient as an alerting sense. They also highlight the importance of taking into account the auditory modality when investigating the way object concepts of biologically relevant categories are stored and retrieved.\n\nLooming auditory collision warnings for driving.\n\nPubMed\n\nGray, Rob\n\n2011-02-01\n\nA driving simulator was used to compare the effectiveness of increasing intensity (looming) auditory warning signals with other types of auditory warnings. Auditory warnings have been shown to speed driver reaction time in rear-end collision situations; however, it is not clear which type of signal is the most effective. Although verbal and symbolic (e.g., a car horn) warnings have faster response times than abstract warnings, they often lead to more response errors. Participants (N=20) experienced four nonlooming auditory warnings (constant intensity, pulsed, ramped, and car horn), three looming auditory warnings (\"veridical,\" \"early,\" and \"late\"), and a no-warning condition. In 80% of the trials, warnings were activated when a critical response was required, and in 20% of the trials, the warnings were false alarms. For the early (late) looming warnings, the rate of change of intensity signaled a time to collision (TTC) that was shorter (longer) than the actual TTC. Veridical looming and car horn warnings had significantly faster brake reaction times (BRT) compared with the other nonlooming warnings (by 80 to 160 ms). However, the number of braking responses in false alarm conditions was significantly greater for the car horn. BRT increased significantly and systematically as the TTC signaled by the looming warning was changed from early to veridical to late. Looming auditory warnings produce the best combination of response speed and accuracy. The results indicate that looming auditory warnings can be used to effectively warn a driver about an impending collision.\n\nHuman Engineer’s Guide to Auditory Displays. Volume 2. Elements of Signal Reception and Resolution Affecting Auditory Displays.\n\nDTIC Science & Technology\n\n1984-08-01\n\n90de It noce..etrv wnd identify by block numberl .’-- This work reviews the areas of monaural and binaural signal detection, auditory discrimination And...AUDITORY DISPLAYS This work reviews the areas of monaural and binaural signal detection, auditory discrimination and localization, and reaction times to...pertaining to the major areas of auditory processing in humans. The areas covered in the reviews presented here are monaural and binaural siqnal detection\n\nIs auditory perceptual timing a core deficit of developmental coordination disorder?\n\nPubMed\n\nTrainor, Laurel J; Chang, Andrew; Cairney, John; Li, Yao-Chuen\n\n2018-05-09\n\nTime is an essential dimension for perceiving and processing auditory events, and for planning and producing motor behaviors. Developmental coordination disorder (DCD) is a neurodevelopmental disorder affecting 5-6% of children that is characterized by deficits in motor skills. Studies show that children with DCD have motor timing and sensorimotor timing deficits. We suggest that auditory perceptual timing deficits may also be core characteristics of DCD. This idea is consistent with evidence from several domains, (1) motor-related brain regions are often involved in auditory timing process; (2) DCD has high comorbidity with dyslexia and attention deficit hyperactivity, which are known to be associated with auditory timing deficits; (3) a few studies report deficits in auditory-motor timing among children with DCD; and (4) our preliminary behavioral and neuroimaging results show that children with DCD at age 6 and 7 have deficits in auditory time discrimination compared to typically developing children. We propose directions for investigating auditory perceptual timing processing in DCD that use various behavioral and neuroimaging approaches. From a clinical perspective, research findings can potentially benefit our understanding of the etiology of DCD, identify early biomarkers of DCD, and can be used to develop evidence-based interventions for DCD involving auditory-motor training. Â© 2018 The Authors. Annals of the New York Academy of Sciences published by Wiley Periodicals, Inc. on behalf of The New York Academy of Sciences.\n\nEffect of Auditory Motion Velocity on Reaction Time and Cortical Processes\n\nERIC Educational Resources Information Center\n\nGetzmann, Stephan\n\n2009-01-01\n\nThe study investigated the processing of sound motion, employing a psychophysical motion discrimination task in combination with electroencephalography. Following stationary auditory stimulation from a central space position, the onset of left- and rightward motion elicited a specific cortical response that was lateralized to the hemisphereâ¦\n\nAuditory reafferences: the influence of real-time feedback on movement control.\n\nPubMed\n\nKennel, Christian; Streese, Lukas; Pizzera, Alexandra; Justen, Christoph; Hohmann, Tanja; Raab, Markus\n\n2015-01-01\n\nAuditory reafferences are real-time auditory products created by a person's own movements. Whereas the interdependency of action and perception is generally well studied, the auditory feedback channel and the influence of perceptual processes during movement execution remain largely unconsidered. We argue that movements have a rhythmic character that is closely connected to sound, making it possible to manipulate auditory reafferences online to understand their role in motor control. We examined if step sounds, occurring as a by-product of running, have an influence on the performance of a complex movement task. Twenty participants completed a hurdling task in three auditory feedback conditions: a control condition with normal auditory feedback, a white noise condition in which sound was masked, and a delayed auditory feedback condition. Overall time and kinematic data were collected. Results show that delayed auditory feedback led to a significantly slower overall time and changed kinematic parameters. Our findings complement previous investigations in a natural movement situation with non-artificial auditory cues. Our results support the existing theoretical understanding of action-perception coupling and hold potential for applied work, where naturally occurring movement sounds can be implemented in the motor learning processes.\n\nEvidence for auditory-visual processing specific to biological motion.\n\nPubMed\n\nWuerger, Sophie M; Crocker-Buque, Alexander; Meyer, Georg F\n\n2012-01-01\n\nBiological motion is usually associated with highly correlated sensory signals from more than one modality: an approaching human walker will not only have a visual representation, namely an increase in the retinal size of the walker's image, but also a synchronous auditory signal since the walker's footsteps will grow louder. We investigated whether the multisensorial processing of biological motion is subject to different constraints than ecologically invalid motion. Observers were presented with a visual point-light walker and/or synchronised auditory footsteps; the walker was either approaching the observer (looming motion) or walking away (receding motion). A scrambled point-light walker served as a control. Observers were asked to detect the walker's motion as quickly and as accurately as possible. In Experiment 1 we tested whether the reaction time advantage due to redundant information in the auditory and visual modality is specific for biological motion. We found no evidence for such an effect: the reaction time reduction was accounted for by statistical facilitation for both biological and scrambled motion. In Experiment 2, we dissociated the auditory and visual information and tested whether inconsistent motion directions across the auditory and visual modality yield longer reaction times in comparison to consistent motion directions. Here we find an effect specific to biological motion: motion incongruency leads to longer reaction times only when the visual walker is intact and recognisable as a human figure. If the figure of the walker is abolished by scrambling, motion incongruency has no effect on the speed of the observers' judgments. In conjunction with Experiment 1 this suggests that conflicting auditory-visual motion information of an intact human walker leads to interference and thereby delaying the response.\n\nInterhemispheric transfer time in patients with auditory hallucinations: an auditory event-related potential study.\n\nPubMed\n\nHenshall, Katherine R; Sergejew, Alex A; McKay, Colette M; Rance, Gary; Shea, Tracey L; Hayden, Melissa J; Innes-Brown, Hamish; Copolov, David L\n\n2012-05-01\n\nCentral auditory processing in schizophrenia patients with a history of auditory hallucinations has been reported to be impaired, and abnormalities of interhemispheric transfer have been implicated in these patients. This study examined interhemispheric functional connectivity between auditory cortical regions, using temporal information obtained from latency measures of the auditory N1 evoked potential. Interhemispheric Transfer Times (IHTTs) were compared across 3 subject groups: schizophrenia patients who had experienced auditory hallucinations, schizophrenia patients without a history of auditory hallucinations, and normal controls. Pure tones and single-syllable words were presented monaurally to each ear, while EEG was recorded continuously. IHTT was calculated for each stimulus type by comparing the latencies of the auditory N1 evoked potential recorded contralaterally and ipsilaterally to the ear of stimulation. The IHTTs for pure tones did not differ between groups. For word stimuli, the IHTT was significantly different across the 3 groups: the IHTT was close to zero in normal controls, was highest in the AH group, and was negative (shorter latencies ipsilaterally) in the nonAH group. Differences in IHTTs may be attributed to transcallosal dysfunction in the AH group, but altered or reversed cerebral lateralization in nonAH participants is also possible. Copyright Â© 2012 Elsevier B.V. All rights reserved.\n\nAuditory and visual interhemispheric communication in musicians and non-musicians.\n\nPubMed\n\nWoelfle, Rebecca; Grahn, Jessica A\n\n2013-01-01\n\nThe corpus callosum (CC) is a brain structure composed of axon fibres linking the right and left hemispheres. Musical training is associated with larger midsagittal cross-sectional area of the CC, suggesting that interhemispheric communication may be faster in musicians. Here we compared interhemispheric transmission times (ITTs) for musicians and non-musicians. ITT was measured by comparing simple reaction times to stimuli presented to the same hemisphere that controlled a button-press response (uncrossed reaction time), or to the contralateral hemisphere (crossed reaction time). Both visual and auditory stimuli were tested. We predicted that the crossed-uncrossed difference (CUD) for musicians would be smaller than for non-musicians as a result of faster interhemispheric transfer times. We did not expect a difference in CUDs between the visual and auditory modalities for either musicians or non-musicians, as previous work indicates that interhemispheric transfer may happen through the genu of the CC, which contains motor fibres rather than sensory fibres. There were no significant differences in CUDs between musicians and non-musicians. However, auditory CUDs were significantly smaller than visual CUDs. Although this auditory-visual difference was larger in musicians than non-musicians, the interaction between modality and musical training was not significant. Therefore, although musical training does not significantly affect ITT, the crossing of auditory information between hemispheres appears to be faster than visual information, perhaps because subcortical pathways play a greater role for auditory interhemispheric transfer.\n\nAuditory and Visual Interhemispheric Communication in Musicians and Non-Musicians\n\nPubMed Central\n\nWoelfle, Rebecca; Grahn, Jessica A.\n\n2013-01-01\n\nThe corpus callosum (CC) is a brain structure composed of axon fibres linking the right and left hemispheres. Musical training is associated with larger midsagittal cross-sectional area of the CC, suggesting that interhemispheric communication may be faster in musicians. Here we compared interhemispheric transmission times (ITTs) for musicians and non-musicians. ITT was measured by comparing simple reaction times to stimuli presented to the same hemisphere that controlled a button-press response (uncrossed reaction time), or to the contralateral hemisphere (crossed reaction time). Both visual and auditory stimuli were tested. We predicted that the crossed-uncrossed difference (CUD) for musicians would be smaller than for non-musicians as a result of faster interhemispheric transfer times. We did not expect a difference in CUDs between the visual and auditory modalities for either musicians or non-musicians, as previous work indicates that interhemispheric transfer may happen through the genu of the CC, which contains motor fibres rather than sensory fibres. There were no significant differences in CUDs between musicians and non-musicians. However, auditory CUDs were significantly smaller than visual CUDs. Although this auditory-visual difference was larger in musicians than non-musicians, the interaction between modality and musical training was not significant. Therefore, although musical training does not significantly affect ITT, the crossing of auditory information between hemispheres appears to be faster than visual information, perhaps because subcortical pathways play a greater role for auditory interhemispheric transfer. PMID:24386382\n\nI can see what you are saying: Auditory labels reduce visual search times.\n\nPubMed\n\nCho, Kit W\n\n2016-10-01\n\nThe present study explored the self-directed-speech effect, the finding that relative to silent reading of a label (e.g., DOG), saying it aloud reduces visual search reaction times (RTs) for locating a target picture among distractors. Experiment 1 examined whether this effect is due to a confound in the differences in the number of cues in self-directed speech (two) vs. silent reading (one) and tested whether self-articulation is required for the effect. The results showed that self-articulation is not required and that merely hearing the auditory label reduces visual search RTs relative to silent reading. This finding also rules out the number of cues confound. Experiment 2 examined whether hearing an auditory label activates more prototypical features of the label's referent and whether the auditory-label benefit is moderated by the target's imagery concordance (the degree to which the target picture matches the mental picture that is activated by a written label for the target). When the target imagery concordance was high, RTs following the presentation of a high prototypicality picture or auditory cue were comparable and shorter than RTs following a visual label or low prototypicality picture cue. However, when the target imagery concordance was low, RTs following an auditory cue were shorter than the comparable RTs following the picture cues and visual-label cue. The results suggest that an auditory label activates both prototypical and atypical features of a concept and can facilitate visual search RTs even when compared to picture primes. Copyright Â© 2016 Elsevier B.V. All rights reserved.\n\nContinuous time wavelet entropy of auditory evoked potentials.\n\nPubMed\n\nCek, M Emre; Ozgoren, Murat; Savaci, F Acar\n\n2010-01-01\n\nIn this paper, the continuous time wavelet entropy (CTWE) of auditory evoked potentials (AEP) has been characterized by evaluating the relative wavelet energies (RWE) in specified EEG frequency bands. Thus, the rapid variations of CTWE due to the auditory stimulation could be detected in post-stimulus time interval. This approach removes the probability of missing the information hidden in short time intervals. The discrete time and continuous time wavelet based wavelet entropy variations were compared on non-target and target AEP data. It was observed that CTWE can also be an alternative method to analyze entropy as a function of time. 2009 Elsevier Ltd. All rights reserved.\n\nTask modulation of the effects of brightness on reaction time and response force.\n\nPubMed\n\nJaÅkowski, Piotr; WÅodarczyk, Dariusz\n\n2006-08-01\n\nVan der Molen and Keuss [van der Molen, M.W., Keuss, P.J.G., 1979. The relationship between reaction time and intensity in discrete auditory tasks. Quarterly Journal of Experimental Psychology 31, 95-102; van der Molen, M.W., Keuss, P.J.G., 1981. Response selection and the processing of auditory intensity. Quarterly Journal of Experimental Psychology 33, 177-184] showed that paradoxically long reaction times (RT) occur with extremely loud auditory stimuli when the task is difficult (e.g. needs a response choice). It was argued that this paradoxical behavior of RT is due to active suppression of response prompting to prevent false responses. In the present experiments, we demonstrated that such an effect can also occur for visual stimuli provided that they are large enough. Additionally, we showed that response force exerted by participants on response keys monotonically grew with intensity for large stimuli but was independent of intensity for small visual stimuli. Bearing in mind that only large stimuli are believed to be arousing this pattern of results supports the arousal interpretation of the negative effect of loud stimuli on RT given by van der Molen and Keuss.\n\nAn investigation of leg and trunk strength and reaction times of hard-style martial arts practitioners.\n\nPubMed\n\nDonovan, Oliver O; Cheung, Jeanette; Catley, Maria; McGregor, Alison H; Strutton, Paul H\n\n2006-01-01\n\nThe purpose of this study was to investigate trunk and knee strength in practitioners of hard-style martial arts. An additional objective was to examine reaction times in these participants by measuring simple reaction times (SRT), choice reaction times (CRT) and movement times (MT). Thirteen high-level martial artists and twelve sedentary participants were tested under isokinetic and isometric conditions on an isokinetic dynamometer. Response and movement times were also measured in response to simple and choice auditory cues. Results indicated that the martial arts group generated a greater body-weight adjusted peak torque with both legs at all speeds during isokinetic extension and flexion, and in isometric extension but not flexion. In isokinetic and isometric trunk flexion and extension, martial artists tended to have higher peak torques than controls, but they were not significantly different (p > 0.05). During the SRT and CRT tasks the martial artists were no quicker in lifting their hand off a button in response to the stimulus [reaction time (RT)] but were significantly faster in moving to press another button [movement time (MT)]. In conclusion, the results reveal that training in a martial art increases the strength of both the flexors and extensors of the leg. Furthermore, they have faster movement times to auditory stimuli. These results are consistent with the physical aspects of the martial arts. Key PointsMartial artists undertaking hard-style martial arts have greater strength in their knee flexor and extensor muscles as tested under isokinetic testing. Under isometric testing conditions they have stronger knee extensors only.The trunk musculature is generally higher under both conditions of testing in the martial artists, although not significantly.The total reaction times of the martial artists to an auditory stimulus were significantly faster than the control participants. When analysed further it was revealed that the decrease in reaction time\n\nAn Investigation Of Leg And Trunk Strength And Reaction Times Of Hard-Style Martial Arts Practitioners\n\nPubMed Central\n\nDonovan, Oliver O; Cheung, Jeanette; Catley, Maria; McGregor, Alison H.; Strutton, Paul H.\n\n2006-01-01\n\nThe purpose of this study was to investigate trunk and knee strength in practitioners of hard-style martial arts. An additional objective was to examine reaction times in these participants by measuring simple reaction times (SRT), choice reaction times (CRT) and movement times (MT). Thirteen high-level martial artists and twelve sedentary participants were tested under isokinetic and isometric conditions on an isokinetic dynamometer. Response and movement times were also measured in response to simple and choice auditory cues. Results indicated that the martial arts group generated a greater body-weight adjusted peak torque with both legs at all speeds during isokinetic extension and flexion, and in isometric extension but not flexion. In isokinetic and isometric trunk flexion and extension, martial artists tended to have higher peak torques than controls, but they were not significantly different (p > 0.05). During the SRT and CRT tasks the martial artists were no quicker in lifting their hand off a button in response to the stimulus [reaction time (RT)] but were significantly faster in moving to press another button [movement time (MT)]. In conclusion, the results reveal that training in a martial art increases the strength of both the flexors and extensors of the leg. Furthermore, they have faster movement times to auditory stimuli. These results are consistent with the physical aspects of the martial arts. Key Points Martial artists undertaking hard-style martial arts have greater strength in their knee flexor and extensor muscles as tested under isokinetic testing. Under isometric testing conditions they have stronger knee extensors only. The trunk musculature is generally higher under both conditions of testing in the martial artists, although not significantly. The total reaction times of the martial artists to an auditory stimulus were significantly faster than the control participants. When analysed further it was revealed that the decrease in reaction\n\nModerate anxiety modifies the electromyographic activity of a forearm muscle during a time-reaction task in women.\n\nPubMed\n\nLanglet, C; Hainaut, J P; Bolmont, B\n\n2017-03-16\n\nArousal anxiety has a great impact on reaction time, physiological parameters and motor performance. Numerous studies have focused on the influence of anxiety on muscular activity during simple non ecologic task. We investigate the impact of a moderate state-anxiety (arousal stressor) on the specific component of a complex multi-joint ecologic movement during a reaction time task of auditory stimulus-response. Our objective is to know if central and peripheral voluntary motor processes were modulated in the same way by an arousal stressor. Eighteen women volunteers performed simple reaction time tasks of auditory stimulus-response. Video-recorded Stroop test with interferences was used to induced moderate state-anxiety. Electromyographic activity of the wrist extensor was recorded in order to analyse the two components of the reaction time: the premotor and motor time. In anxiogenic condition, an acceleration and an increase of muscular activity of the reaction time was obtained. This increase was due to a stronger muscle activity during the premotor time in the anxiogenic condition. Arousal anxiety has a different impact on central and peripheral voluntary motor processes. The modifications observed could be related to an increase in arousal related to a higher anxiety in order to prepare the body to act. Copyright Â© 2017 Elsevier B.V. All rights reserved.\n\nThe processing of auditory and visual recognition of self-stimuli.\n\nPubMed\n\nHughes, Susan M; Nicholson, Shevon E\n\n2010-12-01\n\nThis study examined self-recognition processing in both the auditory and visual modalities by determining how comparable hearing a recording of one's own voice was to seeing photograph of one's own face. We also investigated whether the simultaneous presentation of auditory and visual self-stimuli would either facilitate or inhibit self-identification. Ninety-one participants completed reaction-time tasks of self-recognition when presented with their own faces, own voices, and combinations of the two. Reaction time and errors made when responding with both the right and left hand were recorded to determine if there were lateralization effects on these tasks. Our findings showed that visual self-recognition for facial photographs appears to be superior to auditory self-recognition for voice recordings. Furthermore, a combined presentation of one's own face and voice appeared to inhibit rather than facilitate self-recognition and there was a left-hand advantage for reaction time on the combined-presentation tasks. Copyright Â© 2010 Elsevier Inc. All rights reserved.\n\nVisual Timing of Structured Dance Movements Resembles Auditory Rhythm Perception.\n\nPubMed\n\nSu, Yi-Huang; Salazar-LÃ³pez, Elvira\n\n2016-01-01\n\nTemporal mechanisms for processing auditory musical rhythms are well established, in which a perceived beat is beneficial for timing purposes. It is yet unknown whether such beat-based timing would also underlie visual perception of temporally structured, ecological stimuli connected to music: dance. In this study, we investigated whether observers extracted a visual beat when watching dance movements to assist visual timing of these movements. Participants watched silent videos of dance sequences and reproduced the movement duration by mental recall. We found better visual timing for limb movements with regular patterns in the trajectories than without, similar to the beat advantage for auditory rhythms. When movements involved both the arms and the legs, the benefit of a visual beat relied only on the latter. The beat-based advantage persisted despite auditory interferences that were temporally incongruent with the visual beat, arguing for the visual nature of these mechanisms. Our results suggest that visual timing principles for dance parallel their auditory counterparts for music, which may be based on common sensorimotor coupling. These processes likely yield multimodal rhythm representations in the scenario of music and dance.\n\nVisual Timing of Structured Dance Movements Resembles Auditory Rhythm Perception\n\nPubMed Central\n\nSu, Yi-Huang; Salazar-LÃ³pez, Elvira\n\n2016-01-01\n\nTemporal mechanisms for processing auditory musical rhythms are well established, in which a perceived beat is beneficial for timing purposes. It is yet unknown whether such beat-based timing would also underlie visual perception of temporally structured, ecological stimuli connected to music: dance. In this study, we investigated whether observers extracted a visual beat when watching dance movements to assist visual timing of these movements. Participants watched silent videos of dance sequences and reproduced the movement duration by mental recall. We found better visual timing for limb movements with regular patterns in the trajectories than without, similar to the beat advantage for auditory rhythms. When movements involved both the arms and the legs, the benefit of a visual beat relied only on the latter. The beat-based advantage persisted despite auditory interferences that were temporally incongruent with the visual beat, arguing for the visual nature of these mechanisms. Our results suggest that visual timing principles for dance parallel their auditory counterparts for music, which may be based on common sensorimotor coupling. These processes likely yield multimodal rhythm representations in the scenario of music and dance. PMID:27313900\n\nOdors Bias Time Perception in Visual and Auditory Modalities\n\nPubMed Central\n\nYue, Zhenzhu; Gao, Tianyu; Chen, Lihan; Wu, Jiashuang\n\n2016-01-01\n\nPrevious studies have shown that emotional states alter our perception of time. However, attention, which is modulated by a number of factors, such as emotional events, also influences time perception. To exclude potential attentional effects associated with emotional events, various types of odors (inducing different levels of emotional arousal) were used to explore whether olfactory events modulated time perception differently in visual and auditory modalities. Participants were shown either a visual dot or heard a continuous tone for 1000 or 4000 ms while they were exposed to odors of jasmine, lavender, or garlic. Participants then reproduced the temporal durations of the preceding visual or auditory stimuli by pressing the spacebar twice. Their reproduced durations were compared to those in the control condition (without odor). The results showed that participants produced significantly longer time intervals in the lavender condition than in the jasmine or garlic conditions. The overall influence of odor on time perception was equivalent for both visual and auditory modalities. The analysis of the interaction effect showed that participants produced longer durations than the actual duration in the short interval condition, but they produced shorter durations in the long interval condition. The effect sizes were larger for the auditory modality than those for the visual modality. Moreover, by comparing performance across the initial and the final blocks of the experiment, we found odor adaptation effects were mainly manifested as longer reproductions for the short time interval later in the adaptation phase, and there was a larger effect size in the auditory modality. In summary, the present results indicate that odors imposed differential impacts on reproduced time durations, and they were constrained by different sensory modalities, valence of the emotional events, and target durations. Biases in time perception could be accounted for by a framework of\n\nImpairment of Auditory-Motor Timing and Compensatory Reorganization after Ventral Premotor Cortex Stimulation\n\nPubMed Central\n\nKornysheva, Katja; Schubotz, Ricarda I.\n\n2011-01-01\n\nIntegrating auditory and motor information often requires precise timing as in speech and music. In humans, the position of the ventral premotor cortex (PMv) in the dorsal auditory stream renders this area a node for auditory-motor integration. Yet, it remains unknown whether the PMv is critical for auditory-motor timing and which activity increases help to preserve task performance following its disruption. 16 healthy volunteers participated in two sessions with fMRI measured at baseline and following rTMS (rTMS) of either the left PMv or a control region. Subjects synchronized left or right finger tapping to sub-second beat rates of auditory rhythms in the experimental task, and produced self-paced tapping during spectrally matched auditory stimuli in the control task. Left PMv rTMS impaired auditory-motor synchronization accuracy in the first sub-block following stimulation (p<0.01, Bonferroni corrected), but spared motor timing and attention to task. Task-related activity increased in the homologue right PMv, but did not predict the behavioral effect of rTMS. In contrast, anterior midline cerebellum revealed most pronounced activity increase in less impaired subjects. The present findings suggest a critical role of the left PMv in feed-forward computations enabling accurate auditory-motor timing, which can be compensated by activity modulations in the cerebellum, but not in the homologue region contralateral to stimulation. PMID:21738657\n\nA psychophysiological evaluation of the perceived urgency of auditory warning signals\n\nNASA Technical Reports Server (NTRS)\n\nBurt, J. L.; Bartolome, D. S.; Burdette, D. W.; Comstock, J. R. Jr\n\n1995-01-01\n\nOne significant concern that pilots have about cockpit auditory warnings is that the signals presently used lack a sense of priority. The relationship between auditory warning sound parameters and perceived urgency is, therefore, an important topic of enquiry in aviation psychology. The present investigation examined the relationship among subjective assessments of urgency, reaction time, and brainwave activity with three auditory warning signals. Subjects performed a tracking task involving automated and manual conditions, and were presented with auditory warnings having various levels of perceived and situational urgency. Subjective assessments revealed that subjects were able to rank warnings on an urgency scale, but rankings were altered after warnings were mapped to a situational urgency scale. Reaction times differed between automated and manual tracking task conditions, and physiological data showed attentional differences in response to perceived and situational warning urgency levels. This study shows that the use of physiological measures sensitive to attention and arousal, in conjunction with behavioural and subjective measures, may lead to the design of auditory warnings that produce a sense of urgency in an operator that matches the urgency of the situation.\n\nAuditory display as feedback for a novel eye-tracking system for sterile operating room interaction.\n\nPubMed\n\nBlack, David; Unger, Michael; Fischer, Nele; Kikinis, Ron; Hahn, Horst; Neumuth, Thomas; Glaser, Bernhard\n\n2018-01-01\n\nThe growing number of technical systems in the operating room has increased attention on developing touchless interaction methods for sterile conditions. However, touchless interaction paradigms lack the tactile feedback found in common input devices such as mice and keyboards. We propose a novel touchless eye-tracking interaction system with auditory display as a feedback method for completing typical operating room tasks. Auditory display provides feedback concerning the selected input into the eye-tracking system as well as a confirmation of the system response. An eye-tracking system with a novel auditory display using both earcons and parameter-mapping sonification was developed to allow touchless interaction for six typical scrub nurse tasks. An evaluation with novice participants compared auditory display with visual display with respect to reaction time and a series of subjective measures. When using auditory display to substitute for the lost tactile feedback during eye-tracking interaction, participants exhibit reduced reaction time compared to using visual-only display. In addition, the auditory feedback led to lower subjective workload and higher usefulness and system acceptance ratings. Due to the absence of tactile feedback for eye-tracking and other touchless interaction methods, auditory display is shown to be a useful and necessary addition to new interaction concepts for the sterile operating room, reducing reaction times while improving subjective measures, including usefulness, user satisfaction, and cognitive workload.\n\nSimple reaction time to the onset of time-varying sounds.\n\nPubMed\n\nSchlittenlacher, Josef; Ellermeier, Wolfgang\n\n2015-10-01\n\nAlthough auditory simple reaction time (RT) is usually defined as the time elapsing between the onset of a stimulus and a recorded reaction, a sound cannot be specified by a single point in time. Therefore, the present work investigates how the period of time immediately after onset affects RT. By varying the stimulus duration between 10 and 500 msec, this critical duration was determined to fall between 32 and 40 milliseconds for a 1-kHz pure tone at 70 dB SPL. In a second experiment, the role of the buildup was further investigated by varying the rise time and its shape. The increment in RT for extending the rise time by a factor of ten was about 7 to 8 msec. There was no statistically significant difference in RT between a Gaussian and linear rise shape. A third experiment varied the modulation frequency and point of onset of amplitude-modulated tones, producing onsets at different initial levels with differently rapid increase or decrease immediately afterwards. The results of all three experiments results were explained very well by a straightforward extension of the parallel grains model (Miller and Ulrich Cogn. Psychol. 46, 101-151, 2003), a probabilistic race model employing many parallel channels. The extension of the model to time-varying sounds made the activation of such a grain depend on intensity as a function of time rather than a constant level. A second approach by mechanisms known from loudness produced less accurate predictions.\n\nHuman engineer's guide to auditory displays. Volume 2: Elements of signal reception and resolution affecting auditory displays\n\nNASA Astrophysics Data System (ADS)\n\nMulligan, B. E.; Goodman, L. S.; McBride, D. K.; Mitchell, T. M.; Crosby, T. N.\n\n1984-08-01\n\nThis work reviews the areas of monaural and binaural signal detection, auditory discrimination and localization, and reaction times to acoustic signals. The review was written from the perspective of human engineering and focuses primarily on auditory processing of information contained in acoustic signals. The impetus for this effort was to establish a data base to be utilized in the design and evaluation of acoustic displays. Appendix 1 also contains citations of the scientific literature on which was based the answers to each question. There are nineteen questions and answers, and more than two hundred citations contained in the list of references given in Appendix 2. This is one of two related works, the other of which reviewed the literature in the areas of auditory attention, recognition memory, and auditory perception of patterns, pitch, and loudness.\n\nAuditory hallucinations and PTSD in ex-POWS.\n\nPubMed\n\nCrompton, Laura; Lahav, Yael; Solomon, Zahava\n\n2017-01-01\n\nLiterature has suggested that auditory hallucinations might be prevalent in the general population and could be linked to the experience of trauma. This prospective study examines the prevalence of auditory hallucinations in trauma survivors and its association with posttraumatic stress disorder (PTSD) symptoms, over time. Former prisoners of war (ex-POWs) from the 1973 Yom Kippur War (nÂ =Â 99) with and without PTSD and comparable veterans (nÂ =Â 103) were assessed twice, in 1991 (T1) and 2003 (T2) in regard to auditory hallucinations and PTSD symptoms. Findings indicated that ex-POWs who suffered from PTSD reported higher levels of auditory hallucinations at T2 as well as increased hallucinations over time, compared to ex-POWs without PTSD and combatants who did not endure captivity. The relation between PTSD and auditory hallucinations was unidirectional, so that the PTSD overall score at T1 predicted an increase in auditory hallucinations between T1 and T2, but not vice versa. Assessing the role of PTSD clusters in predicting hallucinations revealed that intrusion symptoms had a unique contribution, compared to avoidance and hyperarousal symptoms. The findings suggest that auditory hallucinations might be a consequence of the posttraumatic reaction among veterans.\n\nAuditory temporal preparation induced by rhythmic cues during concurrent auditory working memory tasks.\n\nPubMed\n\nCutanda, Diana; Correa, Ãngel; Sanabria, Daniel\n\n2015-06-01\n\nThe present study investigated whether participants can develop temporal preparation driven by auditory isochronous rhythms when concurrently performing an auditory working memory (WM) task. In Experiment 1, participants had to respond to an auditory target presented after a regular or an irregular sequence of auditory stimuli while concurrently performing a Sternberg-type WM task. Results showed that participants responded faster after regular compared with irregular rhythms and that this effect was not affected by WM load; however, the lack of a significant main effect of WM load made it difficult to draw any conclusion regarding the influence of the dual-task manipulation in Experiment 1. In order to enhance dual-task interference, Experiment 2 combined the auditory rhythm procedure with an auditory N-Back task, which required WM updating (monitoring and coding of the information) and was presumably more demanding than the mere rehearsal of the WM task used in Experiment 1. Results now clearly showed dual-task interference effects (slower reaction times [RTs] in the high- vs. the low-load condition). However, such interference did not affect temporal preparation induced by rhythms, with faster RTs after regular than after irregular sequences in the high-load and low-load conditions. These results revealed that secondary tasks demanding memory updating, relative to tasks just demanding rehearsal, produced larger interference effects on overall RTs in the auditory rhythm task. Nevertheless, rhythm regularity exerted a strong temporal preparation effect that survived the interference of the WM task even when both tasks competed for processing resources within the auditory modality. (c) 2015 APA, all rights reserved).\n\nMoving in time: Bayesian causal inference explains movement coordination to auditory beats\n\nPubMed Central\n\nElliott, Mark T.; Wing, Alan M.; Welchman, Andrew E.\n\n2014-01-01\n\nMany everyday skilled actions depend on moving in time with signals that are embedded in complex auditory streams (e.g. musical performance, dancing or simply holding a conversation). Such behaviour is apparently effortless; however, it is not known how humans combine auditory signals to support movement production and coordination. Here, we test how participants synchronize their movements when there are potentially conflicting auditory targets to guide their actions. Participants tapped their fingers in time with two simultaneously presented metronomes of equal tempo, but differing in phase and temporal regularity. Synchronization therefore depended on integrating the two timing cues into a single-event estimate or treating the cues as independent and thereby selecting one signal over the other. We show that a Bayesian inference process explains the situations in which participants choose to integrate or separate signals, and predicts motor timing errors. Simulations of this causal inference process demonstrate that this model provides a better description of the data than other plausible models. Our findings suggest that humans exploit a Bayesian inference process to control movement timing in situations where the origin of auditory signals needs to be resolved. PMID:24850915\n\nDividing time: concurrent timing of auditory and visual events by young and elderly adults.\n\nPubMed\n\nMcAuley, J Devin; Miller, Jonathan P; Wang, Mo; Pang, Kevin C H\n\n2010-07-01\n\nThis article examines age differences in individual's ability to produce the durations of learned auditory and visual target events either in isolation (focused attention) or concurrently (divided attention). Young adults produced learned target durations equally well in focused and divided attention conditions. Older adults, in contrast, showed an age-related increase in timing variability in divided attention conditions that tended to be more pronounced for visual targets than for auditory targets. Age-related impairments were associated with a decrease in working memory span; moreover, the relationship between working memory and timing performance was largest for visual targets in divided attention conditions.\n\nAbsence of both auditory evoked potentials and auditory percepts dependent on timing cues.\n\nPubMed\n\nStarr, A; McPherson, D; Patterson, J; Don, M; Luxford, W; Shannon, R; Sininger, Y; Tonakawa, L; Waring, M\n\n1991-06-01\n\nAn 11-yr-old girl had an absence of sensory components of auditory evoked potentials (brainstem, middle and long-latency) to click and tone burst stimuli that she could clearly hear. Psychoacoustic tests revealed a marked impairment of those auditory perceptions dependent on temporal cues, that is, lateralization of binaural clicks, change of binaural masked threshold with changes in signal phase, binaural beats, detection of paired monaural clicks, monaural detection of a silent gap in a sound, and monaural threshold elevation for short duration tones. In contrast, auditory functions reflecting intensity or frequency discriminations (difference limens) were only minimally impaired. Pure tone audiometry showed a moderate (50 dB) bilateral hearing loss with a disproportionate severe loss of word intelligibility. Those auditory evoked potentials that were preserved included (1) cochlear microphonics reflecting hair cell activity; (2) cortical sustained potentials reflecting processing of slowly changing signals; and (3) long-latency cognitive components (P300, processing negativity) reflecting endogenous auditory cognitive processes. Both the evoked potential and perceptual deficits are attributed to changes in temporal encoding of acoustic signals perhaps occurring at the synapse between hair cell and eighth nerve dendrites. The results from this patient are discussed in relation to previously published cases with absent auditory evoked potentials and preserved hearing.\n\nIntertrial auditory neural stability supports beat synchronization in preschoolers\n\nPubMed Central\n\nCarr, Kali Woodruff; Tierney, Adam; White-Schwoch, Travis; Kraus, Nina\n\n2016-01-01\n\nThe ability to synchronize motor movements along with an auditory beat places stringent demands on the temporal processing and sensorimotor integration capabilities of the nervous system. Links between millisecond-level precision of auditory processing and the consistency of sensorimotor beat synchronization implicate fine auditory neural timing as a mechanism for forming stable internal representations of, and behavioral reactions to, sound. Here, for the first time, we demonstrate a systematic relationship between consistency of beat synchronization and trial-by-trial stability of subcortical speech processing in preschoolers (ages 3 and 4 years old). We conclude that beat synchronization might provide a useful window into millisecond-level neural precision for encoding sound in early childhood, when speech processing is especially important for language acquisition and development. PMID:26760457\n\nTime course of dynamic range adaptation in the auditory nerve\n\nPubMed Central\n\nWang, Grace I.; Dean, Isabel; Delgutte, Bertrand\n\n2012-01-01\n\nAuditory adaptation to sound-level statistics occurs as early as in the auditory nerve (AN), the first stage of neural auditory processing. In addition to firing rate adaptation characterized by a rate decrement dependent on previous spike activity, AN fibers show dynamic range adaptation, which is characterized by a shift of the rate-level function or dynamic range toward the most frequently occurring levels in a dynamic stimulus, thereby improving the precision of coding of the most common sound levels (Wen B, Wang GI, Dean I, Delgutte B. J Neurosci 29: 13797â13808, 2009). We investigated the time course of dynamic range adaptation by recording from AN fibers with a stimulus in which the sound levels periodically switch from one nonuniform level distribution to another (Dean I, Robinson BL, Harper NS, McAlpine D. J Neurosci 28: 6430â6438, 2008). Dynamic range adaptation occurred rapidly, but its exact time course was difficult to determine directly from the data because of the concomitant firing rate adaptation. To characterize the time course of dynamic range adaptation without the confound of firing rate adaptation, we developed a phenomenological âdual adaptationâ model that accounts for both forms of AN adaptation. When fitted to the data, the model predicts that dynamic range adaptation occurs as rapidly as firing rate adaptation, over 100â400 ms, and the time constants of the two forms of adaptation are correlated. These findings suggest that adaptive processing in the auditory periphery in response to changes in mean sound level occurs rapidly enough to have significant impact on the coding of natural sounds. PMID:22457465\n\nTime-Varying Vocal Folds Vibration Detection Using a 24 GHz Portable Auditory Radar.\n\nPubMed\n\nHong, Hong; Zhao, Heng; Peng, Zhengyu; Li, Hui; Gu, Chen; Li, Changzhi; Zhu, Xiaohua\n\n2016-07-28\n\nTime-varying vocal folds vibration information is of crucial importance in speech processing, and the traditional devices to acquire speech signals are easily smeared by the high background noise and voice interference. In this paper, we present a non-acoustic way to capture the human vocal folds vibration using a 24-GHz portable auditory radar. Since the vocal folds vibration only reaches several millimeters, the high operating frequency and the 4 Ã 4 array antennas are applied to achieve the high sensitivity. The Variational Mode Decomposition (VMD) based algorithm is proposed to decompose the radar-detected auditory signal into a sequence of intrinsic modes firstly, and then, extract the time-varying vocal folds vibration frequency from the corresponding mode. Feasibility demonstration, evaluation, and comparison are conducted with tonal and non-tonal languages, and the low relative errors show a high consistency between the radar-detected auditory time-varying vocal folds vibration and acoustic fundamental frequency, except that the auditory radar significantly improves the frequency-resolving power.\n\nIncidental Auditory Category Learning\n\nPubMed Central\n\nGabay, Yafit; Dick, Frederic K.; Zevin, Jason D.; Holt, Lori L.\n\n2015-01-01\n\nVery little is known about how auditory categories are learned incidentally, without instructions to search for category-diagnostic dimensions, overt category decisions, or experimenter-provided feedback. This is an important gap because learning in the natural environment does not arise from explicit feedback and there is evidence that the learning systems engaged by traditional tasks are distinct from those recruited by incidental category learning. We examined incidental auditory category learning with a novel paradigm, the Systematic Multimodal Associations Reaction Time (SMART) task, in which participants rapidly detect and report the appearance of a visual target in one of four possible screen locations. Although the overt task is rapid visual detection, a brief sequence of sounds precedes each visual target. These sounds are drawn from one of four distinct sound categories that predict the location of the upcoming visual target. These many-to-one auditory-to-visuomotor correspondences support incidental auditory category learning. Participants incidentally learn categories of complex acoustic exemplars and generalize this learning to novel exemplars and tasks. Further, learning is facilitated when category exemplar variability is more tightly coupled to the visuomotor associations than when the same stimulus variability is experienced across trials. We relate these findings to phonetic category learning. PMID:26010588\n\nOSP Parameters and the Cognitive Component of Reaction Time to a Missing Stimulus: Linking Brain and Behavior\n\nERIC Educational Resources Information Center\n\nHernandez, Oscar H.; Vogel-Sprott, Muriel\n\n2009-01-01\n\nThis within-subjects experiment tested the relationship between the premotor (cognitive) component of reaction time (RT) to a missing stimulus and parameters of the omitted stimulus potential (OSP) brain wave. Healthy young men (N = 28) completed trials with an auditory stimulus that recurred at 2 s intervals and ceased unpredictably. Premotor RTâ¦\n\nTime-Varying Vocal Folds Vibration Detection Using a 24 GHz Portable Auditory Radar\n\nPubMed Central\n\nHong, Hong; Zhao, Heng; Peng, Zhengyu; Li, Hui; Gu, Chen; Li, Changzhi; Zhu, Xiaohua\n\n2016-01-01\n\nTime-varying vocal folds vibration information is of crucial importance in speech processing, and the traditional devices to acquire speech signals are easily smeared by the high background noise and voice interference. In this paper, we present a non-acoustic way to capture the human vocal folds vibration using a 24-GHz portable auditory radar. Since the vocal folds vibration only reaches several millimeters, the high operating frequency and the 4 Ã 4 array antennas are applied to achieve the high sensitivity. The Variational Mode Decomposition (VMD) based algorithm is proposed to decompose the radar-detected auditory signal into a sequence of intrinsic modes firstly, and then, extract the time-varying vocal folds vibration frequency from the corresponding mode. Feasibility demonstration, evaluation, and comparison are conducted with tonal and non-tonal languages, and the low relative errors show a high consistency between the radar-detected auditory time-varying vocal folds vibration and acoustic fundamental frequency, except that the auditory radar significantly improves the frequency-resolving power. PMID:27483261\n\nAuditory priming improves neural synchronization in auditory-motor entrainment.\n\nPubMed\n\nCrasta, Jewel E; Thaut, Michael H; Anderson, Charles W; Davies, Patricia L; Gavin, William J\n\n2018-05-22\n\nNeurophysiological research has shown that auditory and motor systems interact during movement to rhythmic auditory stimuli through a process called entrainment. This study explores the neural oscillations underlying auditory-motor entrainment using electroencephalography. Forty young adults were randomly assigned to one of two control conditions, an auditory-only condition or a motor-only condition, prior to a rhythmic auditory-motor synchronization condition (referred to as combined condition). Participants assigned to the auditory-only condition auditory-first group) listened to 400 trials of auditory stimuli presented every 800â¯ms, while those in the motor-only condition (motor-first group) were asked to tap rhythmically every 800â¯ms without any external stimuli. Following their control condition, all participants completed an auditory-motor combined condition that required tapping along with auditory stimuli every 800â¯ms. As expected, the neural processes for the combined condition for each group were different compared to their respective control condition. Time-frequency analysis of total power at an electrode site on the left central scalp (C3) indicated that the neural oscillations elicited by auditory stimuli, especially in the beta and gamma range, drove the auditory-motor entrainment. For the combined condition, the auditory-first group had significantly lower evoked power for a region of interest representing sensorimotor processing (4-20â¯Hz) and less total power in a region associated with anticipation and predictive timing (13-16â¯Hz) than the motor-first group. Thus, the auditory-only condition served as a priming facilitator of the neural processes in the combined condition, more so than the motor-only condition. Results suggest that even brief periods of rhythmic training of the auditory system leads to neural efficiency facilitating the motor system during the process of entrainment. These findings have implications for interventions\n\nAnalysis of Reaction Times and Aerobic Capacities of Soccer Players According to Their Playing Positions\n\nERIC Educational Resources Information Center\n\nTaskin, Cengiz; Karakoc, Onder; Taskin, Mine; Dural, Murat\n\n2016-01-01\n\n70 soccer players in Gaziantep amateur league voluntarily participated in this study, (average of their ages 19,17Â±1,34years, average of their heights 181,28Â±5,06 cm, average of their body weights 76,75Â±4,43 kg and average of their sports experiences 3,78Â±0,95 years) to analyze visual and auditory reaction times and aerobic capacities of amateurâ¦\n\nActivity in the left auditory cortex is associated with individual impulsivity in time discounting.\n\nPubMed\n\nHan, Ruokang; Takahashi, Taiki; Miyazaki, Akane; Kadoya, Tomoka; Kato, Shinya; Yokosawa, Koichi\n\n2015-01-01\n\nImpulsivity dictates individual decision-making behavior. Therefore, it can reflect consumption behavior and risk of addiction and thus underlies social activities as well. Neuroscience has been applied to explain social activities; however, the brain function controlling impulsivity has remained unclear. It is known that impulsivity is related to individual time perception, i.e., a person who perceives a certain physical time as being longer is impulsive. Here we show that activity of the left auditory cortex is related to individual impulsivity. Individual impulsivity was evaluated by a self-answered questionnaire in twelve healthy right-handed adults, and activities of the auditory cortices of bilateral hemispheres when listening to continuous tones were recorded by magnetoencephalography. Sustained activity of the left auditory cortex was significantly correlated to impulsivity, that is, larger sustained activity indicated stronger impulsivity. The results suggest that the left auditory cortex represent time perception, probably because the area is involved in speech perception, and that it represents impulsivity indirectly.\n\nSpeech Compensation for Time-Scale-Modified Auditory Feedback\n\nERIC Educational Resources Information Center\n\nOgane, Rintaro; Honda, Masaaki\n\n2014-01-01\n\nPurpose: The purpose of this study was to examine speech compensation in response to time-scale-modified auditory feedback during the transition of the semivowel for a target utterance of /ija/. Method: Each utterance session consisted of 10 control trials in the normal feedback condition followed by 20 perturbed trials in the modified auditoryâ¦\n\nEffects of auditory stimuli in the horizontal plane on audiovisual integration: an event-related potential study.\n\nPubMed\n\nYang, Weiping; Li, Qi; Ochi, Tatsuya; Yang, Jingjing; Gao, Yulin; Tang, Xiaoyu; Takahashi, Satoshi; Wu, Jinglong\n\n2013-01-01\n\nThis article aims to investigate whether auditory stimuli in the horizontal plane, particularly originating from behind the participant, affect audiovisual integration by using behavioral and event-related potential (ERP) measurements. In this study, visual stimuli were presented directly in front of the participants, auditory stimuli were presented at one location in an equidistant horizontal plane at the front (0Â°, the fixation point), right (90Â°), back (180Â°), or left (270Â°) of the participants, and audiovisual stimuli that include both visual stimuli and auditory stimuli originating from one of the four locations were simultaneously presented. These stimuli were presented randomly with equal probability; during this time, participants were asked to attend to the visual stimulus and respond promptly only to visual target stimuli (a unimodal visual target stimulus and the visual target of the audiovisual stimulus). A significant facilitation of reaction times and hit rates was obtained following audiovisual stimulation, irrespective of whether the auditory stimuli were presented in the front or back of the participant. However, no significant interactions were found between visual stimuli and auditory stimuli from the right or left. Two main ERP components related to audiovisual integration were found: first, auditory stimuli from the front location produced an ERP reaction over the right temporal area and right occipital area at approximately 160-200 milliseconds; second, auditory stimuli from the back produced a reaction over the parietal and occipital areas at approximately 360-400 milliseconds. Our results confirmed that audiovisual integration was also elicited, even though auditory stimuli were presented behind the participant, but no integration occurred when auditory stimuli were presented in the right or left spaces, suggesting that the human brain might be particularly sensitive to information received from behind than both sides.\n\nEffects of Auditory Stimuli in the Horizontal Plane on Audiovisual Integration: An Event-Related Potential Study\n\nPubMed Central\n\nYang, Weiping; Li, Qi; Ochi, Tatsuya; Yang, Jingjing; Gao, Yulin; Tang, Xiaoyu; Takahashi, Satoshi; Wu, Jinglong\n\n2013-01-01\n\nThis article aims to investigate whether auditory stimuli in the horizontal plane, particularly originating from behind the participant, affect audiovisual integration by using behavioral and event-related potential (ERP) measurements. In this study, visual stimuli were presented directly in front of the participants, auditory stimuli were presented at one location in an equidistant horizontal plane at the front (0Â°, the fixation point), right (90Â°), back (180Â°), or left (270Â°) of the participants, and audiovisual stimuli that include both visual stimuli and auditory stimuli originating from one of the four locations were simultaneously presented. These stimuli were presented randomly with equal probability; during this time, participants were asked to attend to the visual stimulus and respond promptly only to visual target stimuli (a unimodal visual target stimulus and the visual target of the audiovisual stimulus). A significant facilitation of reaction times and hit rates was obtained following audiovisual stimulation, irrespective of whether the auditory stimuli were presented in the front or back of the participant. However, no significant interactions were found between visual stimuli and auditory stimuli from the right or left. Two main ERP components related to audiovisual integration were found: first, auditory stimuli from the front location produced an ERP reaction over the right temporal area and right occipital area at approximately 160â200 milliseconds; second, auditory stimuli from the back produced a reaction over the parietal and occipital areas at approximately 360â400 milliseconds. Our results confirmed that audiovisual integration was also elicited, even though auditory stimuli were presented behind the participant, but no integration occurred when auditory stimuli were presented in the right or left spaces, suggesting that the human brain might be particularly sensitive to information received from behind than both sides. PMID:23799097\n\nAuditory perception and the control of spatially coordinated action of deaf and hearing children.\n\nPubMed\n\nSavelsbergh, G J; Netelenbos, J B; Whiting, H T\n\n1991-03-01\n\nFrom birth onwards, auditory stimulation directs and intensifies visual orientation behaviour. In deaf children, by definition, auditory perception cannot take place and cannot, therefore, make a contribution to visual orientation to objects approaching from outside the initial field of view. In experiment 1, a difference in catching ability is demonstrated between deaf and hearing children (10-13 years of age) when the ball approached from the periphery or from outside the field of view. No differences in catching ability between the two groups occurred when the ball approached from within the field of view. A second experiment was conducted in order to determine if differences in catching ability between deaf and hearing children could be attributed to execution of slow orientating movements and/or slow reaction time as a result of the auditory loss. The deaf children showed slower reaction times. No differences were found in movement times between deaf and hearing children. Overall, the findings suggest that a lack of auditory stimulation during development can lead to deficiencies in the coordination of actions such as catching which are both spatially and temporally constrained.\n\nSeasonal Plasticity of Precise Spike Timing in the Avian Auditory System\n\nPubMed Central\n\nSen, Kamal; Rubel, Edwin W; Brenowitz, Eliot A.\n\n2015-01-01\n\nVertebrate audition is a dynamic process, capable of exhibiting both short- and long-term adaptations to varying listening conditions. Precise spike timing has long been known to play an important role in auditory encoding, but its role in sensory plasticity remains largely unexplored. We addressed this issue in Gambel's white-crowned sparrow (Zonotrichia leucophrys gambelii), a songbird that shows pronounced seasonal fluctuations in circulating levels of sex-steroid hormones, which are known to be potent neuromodulators of auditory function. We recorded extracellular single-unit activity in the auditory forebrain of males and females under different breeding conditions and used a computational approach to explore two potential strategies for the neural discrimination of sound level: one based on spike counts and one based on spike timing reliability. We report that breeding condition has robust sex-specific effects on spike timing. Specifically, in females, breeding condition increases the proportion of cells that rely solely on spike timing information and increases the temporal resolution required for optimal intensity encoding. Furthermore, in a functionally distinct subset of cells that are particularly well suited for amplitude encoding, female breeding condition enhances spike timing-based discrimination accuracy. No effects of breeding condition were observed in males. Our results suggest that high-resolution temporal discharge patterns may provide a plastic neural substrate for sensory coding. PMID:25716843\n\nImpact of language on development of auditory-visual speech perception.\n\nPubMed\n\nSekiyama, Kaoru; Burnham, Denis\n\n2008-03-01\n\nThe McGurk effect paradigm was used to examine the developmental onset of inter-language differences between Japanese and English in auditory-visual speech perception. Participants were asked to identify syllables in audiovisual (with congruent or discrepant auditory and visual components), audio-only, and video-only presentations at various signal-to-noise levels. In Experiment 1 with two groups of adults, native speakers of Japanese and native speakers of English, the results on both percent visually influenced responses and reaction time supported previous reports of a weaker visual influence for Japanese participants. In Experiment 2, an additional three age groups (6, 8, and 11 years) in each language group were tested. The results showed that the degree of visual influence was low and equivalent for Japanese and English language 6-year-olds, and increased over age for English language participants, especially between 6 and 8 years, but remained the same for Japanese participants. This may be related to the fact that English language adults and older children processed visual speech information relatively faster than auditory information whereas no such inter-modal differences were found in the Japanese participants' reaction times.\n\nIndependence of reaction time and response force control during isometric leg extension.\n\nPubMed\n\nFukushi, Tamami; Ohtsuki, Tatsuyuki\n\n2004-04-01\n\nIn this study, we examined the relative control of reaction time and force in responses of the lower limb. Fourteen female participants (age 21.2 +/- 1.0 years, height 1.62 +/- 0.05 m, body mass 54.1 +/- 6.1 kg; mean +/- s) were instructed to exert their maximal isometric one-leg extension force as quickly as possible in response to an auditory stimulus presented after one of 13 foreperiod durations, ranging from 0.5 to 10.0 s. In the 'irregular condition' each foreperiod was presented in random order, while in the 'regular condition' each foreperiod was repeated consecutively. A significant interactive effect of foreperiod duration and regularity on reaction time was observed (P < 0.001 in two-way ANOVA with repeated measures). In the irregular condition the shorter foreperiod induced a longer reaction time, while in the regular condition the shorter foreperiod induced a shorter reaction time. Peak amplitude of isometric force was affected only by the regularity of foreperiod and there was a significant variation of changes in peak force across participants; nine participants were shown to significantly increase peak force for the regular condition (P < 0.001), three to decrease it (P < 0.05) and two showed no difference. These results indicate the independence of reaction time and response force control in the lower limb motor system. Variation of changes in peak force across participants may be due to the different attention to the bipolar nature of the task requirements such as maximal force and maximal speed.\n\nSelf-Regulation of the Primary Auditory Cortex Attention Via Directed Attention Mediated By Real Time fMRI Neurofeedback\n\nDTIC Science & Technology\n\n2017-05-05\n\nDirected Attention Mediated by Real -Time fMRI Neurofeedback presented at/published to 2017 Radiological Society of North America Conference in...DATE Sherwood - p.1 Self-regulation of the primary auditory cortex attention via directed attention mediated by real -time fMRI neurofeedback M S...auditory cortex hyperactivity by self-regulation of the primary auditory cortex (A 1) based on real -time functional magnetic resonance imaging neurofeedback\n\nAuditory Reserve and the Legacy of Auditory Experience\n\nPubMed Central\n\nSkoe, Erika; Kraus, Nina\n\n2014-01-01\n\nMusical training during childhood has been linked to more robust encoding of sound later in life. We take this as evidence for an auditory reserve: a mechanism by which individuals capitalize on earlier life experiences to promote auditory processing. We assert that early auditory experiences guide how the reserve develops and is maintained over the lifetime. Experiences that occur after childhood, or which are limited in nature, are theorized to affect the reserve, although their influence on sensory processing may be less long-lasting and may potentially fade over time if not repeated. This auditory reserve may help to explain individual differences in how individuals cope with auditory impoverishment or loss of sensorineural function. PMID:25405381\n\nStrategy Choice Mediates the Link between Auditory Processing and Spelling\n\nPubMed Central\n\nKwong, Tru E.; Brachman, Kyle J.\n\n2014-01-01\n\nRelations among linguistic auditory processing, nonlinguistic auditory processing, spelling ability, and spelling strategy choice were examined. Sixty-three undergraduate students completed measures of auditory processing (one involving distinguishing similar tones, one involving distinguishing similar phonemes, and one involving selecting appropriate spellings for individual phonemes). Participants also completed a modified version of a standardized spelling test, and a secondary spelling test with retrospective strategy reports. Once testing was completed, participants were divided into phonological versus nonphonological spellers on the basis of the number of words they spelled using phonological strategies only. Results indicated a) moderate to strong positive correlations among the different auditory processing tasks in terms of reaction time, but not accuracy levels, and b) weak to moderate positive correlations between measures of linguistic auditory processing (phoneme distinction and phoneme spelling choice in the presence of foils) and spelling ability for phonological spellers, but not for nonphonological spellers. These results suggest a possible explanation for past contradictory research on auditory processing and spelling, which has been divided in terms of whether or not disabled spellers seemed to have poorer auditory processing than did typically developing spellers, and suggest implications for teaching spelling to children with good versus poor auditory processing abilities. PMID:25198787\n\nStrategy choice mediates the link between auditory processing and spelling.\n\nPubMed\n\nKwong, Tru E; Brachman, Kyle J\n\n2014-01-01\n\nRelations among linguistic auditory processing, nonlinguistic auditory processing, spelling ability, and spelling strategy choice were examined. Sixty-three undergraduate students completed measures of auditory processing (one involving distinguishing similar tones, one involving distinguishing similar phonemes, and one involving selecting appropriate spellings for individual phonemes). Participants also completed a modified version of a standardized spelling test, and a secondary spelling test with retrospective strategy reports. Once testing was completed, participants were divided into phonological versus nonphonological spellers on the basis of the number of words they spelled using phonological strategies only. Results indicated a) moderate to strong positive correlations among the different auditory processing tasks in terms of reaction time, but not accuracy levels, and b) weak to moderate positive correlations between measures of linguistic auditory processing (phoneme distinction and phoneme spelling choice in the presence of foils) and spelling ability for phonological spellers, but not for nonphonological spellers. These results suggest a possible explanation for past contradictory research on auditory processing and spelling, which has been divided in terms of whether or not disabled spellers seemed to have poorer auditory processing than did typically developing spellers, and suggest implications for teaching spelling to children with good versus poor auditory processing abilities.\n\nLocal and Global Auditory Processing: Behavioral and ERP Evidence\n\nPubMed Central\n\nSanders, Lisa D.; Poeppel, David\n\n2007-01"
    }
}
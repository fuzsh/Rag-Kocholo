{
    "id": "dbpedia_993_2",
    "rank": 22,
    "data": {
        "url": "https://patents.google.com/patent/US8058544B2/en",
        "read_more_link": "",
        "language": "en",
        "title": "US8058544B2 - Flexible music composition engine - Google Patents",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://patentimages.storage.googleapis.com/65/a6/18/827b583ad4d7c7/US08058544-20111115-D00000.png",
            "https://patentimages.storage.googleapis.com/0c/91/8f/e0c057df67f143/US08058544-20111115-D00001.png",
            "https://patentimages.storage.googleapis.com/c6/73/ad/2c72c16469b558/US08058544-20111115-D00002.png",
            "https://patentimages.storage.googleapis.com/71/c8/07/bb708010ac70fd/US08058544-20111115-D00003.png",
            "https://patentimages.storage.googleapis.com/cc/80/ed/0b4237a0859256/US08058544-20111115-D00004.png",
            "https://patentimages.storage.googleapis.com/d0/3d/59/2c0613cf16b918/US08058544-20111115-D00005.png",
            "https://patentimages.storage.googleapis.com/54/83/e7/39666fc1ce8d3a/US08058544-20111115-D00006.png",
            "https://patentimages.storage.googleapis.com/f5/4d/d8/70fd47ca9d7ada/US08058544-20111115-D00007.png",
            "https://patentimages.storage.googleapis.com/e6/bf/1f/73f2dc21d2d7ad/US08058544-20111115-D00008.png",
            "https://patentimages.storage.googleapis.com/99/51/02/03350eafaf3b04/US08058544-20111115-D00009.png",
            "https://patentimages.storage.googleapis.com/73/bf/59/14f084ab95fdf2/US08058544-20111115-D00010.png",
            "https://patentimages.storage.googleapis.com/f1/07/74/63de070b03b84a/US08058544-20111115-D00011.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2007-09-21T00:00:00",
        "summary": "",
        "meta_description": "An apparatus, method and system for generating music in real time are provided. A pipeline for coordinating generation of a musical piece is created. At least one producer is loaded into the pipeline, the at least one producer for producing at least one high level musical element of the musical piece, independent of other producers in the pipeline. At least one generator is called by the at least one producer, the at least one generator for generating at least one low level music element of the musical piece. The at least one low level musical element and the at least one high level musical element are integrated, such that the musical piece is generated in real time.",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://patents.google.com/patent/US8058544B2/en",
        "text": "Flexible music composition engine Download PDF\n\nInfo\n\nPublication number\n\nUS8058544B2\n\nUS8058544B2 US12/679,086 US67908608A US8058544B2 US 8058544 B2 US8058544 B2 US 8058544B2 US 67908608 A US67908608 A US 67908608A US 8058544 B2 US8058544 B2 US 8058544B2\n\nAuthority\n\nUS\n\nUnited States\n\nPrior art keywords\n\nmusical\n\ngenerator\n\nproducer\n\nmusical piece\n\npattern\n\nPrior art date\n\n2007-09-21\n\nLegal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)\n\nExpired - Fee Related\n\nApplication number\n\nUS12/679,086\n\nOther versions\n\nUS20100307320A1 (en\n\nInventor\n\nMaia Hoeberechts\n\nRyan Demopoulos\n\nMichael Katchabaw\n\nCurrent Assignee (The listed assignees may be inaccurate. Google has not performed a legal analysis and makes no representation or warranty as to the accuracy of the list.)\n\nUniversity of Western Ontario\n\nOriginal Assignee\n\nUniversity of Western Ontario\n\nPriority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.)\n\n2007-09-21\n\nFiling date\n\n2008-09-19\n\nPublication date\n\n2011-11-15\n\n2008-09-19 Application filed by University of Western Ontario filed Critical University of Western Ontario\n\n2008-09-19 Priority to US12/679,086 priority Critical patent/US8058544B2/en\n\n2010-03-19 Assigned to THE UNIVERSITY OF WESTERN ONTARIO reassignment THE UNIVERSITY OF WESTERN ONTARIO ASSIGNMENT OF ASSIGNORS INTEREST (SEE DOCUMENT FOR DETAILS). Assignors: HOEBERECHTS, MAIA, KATCHABAW, MICHAEL, DEMOPOULOS, RYAN\n\n2010-12-09 Publication of US20100307320A1 publication Critical patent/US20100307320A1/en\n\n2011-11-15 Application granted granted Critical\n\n2011-11-15 Publication of US8058544B2 publication Critical patent/US8058544B2/en\n\nStatus Expired - Fee Related legal-status Critical Current\n\n2028-09-19 Anticipated expiration legal-status Critical\n\nLinks\n\nUSPTO\n\nUSPTO PatentCenter\n\nUSPTO Assignment\n\nEspacenet\n\nGlobal Dossier\n\nDiscuss\n\nImages\n\nClassifications\n\nG—PHYSICS\n\nG10—MUSICAL INSTRUMENTS; ACOUSTICS\n\nG10H—ELECTROPHONIC MUSICAL INSTRUMENTS; INSTRUMENTS IN WHICH THE TONES ARE GENERATED BY ELECTROMECHANICAL MEANS OR ELECTRONIC GENERATORS, OR IN WHICH THE TONES ARE SYNTHESISED FROM A DATA STORE\n\nG10H1/00—Details of electrophonic musical instruments\n\nG10H1/0008—Associated control or indicating means\n\nG10H1/0025—Automatic or semi-automatic music composition, e.g. producing random music, applying rules from music theory or modifying a musical piece\n\nG—PHYSICS\n\nG10—MUSICAL INSTRUMENTS; ACOUSTICS\n\nG10H—ELECTROPHONIC MUSICAL INSTRUMENTS; INSTRUMENTS IN WHICH THE TONES ARE GENERATED BY ELECTROMECHANICAL MEANS OR ELECTRONIC GENERATORS, OR IN WHICH THE TONES ARE SYNTHESISED FROM A DATA STORE\n\nG10H1/00—Details of electrophonic musical instruments\n\nG10H1/36—Accompaniment arrangements\n\nG10H1/361—Recording/reproducing of accompaniment for use with an external source, e.g. karaoke systems\n\nG10H1/368—Recording/reproducing of accompaniment for use with an external source, e.g. karaoke systems displaying animated or moving pictures synchronized with the music or audio part\n\nA—HUMAN NECESSITIES\n\nA63—SPORTS; GAMES; AMUSEMENTS\n\nA63F—CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR\n\nA63F2300/00—Features of games using an electronically generated display having two or more dimensions, e.g. on a television screen, showing representations related to the game\n\nA63F2300/60—Methods for processing data by generating or executing the game program\n\nA63F2300/6063—Methods for processing data by generating or executing the game program for sound processing\n\nA63F2300/6081—Methods for processing data by generating or executing the game program for sound processing generating an output signal, e.g. under timing constraints, for spatialization\n\nG—PHYSICS\n\nG10—MUSICAL INSTRUMENTS; ACOUSTICS\n\nG10H—ELECTROPHONIC MUSICAL INSTRUMENTS; INSTRUMENTS IN WHICH THE TONES ARE GENERATED BY ELECTROMECHANICAL MEANS OR ELECTRONIC GENERATORS, OR IN WHICH THE TONES ARE SYNTHESISED FROM A DATA STORE\n\nG10H2210/00—Aspects or methods of musical processing having intrinsic musical character, i.e. involving musical theory or musical parameters or relying on musical knowledge, as applied in electrophonic musical tools or instruments\n\nG10H2210/021—Background music, e.g. for video sequences, elevator music\n\nG10H2210/026—Background music, e.g. for video sequences, elevator music for games, e.g. videogames\n\nG—PHYSICS\n\nG10—MUSICAL INSTRUMENTS; ACOUSTICS\n\nG10H—ELECTROPHONIC MUSICAL INSTRUMENTS; INSTRUMENTS IN WHICH THE TONES ARE GENERATED BY ELECTROMECHANICAL MEANS OR ELECTRONIC GENERATORS, OR IN WHICH THE TONES ARE SYNTHESISED FROM A DATA STORE\n\nG10H2210/00—Aspects or methods of musical processing having intrinsic musical character, i.e. involving musical theory or musical parameters or relying on musical knowledge, as applied in electrophonic musical tools or instruments\n\nG10H2210/101—Music Composition or musical creation; Tools or processes therefor\n\nG10H2210/111—Automatic composing, i.e. using predefined musical rules\n\nG—PHYSICS\n\nG10—MUSICAL INSTRUMENTS; ACOUSTICS\n\nG10H—ELECTROPHONIC MUSICAL INSTRUMENTS; INSTRUMENTS IN WHICH THE TONES ARE GENERATED BY ELECTROMECHANICAL MEANS OR ELECTRONIC GENERATORS, OR IN WHICH THE TONES ARE SYNTHESISED FROM A DATA STORE\n\nG10H2210/00—Aspects or methods of musical processing having intrinsic musical character, i.e. involving musical theory or musical parameters or relying on musical knowledge, as applied in electrophonic musical tools or instruments\n\nG10H2210/101—Music Composition or musical creation; Tools or processes therefor\n\nG10H2210/141—Riff, i.e. improvisation, e.g. repeated motif or phrase, automatically added to a piece, e.g. in real time\n\nG—PHYSICS\n\nG10—MUSICAL INSTRUMENTS; ACOUSTICS\n\nG10H—ELECTROPHONIC MUSICAL INSTRUMENTS; INSTRUMENTS IN WHICH THE TONES ARE GENERATED BY ELECTROMECHANICAL MEANS OR ELECTRONIC GENERATORS, OR IN WHICH THE TONES ARE SYNTHESISED FROM A DATA STORE\n\nG10H2220/00—Input/output interfacing specifically adapted for electrophonic musical tools or instruments\n\nG10H2220/155—User input interfaces for electrophonic musical instruments\n\nG10H2220/401—3D sensing, i.e. three-dimensional (x, y, z) position or movement sensing.\n\nG—PHYSICS\n\nG10—MUSICAL INSTRUMENTS; ACOUSTICS\n\nG10H—ELECTROPHONIC MUSICAL INSTRUMENTS; INSTRUMENTS IN WHICH THE TONES ARE GENERATED BY ELECTROMECHANICAL MEANS OR ELECTRONIC GENERATORS, OR IN WHICH THE TONES ARE SYNTHESISED FROM A DATA STORE\n\nG10H2240/00—Data organisation or data communication aspects, specifically adapted for electrophonic musical tools or instruments\n\nG10H2240/075—Musical metadata derived from musical analysis or for use in electrophonic musical instruments\n\nG10H2240/085—Mood, i.e. generation, detection or selection of a particular emotional content or atmosphere in a musical piece\n\nDefinitions\n\nthe specification relates generally to automated music composition, and specifically to an apparatus, method, and system for a flexible music composition engine which generates music in real time.\n\na first aspect of the specification provides a flexible music composition engine, comprising a processing unit.\n\nthe processing unit is enabled to create a pipeline for coordinating generation of a musical piece.\n\nthe processing unit is further enabled to load at least one producer into the pipeline, the at least one producer for producing at least one high level musical element of the musical piece, independent of other producers in the pipeline.\n\nthe processing unit is further enabled to call at least one generator, via the at least one producer, the at least one generator for generating at least one low level musical element of the musical piece.\n\nthe processing unit is further enabled to integrate the at least one low level musical element and the at least one high level musical element, such that the processing unit produces the musical piece in real time.\n\nthe processing unit can be further enabled to: call at least one performer object for controlling the generation of the musical piece; and load the pipeline into the performer object upon initialization of the generation of the musical piece.\n\nthe performer object can be enabled to make repeated calls on the pipeline until the musical piece is of a given length, and each call, of the repeated calls, generates at least one block of the musical piece.\n\nthe at least one generator can be associated with a style of the musical piece, such that the at least one low level musical element provides the musical piece with the style.\n\nthe at least one producer can be enabled to call a plurality of generators, including the at least one generator, each of the plurality of generators associated with a different style, such that a character of the musical piece can change from the style to the different style when a new generator is called.\n\nthe processing unit can be further enabled to receive data indicative of the different style and in response trigger the at least one producer to call the generator associated with the different style to change the character of the musical piece in real time.\n\nthe processing unit can be further enabled to monitor at least one setting associated with the generation of the musical piece and, in response to a change in the at least one setting, trigger the at least one producer to call a new generator associated with the setting to change the character of the musical piece in real time.\n\nGenerating at least one low level musical element can be based on at least one of selecting a pattern from a pattern library and randomly generating the at least one low level musical element. Randomly generating the at least one low level musical element can comprise pseudo-randomly generating the at least one low level musical element such that the same low level musical element is generated for a given seed value.\n\nthe at least one pattern library can comprise at least one of a harmonic pattern library, a motif pattern library, a meter pattern library and a mode pattern library.\n\nthe at least one producer can comprise at least one of:\n\na section producer for producing at least one section of the musical piece\n\na block producer for producing at least one block of a section of the musical piece\n\na line producer for producing at least one musical line\n\nan output producer for converting the musical piece to an output format.\n\nthe at least one generator can comprise a structure generator callable by the section producer, the structure generator for generating the at least one section, such that the section producer produces a linear progression of sections to form a structure of the musical piece.\n\nProducing at least one section can comprise producing at least one section according to at least one of length, section number, and section type.\n\nthe section type can comprise at least one of a regular section and an end section.\n\nProducing the at least one block of a section of the musical piece can comprise sequentially producing blocks until the section is of a given length.\n\nthe at least one generator is callable by the block producer, and can comprise at least one of a harmonic generator for generating a harmonic pattern, a meter generator for generating a meter, and a mode generator for generating a mode.\n\nthe at least one generator is callable by the line producer and can comprise a motif generator for generating a motif pattern independent of a mode and a harmonic pattern.\n\nthe line producer can be further enabled to map the motif pattern onto a previously generated harmonic pattern\n\neach the note in the motif pattern into at least one of a pitch of the previously generated mode and a nearby dissonant note, based on the harmonic chords in the previously generated harmonic pattern.\n\nthe processing unit further can be enabled to convert the musical piece to an output format that is at least one of playable by an output device and storable in a data file.\n\nthe flexible music composition engine can further comprise the output device, the output device controllable by the processing unit.\n\nthe output device can be enabled to output the musical piece.\n\nthe flexible music composition engine can further comprise a memory for storing the data file.\n\nthe processing unit can be further enabled to adjust at least one musical element of the musical piece, such that the musical piece reflects a given emotional character, by:\n\nAt least one mood parameter associated with at least one musical element the at least one mood parameter specifying how the at least one musical element is to be adjusted to reflect the given emotional character\n\nthe processing unit can be further enabled to adjust the at least one musical element by:\n\nthe at least one weight parameter can comprise a percentage that the music is to be adjusted to reflect the given emotional character\n\nthe adjusting the at least one mood parameter based on the at least one weight parameter can comprise adjusting the at least one mood parameter based on the percentage\n\nthe flexible music composition engine can further comprise an interface for receiving control data from at least one of a media device and a multimedia application, the interface in communication with the processing unit, such that the processing unit produces the musical piece in real time based on the control data.\n\nthe media device can comprise at least one of a video device, a videogame device, a telephonic device.\n\nthe flexible music composition engine can further comprise at least one of the media device and the multimedia application.\n\na second aspect of the specification provides a method of generating music in real-time, in a computing device including a processing unit, the method executable in the processing unit.\n\nthe method comprises creating a pipeline for coordinating generation of a musical piece.\n\nthe method further comprises loading at least one producer into the pipeline, the at least one producer for producing at least one high level musical element of the musical piece, independent of other producers in the pipeline.\n\nthe method further comprises calling at least one generator, by the at least one producer, the at least one generator for generating at least one low level musical element of the musical piece.\n\nthe method further comprises integrating the at least one low level musical element and the at least one high level musical element, such that the processing unit produces the musical piece in real time.\n\na third aspect of the specification provides a system for generating music in real-time.\n\nthe system comprises a processing unit enabled to create a pipeline for coordinating generation of a musical piece.\n\nthe processing unit is further enabled to load at least one producer into the pipeline, the at least one producer for producing at least one high level musical element of the musical piece, independent of other producers in the pipeline;\n\nthe processing unit is further enabled to call at least one generator, by the at least one producer, the at least one generator for generating at least one low level musical element of the musical piece.\n\nthe processing unit is further enabled to integrate the at least one low level musical element and the at least one high level musical element, such that the processing unit produces the musical piece in real time.\n\nthe system further comprises at least one output device, in communication with the processing unit, enabled to output the musical piece.\n\nthe system further comprises at least one media device, in communication with the processing unit, enabled to produce multimedia data and control data, the control data for triggering the processing unit to change a style of the musical piece synchronous with the multimedia data.\n\nFIG. 1 depicts a system for generating music in real-time, according to non-limiting embodiments\n\nFIG. 2 depicts an architecture of an application for generating music in real-time, according to non-limiting embodiments\n\nFIG. 3 depicts the architecture of a performer, according to non-limiting embodiments\n\nFIG. 4 depicts a pipeline process, according to non-limiting embodiments\n\nFIG. 5 depicts a method for producing a section, according to non-limiting embodiments\n\nFIG. 6 depicts a method for producing a block, according to non-limiting embodiments\n\nFIG. 7 depicts a method for filling a block, according to non-limiting embodiments.\n\nFIG. 8 depicts a method for producing a musical line, according to non-limiting embodiments\n\nFIG. 9 depicts a method for generating music in real-time, according to non-limiting embodiments.\n\nFIG. 10 depicts a method for generating music in real-time, according to non-limiting embodiments.\n\nFIG. 11 depicts the opening phrase of Mozart's Sonata in A+, K.331.\n\nFIG. 1 depicts a system 100 for generating music in real-time, according to non-limiting embodiments.\n\nthe system 100 comprises a processing unit 110 for processing an application M, the processing unit 110 in communication with an output device 115 .\n\nthe processing unit 110 is enabled to generate music in real time, in a manner described below, and in turn control the output device 115 to play the music being generated as described below.\n\nthe output device 115 generally comprises an audio output device, such as a speaker.\n\nthe processing unit 110 is in communication with a media device 120 , and further enabled to receive control data 123 from the media device 120 , such that the processing unit 110 generates the music in real time based on the control data 123 .\n\nthe media device can comprise at least one of a video device, a videogame device, and a telephonic device, and can include, but is not limited to, any suitable combination of a processing unit, memory, communication interface, input devices, output devices, etc.\n\nthe music being generated can be adjusted to reflect the data at the media device 120 .\n\nthe media device 120 comprises a videogame device, as events occur in a videogame, the music can be generated to reflect the events (e.g. happy music for happy events and sad music for sad events).\n\nthe media device 120 comprises a telephonic device.\n\nthe processing unit 110 is in communication with an input device 125 , and control data 123 a can be received from the input device 125 . Hence a user interacting with the input device 125 can determine the control data 123 a .\n\nthe input device 125 can include, but is not limited to, a keyboard, a pointing device, a touch screen, etc.\n\nthe processing unit is an element of a computing device 130 , the computing device 130 comprising an interface 132 and a memory 134 , the interface for receiving the control data 123 and/or control data 123 a , and the memory 134 for storing the application M until the application M is processed by the processing unit 110 .\n\nthe memory 134 can also store any data used in the processing of the application M, and/or any data generated during the processing of the application M.\n\nthe computing device 130 can include, but is not limited to, a personal computer, a laptop computer, and a mobile computing device.\n\nthe memory 134 can comprise any suitable combination of persistent memory and volatile memory, including but not limited to, any suitable combination of a removable diskette, CD-ROM, ROM, fixed disk, USB drive, hard drive, RAM, etc.\n\nthe processing unit 110 is in communication with a memory 136 external to the computing device 130 (e.g. via a communications network, not depicted), the memory 136 for storing the application M until the application M is processed by the processing unit 110 .\n\nthe memory 136 can also store any data used in the processing of the application M, and/or any data generated during the processing of the application M.\n\nthe processing unit 110 , the output device 115 and the media device 120 can be elements of a computing device 199 , with processing for the media device 120 also occurring in the processing unit 110 .\n\nthe computing device 199 can include, but is not limited to, a computer, a laptop computer, a mobile computer, a mobile computing device, video device, a videogame device, and a telephonic device.\n\nthe processing unit 110 upon processing the application M, generally comprises a flexible music composition engine enabled to generate music in real time and/or generate music on demand, for example during game play of a video game, such that the music generated can be influenced by game events and change character on the fly.\n\na flexible music composition engine enabled to generate music in real time and/or generate music on demand, for example during game play of a video game, such that the music generated can be influenced by game events and change character on the fly.\n\nthe processing unit 110 in combination with the application M, described hereafter, meets several goals:\n\nthe processing unit 110 can either generate music without any restrictions, or a user (e.g. a human composer) can guide musical choices.\n\n(c) Incorporate a multi-level application programming interface which makes the functionality of the processing unit 110 accessible to users with varying levels of musical and/or programming knowledge.\n\nthe application M is generally an object-oriented system written in any suitable programming language, including but not limited to C#, C++, and/or Java. It includes high-level classes such as Musician and Instrument that model real-world entities involved in music composition. Furthermore, the application M comprises a pipelined architecture in the generation process to allow a structured, yet flexible approach to composition, the inclusion of optional pattern libraries for storing and accessing musical information, and an optional emotion mapper 240 which allows music to be altered according to emotional characteristics. Each of these will be described below.\n\nthe application M can be embedded within application software to facilitate online, dynamic composition of music for immediate use within that application. For example, using the engine in a video game would allow endless variety in the game music, and since composition is done dynamically, the generated music could be tuned to reflect emotional context during game play. Alterations to the music could be initiated from within the game, by the game player, or both.\n\nthe application M can be used as a basis for stand-alone composition tools. For example, consider a system which would permit collaboration among human composers who could exchange parts of pieces created with the engine, or share virtual musicians, instruments, and musical elements. the application M's architecture can also support the creation of virtual bands and jam sessions.\n\nFIG. 2 depicts the architecture of the application M, according to non-limiting embodiments, as well as a block diagram of output from the application M (e.g. a musical piece), which can be used by the processing unit 110 to control the output device 115 .\n\nthe application M e.g. a musical piece\n\nthe application M comprises several groups of classes:\n\nEach of the pipeline 210 , the producers 220 , the generators 230 and the other high-level classes can be stored in the memory 134 and/or the memory 136 until processed by the processing unit 110 and/or called by the appropriate element of the application M (e.g. generators 230 are stored until called by a producer 220 ).\n\nthe application M can also comprise an emotion mapper 240 , described below, for adjusting at least one musical element of a musical piece such that the musical piece reflects a given emotional character.\n\nthe producers 220 and generators 230 are loaded into the pipeline 210 .\n\nthe producers 220 and generators 230 are created before being loaded into the pipeline 210 .\n\na Generator Factory (not depicted) can be used create the generators 230 .\n\nthe structure generator 230 a is enabled to create the overall sectional structure of a musical piece (e.g. ABA form).\n\nthe harmonic generator 230 b is enabled to create a sequence of chords for each section (e.g. I-IV-V-I) of a musical piece.\n\nthe meter generator 230 c is enabled to create a meter (e.g. 4/4 time) for a musical piece.\n\nthe mode generator 230 d is enabled to create modes for a musical piece (e.g. start in F+, progress to C+, divert to D â and return to F+).\n\nthe motif generator 230 e is enabled to create sequences of notes (e.g. a four note ascending scale of sixteenth notes) for a musical piece.\n\nEach generator 230 can contain at least one random and/or pseudo-random number component which is used for decisions it needs to make.\n\nEach generator 230 can also contain at least one Pattern Library (not depicted) which provides the generator 230 with musical elements it can use directly, or as starting points for musical element generation.\n\nPattern libraries can be created prior to processing the application M, and generally comprise data embodying musical knowledge.\n\na âBachâ MotifPattern library can contain motifs from compositions by Johann Sebastian Bach.\n\na âBachâ Harmonic pattern library can contain harmonic patterns from compositions by Johann Sebastian Bach.\n\npattern libraries can be added to a generator 230 , enabling distribution of new pattern libraries after the application M has been installed in a computing device.\n\nusers and/or the application M can add to the pattern libraries.\n\nthe pattern library or libraries can be stored in the memory 134 and/or the memory 136 .\n\neach of the generators 230 a - 230 e there can be a plurality of each of the generators 230 a - 230 e , with each group of generators 230 a - 230 e associated with a different style.\n\na group can comprise a structure generator 230 a , a harmonic generator 230 b , a meter generator 230 c , a mode generator 230 d and a motif generator 230 e , each associated with a âjazzâ style, and another group of a structure generator 230 a , a harmonic generator 230 b , a meter generator 230 c , a mode generator 230 d and a motif generator 230 e , each associated with a âclassicalâ style.\n\na Producer Factory (not depicted) can be used create the producers 220 .\n\nthe section producer 220 a is enabled to use (e.g. call) the structure generator 230 a to produce at least one section 250 , the section 250 comprising a chunk of a musical piece with an associated length, for example in seconds.\n\nEach section 250 contains a number of blocks 260 , which comprise segments of the piece (for example, 4 bars) composed of a musical line played by each musician 320 .\n\nthe block producer 220 b is enabled for producing at least one block 260 of a section 250 of the musical piece using a harmonic pattern 280 created by the harmonic generator 230 b , when the harmonic generator 230 b is called by the block producer 220 b .\n\nthe block producer 220 b is further enabled to call each of the meter generator 230 c and the mode generator 230 d to produce a meter and a mode, respectively, for the block 260 .\n\nthe line producer 220 c is enabled to produce the musical lines 270 using the motif generator 230 e to create the actual note sequences in musical lines 270 played by each musician 320 .\n\nthe line producer 220 c is enabled to produce musical lines with varied articulation (e.g. staccato vs. legato playing), or any other articulation known to a person of skill in the art.\n\nthe output producer 220 d is enabled to convert the musical piece to any suitable output format, including but not limited to MIDI, way, mp3 and/or streamed formats. Furthermore, the output producer 220 b can be enabled to store an output data file (e.g. in the memory 134 and/or the memory 136 ) comprising the musical piece and/or output the musical piece to the output device 115 .\n\nan output data file e.g. in the memory 134 and/or the memory 136 .\n\nthe processing unit 110 performs the associated functionality upon processing of each producer 220 and generator 230 .\n\nFIG. 9 depicts a method 900 for generating music in real-time, according to non-limiting embodiments.\n\nthe method 900 is performed using the system 100 using the architecture of the application M depicted in FIG. 2 .\n\nthe following discussion of the method 900 will lead to a further understanding of the system 100 and the architecture of FIG. 2 , and their various components.\n\nthe system 100 and/or the architecture of FIG. 2 and/or the method 900 can be varied, and need not work exactly as discussed herein in conjunction with each other, and that such variations are within the scope of present embodiments.\n\na play request is received by the performer 310 , for example by the processing unit 110 upon processing the application M.\n\nthe performer 310 has been pre-configured to initiate with a given number of musicians 320 , each with the pre-configured properties, as described above, though as will be described the number of musicians 320 and/or their properties can be changed such that musical piece being generated changes accordingly in real time.\n\nthe performer 310 can further receive a given style, which is subsequently passed to each of the producers 220 in the pipeline 210 , such that each producer 220 can call generators 230 associated with the given style.\n\nthe performer 310 activates the pipeline 210 based on settings such as pre-configured settings and/or the given style and/or the control data 123 or 123 a .\n\na pipeline process P is initiated and/or continues at step 950 , the pipeline process P described below.\n\nthe pipeline process P is enabled to generate sections and/or blocks of the musical piece.\n\nstep 940 the pipeline process P is interrupted and at step 960 the performer 310 and the pipeline 210 are updated such that the musical piece is now generated according to the new settings.\n\ngame play in a videogame may become more intense or less intense, and the emotional characteristics of the musical piece can be adjusted accordingly, and/or the tempo or style of the musical piece can be adjusted accordingly.\n\na user interacting with the processing unit 110 via the input device 125 can add or subtract musicians 320 , change style, mood, instruments etc.\n\nstep 970 it is determined if the musical piece is complete, based on the pre-configured settings and/or the control data 123 or 123 a .\n\nthe pre-configured settings may indicate that the musical piece is to be of a given length and/or a given number of sections. If the musical piece is finished, the processing unit 110 waits for a new play request at step 980 . If the musical piece is not finished, steps 930 - 970 are repeated to continue to generate the musical piece.\n\nFIG. 4 depicts the pipeline process P, according to a non-limiting embodiment.\n\nthe emotion mapper 240 can be called to adjust global settings (e.g. musical characteristics that are not changed elsewhere, such as tempo, volume etc.).\n\nstep 430 it is determined if a new section 250 is needed. If the application M is being initialized then by default at least one new section 250 is needed. If a new section 250 is needed, at step 440 the section 250 is requested from the section producer 220 a , in a method âAâ depicted in FIG. 5 . Turning to âAâ in FIG. 5 , it is determined if a structural pattern is needed. If so, at step 520 the section producer 220 a calls the structure generator 230 a , which provides a structural pattern. In any event, at step 530 , the section producer 220 a gets a new and/or the next section 250 in the structural pattern.\n\neach section 250 can be determined via control data 123 or control data 123 a , which can be passed to or retrieved by the structure generator 230 a .\n\nthe new section 250 is returned to pipeline 210 (e.g. in step 440 in âPâ of FIG. 4 ).\n\na block 260 is requested from the block producer 220 b in a method âBâ depicted in FIG. 6 .\n\na new block is created of a given pre-configured and/or randomly and/or pseudo-randomly determined length (for example 4 bars).\n\nthe block producer 220 b gets a meter by calling the meter generator 230 c\n\nthe block producer 220 b gets a mode by calling the mode generator 230 d .\n\na meter and mode, respectively, are subsequently returned based on pre-configured settings and/or the given style and/or the control data 123 or 123 a.\n\nthe emotion mapper 240 can adjust the mode according to pre-configured settings and/or the given style and/or the control data 123 or 123 a.\n\nthe block producer 220 b gets a harmonic pattern by calling the harmonic generator 230 b .\n\na harmonic pattern is subsequently returned based on pre-configured settings and/or the given style and/or the control data 123 or 123 a .\n\nthe emotion mapper 240 can adjust the harmonic pattern according to pre-configured settings and/or the given style and/or the control data 123 or 123 a.\n\nstep 670 the block 250 is assembled and returned to the pipeline 210 (i.e. in step 450 in âPâ of FIG. 4 ).\n\nthe musical line 270 (or a plurality of musical lines 270 according to the number of musicians 320 in the performer 310 ) is requested from the line producer 220 c , in a method âCâ depicted in FIG. 7 .\n\na musician 320 is retrieved by the line producer 220 c (i.e. the data representing an instrument 330 , a mood 340 , an ability 350 and a style 360 associated with a musician 320 is retrieved).\n\nthe line producer 220 c calls a routine âDâ (or âProduce Musical Lineâ) to produce the musical line 270 for the retrieved musician 320 , the routine D described below.\n\nstep 730 it is determined if there are more musicians 320 for which a musical line 270 is to be produced. If so, the next musician 320 is retrieved at step 710 . If not, the block 260 is assembled and returned at step 740 to the pipeline 210 (i.e. in step 460 in âPâ of FIG. 4 ), the block 260 now populated with the musical line(s) 270 .\n\nFIG. 8 depicts the routine âDâ for producing the musical line(s) 270 .\n\nthe routine starts at step 810 where it is determined if the musical line 270 being produced is finished. If not (e.g. the routine D is initializing or the length of the block has not been reached), then at steps 820 to 840 , the style of the motif to be returned is determined, based on pre-configured settings and/or the given style and/or the control data 123 or 123 a .\n\na global style e.g. the given style\n\nthe motif generator 230 e associated with the global style i.e. the given style\n\nstep 830 it is determined if the style of a musician 320 (e.g. a style 360 ) is to be used. If so, at step 835 , the motif generator 230 e associated with the style of the musician 320 is selected. If not, at step 840 , the motif generator 230 e associated with a default style associated with the performer 310 and/or the pipeline 210 is selected. In any event, once the appropriate motif generator 230 e has been selected, at step 850 the motif pattern is returned from the appropriate motif generator 230 e.\n\nthe motif generator 230 e e.g. a style 360\n\nsteps similar to steps 820 - 850 can be repeated when any producer 220 makes a call to a generator 230 based on style. For example, in steps 520 , 620 , 630 and 650 , calls to the appropriate generator 230 can be determined via decisions similar to those made in step 820 and 830 .\n\nthe emotion mapper 240 can adjust the motif according to pre-configured settings and/or the given style and/or the control data 123 or 123 a.\n\nthe motif is harmonically adjusted and added to the musical line 870 (described below).\n\nstep 810 it is again determined if the musical line 270 is finished (i.e. the length of the block has been reached). If not, step 820 - 870 are repeated and, if so, at step 880 , the musical line is returned to the pipeline 210 (i.e. in step 720 in âCâ of FIG. 7 ).\n\nthe output producer 220 d generates output data.\n\nthe output data can be saved to a file (i.e. in the memory 134 and/or the memory 136 ) or used to control the output device 115 to play the musical piece that has been generated.\n\nsteps 930 - 960 can occur before, during or after any of the steps depicted in FIGS. 4-9 , such that the pipeline process P is interruptible at step 940 at any appropriate step depicted in FIGS. 4-9 , such that the performer 310 and the pipeline 210 can be updated at step 960 .\n\nthe interruption of the pipeline process P can be delayed if interruption of the pipeline process P results in a discontinuity in the generation of the musical piece (e.g. a dead space where no dead space is desired).\n\nthe musical piece is updated between blocks 260 .\n\nmusic generation can occur much more quickly than playback (i.e. by the output device 115 ), and thus, many blocks 260 can be generated while the first block 260 is playing.\n\nsome of these embodiments comprise a means of keeping track of which block 260 is currently being played, a means of altering and/or replacing subsequent blocks 260 when a change to a setting occurs.\n\nalterations can occur gradually rather than abruptly and/or sometimes gradually and sometimes abruptly.\n\nparameters for starting and end points of the transition are determined and blocks 260 in between are generated accordingly.\n\npresent and future parameters are tracked.\n\nFIG. 10 depicts a method 1000 for generating music in real-time.\n\nthe method 1000 is performed using the system 100 using the architecture of the application M depicted in FIG. 2 .\n\nthe following discussion of the method 1000 will lead to a further understanding of the system 100 and the architecture of FIG. 2 , and their various components.\n\nthe system 100 and/or the architecture of FIG. 2 and/or the method 1000 can be varied, and need not work exactly as discussed herein in conjunction with each other, and that such variations are within the scope of present embodiments.\n\nthe method 1000 is a summary of the methods, processes and routines of FIGS. 4-9 . It is hence understood that method 1000 is generally performed by the processing device 110 .\n\nthe pipeline 210 is created.\n\nthe pipeline 210 is created by the performer 310 , but in other embodiments the pipeline 210 may be created by another entity.\n\nAt step 1020 at least one producer 220 is loaded into the pipeline 210 , the at least one producer 220 for producing at least one high level musical element of the musical piece, independent of other producers 220 in the pipeline.\n\na high level musical element include, but are not limited to, sections 250 , blocks 260 and musical lines 270 .\n\nthe high level musical elements are produced by the at least one producer 220 .\n\nAt step 1040 at least one generator 230 is called within the at least one producer 220 , the at least one generator 230 for generating at least one low level musical element of the musical piece.\n\na low level musical elements include, but are not limited to structure, meter, mode, harmonic pattern, and motif pattern.\n\nthe low level musical elements are produced by the at least one generator 230 .\n\nthe at least one low level musical element and the at least one high level musical element are integrated, for example by the processing unit 110 , such that the musical piece is produced in real time.\n\nthe musical piece is output, for example to an output file and/or to the output device 115 .\n\nsteps 1030 - 1070 can be repeated and/or interleaved as desired.\n\nnew high level elements can be produced before or after the low level elements, the new high level elements to be integrated with new low level elements.\n\nblocks 260 (and/or sections 250 ) can be output as they are produced, before or after new high level elements are produced and/or new low level elements are produced and/or integrated with the new high level elements.\n\nsteps 1030 - 1070 can be repeated as required to continue generation of the musical piece, until the musical piece of a given length, or the method 1000 is interrupted. Furthermore, it is understood that due to the pipeline architecture, steps 1030 - 1070 could occur in parallel.\n\nsteps 1030 - 1070 are being executed by the processing device 110 , settings for the high level and low level musical elements are being monitored to determine a change to the settings, for example at step 1080 . If no change is detected, the method 1000 continues at step 1090 . If, however, a change to the settings is detected at step 1080 , at step 1095 the method 1000 is interrupted at any suitable point to update the pipeline 210 and/or performer 310 . In effect, the method 1000 then returns either to step 1030 such that new high level elements can be produced, or to step 1040 (or step 1050 ), such that new low level elements can be produced. Hence, the musical piece can be changed in real-time, âon the flyâ.\n\nthe emotion mapper 240 can be used to adjust the low level musical elements for mood dependent adjustments.\n\nFIGS. 1-3 Various aspects of the application M will now be described with reference to FIGS. 1-3 .\n\nthe elements of the application M that are involved in music creation generally reflect real-life counterparts.\n\nthe application M deals with the following classes: musician 320 , Instrument 330 , Performer 310 , various piece characteristics, style, mode, meter, and mood 340 .\n\na musician 320 plays an instrument 330 and has a mood 340 . Also, a musician 320 has an ability 350 , and knows a number of styles 360 . The intention is to model a real musician, who can play one instrument at a time, but has the ability to play that instrument in different styles with varying ability\n\na pianist who is classically trained, but also plays some improvisational jazz. This pianist can be modelled as a musician 320 who knows (at least) three styles 360 : classical, jazz and her own style 360 âa personal repertoire of improvisational riffs.\n\nan ability 350 for the musician 320 enables modelling of âgoodâ and âbadâ musicians 320 : Further aspects of ability 350 can be: ability 350 to play an instrument 330 in tune; ability 350 to follow a beat; ability 350 to contain a mood 340 and play in the manner desired by a conductor.\n\nthe styles 360 âknownâ by a musician 320 can also reflect ability 350 .\n\nusers can develop and trade musicians 320 with one another.\n\nusers can have collections of musicians 320 each with their own skill set.\n\nthe musicians 320 can be shared, traded, combined into groups, and marketed.\n\npattern libraries accessible by the generators 230 enables new music to be generated by reusing compositional elements: either elements from existing pieces or elements which have been previously generated.\n\na pattern library can be thought of as a repository of musical ideas.\n\nthe four libraries which can be used by the application M include, but are not limited to, a harmonic pattern library, a motif pattern library, a mode pattern library and a meter pattern library.\n\nthe pattern libraries enable the system 100 to compose in different styles, and to provide a mechanism for exchanging and storing musical ideas. Each of these goals will be discussed in turn.\n\na generator 230 could use the pattern libraries as its only compositional resource, that is, all the musical elements returned by the generator 230 are those taken from the library. In other embodiments, the generator 230 could use patterns from the pattern libraries as starting points which are then modified. In yet further embodiments, a generator 230 could return a mixture of patterns from the pattern libraries and generated patterns. In yet other embodiments, a generator 230 could ignore the pattern libraries entirely and return only generated patterns.\n\nthe pattern libraries comprise rich musical resources which can be flexibly used depending on the desired musical outcome.\n\nmore than one type of pattern library can be used by and/or loaded into the generators 230 , such that sources from more than one style can be combined in the same musical piece. Further, in these embodiments, a user can try pattern library combinations without having to make any changes to the pattern libraries themselves.\n\nmusicians 320 can be enabled to use their own pattern libraries, rather than a common pattern library determined by the generator 230 .\n\npattern libraries can be added and/or modified during composition, enabling repetition in a musical piece, and reuse of motifs in future compositions.\n\nthe pipeline 210 architecture described with reference to FIGS. 2-10 enables features of real-time music generation which are difficult to achieve in the prior art.\n\nthe music generation process within the pipeline 210 can be pictured as an assembly line for constructing musical blocks.\n\nEach of the producers 220 along the pipeline 210 fills in the elements its generators 230 create, until the finished block 260 is eventually passed to the output producer 220 d for playback.\n\nthe producers 220 all work independently, which means the generation process can be parallelized.\n\na different generator 230 can be substituted in a producer 220 without affecting the rest of the pipeline 210 (e.g. when a style and/or setting changes).\n\nMood is considered in two contexts: individual musicians 320 have a mood 340 which can be adjusted independently of other musicians 320 , and a piece can have a mood.\n\nan orchestra with 27 members.\n\nthe bassoon player is depressed because she just learned that she can no longer afford her car payments on her musician's salary.\n\nthe orchestra conductor is trying to achieve a âhappyâ sound at the end of the piece currently being played.\n\nshe will play in a way which reflects her own âsadâ mood 340 , and the desired âhappyâ mood 340 to varying degrees.\n\nthe emotion mapper 240 is an implementation of a class which makes adjustments to musical elements based, for example, on the emotions âhappyâ and âsadâ, though in other embodiments, adjustments to musical elements can be based on other emotions.\n\nthe emotion mapper 240 can be configured to adjust musical characteristics such as the mode, motif pattern/pitch and tempo etc.\n\nthe logic behind the emotion-based changes can be based on research in music psychology.\n\nall mood adjustments can be made based on the mood characteristics of the first musician 320 in a performer 310 or based on a global mood setting. In other embodiments, mood adjustments can be made based on the mood 340 of each musician 320 in a performer 310 . However, in these embodiments, some characteristics of a musical piece may be adjusted independent of each mood 340 of a musician 320 . For example, mood can affect tempo of the piece, however it may be desired that tempo be constant for all musicians 320 . In other embodiments, interesting musical results may occur if musicians 320 play at different tempos depending on their mood 340 .\n\nthe emotion mapper 240 alters music based on a subset of the plurality of emotional descriptors. For example, in some embodiments, the emotion mapper 240 alters music based on the emotions âhappyâ and âsadâ. However, it is understood that the emotion mapper 240 can be configured to alter music based on any suitable subset. It is also understood that the emotion mapper 240 can be updated to include further emotional descriptors and/or updated if understanding changes on how emotions translate into changes to musical elements.\n\nthe emotion mapper 240 adjusts at least one musical element of the musical piece such that the musical piece reflects a given emotional characteristic by: the receiving at least one weight parameter specifying the degree to which the music is to be adjusted to reflect the given emotional character, wherein the at least one weight parameter comprises a percentage that the music is to be adjusted to reflect the given emotional character, and wherein the adjusting the at least one mood parameter based on the at least one weight parameter comprises adjusting the at least one mood parameter based on the percentage.\n\nthe weight parameters can be received from the media device 120 and/or the input device 125 via the control data 123 and 123 a , respectively.\n\nthe emotion mapper 240 then adjusts the at least one musical element based on the at least one mood parameter.\n\nthe motif generator 230 e is enabled to generate sequences of notes and rests with associated timing, independent of both mode and harmony. This is best illustrated by a non-limiting example, described hereafter.\n\nâPitchesâ are positions in the mode relative to the root of the harmonic chord (with the root as 0), âLocationsâ indicate an offset from the beginning of the bar (location 0.0), and âDurationsâ specify the length of the note. Locations and durations are expressed in terms of number of beats (for example, in 6/8 time, an eighth note is one beat).\n\nEncoding motifs in this manner enables capturing a musical pattern associated with a sequence of notes, without restriction to a specific mode or harmonic chord, and further enables composed pieces and/or musical lines to be transposed and reinterpreted in any mode. Moreover, as illustrated in the above example, a particular pattern of notes often appears more than once during a piece, but serves a different function depending on the underlying harmony.\n\nthe motif generator 230 e can operate in at least one of a pattern library-based mode and pseudo-random mode (described below). Hence, when a motif is requested (e.g. by the line producer 220 c ), the motif generator 230 e first checks whether a pattern library has been loaded. If it has, the motif generator 230 e attempts to retrieve a motif of a specified and/or randomly and/or pseudo-randomly determined length (e.g. a given number of beats), and a desired type (e.g. an end pattern, which is one that could be found at the end of a section 250 , or regular pattern, which is one that could be found elsewhere in a section 250 , or either). If any suitable patterns are found, they are returned. Otherwise, a new pattern will be generated randomly and/or pseudo-randomly.\n\na motif library-based mode e.g. a specified and/or randomly and/or pseudo-randomly determined length (e.g. a given number of beats), and a desired type\n\nAny suitable method of randomly generating motifs may be used in motif creation.\n\ntwo random generators and/or pseudo-random generators may be used, including but not limited to a number generator and a pitch generator.\n\na loop continues generating notes/rests as long as the desired number of beats has not been filled.\n\nthe motifs are generated according to musically plausible rules and probabilities, which may be stored in the memory 134 and/or the memory 136 .\n\na pseudo-random number generator may be used to generate motifs. Indeed, as pseudo-random number generators are deterministic, if the motif generator 230 e is initialized with the same seed values during two different runs, the motifs produced will be exactly the same.\n\nany of the musical elements, generated by any of the generators 230 may be generated using either a pattern library, or randomly and/or pseudo-randomly, the latter generated using a pseudo-random number generator.\n\na pseudo-random number generator include, but are not limited to, a Mersenne Twister, a Blum Blum Shub, an inversive congruential generator, ISAAC (cipher), Lagged Fibonacci generator, Linear congruential generator, Linear feedback shift register, and Multiply-with-carry.\n\nOther pseudo-random number generators will occur to persons of skill in the art and are within the scope of present embodiments.\n\nUse of a pseudo-random number generator further enables checkpointing of the pipeline 210 during music generation so that musical elements can be saved and repeated.\n\nthe motif is encoded in a mode-independent and harmony-independent manner.\n\nthe âpitchesâ that are generated and stored are actually relative pitches to the root of the harmonic chord in a mode.\n\nthe motif contains the pitches values 0-1-0. If that motif is eventually resolved in a position where it appears as part of the I chord in C+, would be resolved to the actual pitches for C-D-C. However, if that same motif were used in a position where the harmony required the V chord in C+, the motif would now be resolved to G-A-G. Suppose now that the motif contains the pitch values 0-0.5-1-0.\n\nmotifs can be generated based on a desired type (e.g. an end motif, which is one that could be found at the end of a block 260 or section 250 , or a regular motif, which is one that could be found elsewhere in a block 260 or section 250 , or either).\n\nmotifs can be generated that are typical for different instruments 330 (piano vs. violin vs. guitar vs. electronic etc.).\n\nthe line producer 220 c maps the motif pattern onto a previously generated harmonic pattern by: converting the motif pattern to a harmony adjusted motif based on the previously generated harmonic pattern; bringing each note in the motif pattern into a range of a previously generated mode; and resolving each note in the motif pattern into at least one of a pitch of the previously generated mode and a nearby dissonant note, based on the harmonic chords in the previously generated harmonic pattern.\n\nGenerating a harmonic structure for a musical piece enables groups of musicians 320 to perform a piece together which will sound musically coordinated.\n\nthe line producer 220 c is the entity in the pipeline 210 which resolves motifs into notes that fit into chords within a mode.\n\nthe harmonic structure of the block being generated is determined (e.g. a harmonic pattern is chosen), and then each musical line 270 is generated based on this same harmonic pattern. The result is that even though each musician 320 is playing a different musical line 270 from the others (and possibly different instruments 330 , each with its own range), at any given time, each musician 320 can be playing a motif which fits into the current harmonic chord and mode for the piece.\n\nMusic can be generated in the application M in any mode which can be supported by the technology used to drive the output device 115 .\n\nmusic can be generated in the application M in any mode which can be supported by the output format.\n\nThis is a very flexible implementation which allows music played to be played in any major or minor key, or using a whole-tone scale, chromatic scale, blues scale, Japanese scale etc.\n\nthe restrictions imposed by an output format are illustrated using the non-limiting example of the output format being the MIDI format.\n\nthe restrictions imposed by the MIDI format on the scale are that the period of repetition for the scale is an octave, and that the smallest distance between two notes is a semi-tone.\n\nan octave is divided into 12 semi-tones, designated by consecutive whole numbers in the MIDI format (i.e. 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11). Note that these are ânormalâ restrictions for Western music.\n\nthe pattern of the scale is stored using offsets from the starting note of the scale. These offsets specify the number of semi-tones between steps in the scale. For example, every major scale has the following step pattern: 0, 2, 4, 5, 7, 9, 11. To specify which major scale is desired, we also store the starting note of the scale, which is a whole number between 0 and 11 which corresponds to an offset from MIDI pitch 0, which represents C.\n\nmotifs can be resolved into any mode, regardless of the mode in which they were originally specified. While some motifs might sound strange if the mode into which they are resolved contains less tones than the mode in which they were designed, the situation can be resolved using any suitable method.\n\na resolution method which can be used is that the notes are âwrapped aroundâ using a modulus-style 360Â° computation (i.e., the sixth note in a five note mode becomes the first note one octave higher).\n\nflexibility in the application M is enabled via implementing all the key classes as Abstract classes which can be subclassed according to the goals of a programmer.\n\nAbstract class âMotifGeneratorâ (i.e. motif generator 230 e ).\n\nmotif generator 230 e motif generator 230 e\n\na developer wishes to have a âMotifGeneratorâ which is designed to create jazz style motifs. This can be accomplished by writing a class, âJazzMotifGeneratorâ, which extends âMotifGeneratorâ, and provides implementations of all the abstract methods. All the other principal classes in the application M can follow this same pattern, and thus the application M is easily extensible and enhanced.\n\nData structures for the musical elements are also flexible.\n\nthe mode can be anything which is supported by the output format.\n\nthe piece length can be user defined or determined by the application M.\n\nAny available instruments 330 compatible with the output format can be used (e.g. MIDI instruments).\n\nMotifs can be as long or as short as desired.\n\nAny number of pattern libraries can be developed and used.\n\nAny harmonic chords can be defined, which include any number of notes.\n\nMusical styles are completely user defined. And so on.\n\nmusicians 320 can play together as a coordinated ensemble, in which the musicians perform a piece with a harmonic structure followed by all the musicians 320 .\n\nmusicians 320 can play together in a jamming mode, in which musicians 320 rely on their own musical knowledge to decide what to play.\n\nthe application M can be processed within a processing unit of the media device 120 , such that the application M is an embedded component in an application product such as a video game. In other embodiments, the application M can be processed by the processing unit 110 as a stand-alone music composition application.\n\nthe application M can be implemented as an emotional equalizer.\n\na mood is generally a collection of emotional descriptors, each present to a different degree.\n\nan âemotional equalizerâ comprising the computing device 199 , the output device 115 and the input device 125 , could enable a listener to alter a music's mood as it is playing, for example via the input device 125 .\n\nthe âemotional equalizerâ could also be either a hardware or a software device which would operate like a stereo equalizer, except that the sliders would be marked with emotional descriptors rather than frequency ranges. So, while listening, rather than turning up the bass in the song, a listener could turn up the âhappy.â\n\nenvironmental music could altered based on data received from sensors.\n\nthe processing unit 110 can be generating music played in an elevator. Sensors in the elevator could detect the number of persons in the elevator and/or their mood. Sensors would then transmit control data (similar to control data 123 or 123 a ) and the application M can adjust the music accordingly.\n\nthe application M can further enable Internet based musical collaboration. For example, three users collaborating via the Internet, each of whom has a collection of musicians 320 , with different qualities and abilities, could cause those musicians 320 to perform together and the results could be heard by all the users.\n\nthe functionality of the application M may be implemented using pre-programmed hardware or firmware elements (e.g., application specific integrated circuits (ASICs), electrically erasable programmable read-only memories (EEPROMs), etc.), or other related components.\n\nthe functionality of the application M may be achieved using a computing apparatus that has access to a code memory (not shown) which stores computer-readable program code for operation of the computing apparatus.\n\nthe computer-readable program code could be stored on a computer readable storage medium which is fixed, tangible and readable directly by these components, (e.g., removable diskette, CD-ROM, ROM, fixed disk, USB drive).\n\nthe computer-readable program code could be stored remotely but transmittable to these components via a modem or other interface device connected to a network (including, without limitation, the Internet) over a transmission medium.\n\nthe transmission medium may be either a non-wireless medium (e.g., optical and/or digital and/or analog communications lines) or a wireless medium (e.g., microwave, infrared, free-space optical or other transmission schemes) or a combination thereof.\n\nLandscapes\n\nEngineering & Computer Science (AREA)\n\nPhysics & Mathematics (AREA)\n\nAcoustics & Sound (AREA)\n\nMultimedia (AREA)\n\nTheoretical Computer Science (AREA)\n\nElectrophonic Musical Instruments (AREA)\n\nAbstract\n\nAn apparatus, method and system for generating music in real time are provided. A pipeline for coordinating generation of a musical piece is created. At least one producer is loaded into the pipeline, the at least one producer for producing at least one high level musical element of the musical piece, independent of other producers in the pipeline. At least one generator is called by the at least one producer, the at least one generator for generating at least one low level music element of the musical piece. The at least one low level musical element and the at least one high level musical element are integrated, such that the musical piece is generated in real time.\n\nDescription\n\nCROSS REFERENCE TO RELATED APPLICATIONS\n\nThis application is a National Phase entry of International Patent Application Serial No. PCT/CA2008/001648 filed 19 Sep. 2008, and claims the benefit of priority of U.S. Provisional Patent Application No. 60/974,109 filed 21 Sep. 2007, which are hereby incorporated by reference.\n\nFIELD\n\nThe specification relates generally to automated music composition, and specifically to an apparatus, method, and system for a flexible music composition engine which generates music in real time.\n\nBACKGROUND\n\nThere is increasing interest and demand for adaptive music composition systems, which can change the character of generated music in real time, for use in diverse areas such as video game music generation, film score composition, and development of interactive composition tools. Previous music composition systems have tended to be monolithic and complex in their approach to automated music composition and have not been successful in mimicking human composed pieces of music. Furthermore, previous music composition systems have not been successful at adapting the music being generated to mood as it develops in a game or a film etc. Rather, the previous music composition systems rely on calling up different snippets of music that are classified under the given mood. This can be expensive for the makers of a video game as a composing and/or licensing fee must be paid for each snippet of music used. The complexity of the previous music composition systems have also made them difficult to use by a non-specialist.\n\nSUMMARY\n\nA first aspect of the specification provides a flexible music composition engine, comprising a processing unit. The processing unit is enabled to create a pipeline for coordinating generation of a musical piece. The processing unit is further enabled to load at least one producer into the pipeline, the at least one producer for producing at least one high level musical element of the musical piece, independent of other producers in the pipeline. The processing unit is further enabled to call at least one generator, via the at least one producer, the at least one generator for generating at least one low level musical element of the musical piece. The processing unit is further enabled to integrate the at least one low level musical element and the at least one high level musical element, such that the processing unit produces the musical piece in real time.\n\nThe processing unit can be further enabled to: call at least one performer object for controlling the generation of the musical piece; and load the pipeline into the performer object upon initialization of the generation of the musical piece. The performer object can be enabled to make repeated calls on the pipeline until the musical piece is of a given length, and each call, of the repeated calls, generates at least one block of the musical piece.\n\nThe at least one generator can be associated with a style of the musical piece, such that the at least one low level musical element provides the musical piece with the style. The at least one producer can be enabled to call a plurality of generators, including the at least one generator, each of the plurality of generators associated with a different style, such that a character of the musical piece can change from the style to the different style when a new generator is called. The processing unit can be further enabled to receive data indicative of the different style and in response trigger the at least one producer to call the generator associated with the different style to change the character of the musical piece in real time.\n\nThe processing unit can be further enabled to monitor at least one setting associated with the generation of the musical piece and, in response to a change in the at least one setting, trigger the at least one producer to call a new generator associated with the setting to change the character of the musical piece in real time.\n\nGenerating at least one low level musical element can be based on at least one of selecting a pattern from a pattern library and randomly generating the at least one low level musical element. Randomly generating the at least one low level musical element can comprise pseudo-randomly generating the at least one low level musical element such that the same low level musical element is generated for a given seed value. The at least one pattern library can comprise at least one of a harmonic pattern library, a motif pattern library, a meter pattern library and a mode pattern library.\n\nThe at least one producer can comprise at least one of:\n\na section producer for producing at least one section of the musical piece;\n\na block producer for producing at least one block of a section of the musical piece;\n\na line producer for producing at least one musical line; and\n\nan output producer for converting the musical piece to an output format.\n\nThe at least one generator can comprise a structure generator callable by the section producer, the structure generator for generating the at least one section, such that the section producer produces a linear progression of sections to form a structure of the musical piece. Producing at least one section can comprise producing at least one section according to at least one of length, section number, and section type. The section type can comprise at least one of a regular section and an end section. Producing the at least one block of a section of the musical piece can comprise sequentially producing blocks until the section is of a given length. The at least one generator is callable by the block producer, and can comprise at least one of a harmonic generator for generating a harmonic pattern, a meter generator for generating a meter, and a mode generator for generating a mode. The at least one generator is callable by the line producer and can comprise a motif generator for generating a motif pattern independent of a mode and a harmonic pattern. The line producer can be further enabled to map the motif pattern onto a previously generated harmonic pattern by:\n\nconverting the motif pattern to a harmony adjusted motif based on the previously generated harmonic pattern;\n\nbringing each note in the motif pattern into a range of a previously generated mode; and\n\nresolving each the note in the motif pattern into at least one of a pitch of the previously generated mode and a nearby dissonant note, based on the harmonic chords in the previously generated harmonic pattern.\n\nThe processing unit further can be enabled to convert the musical piece to an output format that is at least one of playable by an output device and storable in a data file. The flexible music composition engine can further comprise the output device, the output device controllable by the processing unit. The output device can be enabled to output the musical piece.\n\nThe flexible music composition engine can further comprise a memory for storing the data file.\n\nThe processing unit can be further enabled to adjust at least one musical element of the musical piece, such that the musical piece reflects a given emotional character, by:\n\nreceiving at least one indication of a given emotional character;\n\nretrieving at least one mood parameter associated with at least one musical element, the at least one mood parameter specifying how the at least one musical element is to be adjusted to reflect the given emotional character;\n\nadjusting the at least one musical element of the music based on the at least one mood parameter.\n\nThe processing unit can be further enabled to adjust the at least one musical element by:\n\nreceiving at least one weight parameter specifying the degree to which the music is to be adjusted to reflect the given emotional character, wherein the at least one weight parameter can comprise a percentage that the music is to be adjusted to reflect the given emotional character, and wherein the adjusting the at least one mood parameter based on the at least one weight parameter can comprise adjusting the at least one mood parameter based on the percentage; and\n\nadjusting the at least one mood parameter based on the at least one weight parameter, prior to the adjusting the at least one musical element.\n\nThe flexible music composition engine can further comprise an interface for receiving control data from at least one of a media device and a multimedia application, the interface in communication with the processing unit, such that the processing unit produces the musical piece in real time based on the control data. The media device can comprise at least one of a video device, a videogame device, a telephonic device. The flexible music composition engine can further comprise at least one of the media device and the multimedia application.\n\nA second aspect of the specification provides a method of generating music in real-time, in a computing device including a processing unit, the method executable in the processing unit. The method comprises creating a pipeline for coordinating generation of a musical piece. The method further comprises loading at least one producer into the pipeline, the at least one producer for producing at least one high level musical element of the musical piece, independent of other producers in the pipeline. The method further comprises calling at least one generator, by the at least one producer, the at least one generator for generating at least one low level musical element of the musical piece. The method further comprises integrating the at least one low level musical element and the at least one high level musical element, such that the processing unit produces the musical piece in real time.\n\nA third aspect of the specification provides a system for generating music in real-time. The system comprises a processing unit enabled to create a pipeline for coordinating generation of a musical piece. The processing unit is further enabled to load at least one producer into the pipeline, the at least one producer for producing at least one high level musical element of the musical piece, independent of other producers in the pipeline; The processing unit is further enabled to call at least one generator, by the at least one producer, the at least one generator for generating at least one low level musical element of the musical piece. The processing unit is further enabled to integrate the at least one low level musical element and the at least one high level musical element, such that the processing unit produces the musical piece in real time. The system further comprises at least one output device, in communication with the processing unit, enabled to output the musical piece. The system further comprises at least one media device, in communication with the processing unit, enabled to produce multimedia data and control data, the control data for triggering the processing unit to change a style of the musical piece synchronous with the multimedia data.\n\nBRIEF DESCRIPTIONS OF THE DRAWINGS\n\nEmbodiments are described with reference to the following figures, in which:\n\nFIG. 1 depicts a system for generating music in real-time, according to non-limiting embodiments;\n\nFIG. 2 depicts an architecture of an application for generating music in real-time, according to non-limiting embodiments;\n\nFIG. 3 depicts the architecture of a performer, according to non-limiting embodiments;\n\nFIG. 4 depicts a pipeline process, according to non-limiting embodiments;\n\nFIG. 5 depicts a method for producing a section, according to non-limiting embodiments;\n\nFIG. 6 depicts a method for producing a block, according to non-limiting embodiments;\n\nFIG. 7 depicts a method for filling a block, according to non-limiting embodiments;\n\nFIG. 8 depicts a method for producing a musical line, according to non-limiting embodiments;\n\nFIG. 9 depicts a method for generating music in real-time, according to non-limiting embodiments;\n\nFIG. 10 depicts a method for generating music in real-time, according to non-limiting embodiments; and\n\nFIG. 11 depicts the opening phrase of Mozart's Sonata in A+, K.331.\n\nDETAILED DESCRIPTION OF THE EMBODIMENTS\n\nFIG. 1 depicts a system 100 for generating music in real-time, according to non-limiting embodiments. The system 100 comprises a processing unit 110 for processing an application M, the processing unit 110 in communication with an output device 115. In general, the processing unit 110 is enabled to generate music in real time, in a manner described below, and in turn control the output device 115 to play the music being generated as described below.\n\nThe output device 115 generally comprises an audio output device, such as a speaker.\n\nIn some embodiments, the processing unit 110 is in communication with a media device 120, and further enabled to receive control data 123 from the media device 120, such that the processing unit 110 generates the music in real time based on the control data 123. For example, the media device can comprise at least one of a video device, a videogame device, and a telephonic device, and can include, but is not limited to, any suitable combination of a processing unit, memory, communication interface, input devices, output devices, etc. As data is generated at the media device 120, the music being generated can be adjusted to reflect the data at the media device 120. In a non-limiting example, if the media device 120 comprises a videogame device, as events occur in a videogame, the music can be generated to reflect the events (e.g. happy music for happy events and sad music for sad events). In another non-limiting example, the media device 120 comprises a telephonic device.\n\nIn some embodiments, the processing unit 110 is in communication with an input device 125, and control data 123 a can be received from the input device 125. Hence a user interacting with the input device 125 can determine the control data 123 a. The input device 125 can include, but is not limited to, a keyboard, a pointing device, a touch screen, etc.\n\nIn some embodiments the processing unit is an element of a computing device 130, the computing device 130 comprising an interface 132 and a memory 134, the interface for receiving the control data 123 and/or control data 123 a, and the memory 134 for storing the application M until the application M is processed by the processing unit 110. The memory 134 can also store any data used in the processing of the application M, and/or any data generated during the processing of the application M. The computing device 130 can include, but is not limited to, a personal computer, a laptop computer, and a mobile computing device.\n\nThe memory 134 can comprise any suitable combination of persistent memory and volatile memory, including but not limited to, any suitable combination of a removable diskette, CD-ROM, ROM, fixed disk, USB drive, hard drive, RAM, etc.\n\nIn yet further embodiments, the processing unit 110 is in communication with a memory 136 external to the computing device 130 (e.g. via a communications network, not depicted), the memory 136 for storing the application M until the application M is processed by the processing unit 110. The memory 136 can also store any data used in the processing of the application M, and/or any data generated during the processing of the application M.\n\nIn yet further embodiments, the processing unit 110, the output device 115 and the media device 120 can be elements of a computing device 199, with processing for the media device 120 also occurring in the processing unit 110. For example, the computing device 199 can include, but is not limited to, a computer, a laptop computer, a mobile computer, a mobile computing device, video device, a videogame device, and a telephonic device.\n\nIn any event, the processing unit 110, upon processing the application M, generally comprises a flexible music composition engine enabled to generate music in real time and/or generate music on demand, for example during game play of a video game, such that the music generated can be influenced by game events and change character on the fly.\n\nHence, the processing unit 110, in combination with the application M, described hereafter, meets several goals:\n\n(a) Permit flexibility in the composition process. The processing unit 110 can either generate music without any restrictions, or a user (e.g. a human composer) can guide musical choices.\n\n(b) Provide an extensible architecture that can be integrated with other software.\n\n(c) Incorporate a multi-level application programming interface which makes the functionality of the processing unit 110 accessible to users with varying levels of musical and/or programming knowledge.\n\n(d) Reuse musical elements, such as note sequences and harmonic structure, both from existing composed pieces or computer generated material.\n\n(e) Alternatively allow music to be altered based on emotional characteristics such as happiness, sadness, anxiety, liveliness etc.\n\nIn some non-limiting embodiments the application M is generally an object-oriented system written in any suitable programming language, including but not limited to C#, C++, and/or Java. It includes high-level classes such as Musician and Instrument that model real-world entities involved in music composition. Furthermore, the application M comprises a pipelined architecture in the generation process to allow a structured, yet flexible approach to composition, the inclusion of optional pattern libraries for storing and accessing musical information, and an optional emotion mapper 240 which allows music to be altered according to emotional characteristics. Each of these will be described below.\n\nFurthermore, the application M can be embedded within application software to facilitate online, dynamic composition of music for immediate use within that application. For example, using the engine in a video game would allow endless variety in the game music, and since composition is done dynamically, the generated music could be tuned to reflect emotional context during game play. Alterations to the music could be initiated from within the game, by the game player, or both. In addition, the application M can be used as a basis for stand-alone composition tools. For example, consider a system which would permit collaboration among human composers who could exchange parts of pieces created with the engine, or share virtual musicians, instruments, and musical elements. the application M's architecture can also support the creation of virtual bands and jam sessions.\n\nAttention is now directed to FIG. 2, which depicts the architecture of the application M, according to non-limiting embodiments, as well as a block diagram of output from the application M (e.g. a musical piece), which can be used by the processing unit 110 to control the output device 115.\n\nthe application M comprises several groups of classes:\n\nA pipeline 210. The pipeline 210 controls the flow of the music generation process, and is responsible for calling methods on the generating classes, described below. In some embodiments, the application M can comprise more than one pipeline, including the pipeline 210.\n\nAt least one producer 220, including but not limited to a section producer 220 a, a block producer 220 b, a line producer 220 c and an output producer 220 d, each of which is described in further detail below (referred to generically as a producer 220, and collectively as producers 220). The producers 220 produce high level musical elements of the musical piece. Non-limiting examples of high level musical elements include, but are not limited to, sections, blocks and musical lines.\n\nAt least one generator 230, including but not limited to a structure generator 230 a, a harmonic generator 230 b, a meter generator 230 c, a mode generator 230 d and a motif generator 230 e (referred to generically as a generator 230 and collectively as generators 230). The generators 230 are callable by the producers 220, with specific producers 220 calling specific generators 230. For example, the section producer 220 a calls the structure generator 230 a, the block producer 220 b calls the harmonic generator 230 b, the meter generator 230 c and the mode generator 230 d, and the line producer 220 c calls the motif generator 230 e. The generators 230 are generally enabled to create low level musical elements (harmonic patterns, motif patterns, modes, and meters) which are integrated with the high level musical elements, to create the musical piece. Generators 230 can have library-based or pseudo-random implementations, as described below.\n\nOther high-level classes, including but not limited to at least one of Musician, Instrument, Performer, Piece Characteristics, Style, Mode, Meter, and Mood. These classes implement the real-world entities modelled in the application M, as described below. For example, FIG. 3 depicts a non-liming example of a performer 310, which comprises the pipeline 210, a first musician 320 a and a second musician 320 b (generically a musician 320 and collectively musicians 320). Each musician 320 a and 320 b comprises at least one instrument 330 a and 330 b, respectively, a mood 340 a and 340 b, respectively and an ability 350 a and 350 b, respectively. In addition, in some embodiments each musician 320 can comprise at least one style 360-1, 360-2, 360-3, etc. However, in other embodiments a musician 320 may not comprise any styles 360. In practice, each of the high level classes can comprise data which, when processed by the processing unit 110, causes the processing unit 110 to generate music in accordance with the high-level class, as described below. It is understood that the performer 310 can comprise any suitable number of musicians 320. For example, the performer can comprise a single musician 320 (e.g. a soloist), four musicians 320 (e.g. a quartet), tens or hundreds of musicians 320 (e.g. an orchestra), and further that each musician 320 can be enabled to play the same or different instrument 330, in the same or different style 360, in the same or different mood 340 and with the same or different ability 350. Hence, by changing the number of musicians 320 and the properties of each musician 320, the performer 310 can be customized to any desired number and type of musician 320. In some embodiments, the configuration of the performer 310 and the musician(s) 320 can be controlled via the input device 125 and/or a graphical user interface (not depicted) and/or the media device 120.\n\nEach of the pipeline 210, the producers 220, the generators 230 and the other high-level classes can be stored in the memory 134 and/or the memory 136 until processed by the processing unit 110 and/or called by the appropriate element of the application M (e.g. generators 230 are stored until called by a producer 220).\n\nthe application M can also comprise an emotion mapper 240, described below, for adjusting at least one musical element of a musical piece such that the musical piece reflects a given emotional character.\n\nWhen initiating the system 100, the producers 220 and generators 230 are loaded into the pipeline 210. In some non-limiting embodiments, the producers 220 and generators 230 are created before being loaded into the pipeline 210. In these embodiments, a Generator Factory (not depicted) can be used create the generators 230.\n\nEach of the generators 230 are summarized hereafter:\n\nThe structure generator 230 a is enabled to create the overall sectional structure of a musical piece (e.g. ABA form).\n\nThe harmonic generator 230 b is enabled to create a sequence of chords for each section (e.g. I-IV-V-I) of a musical piece.\n\nThe meter generator 230 c is enabled to create a meter (e.g. 4/4 time) for a musical piece.\n\nThe mode generator 230 d is enabled to create modes for a musical piece (e.g. start in F+, progress to C+, divert to Dâ and return to F+).\n\nThe motif generator 230 e is enabled to create sequences of notes (e.g. a four note ascending scale of sixteenth notes) for a musical piece.\n\nEach generator 230 can contain at least one random and/or pseudo-random number component which is used for decisions it needs to make.\n\nEach generator 230 can also contain at least one Pattern Library (not depicted) which provides the generator 230 with musical elements it can use directly, or as starting points for musical element generation. Pattern libraries can be created prior to processing the application M, and generally comprise data embodying musical knowledge. For example, a âBachâ MotifPattern library can contain motifs from compositions by Johann Sebastian Bach. Similarly, a âBachâ Harmonic pattern library can contain harmonic patterns from compositions by Johann Sebastian Bach. In some embodiments, pattern libraries can be added to a generator 230, enabling distribution of new pattern libraries after the application M has been installed in a computing device. In yet further embodiments, users and/or the application M can add to the pattern libraries. In some embodiments, the pattern library or libraries can be stored in the memory 134 and/or the memory 136.\n\nFurthermore, in some embodiments, there can be a plurality of each of the generators 230 a-230 e, with each group of generators 230 a-230 e associated with a different style. For example, in some embodiments such a group can comprise a structure generator 230 a, a harmonic generator 230 b, a meter generator 230 c, a mode generator 230 d and a motif generator 230 e, each associated with a âjazzâ style, and another group of a structure generator 230 a, a harmonic generator 230 b, a meter generator 230 c, a mode generator 230 d and a motif generator 230 e, each associated with a âclassicalâ style.\n\nIn some embodiments, a Producer Factory (not depicted) can be used create the producers 220.\n\nEach of the producers 220 are summarized hereafter, with reference to FIG. 2:\n\nThe section producer 220 a is enabled to use (e.g. call) the structure generator 230 a to produce at least one section 250, the section 250 comprising a chunk of a musical piece with an associated length, for example in seconds. Each section 250 contains a number of blocks 260, which comprise segments of the piece (for example, 4 bars) composed of a musical line played by each musician 320.\n\nThe block producer 220 b is enabled for producing at least one block 260 of a section 250 of the musical piece using a harmonic pattern 280 created by the harmonic generator 230 b, when the harmonic generator 230 b is called by the block producer 220 b. The block producer 220 b is further enabled to call each of the meter generator 230 c and the mode generator 230 d to produce a meter and a mode, respectively, for the block 260.\n\nThe line producer 220 c is enabled to produce the musical lines 270 using the motif generator 230 e to create the actual note sequences in musical lines 270 played by each musician 320. In some embodiments, the line producer 220 c is enabled to produce musical lines with varied articulation (e.g. staccato vs. legato playing), or any other articulation known to a person of skill in the art.\n\nThe output producer 220 d is enabled to convert the musical piece to any suitable output format, including but not limited to MIDI, way, mp3 and/or streamed formats. Furthermore, the output producer 220 b can be enabled to store an output data file (e.g. in the memory 134 and/or the memory 136) comprising the musical piece and/or output the musical piece to the output device 115.\n\nIt is understood that while each of the producers 220, the generators 230 etc., are described with respect to given functionality, the processing unit 110 performs the associated functionality upon processing of each producer 220 and generator 230.\n\nAttention is now directed to FIG. 9 which depicts a method 900 for generating music in real-time, according to non-limiting embodiments. In order to assist in the explanation of the method 900, it will be assumed that the method 900 is performed using the system 100 using the architecture of the application M depicted in FIG. 2. Furthermore, the following discussion of the method 900 will lead to a further understanding of the system 100 and the architecture of FIG. 2, and their various components. However, it is to be understood that the system 100 and/or the architecture of FIG. 2 and/or the method 900 can be varied, and need not work exactly as discussed herein in conjunction with each other, and that such variations are within the scope of present embodiments.\n\nAt step 910, a play request is received by the performer 310, for example by the processing unit 110 upon processing the application M. Furthermore, it is understood that the performer 310 has been pre-configured to initiate with a given number of musicians 320, each with the pre-configured properties, as described above, though as will be described the number of musicians 320 and/or their properties can be changed such that musical piece being generated changes accordingly in real time. In some embodiments, the performer 310 can further receive a given style, which is subsequently passed to each of the producers 220 in the pipeline 210, such that each producer 220 can call generators 230 associated with the given style.\n\nAt step 920, the performer 310 activates the pipeline 210 based on settings such as pre-configured settings and/or the given style and/or the control data 123 or 123 a. At step 930, it is determined if there any changes to the settings: for example, pre-configured settings may be changed to a new configuration via the input device 125, the given style may change to a new style, and/or the control data 123 or 123 a may indicate that settings have changed (e.g. new style, different events occurring at the media device 120, such as new events in a video game).\n\nIf no changes to the settings have occurred (for example, as will be the case when processing unit 110 first processes the application M), then a pipeline process P is initiated and/or continues at step 950, the pipeline process P described below. In general, however, the pipeline process P is enabled to generate sections and/or blocks of the musical piece.\n\nIf however, it is determined at step 930 that changes to the settings have occurred, then at step 940 the pipeline process P is interrupted and at step 960 the performer 310 and the pipeline 210 are updated such that the musical piece is now generated according to the new settings. In a non-limiting example, game play in a videogame may become more intense or less intense, and the emotional characteristics of the musical piece can be adjusted accordingly, and/or the tempo or style of the musical piece can be adjusted accordingly. In another non-limiting example, a user interacting with the processing unit 110 via the input device 125 can add or subtract musicians 320, change style, mood, instruments etc.\n\nOnce the performer 310 and the pipeline 210 are updated, the pipeline process continues at step 950. At step 970 it is determined if the musical piece is complete, based on the pre-configured settings and/or the control data 123 or 123 a. For example, the pre-configured settings may indicate that the musical piece is to be of a given length and/or a given number of sections. If the musical piece is finished, the processing unit 110 waits for a new play request at step 980. If the musical piece is not finished, steps 930-970 are repeated to continue to generate the musical piece.\n\nAttention is now directed to FIG. 4, which depicts the pipeline process P, according to a non-limiting embodiment. At an optional step 420 the emotion mapper 240 can be called to adjust global settings (e.g. musical characteristics that are not changed elsewhere, such as tempo, volume etc.).\n\nAt step 430, it is determined if a new section 250 is needed. If the application M is being initialized then by default at least one new section 250 is needed. If a new section 250 is needed, at step 440 the section 250 is requested from the section producer 220 a, in a method âAâ depicted in FIG. 5. Turning to âAâ in FIG. 5, it is determined if a structural pattern is needed. If so, at step 520 the section producer 220 a calls the structure generator 230 a, which provides a structural pattern. In any event, at step 530, the section producer 220 a gets a new and/or the next section 250 in the structural pattern. The length of each section 250 can be determined via control data 123 or control data 123 a, which can be passed to or retrieved by the structure generator 230 a. At step 540, the new section 250 is returned to pipeline 210 (e.g. in step 440 in âPâ of FIG. 4).\n\nReturning to FIG. 4, if no section is needed at step 430 (or alternatively, once the new section is returned at step 540), at step 450, a block 260 is requested from the block producer 220 b in a method âBâ depicted in FIG. 6. Turning to âBâ in FIG. 6, at step 610 a new block is created of a given pre-configured and/or randomly and/or pseudo-randomly determined length (for example 4 bars). At step 620, the block producer 220 b gets a meter by calling the meter generator 230 c, and at step 630 the block producer 220 b gets a mode by calling the mode generator 230 d. A meter and mode, respectively, are subsequently returned based on pre-configured settings and/or the given style and/or the control data 123 or 123 a.\n\nAt an optional step 640, the emotion mapper 240 can adjust the mode according to pre-configured settings and/or the given style and/or the control data 123 or 123 a.\n\nAt step 650, the block producer 220 b gets a harmonic pattern by calling the harmonic generator 230 b. A harmonic pattern is subsequently returned based on pre-configured settings and/or the given style and/or the control data 123 or 123 a. At an optional step 660, the emotion mapper 240 can adjust the harmonic pattern according to pre-configured settings and/or the given style and/or the control data 123 or 123 a.\n\nAt step 670 the block 250 is assembled and returned to the pipeline 210 (i.e. in step 450 in âPâ of FIG. 4).\n\nReturning again, to FIG. 4 at step 460 the musical line 270 (or a plurality of musical lines 270 according to the number of musicians 320 in the performer 310) is requested from the line producer 220 c, in a method âCâ depicted in FIG. 7. Turning to âCâ in FIG. 7, at step 710 a musician 320 is retrieved by the line producer 220 c (i.e. the data representing an instrument 330, a mood 340, an ability 350 and a style 360 associated with a musician 320 is retrieved). At step 720 the line producer 220 c calls a routine âDâ (or âProduce Musical Lineâ) to produce the musical line 270 for the retrieved musician 320, the routine D described below. At step 730, it is determined if there are more musicians 320 for which a musical line 270 is to be produced. If so, the next musician 320 is retrieved at step 710. If not, the block 260 is assembled and returned at step 740 to the pipeline 210 (i.e. in step 460 in âPâ of FIG. 4), the block 260 now populated with the musical line(s) 270.\n\nAttention is now directed to FIG. 8, which depicts the routine âDâ for producing the musical line(s) 270. The routine starts at step 810 where it is determined if the musical line 270 being produced is finished. If not (e.g. the routine D is initializing or the length of the block has not been reached), then at steps 820 to 840, the style of the motif to be returned is determined, based on pre-configured settings and/or the given style and/or the control data 123 or 123 a. At step 820 it is determined if a global style (e.g. the given style) is to be used. If so, at step 825, the motif generator 230 e associated with the global style (i.e. the given style) is selected. If not, at step 830 it is determined if the style of a musician 320 (e.g. a style 360) is to be used. If so, at step 835, the motif generator 230 e associated with the style of the musician 320 is selected. If not, at step 840, the motif generator 230 e associated with a default style associated with the performer 310 and/or the pipeline 210 is selected. In any event, once the appropriate motif generator 230 e has been selected, at step 850 the motif pattern is returned from the appropriate motif generator 230 e.\n\nIndeed, steps similar to steps 820-850 can be repeated when any producer 220 makes a call to a generator 230 based on style. For example, in steps 520, 620, 630 and 650, calls to the appropriate generator 230 can be determined via decisions similar to those made in step 820 and 830.\n\nAt an optional step 860, the emotion mapper 240 can adjust the motif according to pre-configured settings and/or the given style and/or the control data 123 or 123 a.\n\nAt 870, the motif is harmonically adjusted and added to the musical line 870 (described below). At 810 it is again determined if the musical line 270 is finished (i.e. the length of the block has been reached). If not, step 820-870 are repeated and, if so, at step 880, the musical line is returned to the pipeline 210 (i.e. in step 720 in âCâ of FIG. 7).\n\nAttention is again directed to FIG. 4, where once the musical line(s) 270 have been returned, at step 470 the output producer 220 d generates output data. The output data can be saved to a file (i.e. in the memory 134 and/or the memory 136) or used to control the output device 115 to play the musical piece that has been generated.\n\nReturning again to FIG. 9, it is understood that the application M monitors the settings through all of the steps depicted in FIGS. 4-9, and adjusts the generation of the musical piece accordingly. In other words steps 930-960 can occur before, during or after any of the steps depicted in FIGS. 4-9, such that the pipeline process P is interruptible at step 940 at any appropriate step depicted in FIGS. 4-9, such that the performer 310 and the pipeline 210 can be updated at step 960. In some embodiments, the interruption of the pipeline process P can be delayed if interruption of the pipeline process P results in a discontinuity in the generation of the musical piece (e.g. a dead space where no dead space is desired).\n\nIn some embodiments the musical piece is updated between blocks 260. However, as in some embodiments, music generation can occur much more quickly than playback (i.e. by the output device 115), and thus, many blocks 260 can be generated while the first block 260 is playing. Hence, to support dynamic alteration of the musical piece, some of these embodiments comprise a means of keeping track of which block 260 is currently being p"
    }
}
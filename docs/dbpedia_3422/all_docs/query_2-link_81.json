{
    "id": "dbpedia_3422_2",
    "rank": 81,
    "data": {
        "url": "https://developers.google.com/search/docs/crawling-indexing/overview-google-crawlers",
        "read_more_link": "",
        "language": "en",
        "title": "Google Crawler (User Agent) Overview",
        "top_image": "https://developers.google.com/static/search/images/home-social-share-lockup.jpg",
        "meta_img": "https://developers.google.com/static/search/images/home-social-share-lockup.jpg",
        "images": [
            "https://developers.google.com/static/search/images/google-search-central-logo.svg",
            "https://developers.google.com/static/search/images/google-search-central-logo.svg",
            "https://developers.google.com/static/search/images/li.png",
            "https://developers.google.com/static/homepage-assets/images/yt.svg",
            "https://fonts.gstatic.com/s/i/short-term/release/googlesymbols/rss_feed/default/24px.svg",
            "https://developers.google.com/static/search/images/search-off-the-record-podcast-logo.png",
            "https://developers.google.com/static/homepage-assets/images/x.svg",
            "https://www.gstatic.com/devrel-devsite/prod/vd185cef2092d5507cf5d5de6d49d6afd8eb38fe69b728d88979eb4a70550ff03/developers/images/lockup-google-for-developers.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Google crawlers discover and scan websites. This overview will help you understand the common Google crawlers including the Googlebot user agent.",
        "meta_lang": "en",
        "meta_favicon": "https://www.gstatic.com/devrel-devsite/prod/vd185cef2092d5507cf5d5de6d49d6afd8eb38fe69b728d88979eb4a70550ff03/developers/images/favicon-new.png",
        "meta_site_name": "Google for Developers",
        "canonical_link": "https://developers.google.com/search/docs/crawling-indexing/overview-google-crawlers",
        "text": "Overview of Google crawlers and fetchers (user agents)\n\nGoogle uses crawlers and fetchers to perform actions for its products, either automatically or triggered by user request.\n\n\"Crawler\" (sometimes also called a \"robot\" or \"spider\") is a generic term for any program that is used to automatically discover and scan websites by following links from one web page to another. Google's main crawler used for Google Search is called Googlebot.\n\nFetchers, like a browser, are tools that request a single URL when prompted by a user.\n\nThe following tables show the Google crawlers and fetchers used by various products and services, how you may see in your referrer logs, and how to specify them in robots.txt. The lists are not exhaustive, they only cover the most common requestors that may show up in log files.\n\nThe user agent token is used in the User-agent: line in robots.txt to match a crawler type when writing crawl rules for your site. Some crawlers have more than one token, as shown in the table; you need to match only one crawler token for a rule to apply. This list is not complete, but covers most crawlers you might see on your website.\n\nThe full user agent string is a full description of the crawler, and appears in the HTTP request and your web logs.\n\nCommon crawlers\n\nGoogle's common crawlers are used to find information for building Google's search indexes, perform other product specific crawls, and for analysis. They always obey robots.txt rules and generally crawl from the IP ranges published in the googlebot.json object.\n\nSpecial-case crawlers\n\nThe special-case crawlers are used by specific products where there's an agreement between the crawled site and the product about the crawl process. For example, AdsBot ignores the global robots.txt user agent (*) with the ad publisher's permission. The special-case crawlers may ignore robots.txt rules and so they operate from a different IP range than the common crawlers. The IP ranges are published in the special-crawlers.json object.\n\nUser-triggered fetchers\n\nUser-triggered fetchers are initiated by users to perform a product specific fetching function. For example, Google Site Verifier acts on a user's request, or a site hosted on Google Cloud (GCP) has a feature that allows the site's users to retrieve an external RSS feed. Because the fetch was requested by a user, these fetchers generally ignore robots.txt rules. The IP ranges the user-triggered fetchers use are published in the user-triggered-fetchers.json and user-triggered-fetchers-google.json objects.\n\nA note about Chrome/W.X.Y.Z in user agents\n\nWherever you see the string Chrome/W.X.Y.Z in the user agent strings in the table, W.X.Y.Z is actually a placeholder that represents the version of the Chrome browser used by that user agent: for example, 41.0.2272.96. This version number will increase over time to match the latest Chromium release version used by Googlebot.\n\nIf you are searching your logs or filtering your server for a user agent with this pattern, use wildcards for the version number rather than specifying an exact version number.\n\nUser agents in robots.txt\n\nWhere several user agents are recognized in the robots.txt file, Google will follow the most specific. If you want all of Google to be able to crawl your pages, you don't need a robots.txt file at all. If you want to block or allow all of Google's crawlers from accessing some of your content, you can do this by specifying Googlebot as the user agent. For example, if you want all your pages to appear in Google Search, and if you want AdSense ads to appear on your pages, you don't need a robots.txt file. Similarly, if you want to block some pages from Google altogether, blocking the Googlebot user agent will also block all Google's other user agents.\n\nBut if you want more fine-grained control, you can get more specific. For example, you might want all your pages to appear in Google Search, but you don't want images in your personal directory to be crawled. In this case, use robots.txt to disallow the Googlebot-Image user agent from crawling the files in your personal directory (while allowing Googlebot to crawl all files), like this:\n\nUser-agent: Googlebot Disallow: User-agent: Googlebot-Image Disallow: /personal\n\nTo take another example, say that you want ads on all your pages, but you don't want those pages to appear in Google Search. Here, you'd block Googlebot, but allow the Mediapartners-Google user agent, like this:\n\nUser-agent: Googlebot Disallow: / User-agent: Mediapartners-Google Disallow:\n\nControlling crawl speed\n\nEach Google crawler accesses sites for a specific purpose and at different rates. Google uses algorithms to determine the optimal crawl rate for each site. If a Google crawler is crawling your site too often, you can reduce the crawl rate."
    }
}
{
    "id": "dbpedia_1723_2",
    "rank": 33,
    "data": {
        "url": "https://livebook.manning.com/book/software-engineering-for-data-scientists/chapter-7",
        "read_more_link": "",
        "language": "en",
        "title": "7 Memory management with Python · Software Engineering for Data Scientists",
        "top_image": "https://drek4537l1klr.cloudfront.net/treadway/Figures/cover.jpg",
        "meta_img": "https://drek4537l1klr.cloudfront.net/treadway/Figures/cover.jpg",
        "images": [
            "https://www.facebook.com/tr?id=1940497162877014&ev=PageView&noscript=1"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "How to profile your code for memory usage and issues · Handling and consuming large datasets · Optimizing data types for memory · Training an ML model when your data doesn’t fit in memory · Making use of Python’s data structures for memory efficiency",
        "meta_lang": "en",
        "meta_favicon": "https://d19npu3b8zepp3.cloudfront.net/assets/images/favicon.png?v=1",
        "meta_site_name": "",
        "canonical_link": "https://livebook.manning.com/book/software-engineering-for-data-scientists/chapter-7/v-5",
        "text": "7 Memory management with Python\n\nThis chapter covers\n\nHow to profile your code for memory usage and issues\n\nHandling and consuming large datasets\n\nOptimizing data types for memory\n\nTraining an ML model when your data doesn’t fit in memory\n\nMaking use of Python’s data structures for memory efficiency\n\nFor this chapter, our primary dataset will be the ad clicks dataset available at this link: https://www.kaggle.com/competitions/avazu-ctr-prediction/data. The training dataset available here has over 40 million rows. Many of the techniques we will discuss in this chapter are also applicable for even larger datasets, such as ones in the billions of rows.\n\nIn Chapter 6 (Section 71.2), we covered using line-profiler to profile your code for computational speed / efficiency issues. This helped us to easily identify what points in our code are taking the longest to run. We’re going to get started in this chapter by discussing memory profiling, which is a similar mechanism for identifying what points in your code cause the highest amount of memory consumption.\n\n7.1 Memory profiler\n\nA memory profiler is a tool that allows you to identify how much memory is being consumed in various actions in your code. Similar to what we covered in the last chapter around computational profiling, we can perform an analagous check for memory.\n\n7.1.1 High-level memory summaries with guppy\n\n7.1.2 Analyzing your memory consumption line by line with memory-profiler\n\n7.2 Sampling and chunking large datasets\n\n7.2.1 Reading from a large CSV file using chunks\n\n7.2.2 Random selection\n\n7.2.3 Chunking when reading from a database\n\n7.3 Optimizing data types for memory\n\n7.3.1 Checking data types\n\n7.3.2 How to check the memory usage of a data frame\n\n7.3.3 How to check the memory usage of a column\n\n7.3.4 Converting numeric data types to be more memory-efficient\n\n7.3.5 Category data type\n\n7.3.6 Sparse data type\n\n7.3.7 Specifying data types when reading in a dataset\n\n7.3.8 Summary of data types and memory\n\n7.4 Limiting number of columns"
    }
}
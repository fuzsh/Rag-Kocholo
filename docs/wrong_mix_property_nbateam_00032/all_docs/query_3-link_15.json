{
    "id": "wrong_mix_property_nbateam_00032_3",
    "rank": 15,
    "data": {
        "url": "https://arxiv.org/html/2402.13593v1",
        "read_more_link": "",
        "language": "en",
        "title": "Knowledge Graph Enhanced Large Language Model Editing",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png",
            "https://arxiv.org/html/x8.png",
            "https://arxiv.org/html/x9.png",
            "https://arxiv.org/html/x10.png",
            "https://arxiv.org/html/x11.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.\n\nfailed: inconsolata\n\nAuthors: achieve the best HTML results from your LaTeX submissions by following these best practices.\n\nLicense: arXiv.org perpetual non-exclusive license\n\narXiv:2402.13593v1 [cs.CL] 21 Feb 2024\n\nKnowledge Graph Enhanced Large Language Model Editing\n\nMengqi Zhang11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT , Xiaotian Ye 22{}^{2}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT , Qiang Liu33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT , Pengjie Ren11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT, Shu Wu33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT , Zhumin Chen11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT\n\n11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPTSchool of Computer Science and Technology, Shandong University\n\n22{}^{2}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPTSchool of Computer Science, Beijing University of Posts and Telecommunications\n\n33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPTCenter for Research on Intelligent Perception and Computing\n\nState Key Laboratory of Multimodal Artificial Intelligence Systems\n\nInstitute of Automation, Chinese Academy of Sciences\n\n{mengqi.zhang, renpengjie, chenzhumin}@sdu.edu.cn\n\nyexiaotian@bupt.edu.cn\n\n{qiang.liu,shu.wu}@nlpr.ia.ac.cn\n\nAbstract\n\n\\Acp\n\nLLM are pivotal in advancing natural language processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and outdated knowledge. Model editing emerges as a promising solution to address these challenges. However, existing editing methods struggle to track and incorporate changes in knowledge associated with edits, which limits the generalization ability of post-edit large language models in processing edited knowledge. To tackle these problems, we propose a novel model editing method that leverages knowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we first utilize a knowledge graph augmentation module to uncover associated knowledge that has changed due to editing, obtaining its internal representations within LLMs. This approach allows knowledge alterations within LLMs to be reflected through an external graph structure. Subsequently, we design a graph-based knowledge edit module to integrate structured knowledge into the model editing. This ensures that the updated parameters reflect not only the modifications of the edited knowledge but also the changes in other associated knowledge resulting from the editing process. Comprehensive experiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME significantly improves the generalization capabilities of post-edit LLMs in employing edited knowledge.\n\nKnowledge Graph Enhanced Large Language Model Editing\n\nMengqi Zhang11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT , Xiaotian Ye 22{}^{2}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT , Qiang Liu33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT , Pengjie Ren11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT, Shu Wu33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT , Zhumin Chen11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT 11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPTSchool of Computer Science and Technology, Shandong University 22{}^{2}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPTSchool of Computer Science, Beijing University of Posts and Telecommunications 33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPTCenter for Research on Intelligent Perception and Computing State Key Laboratory of Multimodal Artificial Intelligence Systems Institute of Automation, Chinese Academy of Sciences {mengqi.zhang, renpengjie, chenzhumin}@sdu.edu.cn yexiaotian@bupt.edu.cn {qiang.liu,shu.wu}@nlpr.ia.ac.cn\n\n1 Introduction\n\n\\Acfp\n\nLLM have achieved impressive results in various natural language processing (NLP) tasks due to their strong general capabilities and inherent rich world knowledge Zhao et al. (2023). However, the knowledge in LLMs may be factually incorrect or outdated Cao et al. (2021), thereby limiting their capabilities. To address these issues, model editing of LLMs has been proposed, distinguishing themselves from the traditional fine-tuning approaches. Model editing employs a more efficient and precise method to update the knowledge embedded in LLMs and has garnered widespread attention from researchers in recent years.\n\nModel editing primarily comprises three categories of methods: Memory-based, Meta-learning, and Locate-then-edit methods. Memory-based methods, exemplified by SERAC Mitchell et al. (2022), store edited knowledge in the external memory outside of LLMs, enabling the retrieval of this knowledge from memory during the inference process of LLMs. Meta-learning methods typically adopt a hyper-network to learn the weight changes for editing LLMs, such as KE De Cao et al. (2021) and MEND Mitchell et al. (2021). To achieve more precise knowledge editing, locate-then-edit methods have been proposed. For instance, ROME Meng et al. (2022a) and MEMIT Meng et al. (2022b) directly target and update parameters corresponding to specific knowledge.\n\nWhile these methods demonstrate promising results in knowledge editing of LLMs, they still face the challenge of capturing the associated knowledge changes related to edited knowledge. Specifically, existing work primarily focuses on the editing of target knowledge, such as modifying knowledge from (s,r,o)ùë†ùëüùëú(s,r,o)( italic_s , italic_r , italic_o ) to (s,r,o*)ùë†ùëüsuperscriptùëú(s,r,o^{*})( italic_s , italic_r , italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ). However, such single-knowledge modification often triggers a series of consequential alterations in associated knowledge. As shown in Figure 1, an edit that changes the knowledge from ‚ÄúLeBron James plays for the Miami Heat‚Äù to ‚ÄúLeBron James plays for the Los Angeles Lakers‚Äù would necessitate a corresponding update from ‚ÄúLeBron James works in Miami‚Äù to ‚ÄúLeBron James works in Los Angeles‚Äù. Existing editing methods fail to account for the impact on associated knowledge resulting from the modification of target knowledge, which limits the generalizability of post-edited LLMs in processing such edited knowledge. The black-box nature of LLMs makes capturing the associations between pieces of knowledge within the models exceedingly complex, further challenging the detection of such associated knowledge changes during editing.\n\nTo deal with the above challenge, we propose a novel locate-then-edit method enhanced by knowledge Graphs for LArge language Model Editing, namely GLAME. Specifically, for each target edit knowledge, we first present a knowledge graph augmentation (KGA) module (¬ß4.1) to construct a subgraph that captures the new associations resulting from the edit. Directly editing high-order relationships from the subgraph into LLMs in a simplistic way requires multiple alterations to the models and might disrupt the targeted edited knowledge, potentially exerting significant adverse effects and diminishing post-edit model performance (¬ß5.2). Therefore, we further develop a graph-based knowledge edit (GKE) module (¬ß4.2) that integrates the subgraph encoding into the rank-one model editing framework. With just a single edit, it ensures that the edited parameters can recognize not only the edited knowledge but also the broader scope of knowledge impacted by such edits.\n\nWe summarize our contributions as follows:\n\n‚Ä¢\n\nWe emphasize and investigate the necessity of capturing the changes of associated knowledge induced by edited knowledge in model editing.\n\n‚Ä¢\n\nWe integrate knowledge graphs into model editing and propose a novel and effective editing method to structure knowledge changes induced by editing and incorporate them into specific parameters.\n\n‚Ä¢\n\nWe conduct extensive experiments on GPT-2 XL and GPT-J, which demonstrate the effectiveness of our proposed model.\n\n2 Related Work\n\nIn this section, we introduce the related work on model editing, which aims to inject new knowledge into LLMs or modify their existing internal knowledge, while ensuring it does not impact other unrelated knowledge. Model editing methodologies can be broadly classified into three distinct categories Yao et al. (2023): memory-based, meta-learning, and locate-then-edit approaches.\n\nMemory-based strategies choose to augment LLMs with external memory modules, thereby offering a pathway to knowledge updates without modifying the parameters of LLMs. For example, SERAC Mitchell et al. (2022) method introduces a gating network in conjunction with an additional model specifically designed to manage edited knowledge. However, the memory-based approaches all highlight a fundamental limitation in their scalability: the external model‚Äôs management complexity escalates with each additional edit, potentially hampering its practical applicability.\n\nConversely, meta-learning methods eliminate the necessity for complex external memory modules by focusing on the training of a hyper-network capable of generating updated weights for the LLMs. This strategy was initially investigated by KE De Cao et al. (2021), utilizing a bi-directional LSTM to predict model weight updates. However, this approach encountered limitations when applied to larger models due to their extensive parameter spaces. To deal with this challenge, MEND Mitchell et al. (2021) adopts a low-rank decomposition of fine-tuning gradients, showcasing an efficient mechanism for updating weights in LLMs. Nevertheless, these approaches still require extensive computational resources for training and risk affecting unrelated knowledge.\n\nTo overcome these issues, recent works have explored knowledge location within LLMs, aiming for more interpretable and precise knowledge editing by targeting parameters directly associated with specific information. The early attempts include KN Dai et al. (2022), which proposes a knowledge attribution method to identify knowledge neurons but falls short in making precise changes to the model‚Äôs weights. Subsequently, the progress in comprehending the fundamental mechanism of Transformer Vaswani et al. (2017) models has introduced the hypothesis that the Feed Forward Network (FFN) modules might function as key-value memories Geva et al. (2021, 2023), thereby laying the groundwork for more precise editing strategies. The ROME Meng et al. (2022a) method, building on this insight, employed causal tracing to pinpoint knowledge-relevant layers and then edit its FFN module, achieving superior outcomes. Building upon this, MEMIT Meng et al. (2022b) tackles batch editing tasks, enabling large-scale knowledge integration.\n\nDespite these advancements, all of the above models primarily concentrate on editing isolated pieces of knowledge, overlooking the potential ripple effects across the model‚Äôs knowledge base Cohen et al. (2023). This omission can impair the model‚Äôs generalization ability post-editing and hinder its capacity for further reasoning with newly integrated knowledge Zhong et al. (2023).\n\n3 Preliminaries\n\nIn this section, we introduce the definition of model editing and knowledge graphs, and the rank-one model editing framework used in our study.\n\nDefinition 1 (Model Editing for LLMs).\n\nModel editing Yao et al. (2023) aims to adjust an LLM ‚Ñ±‚Ñ±\\mathcal{F}caligraphic_F‚Äôs behavior to modify the knowledge (s,r,o)ùë†ùëüùëú(s,r,o)( italic_s , italic_r , italic_o ) encoded in the model into the target knowledge (s,r,o*)ùë†ùëüsuperscriptùëú(s,r,o^{*})( italic_s , italic_r , italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ), where knowledge is denoted as a triple, consisting of the subject sùë†sitalic_s, relation rùëüritalic_r, and object oùëúoitalic_o. Each edit sample eùëíeitalic_e can be represented as (s,r,o,o*)ùë†ùëüùëúsuperscriptùëú(s,r,o,o^{*})( italic_s , italic_r , italic_o , italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ). The post-edit LLM is defined as ‚Ñ±‚Ä≤superscript‚Ñ±‚Ä≤\\mathcal{F}^{\\prime}caligraphic_F start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT.\n\nDefinition 2 (Knowledge Graph).\n\nA knowledge graph (KG) Ji et al. (2021) stores structured knowledge as a collection of triples {(s,r,o)‚äÜ‚Ñ∞√ó‚Ñõ√ó‚Ñ∞}ùë†ùëüùëú‚Ñ∞‚Ñõ‚Ñ∞\\{(s,r,o)\\subseteq\\mathcal{E}\\times\\mathcal{R}\\times\\mathcal{E}\\}{ ( italic_s , italic_r , italic_o ) ‚äÜ caligraphic_E √ó caligraphic_R √ó caligraphic_E }, where ‚Ñ∞‚Ñ∞\\mathcal{E}caligraphic_E and ‚Ñõ‚Ñõ\\mathcal{R}caligraphic_R represent the set of entities and relations, respectively.\n\n3.1 Rank-one Model Editing Framework\n\nRank-one model editing (ROME) Meng et al. (2022a) is a Locate-then-edit method, this method assumes that the factual knowledge is stored in the Feedforward Neural Networks (FFNs), conceptualizing as key-value memories Geva et al. (2021); Kobayashi et al. (2023). Specifically, the output of the lùëôlitalic_l-th layer FFN for the iùëñiitalic_i-th token is formulated as:\n\nùê¶il=f‚Å¢(ùêñi‚Å¢nl‚ãÖùê°il‚àí1)‚ãÖùêñl,superscriptsubscriptùê¶ùëñùëô‚ãÖùëì‚ãÖsuperscriptsubscriptùêñùëñùëõùëôsuperscriptsubscriptùê°ùëñùëô1superscriptùêñùëô\\mathbf{m}_{i}^{l}=f(\\mathbf{W}_{in}^{l}\\cdot\\mathbf{h}_{i}^{l-1})\\cdot\\mathbf% {W}^{l},bold_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = italic_f ( bold_W start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ‚ãÖ bold_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l - 1 end_POSTSUPERSCRIPT ) ‚ãÖ bold_W start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , (1)\n\nwhere f‚Å¢(‚ãÖ)ùëì‚ãÖf(\\cdot)italic_f ( ‚ãÖ ) denotes the activation function, and ùê°il‚àí1superscriptsubscriptùê°ùëñùëô1\\mathbf{h}_{i}^{l-1}bold_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l - 1 end_POSTSUPERSCRIPT is the input of FFN. To facilitate representation, we omit the superscript lùëôlitalic_l in the subsequent discussion.\n\nIn this setup, the output of the first layer, f‚Å¢(ùêñi‚Å¢n‚ãÖùê°i)ùëì‚ãÖsubscriptùêñùëñùëõsubscriptùê°ùëñf(\\mathbf{W}_{in}\\cdot\\mathbf{h}_{i})italic_f ( bold_W start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT ‚ãÖ bold_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), serves as the keys denoted as ùê§isubscriptùê§ùëñ\\mathbf{k}_{i}bold_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. The outputs of the subsequent layer represent the corresponding values. Based on the hypothesis, this method utilizes casual tracing Pearl (2022); Vig et al. (2020) to select a specific FFN layer for editing, thereby updating the weight ùêñùêñ\\mathbf{W}bold_W of the second layer by solving a constrained least-squares problem:\n\nminimize‚Äñùêñùêä‚àíùêå‚Äñ,subject toùêñùê§*=ùê¶*.missing-subexpressionminimizemissing-subexpressionnormùêñùêäùêåmissing-subexpressionsubject tomissing-subexpressionsubscriptùêñùê§subscriptùê¶\\displaystyle\\begin{aligned} &{\\text{minimize}}&&\\|\\mathbf{{W}}\\mathbf{K}-% \\mathbf{M}\\|,\\\\ &\\text{subject to}&&\\mathbf{{W}}\\mathbf{k}_{*}=\\mathbf{m}_{*}.\\end{aligned}start_ROW start_CELL end_CELL start_CELL minimize end_CELL start_CELL end_CELL start_CELL ‚à• bold_WK - bold_M ‚à• , end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL subject to end_CELL start_CELL end_CELL start_CELL bold_Wk start_POSTSUBSCRIPT * end_POSTSUBSCRIPT = bold_m start_POSTSUBSCRIPT * end_POSTSUBSCRIPT . end_CELL end_ROW (2)\n\nHere, the objective function aims to maintain the knowledge, irrelevant to the edited sample unchanged within the LLM, where ùêä=[ùê§1;ùê§2;,‚Ä¶,;ùê§p]\\mathbf{K}=[\\mathbf{k}_{1};\\mathbf{k}_{2};,\\dots,;\\mathbf{k}_{p}]bold_K = [ bold_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; bold_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ; , ‚Ä¶ , ; bold_k start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ] denotes the sets of keys encoding subjects unrelated to the edited fact, and ùêå=[ùê¶1;ùê¶2;,‚Ä¶,;ùê¶p]\\mathbf{M}=[\\mathbf{m}_{1};\\mathbf{m}_{2};,\\dots,;\\mathbf{m}_{p}]bold_M = [ bold_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; bold_m start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ; , ‚Ä¶ , ; bold_m start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ] are the corresponding values. The constraint is to ensure that edited knowledge can be incorporated into the FFN layer, specifically by enabling the key ùê§*subscriptùê§\\mathbf{k}_{*}bold_k start_POSTSUBSCRIPT * end_POSTSUBSCRIPT (encoding subject sùë†sitalic_s) to retrieve the value ùê¶*subscriptùê¶\\mathbf{m}_{*}bold_m start_POSTSUBSCRIPT * end_POSTSUBSCRIPT about the new object o*superscriptùëúo^{*}italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT.\n\nAs explicated in Meng et al. (2022a), a closed-form solution to the above optimization problem can be derived:\n\nùêñ^=ùêñ+(ùê¶*‚àíùêñùê§*)‚Å¢(ùêÇ‚àí1‚Å¢ùê§*)T(ùêÇ‚àí1‚Å¢ùê§*)T‚Å¢ùê§*,^ùêñùêñsubscriptùê¶subscriptùêñùê§superscriptsuperscriptùêÇ1subscriptùê§TsuperscriptsuperscriptùêÇ1subscriptùê§Tsubscriptùê§\\mathbf{\\hat{W}}=\\mathbf{W}+\\frac{(\\mathbf{m}_{*}-\\mathbf{W}\\mathbf{k}_{*})(% \\mathbf{C}^{-1}\\mathbf{k}_{*})^{\\mathrm{T}}}{(\\mathbf{C}^{-1}\\mathbf{k}_{*})^{% \\mathrm{T}}\\mathbf{k}_{*}},over^ start_ARG bold_W end_ARG = bold_W + divide start_ARG ( bold_m start_POSTSUBSCRIPT * end_POSTSUBSCRIPT - bold_Wk start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ) ( bold_C start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_k start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT end_ARG start_ARG ( bold_C start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_k start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT bold_k start_POSTSUBSCRIPT * end_POSTSUBSCRIPT end_ARG , (3)\n\nwhere ùêÇ=ùêäùêäTùêÇsuperscriptùêäùêäT\\mathbf{C}=\\mathbf{K}\\mathbf{K}^{\\mathrm{T}}bold_C = bold_KK start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT represents a constant matrix, pre-cached by estimating the uncentered covariance of ùê§ùê§\\mathbf{k}bold_k based on a sample of Wikipedia text (Appendix E). Therefore, solving the optimal parameter ùêñ^^ùêñ\\mathbf{\\hat{W}}over^ start_ARG bold_W end_ARG is transformed into calculating ùê§*subscriptùê§\\mathbf{k}_{*}bold_k start_POSTSUBSCRIPT * end_POSTSUBSCRIPT and ùê¶*subscriptùê¶\\mathbf{m}_{*}bold_m start_POSTSUBSCRIPT * end_POSTSUBSCRIPT.\n\nExtending this framework, our research delineates a method to integrate graph-structured knowledge, newly and intrinsically associated with the edited knowledge, into the editing of model parameters. We will provide a detailed description of our approach in the following sections.\n\n4 Methodology\n\nIn this section, we introduce the proposed GLAME, the architecture of which is illustrated in Figure 2. The graphs for large language model editing (GLAME) framework principally comprises two key components: (1) Knowledge Graph Augmentation (KGA), which associates the knowledge of internal changes in LLMs by utilizing external knowledge graphs, and (2) Graph-based Knowledge Edit (GKE), which injects knowledge of edits and edit-induced changes into specific parameters of LLMs.\n\n4.1 Knowledge Graph Augmentation\n\nTo accurately capture the changes in associated knowledge induced by editing in LLMs, we propose using external knowledge graphs. This approach is divided into two operational parts: First, it leverages an external knowledge graph to construct a subgraph, capturing the altered knowledge. Then, the LLM is employed to extract the corresponding representations of entities and relations within this subgraph, serving as the initial representations.\n\n4.1.1 Subgraph construction\n\nWe first introduce how to utilize an external knowledge graph to construct a subgraph that encapsulates the newly formed associations due to the edit.\n\nSpecifically, for a given target edit sample e=(s,r,o,o*)ùëíùë†ùëüùëúsuperscriptùëúe=(s,r,o,o^{*})italic_e = ( italic_s , italic_r , italic_o , italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ), we initially employ o*superscriptùëúo^{*}italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT to match the most relevant entity within an external knowledge graph, such as Wikipedia . This step is followed by the sampling of neighboring entities and their relations centered on this entity, represented as (o*,r1,o1)superscriptùëúsubscriptùëü1subscriptùëú1(o^{*},r_{1},o_{1})( italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ), (o*,r2,o2)superscriptùëúsubscriptùëü2subscriptùëú2(o^{*},r_{2},o_{2})( italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ), ‚ãØ‚ãØ\\cdots‚ãØ, (o*,rn,om)superscriptùëúsubscriptùëüùëõsubscriptùëúùëö(o^{*},r_{n},o_{m})( italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_r start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ). These are used to construct new two-order relationships: (s,r,o*,r1,o1)ùë†ùëüsuperscriptùëúsubscriptùëü1subscriptùëú1(s,r,o^{*},r_{1},o_{1})( italic_s , italic_r , italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ), (s,r,o*,r2,o2)ùë†ùëüsuperscriptùëúsubscriptùëü2subscriptùëú2(s,r,o^{*},r_{2},o_{2})( italic_s , italic_r , italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ), ‚ãØ‚ãØ\\cdots‚ãØ, (s,r,o*,rn,om)ùë†ùëüsuperscriptùëúsubscriptùëüùëõsubscriptùëúùëö(s,r,o^{*},r_{n},o_{m})( italic_s , italic_r , italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_r start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ), thereby generating new associated knowledge as a consequence of editing. Here mùëömitalic_m denotes the maximum number of samples for each entity. Following this approach, we can sequentially sample the neighboring entities of o1subscriptùëú1o_{1}italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, o2subscriptùëú2o_{2}italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, ‚ãØ‚ãØ\\cdots‚ãØ, omsubscriptùëúùëöo_{m}italic_o start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT, thereby constructing higher-order new knowledge associations for sùë†sitalic_s. We define the maximum order of the newly constructed relationships as nùëõnitalic_n. The target edit knowledge (s,r,o*)ùë†ùëüsuperscriptùëú(s,r,o^{*})( italic_s , italic_r , italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ), along with these new high-order relations, forms a subgraph, termed ùí¢nm‚Å¢(e)superscriptsubscriptùí¢ùëõùëöùëí\\mathcal{G}_{n}^{m}(e)caligraphic_G start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ( italic_e ), which can record changes in associated knowledge partially caused by editing knowledge. nùëõnitalic_n is also the maximum order of the subgraph, and together with mùëömitalic_m serve as hyper-parameters to control the size of the graph.\n\n4.1.2 Subgraph initialization\n\nTo further explicitly associate the knowledge within the LLM that is affected by the edit, we extract hidden vectors of entities and relations from the early layers of LLM Geva et al. (2023) as the initial representations for entities and relations in the constructed subgraph.\n\nIn specific, we input entity and relation text into the LLM separately, and then select the hidden state vector of the last token of both the entity and the relation text in kùëòkitalic_k-th layer as their initial representations in the subgraph:\n\nùê≥s,ùê≥r,ùê≥o=ùê°[s]k‚Å¢(s),ùê°[r]k‚Å¢(r),ùê°[o]k‚Å¢(o),formulae-sequencesubscriptùê≥ùë†subscriptùê≥ùëüsubscriptùê≥ùëúsubscriptsuperscriptùê°ùëòdelimited-[]ùë†ùë†subscriptsuperscriptùê°ùëòdelimited-[]ùëüùëüsubscriptsuperscriptùê°ùëòdelimited-[]ùëúùëú\\mathbf{z}_{s},\\mathbf{z}_{r},\\mathbf{z}_{o}=\\mathbf{h}^{k}_{[s]}(s),\\mathbf{h% }^{k}_{[r]}(r),\\mathbf{h}^{k}_{[o]}(o),bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , bold_z start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , bold_z start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT = bold_h start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT [ italic_s ] end_POSTSUBSCRIPT ( italic_s ) , bold_h start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT [ italic_r ] end_POSTSUBSCRIPT ( italic_r ) , bold_h start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT [ italic_o ] end_POSTSUBSCRIPT ( italic_o ) , (4)\n\nwhere ùê°[x]k‚Å¢(x)subscriptsuperscriptùê°ùëòdelimited-[]ùë•ùë•\\mathbf{h}^{k}_{[x]}(x)bold_h start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT [ italic_x ] end_POSTSUBSCRIPT ( italic_x ) is the hidden state vector of the last token of text xùë•xitalic_x at the kùëòkitalic_k-th layer of the LLM.\n\n4.2 Graph-based Knowledge Edit\n\nAfter obtaining the knowledge-enhanced subgraph, this section designs a graph-based knowledge edit module to integrate the new associated knowledge contained in the subgraph into the modified parameters of the LLM.\n\n4.2.1 Subgraph encoding\n\nTo enhance the subject sùë†sitalic_s with the newly constructed associated knowledge resulting from the editing of target knowledge, we perform message propagation and aggregation operations on the subgraph through a relational graph neural network (RGNN) Schlichtkrull et al. (2018); Lv et al. (2021).\n\nFormally, we encode the subgraph as follows:\n\nùê≥sl+1=g‚Å¢(‚àëo‚ààùí©sùêñ1‚Å¢(ùê≥ol+ùê≥r)+ùêñ2‚Å¢ùê≥sl),superscriptsubscriptùê≥ùë†ùëô1ùëîsubscriptùëúsubscriptùí©ùë†subscriptùêñ1superscriptsubscriptùê≥ùëúùëôsubscriptùê≥ùëüsubscriptùêñ2superscriptsubscriptùê≥ùë†ùëô\\mathbf{z}_{s}^{l+1}={g}\\left(\\sum_{o\\in\\mathcal{N}_{s}}\\mathbf{W}_{1}\\left(% \\mathbf{z}_{o}^{l}+\\mathbf{z}_{r}\\right)+\\mathbf{W}_{2}\\mathbf{z}_{s}^{l}% \\right),bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l + 1 end_POSTSUPERSCRIPT = italic_g ( ‚àë start_POSTSUBSCRIPT italic_o ‚àà caligraphic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT bold_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT + bold_z start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) + bold_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ) , (5)\n\nwhere ùí©ssubscriptùí©ùë†\\mathcal{N}_{s}caligraphic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT is the set of neighbors of sùë†sitalic_s in ùí¢nm‚Å¢(e)superscriptsubscriptùí¢ùëõùëöùëí\\mathcal{G}_{n}^{m}(e)caligraphic_G start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ( italic_e ), g‚Å¢(‚ãÖ)ùëî‚ãÖg(\\cdot)italic_g ( ‚ãÖ ) is the ReLU function, ùêñ1subscriptùêñ1\\mathbf{W}_{1}bold_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and ùêñ2subscriptùêñ2\\mathbf{W}_{2}bold_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚àà‚Ñùd√ódabsentsuperscript‚Ñùùëëùëë\\in\\mathbb{R}^{d\\times d}‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_d end_POSTSUPERSCRIPT are trainable weight parameter matrices in each layer, and ùê≥s0superscriptsubscriptùê≥ùë†0\\mathbf{z}_{s}^{0}bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT, ùê≥o0superscriptsubscriptùê≥ùëú0\\mathbf{z}_{o}^{0}bold_z start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT, and ùê≥rsubscriptùê≥ùëü\\mathbf{z}_{r}bold_z start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT are the corresponding entity and relation representations obtained from ¬ß4.1.2. To capture the semantic dependencies among nodes in the subgraph comprehensively, the number of layers of RGNN is set to the subgraph‚Äôs maximum order nùëõnitalic_n, yielding the entity representation ùê≥snsuperscriptsubscriptùê≥ùë†ùëõ\\mathbf{z}_{s}^{n}bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT after nùëõnitalic_n-layer operation.\n\n4.2.2 Knowledge editing\n\nFollowing the ROME framework Meng et al. (2022a), in this subsection, we target specific layer lùëôlitalic_l for the computation of ùê¶*subscriptùê¶\\mathbf{m}_{*}bold_m start_POSTSUBSCRIPT * end_POSTSUBSCRIPT and ùê§*subscriptùê§\\mathbf{k}_{*}bold_k start_POSTSUBSCRIPT * end_POSTSUBSCRIPT. Subsequently, we employ Equation (3) to update the parameters of the second layer of the FNN, thereby accomplishing the editing of knowledge.\n\nComputing ùê¶*subscriptùê¶\\mathbf{m}_{*}bold_m start_POSTSUBSCRIPT * end_POSTSUBSCRIPT. Given that ùê≥snsuperscriptsubscriptùê≥ùë†ùëõ\\mathbf{z}_{s}^{n}bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT aggregates the information of neighbors under new association relationships, we utilize ùê≥snsuperscriptsubscriptùê≥ùë†ùëõ\\mathbf{z}_{s}^{n}bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT to enhance the representation at the last token of sùë†sitalic_s in lùëôlitalic_l-th FFN layer of the pre-edit LLM:\n\nùê¶*subscriptùê¶\\displaystyle\\mathbf{{m}}_{*}bold_m start_POSTSUBSCRIPT * end_POSTSUBSCRIPT =ùê¶sl+ùê≥sn,absentsuperscriptsubscriptùê¶ùë†ùëôsuperscriptsubscriptùê≥ùë†ùëõ\\displaystyle=\\mathbf{m}_{s}^{l}+\\mathbf{z}_{s}^{n},= bold_m start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT + bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , (6)\n\nwhere ùê¶slsuperscriptsubscriptùê¶ùë†ùëô\\mathbf{m}_{s}^{l}bold_m start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT denotes the output from the lùëôlitalic_l-th FFN at the last token of sùë†sitalic_s in the pre-edit LLM. Further details of the FFN are delineated in Equation (1).\n\nFor each edit sample (s,r,o,o*)ùë†ùëüùëúsuperscriptùëú(s,r,o,o^{*})( italic_s , italic_r , italic_o , italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ), our objective is to refine an RGNN to produce an enhanced representation, ùê¶*subscriptùê¶\\mathbf{m}_{*}bold_m start_POSTSUBSCRIPT * end_POSTSUBSCRIPT, that enables the LLM to accurately predict the target object o*superscriptùëúo^{*}italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT. Accordingly, the primary loss function is defined as:\n\n‚Ñíp=‚àísubscript‚Ñíùëù\\displaystyle\\mathcal{L}_{p}=-caligraphic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = - 1N‚Å¢‚àëj=1Nlog‚Å°P‚Ñ±‚Å¢(ùê¶sl:=ùê¶*)‚Å¢[o*‚à£xj‚äïp‚Å¢(s,r)],1ùëÅsubscriptsuperscriptùëÅùëó1subscriptP‚Ñ±assignsuperscriptsubscriptùê¶ùë†ùëôsubscriptùê¶delimited-[]conditionalsuperscriptùëúdirect-sumsubscriptùë•ùëóùëùùë†ùëü\\displaystyle\\frac{1}{N}\\sum^{N}_{j=1}\\log\\mathrm{P}_{\\mathcal{F}(\\mathbf{m}_{% s}^{l}:=\\mathbf{m}_{*})}[o^{*}\\mid x_{j}\\oplus p(s,r)],divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚àë start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT roman_log roman_P start_POSTSUBSCRIPT caligraphic_F ( bold_m start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT := bold_m start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT [ italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ‚à£ italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ‚äï italic_p ( italic_s , italic_r ) ] ,\n\nwhere xjsubscriptùë•ùëóx_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is the random prefix generated by the LLM to foster optimization robustness. ‚Ñ±‚Å¢(ùê¶sl:=ùê¶*)‚Ñ±assignsuperscriptsubscriptùê¶ùë†ùëôsubscriptùê¶\\mathcal{F}(\\mathbf{m}_{s}^{l}:=\\mathbf{m}_{*})caligraphic_F ( bold_m start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT := bold_m start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ) indicates the LLM‚Äôs inference alteration through the hidden state ùê¶slsuperscriptsubscriptùê¶ùë†ùëô\\mathbf{m}_{s}^{l}bold_m start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT modification to ùê¶*subscriptùê¶\\mathbf{m}_{*}bold_m start_POSTSUBSCRIPT * end_POSTSUBSCRIPT.\n\nTo mitigate the impact of enhancing sùë†sitalic_s on its intrinsic properties within the LLM, we aim to minimize the KL divergence between ‚Ñ±‚Å¢(ùê¶sl:=ùê¶*)‚Ñ±assignsuperscriptsubscriptùê¶ùë†ùëôsubscriptùê¶\\mathcal{F}(\\mathbf{m}_{s}^{l}:=\\mathbf{m}_{*})caligraphic_F ( bold_m start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT := bold_m start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ) and the original model ‚Ñ±‚Ñ±\\mathcal{F}caligraphic_F without any interventions Meng et al. (2022a):\n\n‚Ñía=DKL(P‚Ñ±‚Å¢(ùê¶sl:=ùê¶*)[x‚à£p‚Ä≤]‚à•P‚Ñ±[x‚à£p‚Äô]),\\displaystyle\\mathcal{L}_{a}=D_{\\text{KL}}\\left(\\mathrm{P}_{\\mathcal{F}(% \\mathbf{m}_{s}^{l}:=\\mathbf{m}_{*})}[x\\mid p^{\\prime}]\\parallel\\mathrm{P}_{% \\mathcal{F}}[x\\mid p\\textquoteright]\\right),caligraphic_L start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT = italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( roman_P start_POSTSUBSCRIPT caligraphic_F ( bold_m start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT := bold_m start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT [ italic_x ‚à£ italic_p start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ] ‚à• roman_P start_POSTSUBSCRIPT caligraphic_F end_POSTSUBSCRIPT [ italic_x ‚à£ italic_p ‚Äô ] ) ,\n\nwhere p‚Ä≤superscriptùëù‚Ä≤p^{\\prime}italic_p start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT denotes prompts in the form of \"subject is a\". This term serves as a regularization loss.\n\nUltimately, the parameters of the RGNN are optimized by minimizing the following objective function:\n\n‚Ñí=‚Ñíp+Œª‚Å¢‚Ñía,‚Ñísubscript‚ÑíùëùùúÜsubscript‚Ñíùëé\\displaystyle\\mathcal{L}=\\mathcal{L}_{p}+\\lambda\\mathcal{L}_{a},caligraphic_L = caligraphic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT + italic_Œª caligraphic_L start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT , (7)\n\nwhere ŒªùúÜ\\lambdaitalic_Œª adjusts the regularization strength. It is important to note that throughout the optimization process, the parameters of the LLM remain unchanged. The modification is instead focused on optimizing the parameters of the RGNN, which in turn influences the inference of the LLM.\n\nComputing ùê§*subscriptùê§\\mathbf{k}_{*}bold_k start_POSTSUBSCRIPT * end_POSTSUBSCRIPT. For each edit sample (s,r,o,o*)ùë†ùëüùëúsuperscriptùëú(s,r,o,o^{*})( italic_s , italic_r , italic_o , italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ), the ùê§*subscriptùê§\\mathbf{k}_{*}bold_k start_POSTSUBSCRIPT * end_POSTSUBSCRIPT is calculated by\n\nùê§*=1N‚Å¢‚àëj=1Nf‚Å¢(ùêñi‚Å¢nl‚ãÖùê°sl‚àí1).subscriptùê§1ùëÅsuperscriptsubscriptùëó1ùëÅùëì‚ãÖsuperscriptsubscriptùêñùëñùëõùëôsuperscriptsubscriptùê°ùë†ùëô1\\mathbf{k}_{*}=\\frac{1}{N}\\sum_{j=1}^{N}f(\\mathbf{W}_{in}^{l}\\cdot\\mathbf{h}_{% s}^{l-1}).bold_k start_POSTSUBSCRIPT * end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚àë start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_f ( bold_W start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ‚ãÖ bold_h start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l - 1 end_POSTSUPERSCRIPT ) . (8)\n\nHere, we also utilize NùëÅNitalic_N random prefixes generated in the same manner as for the computing ùê¶*subscriptùê¶\\mathbf{m}_{*}bold_m start_POSTSUBSCRIPT * end_POSTSUBSCRIPT Meng et al. (2022a).\n\nAfter obtaining the optimized ùê¶*subscriptùê¶\\mathbf{{m}}_{*}bold_m start_POSTSUBSCRIPT * end_POSTSUBSCRIPT and ùê§*subscriptùê§\\mathbf{k}_{*}bold_k start_POSTSUBSCRIPT * end_POSTSUBSCRIPT, we bring them into Equation (3) and then get the edited parameter ùêñ^^ùêñ\\mathbf{\\hat{W}}over^ start_ARG bold_W end_ARG. Algorithm 1 provides the pseudo-code of the overall framework.\n\n5 Experiments\n\nIn this section, we evaluate our editing method GLAME by applying it to two datasets and assessing its performance on two auto-regressive LLMs. We aim to answer the following questions through experiments.\n\n‚Ä¢\n\nQ1: How does GLAME perform in editing knowledge compared with state-of-the-art model editing methods?\n\n‚Ä¢\n\nQ2: How do different components affect the GLAME performance?\n\n‚Ä¢\n\nQ3: How sensitive is GLAME with different hyper-parameter settings?\n\n5.1 Experimental Setups\n\n5.1.1 Datasets and Evaluation Metrics\n\nWe evaluate our GLAME on three representative datasets in our experiments: CounterFact Meng et al. (2022a), CounterFactPlus Yao et al. (2023), and MQuAKE Zhong et al. (2023).\n\nCounterFact is a dataset that focuses on inserting counterfactual knowledge into models. We utilize three metrics on this dataset: Efficacy Score, measuring the success rate of edits directly; Paraphrase Score, indicating the model‚Äôs ability to accurately recall edited knowledge in paraphrased forms, thus testing its generalization ability; and Neighborhood Score, assessing whether irrelevant knowledge in the LLM is disturbed by testing with close, yet unrelated prompts.\n\nCounterFactPlus, an extension of CounterFact, presents more challenging test questions aimed at evaluating the post-edit models‚Äô ability to accurately respond to queries requiring reasoning with edited knowledge. Compared with CounterFact, this assessment has higher requirements for generalization ability. Following Yao et al. (2023), we employ Portability Score to evaluate the performance of all methods on this dataset. This metric offers a superior reflection of the models‚Äô generalization capabilities compared to other indicators.\n\nAn introduction to MQuAKE, further details on CounterFact and CounterFactPlus, as well as the evaluation metrics are shown in Appendix B and C. We provide results on MQuAKE dataset in Appendix F as an additional experiment.\n\n5.1.2 Baselines\n\nOur experiments are conducted on GPT-2 XL (1.5B) Radford et al. (2019) and GPT-J (6B) Wang and Komatsuzaki (2021), and we compare GLAME with the following state-of-the-art editing methods: Constrained Fine-Tuning (FT) Zhu et al. (2020), MEND Mitchell et al. (2021), ROME Meng et al. (2022a), and MEMIT Meng et al. (2022b). To further verify the superiority of our graph-based editing method, we also compare our method with two variant models ROME-KG and MEMIT-KG. These models utilize ROME and MEMIT, respectively, to directly edit the new high-order relations, (s,r,o*,r,o1),‚ãØ,(s,r,o*,r,on)(s,r,o*,r,o_{1}),\\cdots,(s,r,o*,r,o_{n})( italic_s , italic_r , italic_o * , italic_r , italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ‚ãØ , ( italic_s , italic_r , italic_o * , italic_r , italic_o start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) constructed as described in ¬ß4.1.1 and arising from the edited knowledge (s,r,o,o*)(s,r,o,o*)( italic_s , italic_r , italic_o , italic_o * ), into the LLM. We provide implementation details of baselines and GLAME in Appendix D.\n\n5.2 Performance Comparison (RQ1)\n\nThe performance of all editors on the CounterFact and CounterFactPlus is presented in Table 1. From the results, we have the following observations:\n\nOur model GLAME secures the highest performance on the comprehensive evaluation metric, the Editing Score, surpassing other editors across most evaluation metrics. Specifically, GLAME exhibits enhancements of 11.7611.7611.7611.76 % and 10.9810.9810.9810.98 % in Portability Score over the best baseline models for GPT-2 XL and GPT-J, respectively. This demonstrates that our method can effectively improve the generalization ability of post-edit LLM in utilizing edited knowledge, especially in multi-hop reasoning, by effectively introducing external knowledge graphs. The editing methods based on the ROME framework, GLAME, ROME, and MEMIT, are significantly better than other methods in Paraphrase and Neighborhood Scores. The reason might be these methods impose explicit constraints on editing knowledge recall and retention of editing-irrelevant knowledge. Although MEND and FT, which directly optimize parameters, can accurately recall edited knowledge and achieve commendable results on the Efficacy Score, their lack of precision during the editing process leads to poor performance on Paraphrase, Neighborhood, and Portability Scores compared to other editors.\n\nROME-KG and MEMIT-KG, compared to ROME and MEMIT, demonstrate a notable degradation in performance. This indicates that simply adding extra external information for editing does not guarantee improved performance. Specifically, ROME-KG requires multiple adjustments to the model‚Äôs parameters to edit high-order relationships, potentially harming the original parameters. MEMIT-KG‚Äôs unconstrained incorporation of vast amounts of information into the LLM may compromise the editing of target knowledge. In contrast, GLAME, by developing an editing method tailored for graph structures, incorporates multiple pieces of associated knowledge altered due to editing into the model with just a single edit. This approach not only maintains the precision of edits but also substantially improves the efficiency of leveraging external knowledge graphs.\n\n5.3 Ablation Studies (RQ2)\n\nTo investigate the superiority of each component of our method, we compare GLAME with different variants: GLAME w/ GNN, which omits RGNN‚Äôs relational information and employs a GNN Kipf and Welling (2017) for subgraph encoding in the GKE module; GLAME w/ MLP, which neglects graph structural information, relying solely on MLP for encoding entity representations within the GKE module; and GLAME w/o GKE, which removes the GKE module and degenerates into the ROME. The results are shown in Table 2 and we have the following observations:\n\nGLAME outperforms GLAME w/ MLP and GLAME w/o GKE on most evaluation metrics, especially in Portability Score and Editing Score. This confirms that integrating the structured knowledge altered due to edited samples through the GKE module can effectively enhance the generalization ability of the post-edit model. Additionally, GLAME w/ MLP and GLAME w/ GNN also achieve better performance in Editing Score than GLAME w/o GKE. The improvements verify that the effective incorporation of external information: the hidden state vector of the subject entity and its neighbors from the early layers of LLM, contributes to the performance of edits. Compared with GLAME w/ GNN, the performance of GLAME is further improved, which highlights the importance of relations in LLM‚Äôs recognition of complex graph-structured knowledge associations.\n\n5.4 Sensitivity Analysis (RQ3)\n\nTo further explore the sensitivity of GLAME to important hyper-parameters, we examine the impact of key hyperparameters, the maximum order nùëõnitalic_n of subgraph, and the maximum number mùëömitalic_m of sampled neighbors, on the performance of GLAME. Further results are described in Appendix G.\n\n5.4.1 Effect of maximum subgraph order nùëõnitalic_n\n\nSubgraph construction is a vital operation of the Knowledge Graph Augmentation module (¬ß4.1.1). The maximum order of the subgraph decides the scope of associated knowledge affected by the edited knowledge. In this part, we conduct GLAME with different subgraph order mùëömitalic_m in the GKE module on GPT-2 XL and GPT-J in terms of Editing and Portability Score. We set mùëömitalic_m in the range of {0,1,2,3}0123\\{0,1,2,3\\}{ 0 , 1 , 2 , 3 }. The results are shown in Figure 3. The main observations are as follows:\n\nIncreasing the maximum subgraph order mùëömitalic_m significantly improves the post-edit model performance, peaking at m=2ùëö2m=2italic_m = 2 for two LLMs. GLAME with m>0ùëö0m>0italic_m > 0 consistently outperforms GLAME with m=0ùëö0m=0italic_m = 0. We attribute the improvement to the incorporation of associated knowledge that has been altered due to editing. However, as the maximum order exceeds 2222 (m>2ùëö2m>2italic_m > 2), the post-model‚Äôs performance begins to decline, which may be because the use of higher-order information makes it easy to introduce noise to the editing process.\n\n5.4.2 Effect of the maximum number mùëömitalic_m of neighbors\n\nTo further investigate how the size of subgraph affects the editing performance, we conduct experiments with GLAME, varying the maximum numbers mùëömitalic_m of neighbors per node within the KAG module on GPT-2 XL and GPT-J in terms of Editing and Portability Score. The results are depicted in Figure 4. Specifically, we observed a consistent improvement in editing performance as the number of neighbors increased from 5555 to 20202020 for GPT-2 XL, and up to 25252525 for GPT-J. This suggests that incorporating more neighbors can enhance the representation of the central entity, so that the graph structure may better reflect changes caused by edited knowledge. However, as the nùëõnitalic_n continued to increase, the model‚Äôs performance began to decline. This decline could be attributed to the introduction of noise by an excessive number of neighboring nodes, and the increased subgraph size may escalate the optimization difficulty for the RGNN.\n\n6 Conclusion\n\nIn this paper, we have proposed a novel method GLAME for large language model editing. GLAME leverages a Knowledge Graph Augmentation module to capture the changes in associated knowledge due to edit by constructing an external graph. Following this, we introduce a Graph-based Knowledge Edit module that utilizes a relational graph neural network to seamlessly integrate new knowledge associations from the constructed subgraph into the LLM‚Äôs parameter editing framework. Experimental results on two LLMs and extensive analysis demonstrate the effectiveness and superiority of GLAME in model editing tasks.\n\nLimitations\n\nIn this section, we discuss the limitations of our GLAME. Specifically, our framework‚Äôs reliance on knowledge graphs may be limited by the availability and quality of relevant knowledge. In cases where related knowledge is scarce or the knowledge graph is of low quality, the model‚Äôs performance may suffer. In the future, we will develop more sophisticated subgraph sampling strategies to improve subgraph quality and more accurately capture knowledge changes resulting from editing. Additionally, these strategies aim to increase sampling speed and reduce subgraph size.\n\nEthical Considerations\n\nWe realize that there are risks in developing generative LLMs, so it is necessary to pay attention to the ethical issues of LLMs. We use publicly available pre-trained LLMs, i.e., GPT-2 XL (1.5B) and GPT-J (6B). The datasets are publicly available, i.e., CounterFact, CounterFactPlus, and MQuAKE. All models and datasets are carefully processed by their publishers to ensure that there are no ethical problems.\n\nReferences\n\nCao et al. (2021) Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021. Knowledgeable or educated guess? revisiting language models as knowledge bases. In Annual Meeting of the Association for Computational Linguistics, pages 1860‚Äì1874.\n\nCohen et al. (2023) Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. 2023. Evaluating the ripple effects of knowledge editing in language models. arXiv preprint arXiv:2307.12976.\n\nDai et al. (2022) Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2022. Knowledge neurons in pretrained transformers. In Annual Meeting of the Association for Computational Linguistics, pages 8493‚Äì8502.\n\nDe Cao et al. (2021) Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing factual knowledge in language models. In Conference on Empirical Methods in Natural Language Processing, pages 6491‚Äì6506.\n\nGeva et al. (2023) Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023. Dissecting recall of factual associations in auto-regressive language models. In Conference on Empirical Methods in Natural Language Processing, page 12216‚Äì12235.\n\nGeva et al. (2021) Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer feed-forward layers are key-value memories. In Conference on Empirical Methods in Natural Language Processing, pages 5484‚Äì5495.\n\nJi et al. (2021) Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. 2021. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE transactions on neural networks and learning systems, 33(2):494‚Äì514.\n\nKipf and Welling (2017) Thomas N. Kipf and Max Welling. 2017. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations.\n\nKobayashi et al. (2023) Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2023. Feed-forward blocks control contextualization in masked language models. arXiv preprint arXiv:2302.00456.\n\nLoshchilov and Hutter (2018) Ilya Loshchilov and Frank Hutter. 2018. Decoupled weight decay regularization. In International Conference on Learning Representations.\n\nLv et al. (2021) Qingsong Lv, Ming Ding, Qiang Liu, Yuxiang Chen, Wenzheng Feng, Siming He, Chang Zhou, Jianguo Jiang, Yuxiao Dong, and Jie Tang. 2021. Are we really making much progress? revisiting, benchmarking and refining heterogeneous graph neural networks. In Conference On Knowledge Discovery and Data Mining, page 1150‚Äì1160.\n\nMeng et al. (2022a) Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022a. Locating and editing factual associations in gpt. Annual Conference on Neural Information Processing Systems, 35:17359‚Äì17372.\n\nMeng et al. (2022b) Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. 2022b. Mass-editing memory in a transformer. In International Conference on Learning Representations.\n\nMitchell et al. (2021) Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. 2021. Fast model editing at scale. In International Conference on Learning Representations.\n\nMitchell et al. (2022) Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn. 2022. Memory-based model editing at scale. In International Conference on Machine Learning, pages 15817‚Äì15831. PMLR.\n\nPaszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Annual Conference on Neural Information Processing Systems, pages 8024‚Äì8035.\n\nPearl (2022) Judea Pearl. 2022. Direct and indirect effects. In Probabilistic and causal inference: the works of Judea Pearl, pages 373‚Äì392.\n\nRadford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\n\nSchlichtkrull et al. (2018) Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolutional networks. In Extended Semantic Web Conference, pages 593‚Äì607.\n\nVaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Annual Conference on Neural Information Processing Systems.\n\nVig et al. (2020) Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. 2020. Investigating gender bias in language models using causal mediation analysis. Annual Conference on Neural Information Processing Systems, 33:12388‚Äì12401.\n\nWang and Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax.\n\nWang et al. (2019) Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li, Jinjing Zhou, Qi Huang, Chao Ma, Ziyue Huang, Qipeng Guo, Hao Zhang, Haibin Lin, Junbo Zhao, Jinyang Li, Alexander J Smola, and Zheng Zhang. 2019. Deep graph library: Towards efficient and scalable deep learning on graphs. International Conference on Learning Representations Workshop on Representation Learning on Graphs and Manifolds.\n\nYao et al. (2023) Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023. Editing large language models: Problems, methods, and opportunities. In Conference on Empirical Methods in Natural Language Processing, pages 10222‚Äì10240.\n\nZhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.\n\nZhong et al. (2023) Zexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, and Danqi Chen. 2023. MQuAKE: Assessing knowledge editing in language models via multi-hop questions. In Conference on Empirical Methods in Natural Language Processing, page 15686‚Äì15702.\n\nZhu et al. (2020) Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. 2020. Modifying memories in transformer models. arXiv preprint arXiv:2012.00363.\n\nAppendix A Pseudocode\n\nAlgorithm 1 provides the pseudo-code of our editing method GLAME.\n\nAppendix B Datasets Detail\n\nB.1 Details of CounterFact Dataset\n\nTable 3 shows an example from the CounterFact dataset. Each entry contains an edit request, several paraphrase prompts, and neighborhood prompts. In this example entry, the edit request aims to change the model‚Äôs knowledge of Danielle Darrieux‚Äôs mother tongue from French to English. Paraphrase prompts are the semantical paraphrases of the target prompt, and neighborhood prompts are those prompts that have the same relation with the edit request but have a different subject, whose knowledge should remain unchanged by the edit.\n\nOur train/test dataset splits are kept the same as Meng et al. (2022a). Similarly, we evaluate our method using the first 7500750075007500 records on GPT-2 XL, and the first 2000200020002000 records on GPT-J. Note that for methods not employing hypernetworks, including our GLAME, there is no requirement for training with the data from the training set.\n\nB.2 Details of CounterFactPlus Dataset\n\nThe CounterFactPlus dataset serves as a supplementary expansion of the original CounterFact dataset, selecting 1031103110311031 entries as a subset of the original data and enriching them with new test questions based on the original content. Each entry contains the same edit request as found in CounterFact, with additional questions and answers that require LLM to do further reasoning based on the edited knowledge.\n\nAn example entry from the dataset is showcased in Table 4. In this example entry, the edit request entails modifying the model‚Äôs knowledge of Spike Hughes‚Äôs mother tongue from London to Philadelphia. This edit introduces new knowledge associations, such as (Spike Hughes, mother tongue, Philadelphia, known for, cheesesteaks), leading to a multi-hop question What famous food is associated with the city where Spike Hughes originates from?, with the correct answer being Cheesesteaks. The additional knowledge triple (Philadelphia, knowledge for, Cheesesteaks) used to construct the multi-hop question is labeled as ‚ÄúRecalled relation‚Äù in the dataset. In our work we primarily focus on the multi-hop reasoning aspect, aiming to assess GLAME‚Äôs capacity to capture relevant changes in knowledge.\n\nB.3 Details of MQuAKE Dataset\n\nSimilar to CounterFactPlus, MQuAKE is a more challenging dataset that also focuses on evaluating models‚Äô ability to perform further reasoning using newly edited knowledge. Each entry in this dataset may involve multiple edits and contain multi-hop reasoning questions that require reasoning from 2222 to 4444 hops to answer correctly, posing stricter requirements on the post-model‚Äôs generalization capability.\n\nTable 5 illustrates an example from MQuAKE dataset. The example entry requires two edits to the LLM, inserting new knowledge (Betty Carter, plays, instrumental rock) and (USA, head of state, Norodom Sihamoni). Accordingly, a 3-hop question ‚ÄúWho is the head of state of the country from which the music genre associated with Betty Carter originated?‚Äù is constructed to assess the post-edit models‚Äôs ability to employ edited knowledge and its associated knowledge. Following Zhong et al. (2023), our evaluation also focuses on a subset of 3000300030003000 entries, evenly distributed across {2,3,4}234\\{2,3,4\\}{ 2 , 3 , 4 }-hop questions, with each category comprising 1000100010001000 entries.\n\nAppendix C Evaluation Metrics\n\nWe adopt three widely-used metrics Meng et al. (2022a, b), Efficacy Score, Paraphrase Score, and Neighborhood Score to evaluate all editors on CounterFact dataset, and use Portability Score Yao et al. (2023) on CounterFactPlus dataset. We utilize the harmonic mean of four metrics, Editing Score, to evaluate each editor‚Äôs overall capabilities. Each metric is calculated as follows:\n\nEfficacy Score is to test whether the post-edit LLMs can correctly recall the new target entity when given the edit prompt p‚Å¢(s,r)ùëùùë†ùëüp(s,r)italic_p ( italic_s , italic_r ). It is calculated by\n\nùîº‚Å¢[ùïÄ‚Å¢[P‚Ñ±‚Ä≤‚Å¢(o*‚à£p‚Å¢(s,r))>P‚Ñ±‚Ä≤‚Å¢(o‚à£p‚Å¢(s,r))]].ùîºdelimited-[]ùïÄdelimited-[]subscriptPsuperscript‚Ñ±‚Ä≤conditionalsuperscriptùëúùëùùë†ùëüsubscriptPsuperscript‚Ñ±‚Ä≤conditionalùëúùëùùë†ùëü\\mathbb{E}\\left[\\mathbb{\\mathbb{I}}\\left[\\mathrm{P}_{\\mathcal{F}^{\\prime}}% \\left(o^{*}\\mid p(s,r)\\right)>\\mathrm{P}_{\\mathcal{F}^{\\prime}}\\left(o\\mid p(s% ,r)\\right)\\right]\\right].blackboard_E [ blackboard_I [ roman_P start_POSTSUBSCRIPT caligraphic_F start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ‚à£ italic_p ( italic_s , italic_r ) ) > roman_P start_POSTSUBSCRIPT caligraphic_F start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_o ‚à£ italic_p ( italic_s , italic_r ) ) ] ] .\n\nParaphrase Score measures the performance of the post-edit LLM on rephase prompt set PPsuperscriptùëÉùëÉ{P}^{P}italic_P start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT of edit prompt p‚Å¢(s,r)ùëùùë†ùëüp(s,r)italic_p ( italic_s , italic_r ). The calculation is similar to the Efficacy Score:\n\nùîºp‚ààPP‚Å¢[ùïÄ‚Å¢[P‚Ñ±‚Ä≤‚Å¢(o*‚à£p)>P‚Ñ±‚Ä≤‚Å¢(o‚à£p)]].subscriptùîºùëùsuperscriptùëÉùëÉdelimited-[]ùïÄdelimited-[]subscriptPsuperscript‚Ñ±‚Ä≤conditionalsuperscriptùëúùëùsubscriptPsuperscript‚Ñ±‚Ä≤conditionalùëúùëù\\mathbb{E}_{p\\in{P}^{P}}\\left[\\mathbb{\\mathbb{I}}\\left[\\mathrm{P}_{\\mathcal{F}% ^{\\prime}}\\left(o^{*}\\mid p\\right)>\\mathrm{P}_{\\mathcal{F}^{\\prime}}\\left(o% \\mid p\\right)\\right]\\right].blackboard_E start_POSTSUBSCRIPT italic_p ‚àà italic_P start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ blackboard_I [ roman_P start_POSTSUBSCRIPT caligraphic_F start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ‚à£ italic_p ) > roman_P start_POSTSUBSCRIPT caligraphic_F start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_o ‚à£ italic_p ) ] ] .\n\nNeighborhood Score measures whether the post-edit LLM assigns the higher probability to the correct fact on the prompt set PNsuperscriptùëÉùëÅ{P}^{N}italic_P start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT, which consists of distinct but semantically similar prompts p‚Å¢(s,r)ùëùùë†ùëüp(s,r)italic_p ( italic_s , italic_r ). The calculation is defined as:\n\nùîºp‚ààPN‚Å¢[ùïÄ‚Å¢[P‚Ñ±‚Ä≤‚Å¢(o*‚à£p)<P‚Ñ±‚Ä≤‚Å¢(o‚à£p)]].subscriptùîºùëùsuperscriptùëÉùëÅdelimited-[]ùïÄdelimited-[]subscriptPsuperscript‚Ñ±‚Ä≤conditionalsuperscriptùëúùëùsubscriptPsuperscript‚Ñ±‚Ä≤conditionalùëúùëù\\mathbb{E}_{p\\in{P}^{N}}\\left[\\mathbb{\\mathbb{I}}\\left[\\mathrm{P}_{\\mathcal{F}% ^{\\prime}}\\left(o^{*}\\mid p\\right)<\\mathrm{P}_{\\mathcal{F}^{\\prime}}\\left(o% \\mid p\\right)\\right]\\right].blackboard_E start_POSTSUBSCRIPT italic_p ‚àà italic_P start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ blackboard_I [ roman_P start_POSTSUBSCRIPT caligraphic_F start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ‚à£ italic_p ) < roman_P start_POSTSUBSCRIPT caligraphic_F start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_o ‚à£ italic_p ) ] ] .\n\nThis metric can assess the extent of the impact that edits have on unrelated knowledge.\n\nPortability Score measures the accuracy of the post-edit model on the multi-hop question set PùëÉPitalic_P about the edit sample:\n\nùîºp‚ààP[ùïÄ[‚Ñ±(p)‚Ä≤=o*)‚Ä≤]].\\mathbb{E}_{p\\in{P}}\\left[\\mathbb{I}\\left[\\mathcal{F}{{}^{\\prime}}(p)=o^{*}{{}% ^{\\prime}})\\right]\\right].blackboard_E start_POSTSUBSCRIPT italic_p ‚àà italic_P end_POSTSUBSCRIPT [ blackboard_I [ caligraphic_F start_FLOATSUPERSCRIPT ‚Ä≤ end_FLOATSUPERSCRIPT ( italic_p ) = italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ‚Ä≤ end_FLOATSUPERSCRIPT ) ] ] .\n\nGiven the challenges associated with evaluating the data, the Portability Score provides a more accurate reflection of the model‚Äôs generalization capabilities compared to other metrics.\n\nAppendix D Baselines\n\nOur experiments are conducted on GPT-2 XL (1.5B) Radford et al. (2019) and GPT-J (6B) Wang and Komatsuzaki (2021), and we compare GLAME with the following state-of-the-art editing methods:\n\nConstrained Fine-Tuning (FT) Zhu et al. (2020) involves fine-tuning specific layers of the LLM‚Äôs parameters directly using gradient descent, while imposing a norm constraint on the weight changes to prevent catastrophic forgetting.\n\nMEND Mitchell et al. (2021) constructs a hyper-network based on the low-rank decomposition of gradients to perform editing.\n\nROME Meng et al. (2022a) is based on the hypothesis that knowledge in LLMs is stored in the FFN module, and uses optimization to update a FFN layer to insert knowledge.\n\nMEMIT Meng et al. (2022b) builds on the ROME method, specializing in batch-editing tasks by performing edits on a range of FFN layers.\n\nTo further verify the superiority of our graph-based editing method, we also compare our method with two variant models ROME-KG and MEMIT-KG. The two baselines aim to evaluate the performance of directly adding the same amount of external information to the LLM without using the GKE module. For each record in our test dataset, we construct edit requests that contain high-order relationships from the knowledge graph. For instance, given the original edit content \"Spike Hughes originates from London ‚Üínormal-‚Üí\\to‚Üí Washington\" and a related knowledge graph triple (Washington, capital of, United States of America), we then create a new edit request to insert this knowledge into the LLM: \"Spike Hughes originates from Washington, capital of United States of America\", using either ROME or MEMIT.\n\nAppendix E Implementation Details\n\nWe implement our GLAME method with PyTorch Paszke et al. (2019) and the DGL Wang et al. (2019). Within the Knowledge Graph Augmentation (KGA) module, we set the maximum subgraph order nùëõnitalic_n to 2222 for both GPT-2 XL and GPT-J, with the maximum number of sampled neighbors mùëömitalic_m set to 20202020 for GPT-2 XL and 40404040 for GPT-J. Hidden vectors for entities and relations are extracted from the 5555th layer of GPT-2 XL (k=5ùëò5k=5italic_k = 5) and the 2222nd layer of GPT-J (k=2ùëò2k=2italic_k = 2), respectively, to initialize the subgraph representations. For the GKE module, we perform editing operations on the 9999th layer of GPT-2 XL (l=9ùëô9l=9italic_l = 9) and the 5555th layer of GPT-J (l=5ùëô5l=5italic_l = 5) based on ROME‚Äôs locating results. The hidden embedding sizes for the RGNN are set to 1600160016001600 for GPT-2 XL and 4096409640964096 for GPT-J. For RGNN optimization, the AdamW Loshchilov and Hutter (2018) optimizer is used with a learning rate of 5√ó10‚àí15superscript1015\\times 10^{-1}5 √ó 10 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, the optimal regularization factor ŒªùúÜ\\lambdaitalic_Œª is 6.25√ó10‚àí26.25superscript1026.25\\times 10^{-2}6.25 √ó 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT for CounterFact and 7.5√ó10‚àí27.5superscript1027.5\\times 10^{-2}7.5 √ó 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT for both CounterFactPlus and MQuAKE. To prevent overfitting, we perform early-stop when the loss is lower than 1√ó10‚àí21superscript1021\\times 10^{-2}1 √ó 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT. Since our method does not require an additional training set for training, we select important hyperparameters on the training set. For the covariance matrix estimation ùêÇùêÇ\\mathbf{C}bold_C, which represents the pre-computed keys in a layer, we directly use the results computed by ROME Meng et al. (2022a), which is collected using 100,000100000100,000100 , 000 samples of Wikitext. The number NùëÅNitalic_N of random prefixes generated for calculating ùê¶*subscriptùê¶\\mathbf{m}_{*}bold_m start_POSTSUBSCRIPT * end_POSTSUBSCRIPT and ùê§*subscriptùê§\\mathbf{k_{*}}bold_k start_POSTSUBSCRIPT * end_POSTSUBSCRIPT is to 50505050, serving as a method of data augmentation for the original edits. For other baselines, we conduct our experiment with the code implemented by ROME Meng et al. (2022a), and all the settings of the baselines we compare, including the hyperparameters, are consistent with Meng et al. (2022a, b).\n\nOur experiments are conducted on NVIDIA Tesla A100 (80G) and AMD EPYC 7742 CPU. Under this configuration, given the pre-prepared subgraph, GLAME requires approximately 7777 seconds to perform an edit on the GPT-J model. For comparison, ROME takes approximately 5555 seconds for a similar task. Given the relatively small parameter size of GNNs, GLAME does not necessitate significant additional GPU memory for optimization compared to other similar locate-then-edit models; in practice, approximately 48GB of GPU memory is sufficient for updating the GPT-J model.\n\nE.1 Wikidata Sampling Details\n\nIn the Knowledge Graph Augmentation (KGA) module, we leverage Wikidata as an external knowledge graph to construct a subgraph for each edit sample (s,r,o,o*)ùë†ùëüùëúsuperscriptùëú(s,r,o,o^{*})( italic_s , italic_r , italic_o , italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ). Specifically, we employ Wikidata‚Äôs API to perform a SPARQL query, retrieving all outgoing edges of the entity o*o*italic_o *. After retrieving these edges, we prioritize the triples by sorting them to foreground the most potentially valuable information. This prioritization is based on the frequency of each relation‚Äôs occurrence across the dataset. Relations that appear less frequently are deemed more valuable as they may embody information of higher specificity or rarity, similar to principles of information entropy where less frequent occurrences convey more information.\n\nAs datasets CounterFact, CounterFactPlus, and MQuAKE are directly constructed using Wikidata, each edited entity within these datasets is linked with its corresponding Wikidata item ID, allowing for precise sampling. Note that in our experiments, the constructed subgraphs are filtered to exclude the standard answers to the multi-hop questions. This operation ensures that the improvement in model performance is attributed to an enhancement in the generalization ability, rather than simply being influenced by specific answer patterns within the subgraphs.\n\nE.2 Evaluation Details\n\nIn our experiments, we assessed the Efficacy Score, Paraphrase Score, and Neighborhood Score on the CounterFact dataset following the method in Meng et al. (2022a). We used specific prompts as inputs to the LLM and examined the model‚Äôs prediction probabilities for both the original entity oùëúoitalic_o and the edited entity o*superscriptùëúo^{*}italic_o start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT. For the CounterFactPlus dataset, our assessment of the Portability Score involved prompting the LLM with multi-hop questions, and then verifying whether the output generated includes the correct answers. To accommodate variations in phrasing or synonyms between the model‚Äôs output and the standard answer, fuzzy matching was employed. In practice, we utilized the partial ratio algorithm from Fuzzywuzzy library, which calculates similarity based on the Levenshtein distance. Regarding the MQuAKE dataset, we adopt the Efficacy Score to evaluate the effectiveness of different editing methods.\n\nAppendix F Results on MQuAKE\n\nTo further demonstrate the capability of GLAME in capturing the associated knowledge changes due to edits, we compare our GLAME with two competitive baseline models, ROME and MEMIT, on the more challenging MQuAKE Zhong et al. (2023) dataset. The results are shown in Table 6. From the results, we find that our GLAME achieves significant improvements over ROME and MEMIT across questions of varying hops. With an increase in the number of hops, which necessitates a greater utilization of edited knowledge, the performance of all editing methods begins to decline. However, GLAME exhibits the highest relative improvement on 4444-hop questions than SOTA methods, which is likely attributed to our model‚Äôs effective capture of associative knowledge, enabling it to construct a more solid knowledge representation. Such an advantage becomes significant in the context of 4444-hop questions, where the complexity of reasoning is markedly higher. This emphatically validates the effectiveness of our model in improving the post-edit model‚Äôs generalization capacity in processing edited knowledge.\n\nAppendix G Sensitivity Analysis\n\nThe maximum order of subgraph nùëõnitalic_n and the maximum number mùëömitalic_m of sampled neighbors are two key hyper-parameters in GLAME. Figure 5 and 6 depict the performance of GLAME across various nùëõnitalic_n and mùëömitalic_m values, as measured by Paraphrase and Neighborhood Score. From Figure 5, we observe that increasing the order of the subgraph can enhance the post-edit model‚Äôs performance in terms of the Paraphrase Score. This demonstrates that incorporating more new associated knowledge with edits can improve the generalization ability of the post-edit model in processing edited knowledge. In contrast, Neighborhood Score exhibits greater stability with respect to the value of nùëõnitalic_n, indicating that our editing method inflicts minimal harm on the model‚Äôs original capabilities. In Figure 6, we can find that the Paraphrase and Neighborhood Scores are more stable than the Editing and Portability Scores in Figure 4. This stability may be attributed to the design of the loss function and those random prefixes added during optimization, which impose certain constraints on scenarios related to these two metrics, resulting in more stable behavior as the subgraph changes.\n\nAppendix H Case Study\n\nIn this section, we present several generation examples on GPT-J utilizing three knowledge editing models: GLAME, ROME, and MEND, to demonstrate the efficacy of knowledge editing through multi-hop questions in CounterFactPlus. We focus on the edited models‚Äô ability to leverage newly inserted knowledge for reasoning in response to a given prompt while maintaining contextual coherence. The generation examples are shown in Figure 7.\n\nExample A [Case 1662 in CounterFactPlus]. In this example, counterfactual knowledge ‚ÄúHeritage Range is in Africa‚Äù was inserted. To answer the multi-hop question correctly, the edited model must first recall the newly inserted knowledge (Heritage Range, located in, Africa), followed by (Africa, highest peak, Mount Kilimanjaro). Notably, GLAME provided the correct answer, whereas ROME and MEND seemed to fail in recalling the inserted knowledge during reasoning, offering answers such as ‚Äúthe Great Plains‚Äù and ‚ÄúMount McKinley‚Äù based on Americas-related knowledge, indicating a weaker generalization.\n\nExample B [Case 5431 in CounterFactPlus]. In this example, a piece of new knowledge ‚ÄúAssociation football originated in Sweden‚Äù was inserted. Answering the multi-hop question required further reasoning to identify Sweden‚Äôs famous athlete, Zlatan Ibrahimovic. GLAME maintained coherence with the context and correctly recalled the answer. Although ROME managed to recall information related to ‚ÄúSweden‚Äù, its answer was inconsistent with the prompt, only mentioning ‚ÄúSweden‚Äù and mistakenly claiming ‚ÄúSweden‚Äù has the largest population in the world outside of China, showing signs of hallucination. MEND, again, failed to recall the newly inserted knowledge, providing an unrelated answer about the Brazilian footballer Pele."
    }
}
{
    "id": "dbpedia_182_0",
    "rank": 88,
    "data": {
        "url": "https://www.iieta.org/journals/isi/paper/10.18280/isi.290133",
        "read_more_link": "",
        "language": "en",
        "title": "Machine Learning for Forest Fire Prediction: A Case Study in North Algeria",
        "top_image": "https://iieta.org/sites/default/files/styles/inline_image/public/medias/2024-02/2_29.png?itok=-0z3gFz8",
        "meta_img": "",
        "images": [
            "https://www.iieta.org/sites/default/files/logo2.jpg",
            "https://www.iieta.org/sites/default/files/styles/inline_image/public/medias/2024-04/qqtu_pian_20240428144739.png?itok=DjaJ8zSh",
            "https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg",
            "https://orcid.org/assets/vectors/orcid.logo.icon.svg",
            "https://orcid.org/assets/vectors/orcid.logo.icon.svg",
            "https://orcid.org/assets/vectors/orcid.logo.icon.svg",
            "https://orcid.org/assets/vectors/orcid.logo.icon.svg",
            "https://orcid.org/assets/vectors/orcid.logo.icon.svg",
            "https://orcid.org/assets/vectors/orcid.logo.icon.svg",
            "https://iieta.org/sites/default/files/styles/inline_image/public/medias/2024-02/1_30.png?itok=08f_BUJi",
            "https://iieta.org/sites/default/files/styles/inline_image/public/medias/2024-02/2_29.png?itok=-0z3gFz8",
            "https://iieta.org/sites/default/files/styles/inline_image/public/medias/2024-02/3_28.png?itok=k_IzFoq_",
            "https://iieta.org/sites/default/files/styles/inline_image/public/medias/2024-02/4_29.png?itok=fMR1av2S",
            "https://iieta.org/sites/default/files/styles/inline_image/public/medias/2024-02/5_23.png?itok=csYPAl14",
            "https://iieta.org/sites/default/files/styles/inline_image/public/medias/2024-02/6_19.png?itok=SxU_dURq",
            "https://iieta.org/sites/default/files/styles/inline_image/public/medias/2024-02/7_16.png?itok=bctrRxNT",
            "https://iieta.org/sites/default/files/styles/inline_image/public/medias/2024-02/8_13.png?itok=cvk6-Bbu",
            "https://iieta.org/sites/default/files/styles/inline_image/public/medias/2024-02/9_13.png?itok=ZbNoG3yS",
            "https://iieta.org/sites/default/files/styles/inline_image/public/medias/2024-02/10_8.png?itok=quuIJ31z",
            "https://www.iieta.org/sites/default/files/2_38.jpg",
            "https://www.iieta.org/sites/default/files/2.gif",
            "https://www.iieta.org/sites/default/files/sisw_mdpfnb8ckkifep.gif",
            "https://www.iieta.org/sites/default/files/21.gif",
            "https://www.iieta.org/sites/default/files/77a530ecc22836739ef2b2ca23ee29b6.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://www.iieta.org/journals/isi/paper/10.18280/isi.290133",
        "text": "Forests are a major natural resource that plays a crucial role in maintaining environmental balance. Moreover, it is the most necessary resource part of our ecosystem. Human health and wealth are inextricably linked to forest health; from the fresh air we breathe to the natural products we rely on. Therefore, the health of the forest in any given area is a real indicator of the ecological condition prevailing in that area.\n\nFire has been closely associated with mankind from the beginning of civilization [1]. The discovery of fire and its uses have directly or indirectly permitted a man to live and survive in the temperate zone. However, fire also can be a danger whose potential for disaster is a source of growing concern all over the world (every year millions of hectares of land are destroyed by fire, which causes some damage to the natural environment [2], the forest fire also causes an increase in the proportion of CO2 in the air, which causes suffocation and respiratory diseases for human.\n\nForest fires are considered to be a potential hazard with physical, biological, ecological, and environmental consequences [3]. Forest fire results in partial or complete degradation of vegetation cover, thus modifying the radiation balance by increasing the surface albedo, and water runoff, and raising soil erosion [4].\n\nForest fires are the most common peril in forests. It is an uncontrollable event that occurs in nature that poses a great deal of threat to the wildlife and the people who live there. It is reported that each year in the last decade, a total number of 4 to 6 million wildfire events happened worldwide [5]. Therefore, the probability of occurrence of it depends on the ignition causes and environmental preconditions [6].\n\nAlgeria is the one considered the fire hotspot on the southern rim of the Mediterranean Basin (MB) [7], which has suffered from forest fires due to the presence of flammable fuels such as shrub lands and forests [2]. These recent forest fires that happened last year in “Algeria” causes huge damage that required international support to stop (Figure 1) and the death of many Algerians to throw the process of rescuing. Around 90 people, including 57 civilians and 33 soldiers (during rescue operations) [8], and according to study [9], the total area of vegetation cover affected by fires during the summer of 2021, more than 100,000 hectares through 1,631 fire outbreaks recorded in 21 wilayas. Around 260,135 hectares of forests, 21,040 hectares of bushes, 16,415 hectares of scrub, 16,160 hectares of fruit trees, and 352 hectares of esparto were ravaged by the fires. Also, 19,178 farm animals burned in the fires [10], and 1,705 homes burned [11].\n\nIndeed, the Algerian forests of high productivity and conservation value have suffered during the recent decades of degradation and fragmentation from repeated fires [12, 13]. According to study [14], fires lead to consumption of six times more than these forest ecosystems could produce.\n\nTo avoid these losses, predicting the forest fire before it happens has effectiveness and influence more than detecting it (At least, we gain some time to take action to avoid many of those losses). Prediction of wildfire occurrence also plays a major role in resource allocation, mitigation, and recovery efforts.\n\nNowadays, there are various technologies to predict forest fires depending on the availability of data collected from remote sensing satellites and location detection systems; Machine learning as a sub-branch of Artificial Intelligence (AI) is one of these technologies.\n\nA variety of machine learning (ML) models with different architectures has been used in literature to predict forest fires in different zones around the world, and some of them used only meteorological data. These studies are capable to perform good results and obtaining high accuracy for predicting forest fires [15-19]. However, some challenges and limitations are encountered by many studies including [20]:\n\n˗The effectiveness of forest fire prediction relies on gathering a substantial volume of data from diverse sources. However, the presence of incomplete or inaccurate data introduces challenges that can impact the reliability and validity of prediction models.\n\n˗In the context of forest fire prediction, models must grapple with the complexity and uncertainty surrounding fire behavior and spread, including interactions and feedbacks among diverse factors and scales. Despite these challenges, accurately measuring, modeling, and validating certain aspects of this intricate system may prove challenging.\n\n˗For forest fire prediction to be effective across diverse regions, scenarios, and conditions, models must be adaptable. However, some prediction models rely on specific assumptions or parameters that may not be universally applicable. Regular testing, calibration, and updates are essential to ensure the accuracy and robustness of these models in various situations and environments.\n\nIn Algeria, a prediction study of fire behavior based on terrain, wind conditions, and fuel characteristics presented in [21]. In study [22], a predictive model based on the decision tree for forest fires prediction using data mining techniques presented three meteorological attributes namely: temperature, relative humidity (RH), and wind speed.\n\nWhile study [23] applied artificial neural networks to predict forest fires in embedded devices using collected meteorological data from wireless sensor networks, nine machine learning algorithms were investigated and compared based on the obtained results, and they propose an embedded forest fire prediction model. The study [24] addresses wildfire prediction using a recent dataset from 2012, employing an artificial neural network (ANN) that outperforms other classifiers in accuracy, precision, and recall. Key features influencing predictions include relative humidity (RH), drought code (DC), and initial spread index (ISI).\n\nThe main objective of our work, is to create a forest fire prediction system to predict the fire early based on the data climate and build a complete system that it can be accessed from any device in the world at any time, and we focus on:\n\n- Create the system with the lowest budget possible.\n\n- Let the users interact with the system from any device.\n\nA comparison between three algorithms was applied by examining the experimental results to obtain the best model, K-Nearest Neighbors (KNN), Decision Trees, and Random Forest, which are diverse machine learning models with distinct characteristics. KNN is an instance-based algorithm that classifies data points based on their proximity to neighboring instances. Decision Trees construct a tree-like structure to make decisions by splitting the dataset at different features. Random Forest, an ensemble method, leverages multiple Decision Trees to enhance predictive accuracy and reduce overfitting. While KNN relies on local relationships, Decision Trees focus on hierarchical decision-making, and Random Forest combines the strengths of multiple trees for robust predictions. Each model suits different scenarios, offering a range of approaches for various machine learning tasks.\n\nSeveral classifiers other than the three mentioned in the comment were evaluated, however empirically KNN, Decision Trees and random forest showed the most promising results.\n\nFurthermore, the random forest classifier yields the best experiment results in terms of prediction accuracy, which makes it a useful model for predicting fires in Algeria.\n\nThe practical application of our forest fire forecasting systems will bring great benefit to the state and individuals on the other hand, including that, firstly, a low-budget, easy to implement, and accessible system for forecasting forest fires. Secondly, predicting fires before they occur facilitates the process of rapid intervention to extinguish them and prevent their spread, and it also avoids losses and damages, that were previously mentioned in the introduction.\n\nThe rest of the paper is organized as follows: The methodology section discusses the method we have pursued to create the prediction system, including the system architecture, the data collection, and the algorithm used to build the forest fire prediction system. While the obtained results will be presented and discussed in the results and discussion section. Finally, we conclude our work with a conclusion and suggest some perspectives.\n\nFigure 1. Wildfire in Algeria, 2021\n\nIn this study Algeria country has been selected owing to:\n\nThe number of fires in Algeria has increased in recent years. Based on the availability of fire data, we chose a study period of 21 years from 2000 to 2020 (Figure 2 presents a map of the occurrence of fires number in northern Algeria of the period 2001-2018).\n\nLack of equipment needed to deal with fires.\n\nLack of early detection systems for fires.\n\nThe difficulty of the region's terrain.\n\nFigure 2. Number of fires in northern Algeria between2001-2018, the enclosed figure (upper left) indicates the fire hotspots and their level of density [7]\n\nIn our work, we used machine learning algorithms to train an AI model that can be applied to the future climate dataset to predict forest fires before they start.\n\nThis will help the authorities to take adequate precautions and make necessary arrangements to reduce possible losses.\n\nIn order to achieve this goal, the designed system uses a trained machine learning model to analyze weather data in the area selected by the user to predict if a potential fire may occur.\n\nTo analyze the data provided by NASA for the climate change and previous fires, the system starts by normalizing this data, and then it applies one of the machine learning algorithms: K-Nearest Neighbor (KNN), decision tree (DT), and random forest (RF).\n\nThe website provides a simple interface that allows the user (local authorities, and civilians) to select a geographical area, then displays the possibility that a fire may start.\n\nThe following figure (Figure 3) summarizes the architecture of our system.\n\nFigure 3. Number general system architecture\n\nThe process of creating the prediction system consists of the following steps:\n\n2.1 Collecting the data\n\nIn this step, we collect two types of data from two different sources:\n\n˗The geographical location and date of actual fires.\n\n˗The weather data corresponding to a specific date and geographical location.\n\nThe fires data. The fire data provided by FIRMS from NASA is divided according to country and year. The data in each file consist of: latitude, longitude, brightness, scan, track, acq_date, acq_time, satellite, instrument, confidence, version, bright y31, frp, daynight, and type, but in our case we need only to:\n\n˗Latitude and longitude: represent the geographical coordinates of the fire.\n\n˗acq_date: represents the date when the fire happens.\n\n˗Confidence: This confidence estimate, which ranges between 0% and 100%, is used to assign one of the three fire classes (low-confidence fire, nominal-confidence fire, or high-confidence fire).\n\nFor our study, we collected fire information in the period between 2000 and 2020 for the region of Algeria.\n\nThe climate data. The climate data is provided using the NASA POWER PROJECT API from the MODIS satellite.\n\nThe climate data that we collect consists of many features described in Table 1, in the following:\n\nTable 1. Features description of climate data\n\nThe features that the table shows cover: coordinates, date, Temperature, Humidity, Wind, Surface Pressure and Precipitation.\n\nProcess of Collecting. The method to download the data is shown in Figure 4.\n\nFigure 4. The process of collecting data\n\nTo create a machine learning model, we need positive and negative samples. To obtain negative samples, we create a Power API request using the same geographical coordinates of positive areas with a date decreased by 10 days.\n\nConsequently, the data consist now of:\n\n1. Fire data: the geographical coordinates and the date of the fires.\n\n2. Positive climate data: the weather data of the day and the fire events consists of Temperature, Humidity, Precipitation, Surface Pressure, Wind and each feature has sub-features that are related to it, with a total of 23 features.\n\n3. Negative climate data: data of normal days without any fire have the same features as the Positive climate data.\n\nWe organized the data into files corresponding to the different year of the studied period. Each data file has a different number of samples labeled as positive or negative:\n\n˗ Positive: represents 50% of samples, indicated by 1 in the column ‘fire’, (meaning there was a fire in that location that day).\n\n˗Negative: represents 50% of samples, indicated by 0 in the column ‘fire’, (meaning there wasn’t a fire in that location that day).\n\n2.2 Pre-processing the data\n\nNormalization alters raw datasets by creating new values and maintaining general distribution as well as a ratio in data. The most used type of normalization in machine learning is: Min-Max Scaling which subtract the minimum value from each column and divide by the range (max - min). Each new column will have a minimum value of 0 and a maximum value of 1.\n\nThere are many techniques in Normalization such: Min-Max, Z-score and more, but we focus on using the Min-Max approach.\n\n2.3 Finding the best models\n\nThe process of choosing a machine learning model depends on many factors ranging from the type of problem at hand to the type of output you are looking for, some of these factors are:\n\n˗ Size of the Training Data.\n\n˗ Accuracy and/or Interpretability of the Output.\n\n˗ Speed (Training Time).\n\n˗ Number of Features.\n\nIn the following, we tested the three models described above:\n\n1. K-Nearest Neighbor.\n\n2. Random forest.\n\n3. Decision tree.\n\n2.4 Training and testing the models\n\nWe evaluated each model using the following train-test ratio:\n\n˗ 70 percent of the data will be used for training and 30 percent of the data will be for testing.\n\n˗ 80 percent of the data will be used for training and 20 percent of the data will be for testing.\n\n˗ 90 percent of the data will be used for training and 10 percent of the data will be for testing.\n\n2.5 Evaluation\n\nWe will evaluate the accuracy of the models on the test collection by calculating the Accuracy Score performance metric which is a scoring system in binary classification (i.e., determining if an answer or returned information is correct or not). It represents the ratio of correctly predicted outputs.\n\nWe restrict our processing only to the data that is related to Algeria, where the confidence of fire is above 80%. The data is grouped by years as indicated in Figure 6.\n\nBecause the data already consists of clean vectors of measurements (features), there wasn’t so much preprocessing needed. The only preprocessing method we applied to the data was the Min-Max normalization (see the equation below) to give the same importance (influence on training) to all the features of our data even if they have different ranges of values.\n\n$X=\\frac{X-X_{\\min }}{X_{\\max }-X_{\\min }}$ (1)\n\nFigure 6. The distribution of samples with respect to each year\n\nFigure 7. The accuracy of the KNN classifier for different years in the period 2000-2020\n\nTable 2. The accuracy of KNN classifier\n\nYear\n\nTest Set Size: 30\n\nTest Set Size: 20\n\nTest Set Size: 10\n\nTrain Score\n\nTest Score\n\nTrain Score\n\nTest Score\n\nTrain Score\n\nTest Score\n\n2000\n\n0.835443\n\n0.661764\n\n0.838888\n\n0.739130\n\n0.857142\n\n0.739130\n\n2001\n\n0.843991\n\n0.685660\n\n0.846044\n\n0.705128\n\n0.851758\n\n0.726495\n\n2002\n\n0.819218\n\n0.674683\n\n0.823794\n\n0.691358\n\n0.826261\n\n0.696394\n\n2003\n\n0.849212\n\n0.701709\n\n0.849951\n\n0.691025\n\n0.847677\n\n0.711538\n\n2004\n\n0.861185\n\n0.735849\n\n0.868710\n\n0.759433\n\n0.870370\n\n0.757861\n\n2005\n\n0.867074\n\n0.746077\n\n0.871922\n\n0.755349\n\n0.871888\n\n0.750356\n\n2006\n\n0.842245\n\n0.731830\n\n0.843002\n\n0.740675\n\n0.843482\n\n0.724941\n\n2007\n\n0.865110\n\n0.770277\n\n0.860234\n\n0.771335\n\n0.859606\n\n0.769795\n\n2008\n\n0.858538\n\n0.783977\n\n0.859039\n\n0.790621\n\n0.857328\n\n0.792317\n\n2009\n\n0.836408\n\n0.716059\n\n0.840863\n\n0.748603\n\n0.845293\n\n0.758064\n\n2010\n\n0.817415\n\n0.696607\n\n0.818102\n\n0.727167\n\n0.820460\n\n0.731791\n\n2011\n\n0.847783\n\n0.723892\n\n0.850372\n\n0.751718\n\n0.845252\n\n0.751145\n\n2012\n\n0.896869\n\n0.820224\n\n0.898525\n\n0.822331\n\n0.902465\n\n0.823033\n\n2013\n\n0.794512\n\n0.665910\n\n0.802656\n\n0.670524\n\n0.801422\n\n0.670748\n\n2014\n\n0.833282\n\n0.737809\n\n0.836382\n\n0.755037\n\n0.836397\n\n0.737288\n\n2015\n\n0.813184\n\n0.705276\n\n0.810112\n\n0.704390\n\n0.809484\n\n0.726823\n\n2016\n\n0.815109\n\n0.730293\n\n0.815915\n\n0.746666\n\n0.812008\n\n0.755504\n\n2017\n\n0.871090\n\n0.794792\n\n0.873972\n\n0.790339\n\n0.873130\n\n0.789311\n\n2018\n\n0.827993\n\n0.686107\n\n0.823211\n\n0.695181\n\n0.824918\n\n0.707269\n\n2019\n\n0.835690\n\n0.720854\n\n0.843875\n\n0.728223\n\n0.848048\n\n0.729805\n\n2020\n\n0.855907\n\n0.752735\n\n0.850820\n\n0.751282\n\n0.851442\n\n0.754871\n\n2000-2020\n\n0.836670\n\n0.719687\n\n0.839329\n\n0.730016\n\n0.840485\n\n0.734478\n\n4.1 Model selection\n\nThe experimental protocol consists of two main parts:\n\n˗ The performance metric: a function computed on the true (ground-truth) labels and the labels predicted by the model to assess how good its performance is. In our study, we focused mainly on the test accuracy (score) defined as the ratio of correctly predicted labels.\n\n˗ The validation protocol: generally, two protocols are used to evaluate machine learning methods: K-Fold Cross Validation and Train-Test-Splitting. In our study, we used the second approach, because the first one is used on small datasets to increase the confidence of the results. The Train-Test protocol requires the definition of a Train-Test-Ratio value that indicates the percentages of samples in each of the two folds (train/test), in our study we used a Train-Test-Ratio of 70%-30%.\n\nAs stated previously, we trained and tested a specific model for each year in the studied period (2000-2020).\n\nSince we ourselves collected the dataset, we thought we wanted to experiment with different test set sizes to set a benchmark for future researchers who may work on this data.\n\nK Nearest Neighbors (KNN)\n\nThe first algorithm we experimented is the K Nearest Neighbors (KNN) approach, the Figure 7 demonstrates and summarizes the obtained results in terms of the accuracy (score) performance metric.\n\nAs we can clearly see from Table 2, the best results in terms of the test accuracy (score) are obtained in the year 2012, with an accuracy of 82% (an error rate of 18%). We can also see that the accuracy of the classifier trained on all the data (2000-2020) is around 72%. The average test score for the individual year is around 73%.\n\nThe parameters value Number of neighbors= 3, weights ='uniform', algorithm='auto'.\n\nDecision Trees\n\nThe second algorithm we experimented is the decision tree model. The scikit-learn library’s implementation of the decision tree algorithm provides a long list of hyper-parameters that can be set manually by the developer or left to their default values. The most important hyper-parameter is the max-depth parameter that limits the depth (number nested feature tests) of the tree.\n\nIn our experiment we set the max-depth hyper-parameter to a value of 21.\n\nThe Figures 8 and Table 3 illustrate and summarize the obtained results in terms of the accuracy performance metric for all years in the period 2000-2020.\n\nFigure 8. The accuracy of the decision tree classifier for different years in the period 2000-2020\n\nFigure 9. The accuracy of the random forest classifier for different years in the period 2000-2020\n\nTable 3. The accuracy of decision tree classifier\n\nYear\n\nTest Set Size: 30\n\nTest Set Size: 20\n\nTest Set Size: 10\n\nTrain Score\n\nTest Score\n\nTrain Score\n\nTrain Score\n\nTest Score\n\nTrain Score\n\n2000\n\n0.968354\n\n0.705882\n\n0.972222\n\n0.717391\n\n0.970443\n\n0.695652\n\n2001\n\n0.928716\n\n0.678062\n\n0.903064\n\n0.678062\n\n0.854292\n\n0.669515\n\n2002\n\n0.887622\n\n0.699367\n\n0.877701\n\n0.694207\n\n0.871859\n\n0.694497\n\n2003\n\n0.890069\n\n0.703418\n\n0.820134\n\n0.682692\n\n0.858364\n\n0.710256\n\n2004\n\n0.938005\n\n0.758909\n\n0.934944\n\n0.773584\n\n0.934661\n\n0.786163\n\n2005\n\n0.927217\n\n0.764621\n\n0.922761\n\n0.767475\n\n0.902172\n\n0.787446\n\n2006\n\n0.867233\n\n0.741158\n\n0.868075\n\n0.738344\n\n0.854236\n\n0.745920\n\n2007\n\n0.901516\n\n0.772727\n\n0.883614\n\n0.777868\n\n0.895181\n\n0.776326\n\n2008\n\n0.899115\n\n0.798523\n\n0.886726\n\n0.799413\n\n0.887224\n\n0.790364\n\n2009\n\n0.893718\n\n0.739652\n\n0.896134\n\n0.749844\n\n0.893734\n\n0.766749\n\n2010\n\n0.862194\n\n0.722821\n\n0.867119\n\n0.741618\n\n0.869554\n\n0.745664\n\n2011\n\n0.869840\n\n0.723892\n\n0.865086\n\n0.741023\n\n0.881943\n\n0.766412\n\n2012\n\n0.940208\n\n0.850187\n\n0.936446\n\n0.855337\n\n0.935861\n\n0.856741\n\n2013\n\n0.810468\n\n0.672719\n\n0.851183\n\n0.695711\n\n0.819433\n\n0.676190\n\n2014\n\n0.886935\n\n0.785865\n\n0.883054\n\n0.780487\n\n0.876944\n\n0.766949\n\n2015\n\n0.836161\n\n0.709810\n\n0.751662\n\n0.666048\n\n0.850171\n\n0.733003\n\n2016\n\n0.835321\n\n0.736862\n\n0.865197\n\n0.765797\n\n0.862775\n\n0.775202\n\n2017\n\n0.912347\n\n0.809181\n\n0.911742\n\n0.813463\n\n0.907845\n\n0.811921\n\n2018\n\n0.876053\n\n0.688073\n\n0.861322\n\n0.715830\n\n0.881530\n\n0.766208\n\n2019\n\n0.903804\n\n0.758012\n\n0.902770\n\n0.772125\n\n0.875464\n\n0.749303\n\n2020\n\n0.902814\n\n0.767783\n\n0.893535\n\n0.773333\n\n0.891574\n\n0.781538\n\n2000-2020\n\n0.779506\n\n0.693731\n\n0.789485\n\n0.699767\n\n0.789507\n\n0.711195\n\nTable 4. The accuracy of random forest classifier\n\nYear\n\nTest Set Size: 30\n\nTest Set Size: 20\n\nTest Set Size: 10\n\nTrain Score\n\nTest Score\n\nTrain Score\n\nTest Score\n\nTrain Score\n\nTest Score\n\n2000\n\n0.968354\n\n0.676470\n\n0.972222\n\n0.760869\n\n0.970443\n\n0.608695\n\n2001\n\n0.936863\n\n0.701804\n\n0.930862\n\n0.712250\n\n0.926512\n\n0.749287\n\n2002\n\n0.903365\n\n0.722784\n\n0.897649\n\n0.731244\n\n0.892970\n\n0.721062\n\n2003\n\n0.910406\n\n0.735470\n\n0.905097\n\n0.720512\n\n0.898831\n\n0.746153\n\n2004\n\n0.942048\n\n0.780922\n\n0.938482\n\n0.802672\n\n0.936582\n\n0.819182\n\n2005\n\n0.934964\n\n0.783166\n\n0.929896\n\n0.791012\n\n0.926906\n\n0.791726\n\n2006\n\n0.907046\n\n0.775359\n\n0.904081\n\n0.784382\n\n0.900362\n\n0.784382\n\n2007\n\n0.904550\n\n0.782798\n\n0.900765\n\n0.783993\n\n0.897086\n\n0.782040\n\n2008\n\n0.901163\n\n0.810247\n\n0.897312\n\n0.813090\n\n0.894173\n\n0.811848\n\n2009\n\n0.904364\n\n0.762831\n\n0.900481\n\n0.780881\n\n0.898150\n\n0.787841\n\n2010\n\n0.880039\n\n0.745181\n\n0.874638\n\n0.759537\n\n0.873281\n\n0.765317\n\n2011\n\n0.909150\n\n0.753948\n\n0.904070\n\n0.771581\n\n0.900628\n\n0.8\n\n2012\n\n0.940208\n\n0.858146\n\n0.936446\n\n0.869382\n\n0.935861\n\n0.863764\n\n2013\n\n0.862424\n\n0.712210\n\n0.860378\n\n0.706603\n\n0.855456\n\n0.697959\n\n2014\n\n0.886935\n\n0.773851\n\n0.883320\n\n0.779427\n\n0.879773\n\n0.802966\n\n2015\n\n0.868151\n\n0.732893\n\n0.865316\n\n0.730364\n\n0.860068\n\n0.749072\n\n2016\n\n0.870775\n\n0.771638\n\n0.868386\n\n0.772753\n\n0.866512\n\n0.780996\n\n2017\n\n0.912347\n\n0.818773\n\n0.911742\n\n0.824768\n\n0.910129\n\n0.825282\n\n2018\n\n0.896571\n\n0.725425\n\n0.890828\n\n0.747295\n\n0.886338\n\n0.754420\n\n2019\n\n0.907388\n\n0.776590\n\n0.905035\n\n0.786062\n\n0.902881\n\n0.795264\n\n2020\n\n0.903400\n\n0.786251\n\n0.898794\n\n0.794358\n\n0.898415\n\n0.797948\n\n2000-2020\n\n0.899495\n\n0.763624\n\n0.895470\n\n0.771494\n\n0.892516\n\n0.780916\n\nTable 5. The accuracy of the 2012 obtained model for different years of the study\n\nYear\n\nTest Score\n\nYear\n\nTest Score\n\n2000\n\n0.485294\n\n2011\n\n0.503821\n\n2001\n\n0.494777\n\n2012\n\n0.858146\n\n2002\n\n0.533544\n\n2013\n\n0.519292\n\n2003\n\n0.511111\n\n2014\n\n0.520141\n\n2004\n\n0.511006\n\n2015\n\n0.510717\n\n2005\n\n0.480266\n\n2016\n\n0.49459\n\n2006\n\n0.5274\n\n2017\n\n0.492634\n\n2007\n\n0.515787\n\n2018\n\n0.519004\n\n2008\n\n0.528875\n\n2019\n\n0.488621\n\n2009\n\n0.463162\n\n2020\n\n0.48119\n\n2010\n\n0.51542\n\n2000-2020\n\n0.52555\n\nFigure 10. The main page of our website (map page)\n\nWe can see that the best results in terms of the test accuracy (score) are obtained in the year 2012, with an accuracy of 85% (an error rate of 15%). We can also see that the accuracy of the classifier when trained on all the data (2000-2020) is around 69%. The average test accuracy for the individual years is around 74%.\n\nThe parameters value: max depth=21, random state=33.\n\nRandom Forest\n\nThe last machine learning approach we tested was the Random Forest Classifier. The random forest classifier shares a similar hyper-parameter ‘max-depth’ with the decision tree classifier. Additionally, the random forest classifier has another hyper-parameter called ‘number of estimators’ (n_estimators in the implementation of scikit-learn) that specifies how many Decision Trees the forest has.\n\nIn our experiment, we set the max-depth hyper-parameter to a value of 21 and the n_estimator hyper-parameter to 600.\n\nWe can infer from Table 4 that the best results in terms of the test accuracy (score) are obtained in the year 2012, with an accuracy of 87% (an error rate of 14%). We can also see that the accuracy of the classifier when trained on all the data (2000-2020) is around 76% (see Figure 9). The average test accuracy for the individual years is around 76%.\n\nThe parameters value: random_ state=42, number of jobs=-1, max depth=21, number of estimators=600, oob score=True, bootstrap=True.\n\nBecause random forest algorithm gives the best score we apply the model that we were created using the data of the year 2012 on all other years and the results presented in the following table (Table 5).\n\nWe also evaluated the models based on the processing time needed to fit each one of them. We run three experiments and record the processing time of each model then average the results of the 5 experiments. The obtained results are shown in Table 6.\n\nBy comparing the three types of models using the best performance (max accuracy) which is obtained in the year 2012 for all models, we can see that the random forest classifier outperforms all the other models with an accuracy of 86% followed by the Decision Tree classifier with an accuracy of 85% while the last place is occupied by the KNN classifier with an accuracy of 82%.\n\nTable 6. Models evaluation based on the processing time\n\nModel\n\nTime in Seconds\n\nAverage Time\n\nExperiment\n\n1\n\n2\n\n3\n\n4\n\n5\n\nKNN\n\n1.13\n\n1.00\n\n1.01\n\n1.00\n\n1.02\n\n1.03\n\nDecision Tree\n\n1.03\n\n1.11\n\n1.06\n\n1.07\n\n1.04\n\n1.06\n\nRandom Forest\n\n3.58\n\n3.63\n\n3.59\n\n3.61\n\n3.56\n\n3.59\n\nA Similar conclusion can be deduced from the comparison using the average test accuracy on the individual years from 2000-2020. The random forest classifier has the highest score: 76% followed by the Decision Tree classifier with an accuracy of 74% while the last place is occupied by the KNN classifier with an accuracy of 73%.\n\nHowever, the test score obtained when training on all the samples from all the years reveals a slightly different conclusion concerning the KNN and Decision Tree classifiers, in which KNN outperforms the Decision Tree classifier with an accuracy of 72% versus 69% for the Decision Tree classifier. Nevertheless, the Random Forest classifier still outperforms both other classifiers with an accuracy of 76%.\n\nClearly, we can conclude that the random forest classifier is the best choice for our solution; we attribute this superiority to its use of the ensemble learning paradigm, which largely reduces its degree of overfitting.\n\nThis conclusion leads us to invest more time in the future test on this model and other models of the ensemble learning family.\n\nIn order to enable different end users (authorities or civilians) to check for possible future forest fires, we deployed our selected machine learning model to a website we built.\n\nThe backend of the website was developed using the framework DJANGO available in the open-source repository of the programming language Python.\n\nThe main page displays a map of the entire world from which the user can select the geographical area he wants to check for possible forest fires. The figure below (Figure 10) shows the different parts of this page.\n\nThe different parts (numbered in the figure) are explained in the following list:\n\nTwo input fields that enable the user to manually type the geographical coordinates (latitude and longitude) of the area he wants to check for forest fires.\n\nThe submit button that allows the user to send the data (latitude and longitude) to our server to check for forest fires.\n\nA container that displays the id of the selected coordinates, the latitude, and the longitude.\n\nA map that shows the different countries and regions of the world. It allows the user to select a geographical area to check for fires in a faster and more user-friendly manner by clicking on it with the mouse or touching it on mobile and tablet devices.\n\nShows the latitude and longitude of the location where the mouse is located.\n\nThe website we built provides a simple interface to end users (local authorities or civilians) that allows them to test our models and predict potential forest fires."
    }
}
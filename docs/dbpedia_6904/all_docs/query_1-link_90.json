{
    "id": "dbpedia_6904_1",
    "rank": 90,
    "data": {
        "url": "https://arxiv.org/html/2405.17677v1",
        "read_more_link": "",
        "language": "en",
        "title": "Understanding differences in applying DETR to natural and medical images",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/5624338/figures/deformable_DETR-Page-4.drawio.png",
            "https://arxiv.org/html/extracted/5624338/figures/vanillaDETR.png",
            "https://arxiv.org/html/extracted/5624338/figures/deformableDETR.png",
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/extracted/5624338/figures/num-of-objects.png",
            "https://arxiv.org/html/extracted/5624338/figures/object-size2.png",
            "https://arxiv.org/html/extracted/5624338/figures/breast_example1.png",
            "https://arxiv.org/html/extracted/5624338/figures/coco_example_cow2.png",
            "https://arxiv.org/html/extracted/5624338/figures/coco_example_cat.png",
            "https://arxiv.org/html/extracted/5624338/figures/query_fauc.png",
            "https://arxiv.org/html/extracted/5624338/figures/query_ap10.png",
            "https://arxiv.org/html/extracted/5624338/figures/query_ap10_50.png",
            "https://arxiv.org/html/extracted/5624338/figures/query_iou_rank.png",
            "https://arxiv.org/html/extracted/5624338/figures/two-mod.png",
            "https://arxiv.org/html/extracted/5624338/figures/classification-overfit.png",
            "https://arxiv.org/html/extracted/5624338/figures/mammo-0.8144.png",
            "https://arxiv.org/html/extracted/5624338/figures/mammo-0.823.png",
            "https://arxiv.org/html/extracted/5624338/figures/mammo-0.825.png",
            "https://arxiv.org/html/extracted/5624338/figures/mammo-0.822.png",
            "https://arxiv.org/html/extracted/5624338/figures/mammo-0.014.png",
            "https://arxiv.org/html/extracted/5624338/figures/mammo-0.018.png",
            "https://arxiv.org/html/extracted/5624338/figures/mammo-0.040.png",
            "https://arxiv.org/html/extracted/5624338/figures/mammo-0.044.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "\\firstnameYanqi \\surnameXu \\emailyx2105@nyu.edu\n\n\\addrCenter for Data Science, New York Univeristy, New York, NY, USA \\AND\\nameYiqiu Shen \\emailys1001@nyu.edu\n\n\\addrNYU Grossman School of Medicine, New York, NY, USA \\AND\\nameCarlos Fernandez-Granda \\emailcfgranda@cims.nyu.edu\n\n\\addrCenter for Data Science, New York Univeristy, New York, NY, USA \\AND\\nameLaura Heacock \\emaillaura.heacock@nyulangone.org\n\n\\addrNYU Langone Health, New York, NY, USA \\AND\\nameKrzysztof J. Geras \\emailk.j.geras@nyu.edu\n\n\\addrNYU Grossman School of Medicine, New York, NY, USA\n\nAbstract\n\nTransformer-based detectors have shown success in computer vision tasks with natural images. These models, exemplified by the Deformable DETR, are optimized through complex engineering strategies tailored to the typical characteristics of natural scenes. However, medical imaging data presents unique challenges such as extremely large image sizes, fewer and smaller regions of interest, and object classes which can be differentiated only through subtle differences. This study evaluates the applicability of these transformer-based design choices when applied to a screening mammography dataset that represents these distinct medical imaging data characteristics. Our analysis reveals that common design choices from the natural image domain, such as complex encoder architectures, multi-scale feature fusion, query initialization, and iterative bounding box refinement, do not improve and sometimes even impair object detection performance in medical imaging. In contrast, simpler and shallower architectures often achieve equal or superior results. This finding suggests that the adaptation of transformer models for medical imaging data requires a reevaluation of standard practices, potentially leading to more efficient and specialized frameworks for medical diagnosis.\n\nKeywords: Machine Learning, Vision Transformers, Object Detection, Breast Cancer\n\n1 Introduction\n\nRecent advancements in computer vision have increasingly focused on leveraging transformer architectures (Vaswani et al., 2017) for image classification and object detection (Dosovitskiy et al., 2020; Liu et al., 2021; Carion et al., 2020; Touvron et al., 2021). Thanks to their inherent self-attention mechanisms, transformers effectively capture global dependencies and understand contextual relations across entire images, enabling them to acquire more expressive features. Owing to these advantages, transformer-based models have become prevalent in natural imaging analysis. Moreover, their application in medical imaging has shown promising results, indicating their potential to transform this field as well. (Chen et al., 2021; Dai et al., 2021b; Valanarasu et al., 2021; Zheng et al., 2022).\n\nObject detection plays a crucial role in medical image analysis as detection models directly identify the locations of abnormalities, which is critically important in application of AI in medical diagnosis. With the rise of transformer-based models, Detection Transformer (DETR), an end-to-end transformer-based object detection framework (Carion et al., 2020) has become popular. DETR does not require Non-Maximum Suppression (NMS) (Girshick et al., 2014), a post-processing step that reduces overlapping detections, which is non-differentiable and sensitive to predefined thresholds. The elimination of NMS enables end-to-end training and direct optimization of the objective function. DETR has established itself as one of the most competitive detection framework by consistently achieving state-of-the-art accuracy on MS-COCO datasets (Zhu et al., 2020; Zhang et al., 2022; Zong et al., 2023). Consequently, the potential of DETR draw significant research interest in improving overall performance and accelerating learning (Zhu et al., 2020; Zhang et al., 2022; Chen et al., 2022b; Wang et al., 2022; Chen et al., 2022a). This has led to the development of some highly engineered DETR-based architectures and a diverse range of design choices within the DETR family.\n\nDespite the success of DETR architectures for natural image data, their application to medical imaging poses challenges. Medical images differ significantly from natural images in several aspects:\n\n1.\n\nHigh resolution and small regions of interest Medical images are usually in very high resolutions with small, critical regions of interest (e.g., lesions, masses and calcifications) (Moawad et al., 2023; Heath et al., 2001).\n\n2.\n\nStandardized acquisition Medical images are acquired through very standardized procedures, resulting in images consisting of three elements: the body part of interest, the surrounding body tissue, and black pixels outside the body. In contrast, the backgrounds in natural images can contain anything existing in the natural world.\n\n3.\n\nSmall number of objects per image Medical images typically focus on a narrow range of abnormalities, resulting in fewer objects of interest within each image from very few classes.\n\n4.\n\nSmall and imbalanced data sets Medical imaging data sets are often smaller and exhibit a more imbalanced class distribution, as positive cases (i.e. unhealthy subjects) are usually a lot less common than negative cases (i.e. healthy subjects)(Galdran et al., 2021; Heath et al., 2001; Wang et al., 2017).\n\nMany highly engineered design choices of DETR-family models, such as multi-scale feature fusion, iterative bounding box refinement, works well for natural image data. However, when applying to medical images, where data characteristics differ markedly, we need to reexamine the effectiveness of those engineering design choices. Based on the differences between medical and natural images, we hypothesize that those design choices make DETR architecture unnecessarily complex, increasing its computation demand and memory consumption, and might not even work well with the unique properties of medical imaging data listed above. We hypothesize that a simplified model, tailored to the characteristics of medical imaging data, might be more efficient and accurate.\n\nTo verify the aforementioned hypothesis, this work investigates whether design choices in the DETR family, originally intended for natural images, remain beneficial or even necessary for medical imaging applications. We use the Deformable DETR as our baseline model and the NYU Breast Cancer Screening Dataset as a representative case study (Wu et al., 2019). This dataset contains high-resolution screening mammography with small lesions, serving as a representative dataset for the characteristics of medical imaging mentioned above. The key findings of our work are:\n\nâ€¢\n\nModels with a reduced number of encoder layers and no multi-scale feature fusion learn faster without compromising detection performance. These changes actually increased performance by 1.1%percent1.11.1\\%1.1 % in AP10,50subscriptAP1050\\mathrm{AP}_{10,50}roman_AP start_POSTSUBSCRIPT 10 , 50 end_POSTSUBSCRIPT, while accelerating the training by 50%.\n\nâ€¢\n\nAdditional techniques applied in the decoder, such as object query initialization methods and iterative bounding box refinement, do not improve detection performance on medical image datasets as they do on natural image datasets. Instead, they resulted in a 0.7% drop in AP10,50subscriptAP1050\\mathrm{AP}_{10,50}roman_AP start_POSTSUBSCRIPT 10 , 50 end_POSTSUBSCRIPT, indicating these techniques may not be suitable for medical imaging tasks.\n\nâ€¢\n\nIncreasing the number of object queries improves detection performance up to a point. However, unlike for natural image data, using an excessive number of queries (e.g., 800) leads to a 2% drop in performance due to an increase in false positives.\n\nâ€¢\n\nDesign choices aimed at localizing ROI candidates, such as object query initialization methods, iterative bounding box refinement and more object queries, do not improve performance, suggesting that correctly classifying ROI candidates is key to performance improvement.\n\n2 Background on DETRs\n\nDETR offers several advantages over commonly applied detection models such as Mask-RCNN (He et al., 2017) and YOLO (Redmon et al., 2016). Its transformer-based architecture enables learning more expressive features than CNN-based models, and end-to-end training improves optimization and performance. However, DETR suffers from slow learning. To address this issue, several studies have proposed techniques to accelerate learning and enhance overall performance (Zhu et al., 2020; Wang et al., 2022; Chen et al., 2022b; Zhang et al., 2022). Deformable DETR stands out for its competitive performance on the MS COCO dataset (Lin et al., 2014). It introduces a deformable attention module, which reduces training time by a factor of 10 and enables multi-scale feature fusion that improve detection, especially for small objects. Given its widespread adoption as a benchmark in the subsequent research works (Roh et al., 2021; Dai et al., 2021a; Zhang et al., 2022; Yao et al., 2021), we choose Deformable DETR as the baseline architecture for our experiments. This section details the essential components of DETR and Deformable DETR architectures.\n\nDETR\n\nDETR consists of a backbone, an encoder-decoder transformer, and a prediction head, as illustrated in Figure 2(a).\n\nGiven an input image xâˆˆâ„C0Ã—W0Ã—H0ğ‘¥superscriptâ„subscriptğ¶0subscriptğ‘Š0subscriptğ»0x\\in\\mathbb{R}^{C_{0}\\times W_{0}\\times H_{0}}italic_x âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT Ã— italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT Ã— italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, the backbone network fğ‘“fitalic_f produces a low-resolution activation map xs=fâ¢(x)âˆˆâ„CÃ—WÃ—Hsubscriptğ‘¥ğ‘ ğ‘“ğ‘¥superscriptâ„ğ¶ğ‘Šğ»x_{s}=f(x)\\in\\mathbb{R}^{C\\times W\\times H}italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = italic_f ( italic_x ) âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_C Ã— italic_W Ã— italic_H end_POSTSUPERSCRIPT. This map is further processed by a 1Ã—1111\\times 11 Ã— 1 convolution to collapse the channel dimension Cğ¶Citalic_C into a smaller size dğ‘‘ditalic_d, resulting in image tokens xf=convâ¢(xs)âˆˆâ„Wâ¢HÃ—dsubscriptğ‘¥ğ‘“convsubscriptğ‘¥ğ‘ superscriptâ„ğ‘Šğ»ğ‘‘x_{f}=\\mathrm{conv}(x_{s})\\in\\mathbb{R}^{WH\\times d}italic_x start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT = roman_conv ( italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_W italic_H Ã— italic_d end_POSTSUPERSCRIPT. To preserve spatial information in the original image, each token is paired with a positional encoding, denoted by xpâˆˆâ„Wâ¢HÃ—dsubscriptğ‘¥ğ‘superscriptâ„ğ‘Šğ»ğ‘‘x_{p}\\in\\mathbb{R}^{WH\\times d}italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_W italic_H Ã— italic_d end_POSTSUPERSCRIPT. The encoder is a standard attention-based transformer where each layer consists of a multi-head self-attention module (MHSA) followed by a feedforward network (FFN). For an in-depth formalization of MHSA, refer to Appendix A.1. Typically, the DETR encoder consists of 6 layers. The encoder preserves the dimension of the input, producing xeâ¢nâ¢câˆˆâ„Wâ¢HÃ—dsubscriptğ‘¥ğ‘’ğ‘›ğ‘superscriptâ„ğ‘Šğ»ğ‘‘x_{enc}\\in\\mathbb{R}^{WH\\times d}italic_x start_POSTSUBSCRIPT italic_e italic_n italic_c end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_W italic_H Ã— italic_d end_POSTSUPERSCRIPT.\n\nThe decoder receives two inputs, the encoded features xeâ¢nâ¢csubscriptğ‘¥ğ‘’ğ‘›ğ‘x_{enc}italic_x start_POSTSUBSCRIPT italic_e italic_n italic_c end_POSTSUBSCRIPT and Nğ‘Nitalic_N object queries qâˆˆâ„NÃ—dğ‘superscriptâ„ğ‘ğ‘‘q\\in\\mathbb{R}^{N\\times d}italic_q âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_N Ã— italic_d end_POSTSUPERSCRIPT. Object queries play a central role in DETR architecture. They are learnable embeddings that work as placeholders for the potential objects in an image. Each of them attends to the specific regions of the image and is individually decoded into a bounding box prediction. Each object query is the sum of two learnable embedding: content embeddings qcâˆˆâ„NÃ—dsubscriptğ‘ğ‘superscriptâ„ğ‘ğ‘‘q_{c}\\in\\mathbb{R}^{N\\times d}italic_q start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_N Ã— italic_d end_POSTSUPERSCRIPT, initialized as zero vectors, and the positional embeddings qpâˆˆâ„NÃ—dsubscriptğ‘ğ‘superscriptâ„ğ‘ğ‘‘q_{p}\\in\\mathbb{R}^{N\\times d}italic_q start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_N Ã— italic_d end_POSTSUPERSCRIPT, indicating each queryâ€™s position. More methods for initializing object queries are discussed in Section 3. Decoder layers consists of a MHSA, enabling inter-query learning, and multi-head (MH) cross-attention to integrate encoder features, and a FFN. The formalization of MH cross-attention is detailed in Appendix A.2.\n\nAfter the decoder, each object query is independently decoded into bounding box coordinates and class scores through a three-layer FFN and a linear layer respectively.\n\nDeformable DETR\n\nDeformable DETR improves upon DETR by introducing a deformable attention module, which accelerates training and enhances the detection of small objects. The architecture of Deformable DETR is illustrated in Figure 2(b).\n\nUnlike the standard attention mechanism that calculates attention scores between all query-key pairs, resulting in (Wâ¢H)2superscriptğ‘Šğ»2(WH)^{2}( italic_W italic_H ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT pairs for a feature map of size WÃ—Hğ‘Šğ»W\\times Hitalic_W Ã— italic_H, deformable attention selectively computes attention scores on a subset of k<<Wâ¢Hmuch-less-thanğ‘˜ğ‘Šğ»k<<WHitalic_k < < italic_W italic_H keys for each query. The subset is selected through a learnable key sampling function, allowing the model to focus on the most informative regions for each query. For a detailed formalization of the deformable attention module, refer to Appendix B.1.\n\nFor dense prediction tasks such as object detection, incorporating higher-resolution feature maps can substantially improve detection performance, especially for smaller objects (He et al., 2017). However, the complexity of the standard attention mechanism is quadratic with respect to the number of tokens, making it infeasible for multiple scales of feature maps. The deformable attention mechanism enables effective multi-scale feature fusion. Specifically, the encoder receives the output feature maps x1,x2,x3subscriptğ‘¥1subscriptğ‘¥2subscriptğ‘¥3x_{1},x_{2},x_{3}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT from the backboneâ€™s last three layers, and a convolutional layer generates the lowest resolution feature map x4subscriptğ‘¥4x_{4}italic_x start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT. All four feature maps undergo a 1Ã—1111\\times 11 Ã— 1 convolution and then are reshaped into a sequence of feature vectors of dimension dğ‘‘ditalic_d, denoted by xfâˆˆâ„MÃ—dsubscriptğ‘¥ğ‘“superscriptâ„ğ‘€ğ‘‘x_{f}\\in\\mathbb{R}^{M\\times d}italic_x start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_M Ã— italic_d end_POSTSUPERSCRIPT. Each token is associated with a positional embedding, as well as a layer embedding to identify feature map level. Section 3 explores the benefits of multi-feature fusion for medical imaging datasets.\n\nMoreover, Deformable DETR introduces reference points in the deformable attention module. In the encoder, each query qğ‘qitalic_q is associated with a 2D reference point pq=[x,y]subscriptğ‘ğ‘ğ‘¥ğ‘¦p_{q}=[x,y]italic_p start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = [ italic_x , italic_y ], denoting its location on the feature map. The key sampling function generates kğ‘˜kitalic_k sampling offsets with respect to the reference point, and thus determines the kğ‘˜kitalic_k keys for the query. Similarly in the decoder, the reference point of each object query qğ‘qitalic_q is defined by a linear projection of its positional embedding qpsubscriptğ‘ğ‘q_{p}italic_q start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT. In this way, each object query can be mapped to a position on the feature map. This approach allows object queries to focus on specific regions, significantly accelerating learning Zhu et al. (2020).\n\nDETR in Medical Imaging\n\nDETR-based architectures have been widely applied to various medical imaging tasks, often with architectural tweaks to improve overall performance. For example, Mathai et al. (2022) leveraged a bounding box fusion technique on DETR to reduce false-positive rate in lymph nodes detection. MyopiaDETR (Li et al., 2023) utilizes a Feature Pyramid Network to improve detection of small objects in lesion detection of pathological myopia. COTR (Shen et al., 2021) embeds convolutional layers into DETR encoders to accelerate learning in polyp detection. Although these works achieved good performances, our experiments indicates that, contrary to the common understanding, simplifying the DETR architecture can improve accuracy and accelerate training. One work we identified that also points in this direction, Cell-DETR (Prangemeier et al., 2020), also reduces the number of parameters by ten times, achieving faster inference speeds while maintaining performance on par with state-of-the-art baselines. Finally, Garrucho et al. (2023) applied out-of-the-box Deformable DETR on mammography for mass detection. However, their focus is the effect of a data augmentation method on its detection performance. Despite these advances, a systematic exploration of the effectiveness and relevance of foundational DETR design choices remains under-explored.\n\n3 Methods\n\n3.1 Design Choices\n\nIn this section, we outline key design choices of Deformable DETR relevant to the unique characteristics of medical images: input resolution, the number of encoder layers, multi-scale feature fusion, the number of object queries, and two techniques enhancing the decoding process, query initialization and iterative bounding box refinement (IBBR). We investigate if incorporating these elements into model design yields improvements in detection performance on medical imaging data to the same degree they do on natural image datasets.\n\nInput resolutions\n\nDownsampling input images is a standard practice in detection models for computational efficiency and to satisfy memory constraints. Natural images can be significantly downsized without losing important features such as edges, shapes, and textures necessary for accurate predictions. Data from many medical imaging modalities, such as X-ray images, CT scans, and whole slide images, are at least an order of magnitude larger. These high-resolution medical images contain fine-grained details such as small lesions or slight changes in tissue density that are crucial for accurate diagnosis (Sabottke and Spieler, 2020; Thambawita et al., 2021). On the other hand, processing high-resolution medical images without any downsampling is often infeasible due to the high computational requirements. Hence, we conduct experiments on medical images with resolution ratios ranging from 25% to 100% to examine the optimal input resolution that balances model accuracy with computational efficiency and memory usage.\n\nEncoder complexity\n\nMedical image datasets have three distinct characteristics. Firstly, they are typically smaller compared to natural image datasets due to the limited number of patients with specific conditions. Secondly, medical images within one dataset are usually very homogeneous, depicting single body part such as the brain, breast, or chest, with uniform textures and grayscale. Furthermore, while natural images contain hundreds or thousands of objects classes, medical image datasets usually have much fewer classes. For example, NIH Chest X-ray (14 disease classes) (Wang et al., 2017), DDSM (Heath et al., 2001) (2 classes), BraTs (4 classes) (Moawad et al., 2023). This means that, overall, there is much less variation in the data that the network has to model. Given the broadly accepted idea that model complexity should match task complexity (Geman et al., 1992), we suspect that simpler, shallower architectures might be better suited to these medical imaging datasets to mitigate overfitting and enhance training efficiency. Additionally, objects in medical images tend to have more uniform sizes compared to those in natural images. For instance, the standard deviation of standardized object sizes in the MS COCO dataset is 0.16, whereas it is only 0.025 in the NYU Breast Cancer Screening Dataset. This observation leads us to question the necessity of employing multi-scale feature fusion for medical images, which is more beneficial for detecting objects of varying sizes. To explore these hypotheses, we experimented with modifications to the encoder of Deformable DETR, including reducing the number of encoder layers and utilizing fewer scales of feature maps from the backbone.\n\nNumber of object queries\n\nIn DETR, each object query is individually decoded into a bounding box prediction. Thus, the total number of object queries determines the number of bounding boxes predicted per image (Carion et al., 2020). Most DETR models are optimized for natural image datasets such as MS COCO, where a single image can contain up to 100100100100 objects. Consequently, the number of object queries is usually set to 300300300300 in DETR models. However, a single medical image typically contains fewer than 10101010 objects, with the majority containing only 1111 or no objects at all. For this reason, the hyperparameters suggested in DETR research might not be suitable for medical images, making it crucial to adjust this number for medical image datasets.\n\nDecoding techniques\n\nObject queries initialization and iterative bounding box refinement (IBBR) are two techniques employed by many DETR models to facilitate the decoding process of the object queries (Zhu et al., 2020; Zhang et al., 2022; Yao et al., 2021). These methods have proven effective in boosting detection performance on natural image datasets, increasing AP by 2.4 on the MS COCO dataset (Zhu et al., 2020). This study aims to evaluate their suitability for medical data. We tested three initialization strategies for the positional and content embeddings of object queries, as characterized by Zhang et al. (2022).\n\nâ€¢\n\nStatic queries: Both positional and content embeddings are initialized as random learnable embeddings. This generic starting point requires the model to learn from scratch where objects are likely located and what features represent those objects. While offering maximum flexibility, this method may lead to slower learning as the model has to discover spatial and content relationships without prior knowledge. The standard Deformable DETR uses this approach.\n\nâ€¢\n\nPure query selection: Both content and positional embeddings are initialized with selected encoder features. To choose Kğ¾Kitalic_K encoder features that most likely contain objects, the model either uses a regional proposal network (Yao et al., 2021; Chen et al., 2022b) or applies the prediction head to encoder features (Zhu et al., 2020). Our experiments employ the latter approach. This method leverages existing knowledge encoded in the encoder features, guiding the model more directly toward accurate object detection and significantly speeding up the learning process.\n\nâ€¢\n\nMixed query selection: This method combines the previous two methods. By using pure query selection for positional embeddings and static embeddings for content, the model is informed about the likely positions of objects through the spatial priors while retaining flexibility in learning content representations from scratch. DINO (Zhang et al., 2022) suggests that mixed query selection methods provide the best model performance among all three methods.\n\nIBBR, first introduced in Deformable DETR, iteratively updates the reference points of object queries towards the objects of interest in each image. Reference points guide the modelâ€™s deformable attention mechanism, indicating which regions to search for objects. These reference points are distributed randomly across the image, ensuring a broad coverage without any prior knowledge about where objects might be located. With IBBR, these reference points can move progressively towards the objects through each decoder layer, providing more accurate signals for the attention. This technique has been applied extensively in DETR family (Zhu et al., 2020; Chen et al., 2022b; Wang et al., 2022; Liu et al., 2022) and has shown to effectively speed up training and improve detection performance. Detailed formalization of this method is in Appendix C.\n\n3.2 Data and task\n\nWe conduct experiments on the NYU Breast Cancer Screening Dataset (Wu et al., 2019), representative for the characteristics of medical image datasets. This dataset contains 229,426229426229,426229 , 426 digital screening mammography exams from 141,472141472141,472141 , 472 patients screened at NYU Langone Health. Each exam includes a minimum of four images, each with a resolution of 2944Ã—1920294419202944\\times 19202944 Ã— 1920, covering two standard screening views: craniocaudal (CC) and mediolateral oblique (MLO), for both the left and right breasts. An example of a mammography exam is shown in Figure 3. The dataset is annotated with breast-level cancer labels indicating biopsy-confirmed benign or malignant findings. Moreover, the dataset also provides bounding box annotations, and class labels (benign or malignant) of each visible positive findings. The entire dataset contains 985985985985 breasts with malignant findings and 5,55655565,5565 , 556 breasts with benign findings. The dataset is divided into training (82%percent8282\\%82 %), validation (5%percent55\\%5 %) and test sets (13%percent1313\\%13 %) ensuring a proportional distribution of both benign and malignant cases across the subsets.\n\n3.3 Evaluation Metrics\n\nIn this study, we focus on evaluating the modelsâ€™ ability to detect malignant lesions. We use Average Precision (AP) (Everingham et al., 2010) and the Free-Response Receiver Operating Characteristic curve area (FAUC) (Bandos et al., 2009), which is a frequently used metric in medical image analysis (Yu et al., 2022; Wang et al., 2018; Petrick et al., 2013). Specifically, we focus on FAUC at the rate of 1 false positive per image, referred to as FAUC1superscriptFAUC1\\text{FAUC}^{1}FAUC start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT, in line with the approach described by Bandos et al. (2009). Following prior works (Jailin et al., 2023; Kolchev et al., 2022; Konz et al., 2023), we define a positive bounding box to have at least 10% 50% Intersection over Union (IoU) with a ground truth box. These thresholds are deemed more appropriate for accurately detecting small-sized objects, such as cancerous lesions. FAUC1superscriptFAUC1\\mathrm{FAUC}^{1}roman_FAUC start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT at 0.1 Iou threhold is denoted as FAUC101subscriptsuperscriptFAUC110\\text{FAUC}^{1}_{10}FAUC start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT. Following the notation of COCO evaluation metrics (Lin et al., 2014), we denote AP at 0.1 IoU threshold as AP10subscriptAP10\\text{AP}_{10}AP start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT. Additionally, we report the average AP across IoU thresholds ranging from 0.1 to 0.5, in steps size of 0.05, denoted as AP10,50subscriptAP1050\\text{AP}_{10,50}AP start_POSTSUBSCRIPT 10 , 50 end_POSTSUBSCRIPT.\n\nTo clearly explain how well our models detect objects, we differentiate between â€œlocalizationâ€ and â€œclassification.â€\n\nâ€¢\n\nLocalization refers to the task of accurately drawing a bounding box around each ground-truth object. To be consistent with the definition of FAUC and AP, an object is considered successfully localized if the model produces a bounding box overlapping the ground truth box by more than 10% IoU. To quantify localization accuracy, we compute the percentage of ground-truth objects successfully detected by the model. Assume there are mğ‘šmitalic_m ground-truth objects Gisubscriptğºğ‘–G_{i}italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT where i=1,â€¦,mğ‘–1â€¦ğ‘ši=1,\\ldots,mitalic_i = 1 , â€¦ , italic_m and pğ‘pitalic_p predicted boxes Pjsubscriptğ‘ƒğ‘—P_{j}italic_P start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT where j=1,â€¦,pğ‘—1â€¦ğ‘j=1,\\ldots,pitalic_j = 1 , â€¦ , italic_p in an image. The maximum IoU for a ground-truth bound box Gisubscriptğºğ‘–G_{i}italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT among all the predicted bounding boxes Pjsubscriptğ‘ƒğ‘—P_{j}italic_P start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is maxjâ¡(IoUâ¢(Pj,Gi))subscriptğ‘—IoUsubscriptğ‘ƒğ‘—subscriptğºğ‘–\\max_{j}(\\mathrm{IoU}(P_{j},G_{i}))roman_max start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( roman_IoU ( italic_P start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ). Localization accuracy Lğ¿Litalic_L is then expressed as\n\nL=1mâ¢âˆ‘i=1mğŸ™maxjâ¡(IoUâ¢(Pj,Gi))â‰¥0.1.ğ¿1ğ‘šsubscriptsuperscriptğ‘šğ‘–1subscript1subscriptğ‘—IoUsubscriptğ‘ƒğ‘—subscriptğºğ‘–0.1L=\\frac{1}{m}\\sum^{m}_{i=1}\\mathds{1}_{\\max_{j}\\left({\\mathrm{IoU}\\left(P_{j},% G_{i}\\right)}\\right)\\geq 0.1}.italic_L = divide start_ARG 1 end_ARG start_ARG italic_m end_ARG âˆ‘ start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT blackboard_1 start_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( roman_IoU ( italic_P start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) â‰¥ 0.1 end_POSTSUBSCRIPT . (1)\n\nâ€¢\n\nClassification involves associating the object inside each predicted box with the correct class. We consider modelsâ€™ classification accuracy using the percentage of successfully localized objects among the predicted bounding boxes with the top 10 highest predicted scores in each image. Among all the predicted bounding boxes Pjsubscriptğ‘ƒğ‘—P_{j}italic_P start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT in an image, let Sğ‘†Sitalic_S be the subset of indices of the top 10 predicted bounding boxes in an image. Localization performance considering classification is expressed as\n\nLtop10=1mâ¢âˆ‘i=1mğŸ™maxjâˆˆSâ¡(IoUâ¢(Pj,Gi))â‰¥0.1.subscriptğ¿top101ğ‘šsubscriptsuperscriptğ‘šğ‘–1subscript1subscriptğ‘—ğ‘†IoUsubscriptğ‘ƒğ‘—subscriptğºğ‘–0.1L_{\\mathrm{top10}}=\\frac{1}{m}\\sum^{m}_{i=1}\\mathds{1}_{\\max_{j\\in S}({\\mathrm% {IoU}(P_{j},G_{i})})\\geq 0.1}.italic_L start_POSTSUBSCRIPT top10 end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_m end_ARG âˆ‘ start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT blackboard_1 start_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_j âˆˆ italic_S end_POSTSUBSCRIPT ( roman_IoU ( italic_P start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) â‰¥ 0.1 end_POSTSUBSCRIPT . (2)\n\n3.4 Experimental Setup\n\nOur baseline model is a Deformable DETR in its default setting, using a Swin-T Transformer (Liu et al., 2021) as the backbone. The backbone is pretrained with a breast cancer classification task on the same dataset (see Appendix D for details). All models are optimized using the AdamW optimizer (Loshchilov and Hutter, 2017) with a step scheduler for 60606060 epochs. The step scheduler reduces the learning rate by a factor of 0.10.10.10.1 for the last 20202020 epochs. We tuned the remaining hyperparameters using random search as detailed in Appendix E. For each experiment, we trained 5555 models with different random seeds and we reported their performance mean and standard deviation.\n\n4 Results\n\nOur experiments with the five design choices, including input resolutions, encoder layer complexity, multi-scale feature fusion, number of object queries, and two decoding techniques, all demonstrate that modifications of the standard Deformable DETR do not align well with the unique characteristics of medical imaging datasets. This misalignment results in unnecessary computational overhead and sub-optimal performance.\n\nInput Resolution\n\nOur experiments reveal a positive correlation between input resolution and detection performance up to a certain threshold (Table 1). Specifically, increasing the resolution ratio from a quarter size (0.250.250.250.25) to half size (0.50.50.50.5) significantly improves the performance, with a 9.8%percent9.89.8\\%9.8 % increase in FAUC101subscriptsuperscriptFAUC110\\text{FAUC}^{1}_{10}FAUC start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT, 8.6%percent8.68.6\\%8.6 % in AP10subscriptAP10\\text{AP}_{10}AP start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT and 6.4%percent6.46.4\\%6.4 % in AP10,50subscriptAP1050\\text{AP}_{10,50}AP start_POSTSUBSCRIPT 10 , 50 end_POSTSUBSCRIPT. Further increasing the resolution to 0.750.750.750.75 continues to boost performance, though with a smaller margin. However, using full-resolution images results in a performance drop across all metrics. In high-resolution images, objects of interest can have defining features spread across a large area, requiring the model to integrate clues from a broader area. The deformable attention mechanism only focuses on a selective set of keys centered around the reference points, which may miss necessary information in high resolution images. This phenomenon of diminished performance at very high resolutions has been observed by prior studies (Sabottke and Spieler, 2020; Thambawita et al., 2021). Richter et al. (2021) also challenged the notion that higher resolution always equates to better performance and pointed out that different CNN networks have a preferred input size.\n\nIt is also noteworthy that the increase in resolution from quarter to full resolution comes at the cost of Ã—15absent15\\times 15Ã— 15 increase in GFLOPs. To balance the training efficiency and detection performance, subsequent experiments are conducted for half-resolution images.\n\nEncoder complexity: number of encoder layers\n\nIn this set of experiments, we examined the impact of the number of encoder layers and the necessity of muilti-scale feature fusion in the Deformable DETR architecture.\n\nAs the backbone critically affects feature extraction, to reach more generalizable conclusions, we conducted this experiment with two distinct backbones, ResNet50 and Swin-T. For both backbones, models with merely 1111 and 3333 encoder layers matched the performance of models with 6666 encoder layers across all three detection metrics (Table 2). Impressively, this reduction in the number of layers resulted in up to 40%percent4040\\%40 % decrease in GFLOPs. Furthermore, when using Swin-T as the backbone, the encoder-free models achieve nearly the same detection performance as their 6-layer counterparts, with less than a 1%percent11\\%1 % drop across all three detection metrics, while cutting the GFLOPs almost by half.\n\nThis observation aligns with the recent development of the encoder-free D2â¢ETRsuperscriptD2ETR\\text{D}^{2}\\text{ETR}D start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ETR (Lin et al., 2022), which outperforms the standard DETR model on the MS COCO dataset (Lin et al., 2014). Such findings challenge the traditional view that encoders are essential for feature transformation and multi-level feature integration within DETR models. Our results suggest that it is possible to create effective DETR architectures without encoders, avoiding significant sacrifices in performance. Additionally, a strong backbone can further reduce DETRâ€™s reliance on the encoders, further supporting the potential for more streamlined designs.\n\nEncoder complexity: Multi-scale feature fusion\n\nStandard Deformable DETR applies the attention mechanism to four scales of feature maps in the encoder: three from the last three layers of the backbone and fourth from a convolution layer applied to final output of the backbone, as illustrated in Figure 2(b). Previous findings highlight that using multi-scale feature fusion improves detection performance by 1.3 on the MS COCO dataset and also on other datasets (He et al., 2017; Zhou et al., 2021; Zeng et al., 2022), our results in Table 3 indicate that comparable performance can be achieved using only the output of the last layer of backbone. This suggests that multi-scale feature fusion may not be crucial for detecting abnormalities in medical images.\n\nThe characteristics of our dataset likely explain this finding. Firstly, our dataset predominantly consists of images containing a single object. Secondly, the sizes of these objects are relatively uniform. These characteristics contrast with natural image datasets such as MS COCO, where object sizes vary widely due to perspective, distance from the camera, and the inherent size differences between object classes, as shown in Figure 4(c). Multi-scale feature fusion allows models to be sensitive to this size variability by utilizing feature maps at different scales, each of which captures details at different resolutions. However, our dataset is more homogeneous, with images containing single objects of similar sizes (Figure 4(b). For such data, the benefits of multi-scale feature fusion are less pronounced. Figure 4(a) contrasts the object size distribution between the two datasets. In our dataset, the majority of images contain only a single object and most objects take only approximately 1%percent11\\%1 % of the entire image, whereas MS COCO dataset exhibits a broader variation in both the sizes of objects and the number of objects per image. Consequently, in a homogeneous dataset, the additional complexity of multi-scale feature fusion may not translate into better performance.\n\nNumber of object queries\n\nFigure 5(a)-(c) depicts the impact of increasing the number of object queries from 5 to 500 on three detection metrics. Increasing the number of object queries from 5555 to 100100100100 consistently improves detection performance. However, exceeding this number does not bring additional benefits and may even slightly hurt performance. The benefit of more object queries for object localization is evident in Figure 5(d), which shows improved localization performance Lğ¿Litalic_L (as defined in Equation 1) as the number of queries increases. However, better localization does not necessarily equate to improved overall detection performance, as indicated by Ltop10subscriptğ¿top10L_{\\mathrm{top10}}italic_L start_POSTSUBSCRIPT top10 end_POSTSUBSCRIPT (cf. Equation 2), the proportion of correctly localized objects that are also among top 10101010 classification scores. Although the model correctly identifies more ground truth objects, it assigns them low classification scores, resulting in fewer objects ranked among the top 10 bounding boxes.\n\nWe hypothesize that having more object queries increases the chances of localizing false positives. More object queries expand modelâ€™s search space, making it more sensitive to variations in subtle patterns or textures that may resemble the characteristics of positive objects. These false positives cause true positives to be ranked lower. In summary, while more object queries improves the chances of detecting an object, they also introduce the risk of misidentifying irrelevant entities as objects of interest, thus adversely impacting the modelâ€™s overall performance. This phenomenon can be more severe in medical imaging datasets, where there are significantly fewer objects (usually a single object) within an image, increasing the likelihood of false positives overwhelming the true positives.\n\nDecoding Techniques\n\nWe implemented two techniques commonly adopted in DETR family using the simplified model with 3333 encoder layers and 1111 feature map: query initialization method and iterative bounding box refinement (IBBF). Surprisingly, empirical results indicate that neither technique significantly improved detection performance across all three metrics, as detailed in Table 4. Following previous analysis, we examine the modelâ€™s localization and classification performance separately by evaluating on Lğ¿Litalic_L and Ltop10subscriptğ¿top10L_{\\mathrm{top10}}italic_L start_POSTSUBSCRIPT top10 end_POSTSUBSCRIPT. We found that while these techniques enhanced localization performance, they adversely affected classification performance, as depicted in Figure 6.\n\nWe visualize training and validation losses for localization and classification separately in Figure 7. Localization losses include the IoU loss and bounding boxes regression loss, while classification loss is the BCE loss on all prediction boxes. The figure reveals that the model equipped with either decoding techniques tends to overfit the training data more than the model without them, particularly in classification loss, as illustrated in Figure 7. This overfitting is likely the reason behind the observed decline in classification performance. We hypothesize that this is due to the significantly smaller number of positive objects in our dataset.\n\nCases Visualization\n\nFinally, to better understand which test examples are easy and hard for our models, we visualized a few exams along with their classification scores. Figures 8 and 9 showcase images where the model assigned cancerous objects high malignant scores (scores â‰¥0.8absent0.8\\geq 0.8â‰¥ 0.8) and low scores (scores â‰¤0.1absent0.1\\leq 0.1â‰¤ 0.1), respectively. We observed that the model correctly localizes abnormal objects in all images. However, it tends to assign high scores to high-density masses featuring non-circumscribed, irregular, or indistinct borders, which are typically indicative of malignancy to the human eye. Conversely, the model usually assigns low scores to low-density masses with circumscribed borders, which can easily be confused with benign cases (Lee et al., 2018).\n\n5 Conclusions\n\nIn this study, we investigated the impact of common design choices in Deformable DETR on medical images object detection, focusing on the NYU Breast Cancer Screening Dataset. We found out that all the design choices we experimented with need to be reconsidered and a simpler architectures typically lead to better performance on medical dataset. Our results further conclude that while the model effectively identifies abnormal tissues, it struggles to distinguish benign from malignant lesions and assigning high classification scores to the latter. This issue partly stems from the relatively small proportion of positive cases, as only about 10%percent1010\\%10 % of the images contain positive objects. Partly because visual features typically used for classification in natural image datasets, such as object size, shape, and color, are less discriminative in medical images. For example malignant lesions in our images often lack distinct borders, have uniform sizes, and are presented in grayscale.\n\nFuture research should prioritize developing techniques for extracting subtle features relevant to medical imaging, including texture variations, density changes, irregular borders, and microcalcifications. Additionally, exploring architectural designs capable of efficiently processing full-resolution images is crucial, enabling models to focus on rich information in the pertinent regions while disregarding extensive background areas. Additionally, implementing regularization methods to mitigate overfitting in classification tasks, especially in datasets with limited positive cases, is vital for improving model performance.\n\n6 Limitations and future work\n\nOur study had several limitations. First, while we focused on Deformable DETR, comparing its performance with other transformer-based object detection architectures, such as DINO, would provide a broader understanding of the strengths and weaknesses of different models. Future studies should explore multiple architectures to reach more generalizable conclusions. Additionally, due to the amount of the resources necessary to conduct the experiments with a large medical dataset, our investigation was confined to a single dataset. To ensure the robustness of our findings, it is important to validate these results across various medical imaging datasets. Considering the vast diversity within medical image datasets, including modalities such as brain MRI or ultrasound images, future research should encompass these distinct characteristics to provide a comprehensive evaluation.\n\nAcknowledgments\n\nThis work was supported in part by a grants from the National Institutes of Health (P41EB017183), the National Science Foundation (1922658), the Gordon and Betty Moore Foundation (9683), and the Mary Kay Ash Foundation (05-22). We also gratefully acknowledge the support of Nvidia Corporation with the donation of some of the GPUs used in this research.\n\nEthical Standards\n\nThe work follows appropriate ethical standards in conducting research and writing the manuscript, following all applicable laws and regulations regarding treatment of animals or human subjects.\n\nConflicts of Interest\n\nThe authors do not declare any conflicts of interest.\n\nData availability\n\nOur internal (NYU Langone Health) dataset is not publicly available due to internal data transfer policies. We released a data report on data curation and preprocessing to encourage reproducibility. The data report can be accessed at this link.\n\nReferences\n\nBandos et al. (2009) Andriy I Bandos, Howard E Rockette, Tao Song, and David Gur. Area under the free-response roc curve (froc) and a related summary index. Biometrics, 65(1):247â€“256, 2009.\n\nCarion et al. (2020) Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part I 16, pages 213â€“229. Springer, 2020.\n\nChen et al. (2021) Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306, 2021.\n\nChen et al. (2022a) Qiang Chen, Xiaokang Chen, Gang Zeng, and Jingdong Wang. Group detr: Fast training convergence with decoupled one-to-many label assignment. arXiv preprint arXiv:2207.13085, 2022a.\n\nChen et al. (2022b) Xiaokang Chen, Fangyun Wei, Gang Zeng, and Jingdong Wang. Conditional detr v2: Efficient detection transformer with box queries. arXiv preprint arXiv:2207.08914, 2022b.\n\nDai et al. (2021a) Xiyang Dai, Yinpeng Chen, Jianwei Yang, Pengchuan Zhang, Lu Yuan, and Lei Zhang. Dynamic detr: End-to-end object detection with dynamic attention. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2988â€“2997, 2021a.\n\nDai et al. (2021b) Yin Dai, Yifan Gao, and Fayu Liu. Transmed: Transformers advance multi-modal medical image classification. Diagnostics, 11(8):1384, 2021b.\n\nDosovitskiy et al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nEveringham et al. (2010) Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88:303â€“338, 2010.\n\nGaldran et al. (2021) Adrian Galdran, Gustavo Carneiro, and Miguel A GonzÃ¡lez Ballester. Balanced-mixup for highly imbalanced medical image classification. In Medical Image Computing and Computer Assisted Interventionâ€“MICCAI 2021: 24th International Conference, Strasbourg, France, September 27â€“October 1, 2021, Proceedings, Part V 24, pages 323â€“333. Springer, 2021.\n\nGarrucho et al. (2023) Lidia Garrucho, Kaisar Kushibar, Richard Osuala, Oliver Diaz, Alessandro Catanese, Javier del Riego, Maciej Bobowicz, Fredrik Strand, Laura Igual, and Karim Lekadir. High-resolution synthesis of high-density breast mammograms: Application to improved fairness in deep learning based mass detection. Frontiers in Oncology, 12:1044496, 01 2023. .\n\nGeman et al. (1992) Stuart Geman, Elie Bienenstock, and RenÃ© Doursat. Neural networks and the bias/variance dilemma. Neural Computation, 4(1):1â€“58, 1992.\n\nGirshick et al. (2014) Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 580â€“587, 2014.\n\nHe et al. (2017) Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 2961â€“2969, 2017.\n\nHeath et al. (2001) Michael Heath, Kevin Bowyer, Daniel Kopans, Richard Moore, and W. Philip Kegelmeyer. The digital database for screening mammography. In M.J. Yaffe, editor, Proceedings of the Fifth International Workshop on Digital Mammography, pages 212â€“218. Medical Physics Publishing, 2001. ISBN 1-930524-00-5.\n\nJailin et al. (2023) ClÃ©ment Jailin, RÄƒzvan Iordache, Pablo Milioni de Carvalho, Salwa Ahmed, Engy Sattar, Amr Moustafa, Mohammed Gomaa, Rashaa Kamal, and Laurence Vancamberg. Ai-based cancer detection model for contrast-enhanced mammography. Bioengineering, 10:974, 08 2023. .\n\nKolchev et al. (2022) Alexey Kolchev, D. Pasynkov, Ivan Egoshin, Ivan Kliouchkin, Olga Pasynkova, and Dmitrii Tumakov. Yolov4-based cnn model versus nested contours algorithm in the suspicious lesion detection on the mammography image: A direct comparison in the real clinical settings. Journal of Imaging, 8:88, 03 2022. .\n\nKonz et al. (2023) Nicholas Konz, Mateusz Buda, Hanxue Gu, Ashirbani Saha, Jichen Yang, Jakub Chledowski, Jungkyu Park, Jan Witowski, Krzysztof J. Geras, Yoel Shoshan, Flora Gilboa-Solomon, Daniel Khapun, Vadim Ratner, Ella Barkan, Michal Ozery-Flato, Robert MartÃ­, Akinyinka Omigbodun, Chrysostomos Marasinou, Noor Nakhaei, William Hsu, Pranjal Sahu, Md Belayat Hossain, Juhun Lee, Carlos Santos, Artur Przelaskowski, Jayashree Kalpathy-Cramer, Benjamin Bearce, Kenny Cha, Keyvan Farahani, Nicholas Petrick, Lubomir Hadjiiski, Karen Drukker, III Armato, Samuel G., and Maciej A. Mazurowski. A Competition, Benchmark, Code, and Data for Using Artificial Intelligence to Detect Lesions in Digital Breast Tomosynthesis. JAMA Network Open, 6(2):e230524â€“e230524, 02 2023. ISSN 2574-3805. . URL https://doi.org/10.1001/jamanetworkopen.2023.0524.\n\nKuhn (1955) Harold W Kuhn. The hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2(1-2):83â€“97, 1955.\n\nLee et al. (2018) Christoph I. Lee, Constance D. Lehman, Lawrence W. Bassett, and Lonie R. Salkowski. 204Mass with Indistinct Margins. In Breast Imaging. Oxford University Press, 01 2018. ISBN 9780190270261. . URL https://doi.org/10.1093/med/9780190270261.003.0024.\n\nLi et al. (2023) Manyu Li, Shichang Liu, Zihan Wang, Xin Li, Zezhong Yan, Renping Zhu, and Zhijiang Wan. Myopiadetr: End-to-end pathological myopia detection based on transformer using 2d fundus images. Frontiers in Neuroscience, 17:1130609, 2023.\n\nLin et al. (2022) Junyu Lin, Xiaofeng Mao, Yuefeng Chen, Lei Xu, Yuan He, and Hui Xue. D^ 2etr: Decoder-only detr with computationally efficient cross-scale attention. arXiv preprint arXiv:2203.00860, 2022.\n\nLin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Visionâ€“ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740â€“755. Springer, 2014.\n\nLin et al. (2017) Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision, pages 2980â€“2988, 2017.\n\nLiu et al. (2022) Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329, 2022.\n\nLiu et al. (2021) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012â€“10022, 2021.\n\nLoshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\nMathai et al. (2022) Tejas Sudharshan Mathai, Sungwon Lee, Daniel C Elton, Thomas C Shen, Yifan Peng, Zhiyong Lu, and Ronald M Summers. Lymph node detection in t2 mri with transformers. In Medical Imaging 2022: Computer-Aided Diagnosis, volume 12033, pages 855â€“859. SPIE, 2022.\n\nMoawad et al. (2023) Ahmed W Moawad, Anastasia Janas, Ujjwal Baid, Divya Ramakrishnan, Leon Jekel, Kiril Krantchev, Harrison Moy, Rachit Saluja, Klara Osenberg, Klara Wilms, et al. The brain tumor segmentation (brats-mets) challenge 2023: Brain metastasis segmentation on pre-treatment mri. arXiv preprint arXiv:2306.00838, 2023.\n\nPetrick et al. (2013) Nicholas Petrick, Berkman Sahiner, Samuel G Armato III, Alberto Bert, Loredana Correale, Silvia Delsanto, Matthew T Freedman, David Fryd, David Gur, Lubomir Hadjiiski, et al. Evaluation of computer-aided detection and diagnosis systems a. Medical Physics, 40(8):087001, 2013.\n\nPrangemeier et al. (2020) Tim Prangemeier, Christoph Reich, and Heinz Koeppl. Attention-based transformers for instance segmentation of cells in microstructures. In 2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 700â€“707. IEEE, 2020.\n\nRedmon et al. (2016) Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 779â€“788, 2016.\n\nRezatofighi et al. (2019) Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union. June 2019.\n\nRichter et al. (2021) Mats L. Richter, Wolf Byttner, Ulf Krumnack, Anna Wiedenroth, Ludwig Schallner, and Justin Shenk. (Input) Size Matters for CNN Classifiers, page 133â€“144. Springer International Publishing, 2021. ISBN 9783030863401. . URL http://dx.doi.org/10.1007/978-3-030-86340-1_11.\n\nRoh et al. (2021) Byungseok Roh, JaeWoong Shin, Wuhyun Shin, and Saehoon Kim. Sparse detr: Efficient end-to-end object detection with learnable sparsity. arXiv preprint arXiv:2111.14330, 2021.\n\nSabottke and Spieler (2020) Carl F Sabottke and Bradley M Spieler. The effect of image resolution on deep learning in radiography. Radiology: Artificial Intelligence, 2(1):e190015, 2020.\n\nShen et al. (2021) Zhiqiang Shen, Rongda Fu, Chaonan Lin, and Shaohua Zheng. Cotr: Convolution in transformer network for end to end polyp detection. In 2021 7th International Conference on Computer and Communications (ICCC), pages 1757â€“1761. IEEE, 2021.\n\nThambawita et al. (2021) Vajira Thambawita, Inga StrÃ¼mke, Steven A Hicks, PÃ¥l Halvorsen, Sravanthi Parasa, and Michael A Riegler. Impact of image resolution on deep learning performance in endoscopy image classification: an experimental study using a large dataset of endoscopic images. Diagnostics, 11(12):2183, 2021.\n\nTouvron et al. (2021) Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and HervÃ© JÃ©gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347â€“10357. PMLR, 2021.\n\nValanarasu et al. (2021) Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, and Vishal M Patel. Medical transformer: Gated axial-attention for medical image segmentation. In Medical Image Computing and Computer Assisted Interventionâ€“MICCAI 2021: 24th International Conference, Strasbourg, France, September 27â€“October 1, 2021, Proceedings, Part I 24, pages 36â€“46. Springer, 2021.\n\nVaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.\n\nWang et al. (2018) Pu Wang, Xiao Xiao, Jeremy R Glissen Brown, Tyler M Berzin, Mengtian Tu, Fei Xiong, Xiao Hu, Peixi Liu, Yan Song, Di Zhang, et al. Development and validation of a deep-learning algorithm for the detection of polyps during colonoscopy. Nature Biomedical Engineering, 2(10):741â€“748, 2018.\n\nWang et al. (2017) Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2097â€“2106, 2017.\n\nWang et al. (2022) Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for transformer-based detector. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 2567â€“2575, 2022.\n\nWu et al. (2019) Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, S.G. Kim, Laura Heacock, Linda Moy, Kyunghyun Cho, and Krzyszrof J. Geras. The nyu breast cancer screening dataset v1.0. Tech. rep., New York Univ., New York, NY, USA, 2019.\n\nYao et al. (2021) Zhuyu Yao, Jiangbo Ai, Boxun Li, and Chi Zhang. Efficient detr: improving end-to-end object detector with dense prior. arXiv preprint arXiv:2104.01318, 2021.\n\nYu et al. (2022) Xiang Yu, Qinghua Zhou, Shuihua Wang, and Yu-Dong Zhang. A systematic survey of deep learning in breast cancer. International Journal of Intelligent Systems, 37(1):152â€“216, 2022.\n\nZeng et al. (2022) Nianyin Zeng, Peishu Wu, Zidong Wang, Han Li, Weibo Liu, and Xiaohui Liu. A small-sized object detection oriented multi-scale feature fusion approach with application to defect detection. IEEE Transactions on Instrumentation and Measurement, 71:1â€“14, 2022.\n\nZhang et al. (2022) Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022.\n\nZheng et al. (2022) Yi Zheng, Rushin H Gindra, Emily J Green, Eric J Burks, Margrit Betke, Jennifer E Beane, and Vijaya B Kolachalama. A graph-transformer for whole slide image classification. IEEE Transactions on Medical Imaging, 41(11):3003â€“3015, 2022.\n\nZhou et al. (2021) Wujie Zhou, Xinyang Lin, Jingsheng Lei, Lu Yu, and Jenq-Neng Hwang. Mffenet: Multiscale feature fusion and enhancement network for rgbâ€“thermal urban road scene parsing. IEEE Transactions on Multimedia, 24:2526â€“2538, 2021.\n\nZhu et al. (2020) Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n\nZong et al. (2023) Zhuofan Zong, Guanglu Song, and Yu Liu. Detrs with collaborative hybrid assignments training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6748â€“6758, 2023.\n\nA DETR architecture\n\nA.1 Multi-head self-attention (MHSA)\n\nA standard MHSA with Mğ‘€Mitalic_M heads is defined as:\n\nMâ¢Hâ¢Sâ¢Aâ¢(Q,K,V)=âˆ‘m=1MWmâ¢oâ¢[softmaxâ¢(Qâ¢Wmâ¢qâ¢(Kâ¢Wmâ¢k)Td/M)â¢Vâ¢Wmâ¢v].ğ‘€ğ»ğ‘†ğ´ğ‘„ğ¾ğ‘‰superscriptsubscriptğ‘š1ğ‘€subscriptğ‘Šğ‘šğ‘œdelimited-[]softmaxğ‘„subscriptğ‘Šğ‘šğ‘superscriptğ¾subscriptğ‘Šğ‘šğ‘˜ğ‘‡ğ‘‘ğ‘€ğ‘‰subscriptğ‘Šğ‘šğ‘£MHSA(Q,K,V)=\\sum_{m=1}^{M}W_{mo}\\left[\\mathrm{softmax}\\left(\\frac{QW_{mq}(KW_{% mk})^{T}}{\\sqrt{d/M}}\\right)VW_{mv}\\right].italic_M italic_H italic_S italic_A ( italic_Q , italic_K , italic_V ) = âˆ‘ start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_W start_POSTSUBSCRIPT italic_m italic_o end_POSTSUBSCRIPT [ roman_softmax ( divide start_ARG italic_Q italic_W start_POSTSUBSCRIPT italic_m italic_q end_POSTSUBSCRIPT ( italic_K italic_W start_POSTSUBSCRIPT italic_m italic_k end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d / italic_M end_ARG end_ARG ) italic_V italic_W start_POSTSUBSCRIPT italic_m italic_v end_POSTSUBSCRIPT ] . (3)\n\nThe Kğ¾Kitalic_K, Qğ‘„Qitalic_Q and Vğ‘‰Vitalic_V represent the query, key, and value matrices respectively, defined with respect to input feature map xfâˆˆâ„Wâ¢HÃ—dsubscriptğ‘¥ğ‘“superscriptâ„ğ‘Šğ»ğ‘‘x_{f}\\in\\mathbb{R}^{WH\\times d}italic_x start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_W italic_H Ã— italic_d end_POSTSUPERSCRIPT and its positional embedding xpâˆˆâ„Wâ¢HÃ—dsubscriptğ‘¥ğ‘superscriptâ„ğ‘Šğ»ğ‘‘x_{p}\\in\\mathbb{R}^{WH\\times d}italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_W italic_H Ã— italic_d end_POSTSUPERSCRIPT:\n\nQ=xf+xp,K=xf+xp,V=xf.formulae-sequenceğ‘„subscriptğ‘¥ğ‘“subscriptğ‘¥ğ‘formulae-sequenceğ¾subscriptğ‘¥ğ‘“subscriptğ‘¥ğ‘ğ‘‰subscriptğ‘¥ğ‘“Q=x_{f}+x_{p},K=x_{f}+x_{p},V=x_{f}.italic_Q = italic_x start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_K = italic_x start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_V = italic_x start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT . (4)\n\nWmâ¢q,Wmâ¢k,Wmâ¢vâˆˆâ„dÃ—d/Msubscriptğ‘Šğ‘šğ‘subscriptğ‘Šğ‘šğ‘˜subscriptğ‘Šğ‘šğ‘£superscriptâ„ğ‘‘ğ‘‘ğ‘€W_{mq},W_{mk},W_{mv}\\in\\mathbb{R}^{d\\times d/M}italic_W start_POSTSUBSCRIPT italic_m italic_q end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT italic_m italic_k end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT italic_m italic_v end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d Ã— italic_d / italic_M end_POSTSUPERSCRIPT linearly transforms K,Q,Vğ¾ğ‘„ğ‘‰K,Q,Vitalic_K , italic_Q , italic_V in mğ‘šmitalic_m-th head and Wmâ¢oâˆˆâ„d/MÃ—dsubscriptğ‘Šğ‘šğ‘œsuperscriptâ„ğ‘‘ğ‘€ğ‘‘W_{mo}\\in\\mathbb{R}^{d/M\\times d}italic_W start_POSTSUBSCRIPT italic_m italic_o end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d / italic_M Ã— italic_d end_POSTSUPERSCRIPT.\n\nA.2 Multi-head (MH) cross-attention\n\nThe MH cross-attention module performs the same computation as MHSA defined in 3, except that K,Q,Vğ¾ğ‘„ğ‘‰K,Q,Vitalic_K , italic_Q , italic_V are defined based on two different set of tokens. The queries Qğ‘„Qitalic_Q is defined by object queries q=qc+qpğ‘subscriptğ‘ğ‘subscriptğ‘ğ‘q=q_{c}+q_{p}italic_q = italic_q start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT + italic_q start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT where qpsubscriptğ‘ğ‘q_{p}italic_q start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT and qcsubscriptğ‘ğ‘q_{c}italic_q start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT are the positional embedding and content embedding of the object queries. The keys Kğ¾Kitalic_K are defined by encoder features xeâ¢nâ¢c+xpsubscriptğ‘¥ğ‘’ğ‘›ğ‘subscriptğ‘¥ğ‘x_{enc}+x_{p}italic_x start_POSTSUBSCRIPT italic_e italic_n italic_c end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT. Specifically,\n\nQ=qc+qp,K=xeâ¢nâ¢c+xp,V=qc.formulae-sequenceğ‘„subscriptğ‘ğ‘subscriptğ‘ğ‘formulae-sequenceğ¾subscriptğ‘¥ğ‘’ğ‘›ğ‘subscriptğ‘¥ğ‘ğ‘‰subscriptğ‘ğ‘Q=q_{c}+q_{p},K=x_{enc}+x_{p},V=q_{c}.italic_Q = italic_q start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT + italic_q start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_K = italic_x start_POSTSUBSCRIPT italic_e italic_n italic_c end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_V = italic_q start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT . (5)\n\nA.3 Set prediction loss\n\nDETR uses a set prediction loss which enables end-to-end training without non-maximum suppression (NMS). DETR produces a fixed number of predictions per image Nğ‘Nitalic_N, and Nğ‘Nitalic_N is set to be significantly larger than the maximum possible number of objects in the image. Let {y^i=(c^i,b^i)}Nsubscriptsubscript^ğ‘¦ğ‘–subscript^ğ‘ğ‘–subscript^ğ‘ğ‘–ğ‘\\{\\hat{y}_{i}=(\\hat{c}_{i},\\hat{b}_{i})\\}_{N}{ over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( over^ start_ARG italic_c end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over^ start_ARG italic_b end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT be all pairs of class and box predictions. The set of Nğ‘Nitalic_N labels is {yi=(ci,bi)}Nsubscriptsubscriptğ‘¦ğ‘–subscriptğ‘ğ‘–subscriptğ‘ğ‘–ğ‘\\{y_{i}=(c_{i},b_{i})\\}_{N}{ italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT where each ground truth label represents an object in the image. If there are fewer objects than Nğ‘Nitalic_N, the rest of the labels are the empty classes (0,âˆ…)0(0,\\emptyset)( 0 , âˆ… ). The set prediction loss is computed in two steps. The first step is to find a permutation Ïƒğœ\\sigmaitalic_Ïƒ on the set of labels {yi}subscriptğ‘¦ğ‘–\\{y_{i}\\}{ italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } that minimizes the matching loss, defined as below:\n\nÏƒ^=argâ¡minÏƒâˆˆÎ£Nâ¢âˆ‘Niâ„’mâ¢aâ¢tâ¢câ¢hâ¢(y^i,yÏƒâ¢(i)).^ğœsubscriptğœsubscriptÎ£ğ‘superscriptsubscriptğ‘ğ‘–subscriptâ„’ğ‘šğ‘ğ‘¡ğ‘â„subscript^ğ‘¦ğ‘–subscriptğ‘¦ğœğ‘–\\mathbf{\\hat{\\sigma}}=\\arg\\min_{\\sigma\\in\\Sigma_{N}}\\sum_{N}^{i}\\mathcal{L}_{% match}(\\hat{y}_{i},y_{\\sigma(i)}).over^ start_ARG italic_Ïƒ end_ARG = roman_arg roman_min start_POSTSUBSCRIPT italic_Ïƒ âˆˆ roman_Î£ start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_POSTSUBSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_m italic_a italic_t italic_c italic_h end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_Ïƒ ( italic_i ) end_POSTSUBSCRIPT ) .\n\nThe matching loss for a matching pair is a linear combination of classification loss, box regression loss and GIoU loss (Rezatofighi et al., 2019). The classification loss is a standard focal loss (Lin et al., 2017). The regression loss and the GIoU loss are only applied to non-empty labels. It is defined as the following:\n\nâ„’mâ¢aâ¢tâ¢câ¢hâ¢(y^i,yÏƒâ¢(i))=Wcâ¢lâ¢sâ¢â„’câ¢lâ¢sâ¢(c^i,cÏƒâ¢(i))+1bâ‰ âˆ…â¢(Wlâ¢1â¢â„’lâ¢1â¢(b^i,bÏƒâ¢(i))+Wgâ¢iâ¢oâ¢uâ¢â„’gâ¢iâ¢oâ¢uâ¢(b^i,bÏƒâ¢(i))),subscriptâ„’ğ‘šğ‘ğ‘¡ğ‘â„subscript^ğ‘¦ğ‘–subscriptğ‘¦ğœğ‘–subscriptğ‘Šğ‘ğ‘™ğ‘ subscriptâ„’ğ‘ğ‘™ğ‘ subscript^ğ‘ğ‘–subscriptğ‘ğœğ‘–subscript1ğ‘subscriptğ‘Šğ‘™1subscriptâ„’ğ‘™1subscript^ğ‘ğ‘–subscriptğ‘ğœğ‘–subscriptğ‘Šğ‘”ğ‘–ğ‘œğ‘¢subscriptâ„’ğ‘”ğ‘–ğ‘œğ‘¢subscript^ğ‘ğ‘–subscriptğ‘ğœğ‘–\\mathcal{L}_{match}(\\hat{y}_{i},y_{\\sigma(i)})=W_{cls}\\mathcal{L}_{cls}(\\hat{c% }_{i},c_{\\sigma(i)})+1_{b\\neq\\emptyset}\\left(W_{l1}\\mathcal{L}_{l1}(\\hat{b}_{i% },b_{\\sigma(i)})+W_{giou}\\mathcal{L}_{giou}(\\hat{b}_{i},b_{\\sigma(i)})\\right),caligraphic_L start_POSTSUBSCRIPT italic_m italic_a italic_t italic_c italic_h end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_Ïƒ ( italic_i ) end_POSTSUBSCRIPT ) = italic_W start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT ( over^ start_ARG italic_c end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT italic_Ïƒ ( italic_i ) end_POSTSUBSCRIPT ) + 1 start_POSTSUBSCRIPT italic_b â‰  âˆ… end_POSTSUBSCRIPT ( italic_W start_POSTSUBSCRIPT italic_l 1 end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_l 1 end_POSTSUBSCRIPT ( over^ start_ARG italic_b end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT italic_Ïƒ ( italic_i ) end_POSTSUBSCRIPT ) + italic_W start_POSTSUBSCRIPT italic_g italic_i italic_o italic_u end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_g italic_i italic_o italic_u end_POSTSUBSCRIPT ( over^ start_ARG italic_b end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT italic_Ïƒ ( italic_i ) end_POSTSUBSCRIPT ) ) ,\n\nwhere Wcâ¢lâ¢s,Wlâ¢1,Wgâ¢iâ¢oâ¢usubscriptğ‘Šğ‘ğ‘™ğ‘ subscriptğ‘Šğ‘™1subscriptğ‘Šğ‘”ğ‘–ğ‘œğ‘¢W_{cls},W_{l1},W_{giou}italic_W start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT italic_l 1 end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT italic_g italic_i italic_o italic_u end_POSTSUBSCRIPT are scalar coefficients that are tuned as hyperparameters to balance the scale of different losses. The Hungarian algorithm (Kuhn, 1955) can efficiently find the optimal matching Ïƒ^^ğœ\\hat{\\sigma}over^ start_ARG italic_Ïƒ end_ARG. The second step is to minimize the loss function âˆ‘Niâ„’mâ¢aâ¢tâ¢câ¢hâ¢(y^i,yÏƒ^â¢(i))superscriptsubscriptğ‘ğ‘–subscriptâ„’ğ‘šğ‘ğ‘¡ğ‘â„subscript^ğ‘¦ğ‘–subscriptğ‘¦^ğœğ‘–\\sum_{N}^{i}\\mathcal{L}_{match}(\\hat{y}_{i},y_{\\hat{\\sigma}(i)})âˆ‘ start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_m italic_a italic_t italic_c italic_h end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT over^ start_ARG italic_Ïƒ end_ARG ( italic_i ) end_POSTSUBSCRIPT ) with the permutation Ïƒ^^ğœ\\hat{\\sigma}over^ start_ARG italic_Ïƒ end_ARG on the label set.\n\nDETR also utilizes auxiliary loss in each decoder layer to provide stronger supervision. At the end of each decoder layer, it predicts Nğ‘Nitalic_N boxes and class scores with MLP prediction heads. All the prediction heads share weights. The above two steps, the matching step and Hungarian loss minimization, are applied to each decoder layerâ€™s output. At inference, only the last layerâ€™s output are used as the final prediction.\n\nB Deformable DETR architecture\n\nB.1 Deformable multi-head self-attention\n\nFormally, deformable MHSA for a single query qâˆˆâ„dğ‘superscriptâ„ğ‘‘q\\in\\mathbb{R}^{d}italic_q âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT in the feature map is given by:\n\nDeformableâ¢_â¢MHSAâ¢(Qq,K,V)=âˆ‘m=1MWmâ¢oâ¢[softmaxâ¢(Kqâ¢Wmâ¢k)â¢Vqâ¢Wmâ¢v]Deformable_MHSAsubscriptğ‘„ğ‘ğ¾ğ‘‰superscriptsubscriptğ‘š1ğ‘€subscriptğ‘Šğ‘šğ‘œdelimited-[]softmaxsubscriptğ¾ğ‘subscriptğ‘Šğ‘šğ‘˜subscriptğ‘‰ğ‘subscriptğ‘Šğ‘šğ‘£\\mathrm{Deformable\\_MHSA}(Q_{q},K,V)=\\sum_{m=1}^{M}W_{mo}\\left[\\mathrm{softmax% }\\left(K_{q}W_{mk}\\right)V_{q}W_{mv}\\right]roman_Deformable _ roman_MHSA ( italic_Q start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_K , italic_V ) = âˆ‘ start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_W start_POSTSUBSCRIPT italic_m italic_o end_POSTSUBSCRIPT [ roman_softmax ( italic_K start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_m italic_k end_POSTSUBSCRIPT ) italic_V start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_m italic_v end_POSTSUBSCRIPT ] (6)\n\nThe Qğ‘„Qitalic_Q, Kğ¾Kitalic_K and Vğ‘‰Vitalic_V represent the query, key, and value matrices respectively, defined as the following,\n\nQ=xf+xpâ¢ and â¢Qq=q,Kq=Î´â¢(K,q)âˆˆâ„kÃ—d,Vq=Î´â¢(V,q)âˆˆâ„kÃ—d.formulae-sequenceğ‘„subscriptğ‘¥ğ‘“subscriptğ‘¥ğ‘ and subscriptğ‘„ğ‘ğ‘subscriptğ¾ğ‘ğ›¿ğ¾ğ‘superscriptâ„ğ‘˜ğ‘‘subscriptğ‘‰ğ‘ğ›¿ğ‘‰ğ‘superscriptâ„ğ‘˜ğ‘‘Q=x_{f}+x_{p}\\text{ and }Q_{q}=q,K_{q}=\\delta(K,q)\\in\\mathbb{R}^{k\\times d},V_% {q}=\\delta(V,q)\\in\\mathbb{R}^{k\\times d}.italic_Q = italic_x start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT and italic_Q start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = italic_q , italic_K start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = italic_Î´ ( italic_K , italic_q ) âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_k Ã— italic_d end_POSTSUPERSCRIPT , italic_V start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = italic_Î´ ( italic_V , italic_q ) âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_k Ã— italic_d end_POSTSUPERSCRIPT . (7)\n\nThe key sampling function Î´ğ›¿\\deltaitalic_Î´ samples kğ‘˜kitalic_k keys from the full set of keys K=xf+xpğ¾subscriptğ‘¥ğ‘“subscriptğ‘¥ğ‘K=x_{f}+x_{p}italic_K = italic_x start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT by generating the sampling offsets Î”â¢pÎ”ğ‘\\Delta proman_Î” italic_p with respect to reference points pqsubscriptğ‘ğ‘p_{q}italic_p start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT: Î´â¢(K,q)=Kâ¢(pq+Î”â¢p)ğ›¿ğ¾ğ‘ğ¾subscriptğ‘ğ‘Î”ğ‘\\delta(K,q)=K(p_{q}+\\Delta p)italic_Î´ ( italic_K , italic_q ) = italic_K ( italic_p start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT + roman_Î” italic_p ). The sampling offsets are obtained through a linear transformation of the query qğ‘qitalic_q.\n\nC Iterative Bounding Box Refinement Technique\n\nIn the standard Deformable DETR, a 2D reference point rqâˆˆ[0,1]2subscriptğ‘Ÿğ‘superscript012r_{q}\\in[0,1]^{2}italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT âˆˆ [ 0 , 1 ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT for each object query qğ‘qitalic_q is derived from its learnable positional embedding pqsubscriptğ‘ğ‘p_{q}italic_p start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT via a linear layer\n\nrq=linearâ¢(pq).subscriptğ‘Ÿğ‘linearsubscriptğ‘ğ‘r_{q}=\\mathrm{linear}(p_{q}).italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = roman_linear ( italic_p start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) .\n\nThroughout the decoder, the locations of these reference points remain constant. They are updated based on the learnable positional embedding pqsubscriptğ‘ğ‘p_{q}italic_p start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT when a backward pass is completed. Formally, let rqisuperscriptsubscriptğ‘Ÿğ‘ğ‘–r_{q}^{i}italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT be the reference points of an object query qğ‘qitalic_q in i-th decoder layer. In standard Deformable DETR,\n\nrq1=rq2=â€¦=rq6.superscriptsubscriptğ‘Ÿğ‘1superscriptsubscriptğ‘Ÿğ‘2â€¦superscriptsubscriptğ‘Ÿğ‘6r_{q}^{1}=r_{q}^{2}=\\ldots=r_{q}^{6}.italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = â€¦ = italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT .\n\nIn IBBR, the reference points rqisuperscriptsubscriptğ‘Ÿğ‘ğ‘–r_{q}^{i}italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT in i-th decoder layer is refined based on the previous reference points rqiâˆ’1superscriptsubscriptğ‘Ÿğ‘ğ‘–1r_{q}^{i-1}italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT and the offsets predicted by auxiliary prediction head.\n\nrqi=Sâ¢(linearâˆ’1â¢(Sâˆ’1â¢(rqiâˆ’1))+MLPâ¢(xdâ¢eâ¢ci)),superscriptsubscriptğ‘Ÿğ‘ğ‘–ğ‘†superscriptlinear1superscriptğ‘†1superscriptsubscriptğ‘Ÿğ‘ğ‘–1MLPsuperscriptsubscriptğ‘¥ğ‘‘ğ‘’ğ‘ğ‘–r_{q}^{i}=S(\\mathrm{linear}^{-1}(S^{-1}(r_{q}^{i-1}))+\\mathrm{MLP}(x_{dec}^{i}% )),italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_S ( roman_linear start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_S start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT ) ) + roman_MLP ( italic_x start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) ) ,\n\nwhere Sğ‘†Sitalic_S and Sâˆ’1superscriptğ‘†1S^{-1}italic_S start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT represent the sigmoid function and its inverse, and xdâ¢eâ¢cisubscriptsuperscriptğ‘¥ğ‘–ğ‘‘ğ‘’ğ‘x^{i}_{dec}italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT is the output of the i-th decoder layer.\n\nD Backbone Pre-training\n\nWe pretrained the Swin-T Transfromer backbone with a cancer classification task on our dataset. This classification task is a binary multi-label classification that predicts two scores indicating if an input image contains benign lesions or malignant lesions.\n\nE Hyperparameter Tuning\n\nOur method for hyper-parameter tuning is random search. We tuned the following hyperparameters and their ranges on quarter resolution images:\n\nâ€¢\n\nlearning rate Î·âˆˆ10[3,5.5]ğœ‚superscript1035.5\\eta\\in 10^{[3,5.5]}italic_Î· âˆˆ 10 start_POSTSUPERSCRIPT [ 3 , 5.5 ] end_POSTSUPERSCRIPT,\n\nâ€¢\n\nscale of the backbone learning rate sâˆˆ[1,0.01]ğ‘ 10.01s\\in[1,0.01]italic_s âˆˆ [ 1 , 0.01 ] (backbone learning rate = sÃ—Î·ğ‘ ğœ‚s\\times\\etaitalic_s Ã— italic_Î· ),\n\nâ€¢\n\nweight decay Î»âˆˆ10[3,6]ğœ†superscript1036\\lambda\\in 10^{[3,6]}italic_Î» âˆˆ 10 start_POSTSUPERSCRIPT [ 3 , 6 ] end_POSTSUPERSCRIPT,\n\nâ€¢\n\nnumber of object queries Nâˆˆ[10,200]ğ‘10200N\\in[10,200]italic_N âˆˆ [ 10 , 200 ],\n\nâ€¢\n\ntwo hyperparameters Î±ğ›¼\\alphaitalic_Î± and Î³ğ›¾\\gammaitalic_Î³ in the focal loss Î±âˆˆ[0,1]ğ›¼01\\alpha\\in[0,1]italic_Î± âˆˆ [ 0 , 1 ], Î³âˆˆ[0,3]ğ›¾03\\gamma\\in[0,3]italic_Î³ âˆˆ [ 0 , 3 ],\n\nâ€¢\n\nthe coefficients on classification loss and GIoU loss âˆˆ[0,1]absent01\\in[0,1]âˆˆ [ 0 , 1 ].\n\nWe train 80808080 jobs in total and choose the best model based on FAUC_1."
    }
}
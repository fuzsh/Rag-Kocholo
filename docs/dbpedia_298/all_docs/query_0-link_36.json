{
    "id": "dbpedia_298_0",
    "rank": 36,
    "data": {
        "url": "https://carnegieendowment.org/2024/03/21/envisioning-global-regime-complex-to-govern-artificial-intelligence-pub-92022",
        "read_more_link": "",
        "language": "en",
        "title": "Envisioning a Global Regime Complex to Govern Artificial Intelligence",
        "top_image": "https://carnegie-production-assets.s3.amazonaws.com/static/media/images/GettyImages-AI-GOV-9.jpg",
        "meta_img": "https://carnegie-production-assets.s3.amazonaws.com/static/media/images/GettyImages-AI-GOV-9.jpg",
        "images": [
            "https://carnegie-production-assets.s3.amazonaws.com/static/media/images/GettyImages-AI-GOV-9.jpg",
            "https://carnegie-production-assets.s3.amazonaws.com/static/media/images/globalorder_hero_mb.png",
            "https://carnegie-production-assets.s3.amazonaws.com/static/media/avatars/Stewart-Patrick-headshot.jpg",
            "https://carnegie-production-assets.s3.amazonaws.com/static/media/images/globalorder_hero_mb.png",
            "https://carnegie-production-assets.s3.amazonaws.com/static/media/avatars/Stewart-Patrick-headshot.jpg",
            "https://carnegieendowment.org/static/media/images/ai-model-iStock-1705828371.jpg",
            "https://carnegieendowment.org/static/media/images/iStock-1089666652-1420x7707-8.jpg",
            "https://carnegieendowment.org/static/media/images/GettyImages-1239563813_2_-9.jpg",
            "https://carnegieendowment.org/static/media/images/GettyImages-India-SemiConducter.jpg",
            "https://carnegieendowment.org/static/media/avatars/Carnegie_Konark_photo__1_-1.JPG",
            "https://carnegieendowment.org/static/media/images/technology-global-iStock-1331260772.jpg",
            "https://carnegie-production-assets.s3.amazonaws.com/_nuxt/avatar-placeholder.BaqAmelg.png",
            "https://carnegie-production-assets.s3.amazonaws.com/_nuxt/avatar-placeholder.BaqAmelg.png",
            "https://carnegieendowment.org/static/media/images/imf-governance-GettyImages-2148760650.jpg",
            "https://carnegie-production-assets.s3.amazonaws.com/_nuxt/avatar-placeholder.BaqAmelg.png",
            "https://carnegieendowment.org/static/media/images/climate-change-iStock-1435058487.jpg",
            "https://carnegieendowment.org/static/media/avatars/Awad-Ibrahim-headshot.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "{\"@type\": \"Person\",\"name\": \"Emma Klein\"},{\"@type\": \"Person\",\"name\": \"Stewart Patrick\"}"
        ],
        "publish_date": "2024-03-21T00:00:00",
        "summary": "",
        "meta_description": "Rather than a single, tidy, institutional solution to govern AI, the world will likely see the emergence of something less elegant: a regime complex, comprising multiple institutions within and across several functional areas.",
        "meta_lang": "en",
        "meta_favicon": "/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://carnegieendowment.orgundefined?lang=en",
        "text": "Introduction\n\nIn spring 2023, OpenAI cofounders Sam Altman, Greg Brockman, and Ilya Sutskever proposed an “IAEA [International Atomic Energy Agency] for superintelligence efforts” to govern high-capability systems, noting the potential genesis of superintelligence in rapidly advancing artificial intelligence (AI) models.1 Soon after, United Nations (UN) Secretary-General António Guterres lent his support to this idea.2 In the following months, others suggested an array of global institutions that could be created to regulate this technology, based on models such as the Intergovernmental Panel on Climate Change (IPCC) and the International Civil Aviation Organization (ICAO).3\n\nLess than a year later, the search for a single institutional solution has faded. The challenges that AI presents are too multifaceted, the relevant actors too varied, and the geopolitical situation too complicated for any one global body to tackle by itself. Instead, many expect the emergence of overlapping institutions designed to advance and govern specific uses and impacts of AI.4 Illustrative of this shift in thinking is the preliminary report of the UN High-Level Advisory Body on AI (HLAB), produced by thirty-nine expert members and released in December 2023.5 Although the report does not address how closely linked global AI governance institutions should be—and whether there should be “individual institutions” or a “network of institutions”—it presumes there will be multiple. The ultimate content and contours of this governance arrangement will reflect several functional imperatives, as refracted through the interests, values, and capabilities of powerful public and private actors with a stake in AI’s future.\n\nAs this alternative approach to AI governance gains traction in policy discussions, several analytical gaps remain. First, few analysts have explicitly framed the anticipated governance framework for AI as a regime complex, much less grappled with the implications of such a complicated institutional design. Second, while various stakeholders, including the HLAB, have enumerated a welter of institutional analogies from other fields for particular governance functions, they have seldom interrogated the relevance of these models in any detail.6 Third, few experts have explored how geopolitical dynamics, including strategic rivalry, will shape, and likely constrain, the creation of international institutions to govern AI. This working paper aims to fill these gaps and alert policymakers to the possibilities, dilemmas, and trade-offs that may arise as they design—and ideally seek to reconcile—multiple governance arrangements.\n\nGlobal AI governance will inevitably involve some fragmentation. The history of internet governance, including debates over the appropriate regulatory role of governments, has illuminated the distinct orientations of the United States, the European Union (EU), and China toward the global digital order, characterized as “market-driven,” “rights-driven,” and “state-driven” models, respectively.7 While the United States has championed a limited government role over the internet and deferred to private technology companies so as to support freedom of speech and technological innovation, the EU has pursued a greater regulatory role to protect other human rights, including privacy, and China has assumed complete state control, with extensive censorship and surveillance capabilities.\n\nThese differences—and the challenge of reaching a multilateral consensus—have played out on the global stage, notably in increasingly contentious elections over leadership of the International Telecommunication Union, a UN-affiliated specialized agency whose mandate encompasses internet regulation.8 Similar fissures over domestic regulatory approaches toward AI are already evident and expected to bleed into nascent global governance initiatives. Managing normative and regulatory fragmentation in an eventual regime complex will thus be essential to advancing global AI cooperation.\n\nCurrent proposals for the design of global AI governance have concentrated on several functions.9 This paper consolidates these into four broad categories to facilitate deep analysis and probe the relevance of their associated institutional analogies. The first function is to provide an authoritative platform for scientific and technical knowledge and information sharing on the latest state of AI capabilities and their potential ramifications. The second is to promote common norms and standards for the responsible uses of AI by both public and private actors, as well as to seek to harmonize national regulatory approaches. The third is to support the broadest possible access to and equitable sharing of the benefits from AI, with a particular focus on the development needs of low- and middle-income countries. The fourth is to foster global collective security by creating frameworks to deter and respond to destabilizing uses of these technologies by state and nonstate actors, as well as to prepare for any existential risks posed by the potential emergence of artificial general intelligence.\n\nRealizing these functional objectives will require a regime complex that is multi-multilateral, comprising several institutions and initiatives, each involving different membership groups. For some functions, building entirely new institutions may be necessary. More commonly, the mandates and capacities of existing institutions will need to be adapted and extended to make them AI-competent. Many institutions for AI governance will be intergovernmental, with membership restricted to sovereign states; some of these will have universal membership, whereas some will be narrower, selective, minilateral frameworks among like-minded nations. Other global arrangements will have multiple stakeholders, involving not only national governments but also corporations and civil society actors. Eventually, some normative commitments may become grounded in binding international law, while others will remain voluntary.\n\nThe Rise of AI and Efforts to Govern It\n\nBreathtaking advances in AI and its integration throughout society have left public authorities scrambling to ensure the safety and transparency of its development and applications.10 If the velocity of innovation accelerates, the governance challenges will become even more daunting.11\n\nAlthough definitions vary, artificial intelligence generally refers to information-processing systems that use models and algorithms to make inferences from data and other inputs to “generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments.”12 The main focus of attention today is on foundation AI models that are trained on large amounts of data to be utilized for a range of tasks, rather than on narrow applications.13 These models have contributed to the increasing sophistication of generative AI, including an ability to produce high-quality text, images, and videos from input data and similarly respond to text and images, including through chatbots like ChatGPT.14\n\nThe potential benefits of AI—for alleviating poverty, transforming medicine, combating climate change, enhancing worker productivity, eradicating infectious diseases, improving access to high-quality education, strengthening the efficiency of local governments, and so much else—are significant.15 Notably, however, these potential benefits have attracted less attention as subjects of international governance than AI’s possible dangers. (This may reflect the underrepresentation in prominent initiatives of policymakers from countries of the Global South, where policy discussions have tended to focus more on the many opportunities that AI presents for development.)16\n\nInstead, many policymakers and researchers in the Global North have emphasized AI’s potential risks.17 Among these concerns are that AI may facilitate political interference and the spread of misinformation and disinformation;18 entrench discrimination through algorithmic bias;19 enable mass surveillance by authoritarian regimes;20 worsen invasions of privacy by private corporations;21 generate mass worker dislocation and unemployment in knowledge and data-intensive sectors;22 exacerbate global inequality;23 facilitate the spread of lethal autonomous weapons systems;24 lower barriers to entry for biological and nuclear weapons;25 weaken security in cyberspace;26 accentuate geopolitical rivalry and the risk of major power war;27 undermine nuclear deterrence and strategic stability;28 and hasten the emergence of an omniscient, omnipresent, and omnipotent “superintelligence” that would act contrary to the interests of its human creators.29\n\nMuch of the action to regulate AI and manage its risks and opportunities will take place at the domestic level.30 The EU, China, the United States, and India are already pursuing regulatory models, with different objectives and enforcement mechanisms. The EU’s AI Act, provisionally approved in December 2023 and passed by the European Parliament in March 2024, seeks to protect “fundamental rights, democracy, the rule of law and environmental sustainability” without hampering innovation.31 It adopts a “risk-based” approach, such that higher-risk AI applications face more stringent rules, especially regarding transparency and quality of data sources, cybersecurity, and safety testing.32 This high-risk category includes foundation models for now, despite misgivings from France, Germany, and Italy about stifling innovation.33 Banned applications include social scoring, emotion recognition in workplaces and educational institutions, and biometric categorization systems.34 (Law enforcement can still use biometric identification systems for solving a narrow list of crimes.) The rules are legally binding, and noncompliant parties are subject to fines, though most provisions will not take hold until two years after the act enters into force.\n\nChina also imposes strict, binding regulations on companies—including with respect to specific components like algorithms, synthetically generated content, and generative AI.35 Unlike the EU, its primary motivation is not to protect individual rights but to exert information control over Chinese society.36 For example, China’s Generative AI Measures, a set of regulations that took effect in August 2023, seek to restrict chatbots from “producing fake and harmful information,” including content related to the “subversion of state power.”37 However, these efforts to contain politically sensitive content create a high regulatory burden for companies. Already Chinese policymakers have relaxed some rules and their enforcement, lest they stifle industry innovation and broader economic competitiveness.38\n\nThe United States is further behind in producing regulations. It has relied on voluntary commitments and nonbinding measures, undergirded by the principle of “responsible innovation.”39 In July 2023, President Joe Biden’s administration reached agreement with seven leading AI companies on “voluntary safeguards,” including security testing and monitoring of bias and privacy risks.40 In October 2023, the White House issued the Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence, a notable first step in creating guardrails for advanced AI systems.41 Still, the executive order primarily directs government agencies to prepare assessments and recommendations on AI in their domains; robust U.S. regulation will require congressional legislation.42 On balance, the current U.S. stance privileges innovation, consistent with a traditional deference to the private sector, whereas the EU prioritizes safety and rights.\n\nFinally, India is seeking to carve out its own role in AI governance and promote an alternative regulatory framework for countries in the Global South that encourages domestic innovation while keeping citizens safe.43 The form and content of the country’s eventual AI regulations remain unclear, even though the Indian government’s public policy think tank released a national AI strategy in 2018 and the Ministry of Electronics and Information Technology proposed the Digital India Act in 2023, which focuses on AI, data governance protection, and cybersecurity.44\n\nBeyond these domestic regulatory steps, international governance will also be critical, given the global reach and ramifications of AI. Although the development of advanced AI systems and chips is currently concentrated in a handful of countries­­­­­­, access to many AI models cannot easily be contained within borders.45 It is thus imperative to develop international rules of the road regarding AI’s development and use, hold governments and private actors within their sovereign jurisdictions accountable for how they employ it, and create backstops in case governments are unable or unwilling to fulfill their regulatory responsibilities. The emerging global framework must also be consistent with established international law, norms, and conventions, including the UN Charter, the Universal Declaration of Human Rights, international humanitarian law, and other multilateral treaties.\n\nMultilateral AI diplomacy has accelerated accordingly. In October 2023, at the Third Belt and Road Forum, Chinese President Xi Jinping announced a Global AI Governance Initiative.46 Soon after, the G7 released the Hiroshima Process International Guiding Principles for Organizations Developing Advanced AI Systems and a related code of conduct (hereafter called the Hiroshima Code of Conduct).47 In November 2023, the United Kingdom (UK) hosted the world’s inaugural AI Safety Summit, where twenty-eight nations agreed to commission an expert-led State of the Science Report, among other outcomes.48 More recently, in December 2023, the HLAB released its interim report, “Governing AI for Humanity.”49 This flurry of activity builds on earlier work by intergovernmental bodies, such as the Organisation for Economic Co-operation and Development’s (OECD) AI Principles; the UN Educational, Scientific and Cultural Organization’s (UNESCO) Recommendation on the Ethics of AI, which was adopted by all 193 member states in 2021 (though the United States has not adopted the recommendation despite rejoining UNESCO in July 2023); and the Global Partnership on AI’s multidisciplinary research reports.50\n\nNotwithstanding these developments, any effort to govern AI at the global level will face powerful incentives working against such regulation, as both major powers and leading companies compete with their respective counterparts to reap the geopolitical and economic rewards of this new technology. Approaches to global governance for AI must therefore account for this barrier if they are to overcome it.\n\nA Regime Complex for AI\n\nThe regulatory complexities presented by AI, as well as the multiplicity of actors involved and the geopolitical context, necessitate multiple institutions at the global level. Beyond the HLAB’s interim report, which outlined a disaggregated global governance framework, other policy developments affirm this expectation.51 For instance, the Bletchley Declaration, issued at the close of the UK AI Safety Summit, anticipates that countries will work together through “existing international fora and other relevant initiatives.”52 In a similar vein, the director of the White House Office of Science and Technology Policy noted in a lecture that “different [multilateral] fora are approaching different aspects of the [AI] problem.”53\n\nA regime complex is a collage of overlapping multilateral arrangements involving different actors, functions, and principles that facilitate international cooperation.54 Similar arrangements have emerged to address other complicated global domains, such as climate change, global health, and cyberspace, though the cyberspace example serves more as a cautionary tale of how discord among major powers can produce extreme fragmentation in global governance.55\n\nKey components of the regime complex for climate change, for example, include multilateral treaties, such as the UN Framework Convention on Climate Change and the Montreal Protocol on the ozone layer; scientific assessment bodies, notably the IPCC; funding mechanisms like the Green Climate Fund and the Global Environment Facility; UN agencies like the World Meteorological Organization (WMO) and the UN Environment Programme (UNEP); the climate initiatives of the International Monetary Fund (IMF), the World Bank, and regional multilateral development banks; narrower bodies like the International Energy Agency; minilateral groupings such as the G20 and the Major Economies Forum on Energy and Climate; networks of subnational actors like the C40 Cities coalition; and private sector coalitions such as the Glasgow Financial Alliance for Net Zero.56\n\nFraming AI governance in terms of a regime complex is useful analytically because it draws attention to the varied practical imperatives and messy geopolitical realities of AI governance. This approach helps policymakers to break down the AI challenge into manageable chunks; anticipate the dilemmas of alternative institutional design choices, including the trade-offs between universal and coalitional approaches to regulation; and contemplate the relevance of analogous institutions that have managed other global challenges.\n\nRegime complexes are nonhierarchical and modular, meaning that no institution holds authority over the other constitutive elements and that the constituent parts of a regime complex can be designed for specific purposes. Such a decentralized approach is well-suited to AI for three reasons.\n\nFirst, AI governance must make simultaneous progress on several fronts, such as improving policymakers’ understanding of AI’s underlying science, ensuring wider access to the technology, and regulating its use in military contexts. Rather than asking a single institution to fill these needs, a division of labor is more appropriate, with different institutions pursuing cooperative objectives across distinct domains.\n\nSecond, using a variety of fora and institutions can permit selectivity in membership, allowing policymakers to adjust who is at the table, depending on the nature of the issue, the interests and competencies of relevant actors, and geopolitical considerations. Some aspects of AI cooperation warrant broad intergovernmental participation within institutions that feature universal membership like the UN and its agencies. Tackling other aspects may only be feasible within, or when restricted to, narrow coalitions of countries that share values and objectives, possess germane AI capabilities, or are able to move with greater dispatch than bodies with more encompassing membership. Still other frameworks will need to have multiple stakeholders, such that technology companies and civil society representatives hold formal membership alongside governments.\n\nThird, regime complexes can advance governance in the absence of multilateral treaties or even formal organizations. Negotiating and ratifying international legal conventions is a painstaking process and will be impossible for many AI issues in the short and medium term. Instead, progress will initially rely on nonbinding agreements and declarations of principles and on the promotion of norms of behavior for states and nonstate actors, which can be gradually incorporated in the activities of existing and new international organizations. Regime complexes can therefore enable an adaptive, multifaceted approach to global governance that permits the iterative evolution of regulation in response to innovation.\n\nRegime complexes have disadvantages, but highlighting them now can encourage policymakers to mitigate these downsides as they begin to build a governance architecture for AI.57 First, the fragmented nature of regime complexes, particularly the absence of an authoritative institution or even high-level conductor to orchestrate actors and activities, can lead to incoherence, gaps, and redundancy across initiatives, undercutting progress on shared challenges and complicating efforts to hold governments accountable. To promote complementarity from the outset, countries must negotiate shared principles and norms about how to address the development and use of AI.\n\nSecond, regime complexes can exacerbate competitive dynamics among countries, providing nations dissatisfied with existing institutions greater leeway to engage in forum shopping or create alternative bodies that undermine the original ones. Such contested multilateralism, which is particularly common between strategic adversaries like the United States and China, can be corrosive to more encompassing forms of collective action, as nations prioritize narrow interests over shared goals.58 While policymakers cannot eliminate competitive dynamics, they can temper them by supporting wider participation on some issues. In this sense, the UK’s decision to include China at the November 2023 AI Safety Summit was prudent.\n\nForm Follows Function\n\nA regime complex is a multidimensional governance system that emerges not through the tightly coordinated actions of any single group of countries but through the cumulative efforts of disparate actors (including sovereign states, intergovernmental organizations, and nonstate actors) to construct institutions that address different aspects of a complicated global challenge. The future regime complex for AI will be no different. Despite this lack of central direction, policymakers in the United States and other major powers can help foster the emergence of an effective, stable, and coherent AI governance system if they focus on several concrete objectives and remain attuned to how these different initiatives and institutions can complement each other.\n\nThe regime complex for AI should notionally fulfill at least four main functions: building scientific understanding about AI’s evolving capabilities and implications, setting standards for its development and use, sharing the benefits of AI globally, and promoting collective security. Given the multifaceted nature of these functions, the diversity of national interests involved, and the multiplicity of state and private actors in this field, the framework for AI governance that emerges in each functional area will likely rest not on a single institution but comprise myriad multilateral, minilateral, and multistakeholder arrangements.59\n\nThe following sections explore these four functions of a future regime complex for AI, as well as the relevance and limitations of prominent analogies to existing multilateral bodies in other fields.\n\nBuilding Scientific Understanding\n\nOne institutional priority is establishing an authoritative intergovernmental framework for synthesizing and sharing the latest scientific and technological breakthroughs related to AI to give policymakers and the public a common baseline of understanding. Generating these reference points is a precondition for international cooperation on managing the risks and opportunities presented by advanced machine learning systems, most of which are developed by private corporations, with insufficient transparency and information sharing.\n\nWith this objective in mind, several recent proposals advocate the creation of an intergovernmental body to regularly assess, build political consensus on, and share the latest scientific and technical knowledge on AI’s capabilities and implications, including its potential social, economic, developmental, environmental, political, and security-related impacts.60 Such an expert-led process would ideally provide objective information to ensure that fact-based assessments undergird the development of national and international policies, including the formulation and harmonization of best practices.\n\nAt the UK AI Safety Summit, participating governments agreed to commission an international report on the state of AI science, under the direction of prominent computer scientist Yoshua Bengio, to better understand the power and risks of cutting-edge “frontier” models.61 It is slated to be released before South Korea hosts a mini virtual AI Safety Summit in mid-2024. Although inspired by the IPCC, this initiative is currently ad hoc, lacking set procedures and bureaucratic infrastructure. Presuming this framework is formalized in a permanent institution, the IPCC and similar entities provide policymakers with some useful lessons. Still, significant differences between climate and AI challenges, as well as the shortcomings of the IPCC itself, could render this analogy less compelling than it initially appears.\n\nProposed Institutional Models\n\nEstablished in 1988, the IPCC is an intergovernmental body under UN auspices, comprising 195 member states.62 Its mandate is to produce scientific assessments of climate change, including current and future impacts. The IPCC elects a bureau of scientists to oversee each assessment report and select the experts who prepare and write that document. This complicated process involves input from thousands of scientists. (It is no surprise, then, that the IPCC’s regular assessments only come out every six to seven years).63\n\nAnalogous bodies exist, including in other environmental fields. One is the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services, created in 2012, which synthesizes the latest scientific findings on the status of and threats to biodiversity and Earth’s natural ecosystems.64 It has a structure similar to the IPCC, but it is not a UN body, and its mandate includes supporting policy development.65 Another model is the Montreal Protocol (formed in 1987), whose three highly respected assessment panels address scientific, technical, and environmental questions related to the health and integrity of the ozone layer.66\n\nWere one to envision a similar entity for AI built on existing initiatives, one practical question is under what auspices it should be created. The above examples suggest a range of options: the IPCC was established by the WMO and the UNEP. The Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services was founded by a coalition of ninety-four governments at a conference in Panama in 2012.67 The three ozone panels, meanwhile, were created pursuant to a formal multilateral treaty.\n\nThe IPCC offers an appealing governance model. First, it is ostensibly policy neutral, meaning it does not adopt a stance on actions that countries should pursue. Given the diversity of governments’ regulatory approaches toward AI, a commitment to policy neutrality should minimize political contestation within a scientific assessment panel for AI and contribute to the panel’s legitimacy, which is important due to low levels of trust toward major powers and private technology companies. Moreover, there is not yet sufficient research or agreement on AI’s capabilities and impact for the proposed panel to advocate policy solutions.68\n\nSecond, the IPCC publishes periodic special reports (on subjects like the implications of global warming of more than 1.5 degrees Celsius or on the ramifications of climate change for Earth’s oceans and frozen regions), which illuminate areas where future governance initiatives are needed.69 For AI, there could be special reports on the extent of existential risk posed by AI or on the state of global investment in research and development on AI safety. Commentators from academia and industry have suggested creating an international AI research institution, along the model of the European Organization for Nuclear Research (CERN) or the International Space Station.70 A similar endeavor for AI would be an expensive undertaking, so it should be preceded by an assessment of current primary research and whether there are gaps that an institution for joint research could fill.\n\nLimitations of Proposed Models\n\nAs a research subject, AI is notably different from climate change, biodiversity, and the ozone layer, so establishing a scientific assessment panel for AI will involve addressing additional complexities and trade-offs. First, the rapid speed of AI innovation conflicts with the IPCC’s painstaking, multiyear assessment cycles. IPCC assessments are designed to achieve a high degree of scientific rigor and solicit widespread participation at every phase.71 Once authors write the chapters (based on primary literature in the field), people can register as expert reviewers to provide feedback on the initial drafts. The authors then write second drafts, submitting them for additional review by experts and governments. Final drafts are then prepared for governments, which submit their last edits and then meet to approve the report in question. This complex process has some advantages. By prioritizing rigorous research and inclusivity, it helps to build scientific consensus and political legitimacy for IPCC assessments.\n\nHowever, a protracted timeline makes little sense for AI because the pace of innovation would render any report outdated by the time it appeared. AI requires a more agile approach to scientific assessment by continually evaluating the technology’s evolving capabilities and their ramifications. Streamlining the assessment process for AI will also require policymakers to make difficult trade-offs between inclusivity and efficiency, particularly when it comes to who determines who should participate in these assessments and how. To reflect the entire range of AI risks and opportunities, any intergovernmental assessment would ideally draw on experts from countries at all income levels and from a wide range of disciplines, including non-technological fields such as ethics, law, and the social sciences. At the same time, such a body will need to move with dispatch. One plausible scenario would be to create separate panels to assess specific dimensions of the AI challenge, analogous to the three panels created under the Montreal Protocol, for a more efficient division of labor. In addition, policymakers should consider including a horizon-scanning function that alerts the international community in real time to emerging dangers and dilemmas, akin to what the Financial Stability Board does for the global financial system.72 This function should include regularly shared risk assessments based on an agreed-upon risk classification system.\n\nSecond, policymakers need to decide on a governance model for the assessment body or bodies for AI and determine the precise role of the private sector. Many representatives from governments, technology companies, academia, and civil society endorse a multistakeholder approach to AI governance, without specifying how this objective should influence the institutional design of any future assessment panel. The IPCC is formally intergovernmental; it is a UN body, and member states oversee how it functions and approve the final report. At the same time, it has multistakeholder dimensions: its authors and reviewers participate in their individual capacities, are nominated and selected based on their expertise, and can come from industry and civil society, not simply academic or government research institutions.73 For an AI assessment panel, is this the model that those who advocate a multistakeholder approach have in mind? Or should the oversight and governance of an AI panel also be multistakeholder, and, if so, how would that be designed?\n\nThe answers could have important implications for AI regulation. Since private industry leads the research and development of advanced AI, any scientific body needs to negotiate standing arrangements with major technology platforms to gain access to and information about the latest models. In addition, private sector actors presumably need to be represented (and perhaps heavily so) in the authorship of assessment reports. But if the panel’s purpose is to provide policymakers with information that will, in part, contribute to regulatory decisions, then governments may want to maintain ultimate oversight.\n\nFinally, a core objective of any scientific assessment body should be to advance transparency in AI’s research and development. Accordingly, governments should assign to this body a role as a registry or clearinghouse for up-to-date information about advanced civilian AI research and development, including any recent breakthroughs in capabilities. (Such information sharing is inherently more challenging when it comes to potential miliary applications, as discussed later.) This clearinghouse role would help compensate for any time lag between the body’s periodic assessments, allow the world to keep abreast of rapid AI innovations, and facilitate multilateral cooperation by reducing uncertainty. Analogous proposals have been made in other fields involving transformative or unconventional technologies, such as solar geoengineering. Some of them have even been implemented, notably the Biosafety Clearing-House created under the Cartagena Protocol on Biosafety to share information on genetically modified organisms.74\n\nAt present, no single international institution exists that can be adapted to fulfill the governance role of building scientific understanding on AI. The twenty-nine-nation Global Partnership for AI, officially launched in June 2020 with sponsorship by Canada and France and now chaired by India, was initially envisioned as an IPCC-like body but has not yet achieved an authoritative role.75 It is unlikely to do so in the near term, in part because membership to the partnership is contingent on endorsing the OECD AI Principles, limiting its expansion to more countries. A more likely path to a scientific body—reflected in the HLAB interim report—may be to create a new standing institution out of the ad hoc effort to produce the report on the state of AI science.\n\nSetting Standards and Harmonizing Regulations\n\nA second core function of a regime complex for AI should be to establish common standards for its responsible development and use by state and nonstate actors and to harmonize the regulations emerging from domestic jurisdictions. This should eventually include mechanisms to monitor the implementation of and verify compliance with agreed-upon standards, even if they are voluntary and nonbinding.\n\nRecent declarations to promote international AI principles—emerging from political platforms as diverse as the G7, OECD, UNESCO, G20, China’s Ministry of Foreign Affairs, and the UK AI Safety Summit—employ the same assortment of high-minded adjectives.76 “Ethical,” “responsible,” “trustworthy,” “human-centered,” “transparent,” “safe,” “accountable,” and “fair” are prominent examples. This consensus suggests that these high-level principles are relatively settled, until they need updating to reflect advancements in AI. Yet this unanimity on the surface about aligning AI with human values belies the fragmentation already characterizing domestic regulatory approaches. It also overlooks the divergences in values among some major powers, which will likely increase the attractiveness of parallel minilateral forums, in which narrower like-minded coalitions can push for higher-quality or alternative standards.\n\nAs more countries develop and implement domestic rules and frameworks to manage AI risks and opportunities—beyond major powers like the EU, China, the United States, and India—pressure will grow to translate existing high-level multilateral principles into common standards and harmonize disparate national regulations, as the HLAB interim report highlighted. One jurisdiction’s regulatory approach is unlikely to become a de facto global benchmark, given the pluralism of current preferences.77 In the meantime, some experts have recommended creating a new UN-affiliated specialized agency to address this burgeoning regulatory fragmentation.78\n\nProposed Institutional Models\n\nTwo specialized UN agencies often invoked as models for harmonizing AI standards and benchmarks are ICAO and the International Maritime Organization (IMO). Each provides an intergovernmental forum to set regulatory and technical standards for a specific domain and encourage their implementation. Established in 1947 after the entry into force of the 1944 Convention on International Civil Aviation, ICAO promotes safe international air transport, including by setting technical standards and developing best practices, monitoring their domestic implementation, and supporting aviation capacity building, based on assessments prepared by representatives and industry experts from the body’s 193 member states.79 IMO, which opened its doors in 1958 after its 1948 convention went into effect, fosters cooperation among its 175 member states on international shipping, with a focus on developing standards and providing technical guidance on the implementation of measures related to issues like safety, security, pollution, and efficiency.80\n\nThe idea of designing a similar multilateral agency to set global AI standards and harmonize domestic AI regulations has proven attractive. First, like civil aviation and shipping, AI is a transnational phenomenon with cross-border effects. To ensure AI safety, security, and efficiency, both the private sector and national governments need to conform to a set of minimum global standards in AI’s development and use. Like airlines and shipping companies, AI technology companies operate across domestic jurisdictions, and they rely on interoperable standards for their own self-governance, policy development, and supply chains. Regulatory harmonization on AI is also imperative for governments. It would help ensure that high-standard jurisdictions are not placed at a competitive disadvantage compared to low-standard ones. Such harmonization would reduce opportunities for companies to engage in regulatory arbitrage by gravitating to jurisdictions with lax standards, and it would discourage the emergence of regulatory black holes that could be exploited by nefarious state and nonstate actors.\n\nSecond, the ICAO/IMO model also includes supervisory and monitoring mechanisms to track compliance and ensure accountability. ICAO, besides providing robust guidance on the implementation of standards, conducts safety and security audits on the actions, including legislation, that countries are taking to implement standards.81 (It does not audit industry actors.) The results of safety audits are made public, whereas security audit results remain confidential among member states. In cases of noncompliance, states can use dispute settlement mechanisms to suspend the voting power of another state and, in principle, impose UN General Assembly or UN Security Council sanctions.82 Additionally, if an airline is found to be violating rules, then member states are all expected to prohibit it from operating in their airspace. By contrast, IMO has a softer monitoring role. Although audits of national implementation of standards became obligatory in 2016 (after previously being voluntary), IMO lacks a dispute settlement mechanism or even minimal enforcement provisions.83\n\nLimitations of Proposed Models\n\nAt the same time, the ICAO/IMO model has significant limitations. First, AI is not limited to a single domain but is rather a general-purpose technology with the potential to affect all aspects of society, akin to electricity. The world thus needs multiple sets of standards to address both AI’s technical dimensions and its development and use in various domains. Rather than seeking to create a single, new, all-encompassing standard-setting body to govern AI’s application to an ever-expanding set of specific use cases, the goal must be to make existing international institutions as AI literate as possible, as soon as possible. This will require promoting international dialogue on domain-specific AI standards so they can be developed and implemented in particular fields.\n\nMuch global standard setting is highly technical, focused on the establishment of universal benchmarks, guidelines, measures, or models that facilitate international coordination.84 Classic examples include the establishment of standard time, mutual recognition of internet protocol (IP) addresses generated by the Internet Assigned Numbers Authority, or the creation of standards for capital adequacy determined by the Basel Committee on Banking Supervision.85 As AI permeates the global economy, demand will increase for common standards in its applications, as well as guidelines for assessing its risks and testing the effectiveness and quality of AI systems.\n\nThis technical work will be relatively straightforward, focused on the interoperability of technologies across jurisdictions.86 Some of these details are already being worked out under the auspices of the International Organization for Standardization, a nongovernmental entity comprising 170 national standards bodies that seeks to forge international consensus on market-relevant global standards.87 Other “sector-specific,” or “vertical,” standards are being produced through the more narrowly focused International Electrotechnical Commission, though the commission is also collaborating with the International Organization for Standardization to create “generic,” or “horizontal,” standards.88 Government entities, including the U.S. Commerce Department’s National Institute of Standards and Technology, are also ramping up these efforts.89\n\nIn parallel with technical standard setting, the world needs sector-specific international standards for the development and use of AI applications, which can inform governments as they develop national regulatory policies. This is analogous to what is already happening at the national level. For instance, the White House’s October 2023 Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence directs individual U.S. government agencies and departments to come up with guidelines on the application of AI in sectors as diverse as employment, financial services, healthcare, and transportation.90 At the multilateral level, too, existing institutions are beginning to develop sector-specific guidelines. The IAEA is starting to work on the application of AI in nuclear science, power, safety and security, and verification, while the World Health Organization (WHO) has begun to develop standards for use of AI in global health. ICAO could likewise be tasked with catalyzing standards for use of AI in civil aviation, building on EU efforts.91 To gain diplomatic traction and exert practical effects, such standard-setting efforts should not be limited to the purview of agency secretariats; they should also involve dialogue among member states.\n\nSecond, the ICAO/IMO model is intergovernmental rather than multistakeholder. This is potentially problematic in the case of AI because the private sector dominates the design, development, and distribution of the technology and its applications. Although the phrase global governance conjures images of formal intergovernmental organizations, in practice many global regulatory and standard-setting bodies are multistakeholder.92 Constituting an organization in this way would grant representatives of the private sector and civil society organizations a role alongside participants from governments and international agencies in the processes of agenda setting, negotiating, implementing, monitoring, and enforcing or evaluating relevant standards and regulations. This approach may hold lessons for AI.\n\nAmong the most prominent examples is the International Labour Organization (ILO), established in 1919 and now with 187 member states.93 It sets labor standards through conventions and nonbinding agreements and provides technical assistance, helping countries draft domestic legislation to meet international obligations.94 Unlike ICAO and IMO, the ILO has a tripartite governance structure involving governments, workers, and employers.95 Each member state delegation to the ILO’s governing body, the International Labour Conference, must include an employer and worker representative, alongside two representatives from the country’s government.96 Another prominent multistakeholder regulatory and standard-setting entity is the Internet Corporation for Assigned Names and Numbers, an independent nonprofit charged with coordinating maintenance of the internet’s Domain Name System of unique identifiers.97 The nonprofit’s Governmental Advisory Committee comprises representatives from 179 sovereign nations and dozens of international organizations. Still, as these examples show, adopting a multistakeholder design is not a complete solution. For AI, a key challenge would be determining the degree and form of representation for each type of party (including governments, the private sector, and civil society).\n\nThe third major limitation of the ICAO/IMO model is that it is grounded in a near-universal membership framework, by way of international treaty law. As such, it is not well-suited to current geopolitical conditions, particularly the resurgence of strategic rivalry and ideological competition between the democratic West and authoritarian China and Russia, to say nothing of the hurdles to treaty ratification (not least in the United States). In this context, high-level intergovernmental standard-setting efforts for AI are likely to unfold along at least two parallel tracks: all-encompassing UN-affiliated settings and narrower coalitions of like-minded participants.\n\nThe UN and its agencies will inevitably foster global dialogue on evolving principles, norms, and standards for the responsible use of AI and will promote greater transparency and harmonization of national regulations. Historically, the global body has played an important norm- and standard-setting function on topics ranging from sustainable development to the prevention of atrocities, drawing on the unparalleled legitimacy conferred by its universal membership and the binding UN Charter.98 The current UN secretary-general, Guterres, has tried to do the same for AI. The HLAB he appointed is slated to prepare its final recommendations for consideration by the UN General Assembly at the Summit of the Future in September 2024. Although “the UN cannot and should not seek to be the sole arbiter of AI governance,” as the HLAB acknowledges, the General Assembly can help promote coherence by negotiating and passing a declaration of principles for the development and use of AI.99 Properly crafted, such a resolution could play a role similar to the nonbinding Universal Declaration of Human Rights, which laid the foundation for subsequent international legal conventions.100 Furthermore, the General Assembly can ensure that standards are consistent with, and where possible take direction from, established international law and norms. Still, current geopolitical fragmentation suggests that any universal approach is likely to reflect lowest-common-denominator outcomes and hardly go beyond UNESCO’s Recommendation on the Ethics of AI, adopted by all 193 member states in 2021.101\n\nBeyond endorsing principles and norms, the UN can foster regulatory harmonization through its technical agencies. One promising approach would be to require that countries self-report domestic AI regulations to an international clearinghouse, much as trading nations are obligated to declare subsidies to the World Trade Organization.102 A possible repository of this information is the International Telecommunication Union, the world’s oldest intergovernmental organization, which already coordinates and reports on AI-related activities conducted across UN entities.103 This notification process could support transparency and eventually lead to greater multilateral agreement on rules.\n\nWhile it is worth pursuing universal approaches, the UN’s standard-setting and regulatory role is still expected to be limited. Ideological differences among major powers may thwart meaningful global consensus, perhaps making it so that resolutions on AI norms and standards reflect the lowest common denominator. As countries translate their stated commitments into actionable standards and regulatory schemes, fissures between open and closed societies are likely to loom large and at times be insurmountable. For example, the advanced G7 market democracies are unlikely to concur with authoritarian China and Russia on the “ethical” or “human-centered” standards that should inform the use and export of AI tools for mass surveillance and censorship, or even on how to reduce algorithmic bias. (China and Russia signed on to UNESCO’s Recommendation on AI Ethics, which states, “AI systems should not be used for social scoring or mass surveillance purposes,” but this is only a voluntary commitment, lacking an enforcement mechanism.)104\n\nAccordingly, the community of advanced market democracies may well pursue parallel minilateral efforts to adopt higher regulatory standards and more demanding monitoring provisions. For instance, the G7 could set up a body to assess national implementation of the nonbinding Hiroshima Code of Conduct among endorsing countries. In this scenario, the G7 would treat any emerging UN standards as a floor while separately pursuing higher standards—with the aspiration to eventually globalize the latter. The United States and other Western nations are no strangers to this approach, having created high-ambition coalitions in other domains and then subsequently inviting other countries to join. The Proliferation Security Initiative, the Artemis Accords, and the Declaration for the Future of the Internet are notable examples.105\n\nAmong the most successful such ventures is the Financial Action Task Force (FATF).106 In 1989, the G7 established the FATF to combat money laundering, and the body’s mandate was later expanded to combat the financing of terrorism and of the proliferation of weapons of mass destruction. Over time, the FATF has developed widely accepted standards that it uses to monitor the strength of countries’ legal and regulatory structures and classify countries as cooperating or noncooperating jurisdictions, depending on whether they implement sound practices. Although this is an informal arrangement, the FATF’s designations have real bite, as private financial institutions tend to reduce their exposure to blacklisted countries. The IMF and the UN Security Council have since accepted FATF standards, demonstrating how a club of like-minded nations can elevate overall global norms and standards. An analogous framework for AI could similarly classify and rate jurisdictions to discourage private corporations from operating or investing in low-standard jurisdictions, limiting their economic prospects.107\n\nOne risk in the minilateral approach is that rival coalitions—for instance, the BRICS (Brazil, Russia, India, China, and South Africa) or the Shanghai Cooperation Organization—could mimic this method, accelerating the fragmentation of the global economy and world order. A related danger is that such an approach could deepen divisions between the Global North and the Global South, if it came across as another attempt by privileged powers to impose their standards on poorer players. To avoid this hazard, advanced market democracies need to collaborate with democracies in the Global South in the development of AI standards and regulatory harmonization. India, which has staked out a distinctive regulatory approach for AI that balances opportunity with rights, will be an important partner in any such effort.108\n\nFinally, Western countries will have to overcome some of their own divisions if they are to advance common standards, an obstacle exemplified by ongoing debates about the Council of Europe’s Framework Convention on Artificial Intelligence, Human Rights, Democracy, and the Rule of Law.109 The draft treaty—which if finalized would be the first binding international AI convention—offers the West an opportunity to present a united front. Yet that outcome depends on whether negotiators can resolve big disagreements among both the council’s forty-six member states and observer nations like the United States, including over whether the treaty applies to the private sector.110\n\nUltimately, an effective regime complex for AI needs to include mechanisms for monitoring and verifying compliance with standards negotiated in multilateral or minilateral settings. While a robust global regulatory scheme grounded in international law remains a distant prospect, lessons from other domains suggest that well-designed frameworks can help governments verify and improve compliance with nonbinding standards. Examples include the UN Human Rights Council’s process of Universal Periodic Review, through which countries assess each other’s human rights records; the obligation of ILO members to submit periodic self-assessments of compliance to the International Labour Conference; and the Enhanced Transparency Framework under the Paris Agreement, which mandates that countries, starting in 2024, report on the mitigation and adaptation measures they have taken to address climate change.111 Similar self-reporting and peer review arrangements for AI standards may be feasible even for countries that are not inherently like-minded.\n\nSharing Access and Benefits\n\nA third function of the regime complex for AI should be to expand access to and the sharing of benefits from this technology, as its development and use remain concentrated in a few advanced economies. About 2.6 billion people, or approximately one-third of the global population, are still unconnected to the internet, so ensuring that all countries and citizens can benefit from AI will be no small feat.112 Yet doing so is critical. The world is badly off track in terms of meeting the UN Sustainable Development Goals (SDGs), a set of globally agreed-upon objectives for bettering the human condition by 2030, and AI has the potential to alleviate or exacerbate this reality.113\n\nThe applications of AI for achieving the SDGs appear almost limitless, including AI’s potential to deliver high-quality education and healthcare services, advance early warning systems for extreme weather events, make agriculture more climate resilient, and support biodiversity and ecological conservation.114 Yet this outcome is not guaranteed. The countries leading in AI development have a moral obligation, and practical incentives, to share its benefits. Supporting critical investments in the Global South is a start, since across these otherwise heterogenous countries, AI has been perceived as a beacon of opportunity. (This view contrasts with the focus on risk in the Global North.)115\n\nTraditional development actors—including intergovernmental organizations, national development agencies, and nongovernmental organizations—are already using AI applications to improve service delivery, especially for SDG-related projects. For example, the UN Development Programme has leveraged AI tools to identify trends in hate speech in Sudan and electoral misinformation in Zambia and Honduras, strengthen government policy evaluation in Mexico, and accelerate cash transfer processes for beneficiaries in Togo and Bangladesh.116 The UN Office of the High Commissioner for Refugees has used AI-based predictive analytics to improve the agency’s responses by forecasting movements of internally displaced people in Somalia and of Venezuelan refugees and migrants into Brazil.117 The U.S. Agency for International Development has also funded projects for machine learning applications in sectors like healthcare, democracy and governance, humanitarian response, and education.118\n\nMore ambitious capacity-building efforts are needed to promote broader access to AI technology itself, not just the delivery of benefits derived from its use. As the HLAB’s preliminary report highlighted, this entails distributing to low- and middle-income countries (LMICs) key AI inputs like computational power (compute)—the hardware and software needed to build new models—data, and existing models.119 Supporting training for public and private sector workers on the development and use of AI applications is also essential. One recent initiative in this mold is the UK-led AI for Development Programme with the United States, Canada, and the Bill and Melinda Gates Foundation. This coalition has committed to funding $100 million in programs across the African continent to “support home-grown AI expertise and computing power” and “solve some of the developing world’s most pressing challenges.”120 An ancillary goal is to support the creation of “sound regulatory frameworks for responsible, equitable, and safe AI.” Such a comprehensive approach is critical to combating the digital divide and ensuring that developing countries and their citizens gain AI capabilities.121\n\nAmong other objectives, this agenda for cooperation on AI development should seek to promote joint science and technology research, invest in capacity-building initiatives for entrepreneurs and programmers creating new models and applications, and promote skills training for workers whose economic prospects are likely to depend on AI literacy. Such cooperation should also enhance data collection programs so that models are trained on data that are more culturally, linguistically, and geographically diverse, and it should support governments as they design domestic regulations and increase their own use of AI applications.122\n\nTo be sure, AI capacity-building strategies need to be tailored to specific national contexts, since any society’s ability to leverage AI depends, in part, on situational factors like government effectiveness, digital infrastructure conditions, and human capital levels.123 Still, the nations that lead in AI capabilities ought to make an ambitious, formal commitment to share the benefits of AI, including as part of the Global Digital Compact that UN member states are currently negotiating.124 This action is particularly prudent and warranted, given the impression in many developing countries—one compounded by the experiences of the COVID-19 pandemic and the deepening global climate emergency—that wealthy nations are fickle partners in tackling shared challenges and indifferent to the plight of the world’s less fortunate.\n\nProposed Institutional Models\n\nTwo broad institutional models have been proposed to help expand access to AI technologies and financing for them.125 One is modeled on global health partnerships and another on existing arrangements for peaceful uses of nuclear energy. Among other differences, they diverge on the degree of conditionality, if any, that should govern access to relevant financing, technology, and applications. The ultimate institutional framework for AI-related benefit sharing will likely fall in between these two templates, with moderately conditional access to AI.\n\nThe first model is based on public-private partnerships in global health, in particular the Global Alliance for Vaccines and Immunizations (Gavi) and the Global Fund to Fight AIDS, Tuberculosis, and Malaria (hereafter the Global Fund). This model suggests a potential way to unlock financing and make it possible for LMICs to access markets and innovative technologies.\n\nScarce financing presents a major hurdle to developing nations’ ability to obtain equitable benefits from breakthroughs in AI. Beyond placing existing technology out of reach for many LMICs (including when it comes to licensing), inadequate financing constrains the ability of private corporations in developing countries to obtain inputs for creating new applications and potentially their own models. This scarcity of financing also limits the capacity of LMIC governments to expand digital public infrastructure and launch upskilling programs so that their citizens can take advantage of the AI revolution. Complicating matters further, the profit incentives of leading AI companies may not steer them toward designing applications that address problems predominantly affecting LMICs. For these reasons, some researchers have cited public-private partnerships for public health interventions in developing countries as possible models for expanding global access to AI.126\n\nGavi was founded in 2000 by the Bill and Melinda Gates Foundation, the WHO, the UN Children’s Fund (UNICEF), and the World Bank to expand immunization programs in low-income countries, particularly for diseases like malaria, pneumonia, and rotavirus.127 Gavi continues to make vaccines more affordable for countries by negotiating prices with manufacturers or sharing the costs with governments. For the malaria vaccine, Gavi’s advanced funding commitments ensured that manufacturers continued production.128 As a funding mechanism, Gavi’s country-based operations are conducted by domestic health ministries, alongside the WHO. Founded in 2002, the Global Fund is a similar form of collaboration among international organizations, philanthropic foundations, and national governments.129 It allocates funding based on proposals and implementation plans submitted by countries after multistakeholder consultations.130 The fund itself does not maintain an in-country presence, instead relying on existing national and international public health actors.\n\nA comparable funding and partnership approach for AI offers a potential pathway for developing countries—and domestic private sector actors—to obtain access to existing AI products (many of which are developed with proprietary technology). Such an approach could also support the development of indigenous AI models, applications, computing capabilities, and human capital. Moreover, this type of financing mechanism would afford partner countries autonomy in the design and implementation of country-specific programs, allowing host governments to deploy such capacity-building resources as they see fit. (Some might prioritize developing digital public infrastructure or enhancing data collection, whereas others might prioritize accessing computing power for domestic technology firms.) Finally, this multistakeholder model would involve a role for international development agencies, multilateral development banks, governments of donor countries and developing countries, the private sector, and both global and domestic civil society actors.\n\nHowever, unlike the provision of vaccines and medicines, the introduction of AI technologies and systems could have some destabilizing consequences. International donors are thus likely to insist that initiatives to spread this technology be accompanied by safeguards. An alternative model is a conditional access approach that includes provisions to guard against misuse, loosely analogous to the peaceful uses of nuclear energy pillar of the Treaty on the Non-Proliferation of Nuclear Weapons (NPT).131 Like the public health model, it recognizes the importance of expanding access and benefit sharing but with the caveats that beneficiaries must show restraint and conform to other obligations (such as abstaining from the pursuit of nuclear weapons). For AI, a contingent access framework would require beneficiaries of relevant technologies, applications, compute, financing, and other tools to conform to safety standards and refrain from certain kinds of development and use.\n\nThe NPT is based on a core bargain, whereby non-nuclear-weapons states agree not to acquire such weapons in return for a pledge by the five acknowledged nuclear-weapons states to pursue nuclear disarmament and share the benefits of access to peaceful nuclear technology.132 As a model for sharing AI’s benefits, the NPT’s most relevant element is Article IV, which establishes an “inalienable right” of all parties to “develop research, production and use of nuclear energy for peaceful purposes.”133 That article further commits treaty parties, especially nuclear-weapons states, to ensuring that all nations have access to the equipment, materials, and scientific and technological information required to pursue “nuclear energy for peaceful purposes . . . with due consideration for the needs of the developing areas of the world.” For more than five decades, Article IV has provided an international legal foundation for the transfer of nuclear technology and material to NPT member states to develop safe civilian nuclear energy programs, contingent on having safeguards that meet IAEA standards—to ensure that these inputs are not being diverted to nuclear weapons programs.\n\nIn recent years, the IAEA has enhanced its financing and technical assistance for this conditional access regime, launching the Peaceful Uses Initiative in 2010 to secure extrabudgetary funding for peaceful use projects and the COMPASS program in 2020 to support capacity building on safeguards implementation.134 In addition, the IAEA’s nuclear fuel bank provides low-enriched uranium to member states in circumstances where they are unable to secure it, provided safeguards are in place.135 The IAEA also maintains a Global Nuclear Safety and Security Network, a knowledge- and capacity-building hub for countries with limited nuclear energy programs, so that those programs remain secure and in line with standards.136\n\nLimitations of Proposed Models\n\nWhen it comes to benefit sharing, the regime complex for AI is expected to fall somewhere along the continuum of unrestricted to restricted access. Countries leading in AI innovation will seek to balance the development imperative with safety concerns, tying financing for capacity building with regulatory support, as is the case for the UK’s AI for Development Programme. This will likely entail that recipient countries agree to certain safeguards, though what these should look like, how they should be established, and how stringent they should be all will be matters for debate. At the very least, safeguards will not be as strict or robust as those associated with the NPT because nuclear energy has a clearer link to a catastrophic weapon than AI does with any potentially destabilizing uses.137 Beyond this general point, AI presents challenges very different from both nuclear energy and global health. These models cannot simply be replicated as policymakers seek to design an AI benefit-sharing framework.\n\nFirst, the success of the NPT model owed much to the dynamics of mutually assured destruction, which persuaded the world’s two main nuclear powers—the United States and the Soviet Union—to pursue nonproliferation and arms control, including by curbing the availability of nuclear material. With no alternative provider, countries seeking access to peaceful nuclear energy were forced to accept stringent conditions for access.\n\nIt is uncertain today whether China and the United States, given their intense geopolitical and geoeconomic competition, could reach agreement on establishing parameters for granting developing countries access to advanced AI models and applications, or even reach consensus on what aspects of these technologies could be destabilizing. And if they cannot, the United States and other Western countries will need to think carefully about whether to impose their own strict conditions, given the risk that this could push countries toward China as an alternative supplier. (Control of AI’s broad suite of technologies—and the knowledge associated with their development and use—is also less straightforward than control of nuclear weapons, a challenge further elaborated on in the following section on collective security frameworks.)\n\nMore generally, disagreements between major powers in the standard-setting sphere could spill over into the design of benefit-sharing arrangements, resulting in a fragmentation of approaches to governing access. Divisions between democratic and authoritarian powers are likely to be especially salient, given the different approaches of Western countries and China toward development cooperation and aid conditionality. Whereas Western donors continue to condition their assistance on commitments to good governance, human rights, sustainable environmental policies, and the like, China has sought to distinguish itself through its no-strings-attached (albeit mercantilist) stance, not least through its spree of financing for infrastructure projects throughout the world by way of the Belt and Road Initiative.\n\nWith AI, China will presumably commit to respecting countries’ sovereignty over how they employ these technologies. Indeed, Chinese surveillance technology, empowered by AI, is spreading abroad.138 Western nations, by contrast, are more likely to insist that partner nations develop and use AI in ways that support democracy and human rights. Once again, however, they must walk a fine line, seeking to incentivize rights-based AI governance while recognizing that strict conditionality may drive other countries into China’s embrace.139\n\nA second limitation is that the eventual global framework to finance and support AI access and benefit sharing must be designed to be inclusive, equitable, and non-extractive. Current AI models and related algorithms are not always trained on globally representative data, which can limit their utility and appropriateness in contexts beyond where they are developed.140 This is an issue that broad access approaches do not address.\n\nCapacity-building efforts to support the development of AI models and applications in more countries will help compensate for this problem, but the emergence of such indigenous capacity will take time. Major technology companies in wealthy nations will continue to drive much AI development, and these firms will need to gain access to more diverse data to develop better and more accurate models and applications. This process is inherently extractive, even if the end result winds up better serving the interests of populations in developing nations. This dynamic poses an ethical dilemma—how to compensate the people of developing nations for the use of their data, so that they can share in the economic and other benefits derived from its use (benefits that may otherwise be concentrated in the nations where major technology companies are based).\n\nOne institutional precedent worth exploring is the Nagoya Protocol on Access and Benefit-sharing, which was signed in 2010 and entered into force in 2014 as a supplementary international agreement to the Convention on Biological Diversity.141 The protocol was designed to ensure the fair and equitable sharing of benefits arising from the use of genetic resources, as well as traditional knowledge associated with them. The protocol establishes state obligations with respect to access to and use of these resources (including rules for prior informed consent and the issuance of permits), benefit sharing (including compensation in the form of royalties, knowledge, or technology transfer), and compliance.142 Like many multilateral treaties, it seeks to balance the interests of developing and developed nations, in this case by ensuring that countries and communities in the Global South, where much biodiversity is located, share in the material benefits from any commercial and other exploitation of genetic resources, a domain dominated by companies from the wealthy Global North.\n\nThe Nagoya Protocol provides one possible model for a future framework for AI data governance that would allow national governments, particularly of developing countries, to regulate data harvesting by foreign technology companies and related organizations. Such an effort would admittedly be complicated, in part because while natural resources are considered a sovereign resource under the Convention on Biological Diversity, there is less agreement that data possess the same status.\n\nA third dimension left out of broad access models is the role of joint scientific research. The countries currently leading the development of AI can help improve access and benefit sharing by launching a robust program of cooperation between the Global North and the Global South on scientific research, designed to support capacity building in countries that are hungry to translate technological innovation into development outcomes. This cooperation could seek to replicate existing international models of joint scientific research, such as CERN, the world’s largest particle physics laboratory, or the International Thermonuclear Experimental Reactor, which seeks to build the world’s largest fusion device. In this case, however, the emphasis would be on collaboration between developed and developing countries, similar to the World Climate Research Programme under the auspices of the WMO.143 This approach could provide another way to incentivize agreement on safe, rights-based standards and promote access, without imposing strict conditions.\n\nA final point merits emphasizing. Ensuring AI access and benefit sharing among countries of the Global South is not the same as ensuring participation by those same countries in the global governance of AI, and promoting one of these objectives does not guarantee the other. One opportunity to enhance the influence of developing nations in global AI governance is in the Global Digital Compact, a set of “shared principles for an open, free and secure digital future for all,” slated to be submitted to the UN General Assembly for approval at the 2024 Summit of the Future.144\n\nPromoting Collective Security\n\nThe fourth function of any international regime complex for AI should be to promote collective security as the proliferation of AI reshapes this domain, amplifying existing risks and introducing new, potentially catastrophic, ones.145 Given the fractious geopolitical environment and competing interpretations of what constitutes collective security­—and how AI will affect it­—pursuing this mandate will require diverse mechanisms that seek to deter the malicious use of AI, including by mitigating dual-use risks; prevent a destabilizing AI arms race, through confidence-building measures (CBMs) and other means; and create safeguards, trip wires, and contingency-planning mechanisms to address emerging threats.\n\nTo start, breakthroughs in AI have turbocharged long-standing debates about lethal autonomous weapons systems (LAWS), as these systems transform military affairs.146 Having reached the battlefields in the wars in Ukraine and Gaza, LAWS are increasingly contributing to use of force decisions and actions, and other AI applications are becoming integrated into broader functions like command and control; surveillance, intelligence, and reconnaissance; logistics and training; and information management.147 The arrival of fully autonomous systems portends a paradigm shift in warfare, as countries employ systems that can “identify, track, and prosecute targets without human oversight.”148\n\nDespite fervorous multilateral diplomacy to negotiate legally binding restrictions on LAWS, a treaty that bans or severely limits the use of autonomous weapons is not realistic at present.149 The United States and other major military powers have resisted efforts to restrict the use of LAWS, instead forging ahead with developing these systems, which they deem critical to lessening the destruction of war and, implicitly, maintaining their own military competitiveness. Given these trends, efforts to reach agreement on basic principles of use, especially regarding how existing international humanitarian law applies, hold more promise than a formal arms control treaty.150\n\nWhile the development and deployment of LAWS and other military applications of AI are reshaping the battlefield, the challenges that AI poses to global collective security are far broader.\n\nAn effective regime complex for AI will need to encompass higher-level institutional frameworks that allow the international community to deter and respond to potentially destabilizing uses of these technologies by state and nonstate actors—and perhaps by AI itself.\n\nAI threatens to undermine collective security in at least three ways, beyond LAWS. First, AI will increase the availability and lethality of all weapons—including weapons of mass destruction.151 It will make it easier for state and nonstate actors to design novel pathogens for biological warfare, turn drugs and compounds into more effective chemical weapons, and build more powerful and precise nuclear weapons. The AI revolution will also enable more sophisticated malware, leading to more potent cyber attacks on civilian and defense infrastructure.\n\nSecond, the pace of AI innovation will exacerbate geopolitical competition, as major powers engage in an arms race to dominate this technology and translate these advantages into military supremacy.152 Automated warfare, combined with AI-enabled disinformation, could increase the risk of major power conflict due to escalation, miscalculation, loss of command and control, or algorithm-determined retaliation to a preceding attack.\n\nThird, the rapid advancement of AI capabilities could pose serious, even existential, risks to our species. Such a scenario may seem far-fetched and remains hypothetical, but intense technological competition to develop cutting-edge models and applications could generate selection pressures for selfish AI traits, analogous to biological evolution.153 In principle, these dynamics could encourage the emergence of super intelligent AI that acquires powerful capabilities and uses manipulation to pursue objectives misaligned with its creators’ intentions or survival.\n\nUnfortunately, incentive structures may work against robust international cooperation to address these threats, as both governments and corporations compete with counterparts to reap the technology’s rewards. Zero-sum thinking may lead major AI powers to prioritize short-term gains over long-term global peace and stability, exacerbating the security dilemma inherent in world politics.154 Cutthroat commercial competition could likewise encourage leading AI companies to sacrifice safety for innovation—a risk Silicon Valley itself recognizes. In March 2023, 1,000 technologists and other experts advocated a six-month pause in “the training of AI systems more powerful than GPT-4,” warning that private labs were “locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one—not even their creators—can understand, predict, or reliably control.”155\n\nProposed Institutional Models and Their Limitations\n\nAnalysts have proposed various institutional responses to the diverse security risks of AI, often drawing on analogies from other fields, including arms control and nonproliferation.156 These models include multilateral frameworks for inspection, verification, and enforcement; for the control of exports of dual-use technologies; and for early warning and crisis response. While some models hold relevant lessons, many analogies begin to fall apart when one looks more closely at the distinctive security challenges posed by AI.\n\nIAEA-Type Organization\n\nThe most frequently cited model for addressing AI’s security risks is the IAEA, as noted earlier. In May 2023, OpenAI, the creator of ChatGPT, advocated the establishment of a new international organization based on the IAEA to regulate the pursuit of “superintelligence.”157 Under this proposal, companies or governments that pursue AI capabilities beyond a threshold would “need to be subject to an international authority” empowered by the world community to enforce safety standards, conduct audits, launch inspections, and restrict deployments that endanger security. The UN secretary-general quickly endorsed the idea.158\n\nEstablished in 1957 as an autonomous agency within the UN system, the IAEA was designed to facilitate access to nuclear energy while serving as a nuclear weapons watchdog by ensuring member states’ compliance with the NPT.159 To carry out this mission, it conducts safeguard inspections of civilian nuclear facilities and verifies that fissile and other materials are not diverted to clandestine nuclear weapons programs. The 178-member body reports to the UN Security Council and UN General Assembly. Although the IAEA has had notable failures, particularly regarding North Korea, its overall track record in exposing noncompliance and limiting nuclear proliferation—including thus far by Iran—is impressive.\n\nSince AI’s capabilities are similarly dual-use, with the potential for globally destabilizing military applications, the appeal of the IAEA model is obvious. Emulating its nearly universal membership status is also compelling, as it could underpin the legitimacy of the proposed organization’s mandate to promote collective security.\n\nYet the enormous differences between AI and nuclear challenges limit the utility of this governance model.160 First, the opportunities and dangers AI presents are less straightforward than those posed by nuclear weapons. The latter are difficult to build, require a narrow set of material inputs (including fissile material) that are hard to procure, rely on sophisticated technologies and manufacturing processes, and entail ambitious, large-scale programs difficult for even sovereign governments to conceal. AI, in contrast, comprises a broad suite of general-purpose technologies that, except for the most advanced chips, are easily distributed and wide-ranging in their applications. Furthermore, its development is being driven primarily by private technology platforms with little profit incentive to slow innovation. At a minimum, any regime similar to the IAEA and NPT would need to specify the expectations and legal responsibilities of major technology companies.\n\nSecond, whereas nuclear weapons pose a concrete threat to humanity, the existential risk posed to artificial general intelligence remains (for now) theoretical, and there is no equivalent (yet) to the dynamics of mutually assured destruction that eventually induced nuclear restraint and arms control between the United States and the Soviet Union.161 Rather, there is a pell-mell race, particularly between the United States and China, to dominate this new field, further limiting prospects for negotiating a treaty and setting up an associated universal AI governance agency (at least in the short term).\n\nNon-treaty options offer an alternative route to reduce the potentially destabilizing consequences of AI competition. One avenue would be to negotiate basic codes of conduct that set parameters of responsible behavior for the application of AI in critical domains, such as nuclear weapons or cyberspace. Analogous codes have been proposed for other global issues. Given the difficulties of reopening the Outer Space Treaty, for example, which became effective in 1967, some nations have proposed a code of conduct for outer space activities.162 The United States, similarly, worked with its Arctic Council partners to draft the Ilulissat Declaration, setting out general principles of conduct in the Arctic.163\n\nIn parallel, major powers might borrow from arms control regimes by developing CBMs.164 Even in the absence of treaty-based monitoring and enforcement, CBMs, which are voluntary and can be formal or informal, have stabilized major power relations by reducing ambiguities and mistrust—and associated risks of escalation and miscalculation.165 During the Cold War, CBMs complemented nuclear arms control negotiations between the United States and Western Europe on the one hand and the Soviet Union on the other. Notable measures included the Moscow-Washington hotline following the Cuban Missile Crisis; voluntary observations and inspections of military exercises; and information exchanges on force deployments, weapons programs, and military budgets.166\n\nMilitary CBMs continue to be used globally and may occur unilaterally, bilaterally, regionally, and multilaterally in several forms.167 The UN Office for Disarmament Affairs lists five categories of measures that seek to facilitate trust: communication and coordination, observation and verification, military constraints, training and education, and cooperation and integration.168 For AI, suggested military CBMs beyond dialogues and codes of conduct include promoting shared testing and evaluation standards, sharing information on deployments of AI-enabled systems, and clarifying the expected behavior of autonomous systems, as well as the extent of their autonomy.169 More general CBMs, including among AI labs, have also been proposed, but these diverge from the traditional model.170\n\nMultilateral Export Control Regime\n\nBeyond establishing a new UN agency to serve as an AI watchdog and address dual-use risks, some experts have proposed a multilateral export control regime to control AI’s key inputs, including advanced semiconductors.171 This arrangement would be analogous to existing coalitional schemes for nonproliferation and arms control, particularly the Nuclear Suppliers Group, the Australia Group, the Missile Technology Control Regime, and the Wassenaar Arrangement.\n\nThe Nuclear Suppliers Group is a forty-eight-member voluntary regime established in 1975 to prevent proliferation by controlling the export of materials, equipment, and technology required to manufacture nuclear weapons.172 Members agree to adhere to and implement guidelines for responsible supplier behavior consistent with IAEA safeguards for both nuclear and nuclear-related exports and to exchange relevant information. The Australia Group is a similar arrangement established in the 1980s to control materials needed to produce biological and chemical weapons. It now has forty-two participating countries (plus the EU), who must adhere to common guidelines and control lists and adopt licensing measures.173\n\nThe Missile Technology Control Regime is an informal political understanding among thirty-five countries today; it was created in 1987 under G7 auspices to limit the spread of rockets and unmanned aerial vehicles.174 Members adhere to export control guidelines pertaining to specific equipment, software, and technologies. The Wassenaar Arrangement is a forty-two-nation regime established in 1996 to promote transparency and responsibility in transfers of conventional arms, plus related dual-use technologies, software, and goods.175 It meets annually to review an agreed-upon regulation list and exchange information on deliveries of nine categories of conventional weapons to nonmember nations.176\n\nAt first glance, this sounds feasible, particularly as some aspects of advanced AI technology remain highly concentrated. The United States, the UK, the EU, and China dominate the development of significant machine learning systems, and over 90 percent of specialized hardware chips are designed and produced in the United States, China, Japan, South Korea, and Taiwan.177 In principle, imposing tighter regulation of key inputs for advanced AI models, to ensure that they are available only to actors who meet certain international standards, makes sense.\n\nAny such regime would pose major dilemmas, however. A big one for Western governments is how to create an export control regime that both constrains and restrains China, depriving it of critical technology inputs while somehow also incentivizing its responsible behavior. Consider the export controls that the U.S. Department of Commerce implemented in October 2022, which limit China’s “ability to obtain advanced computing chips, develop and maintain supercomputers, and manufacture advanced semiconductors.”178 Updated provisions, added in October 2023, impose even greater restrictions on China’s access to advanced chips and semiconductor manufacturing equipment.179 Because this is a unilateral U.S. approach, however, China can still try to obtain functional equivalents of U.S.-sourced technology from other countries.\n\nTo close this gap, the United States is starting to globalize these export controls, including through agreements with Japan and the Netherlands.180 Some U.S. national security experts have called for making Washington’s “small yard, high fence” approach even more multilateral, notionally by updating (or even replacing) the Wassenaar Arrangement to restrict trade in the most advanced chips with stronger enforcement mechanisms.181 China is not a member of the Wassenaar Arrangement (or of the Australia Group, or the Missile Technology Control Regime)—though Russia is, adding a diplomatic complication. The risk in such a containment approach is that China will seek to undermine global export controls on AI, forming AI alliances with other irresponsible players.\n\nThe export control model presents two other complications. First, while chips and hardware might lend themselves to such an arrangement, it would be harder to enforce limits on advanced AI models, algorithms, and other digital inputs. Second, like the IAEA model, the success of any export control regime would require unprecedented government oversight of major AI industry players, including new tools for monitoring end users, to verify who uses what models and how.\n\nCrisis Preparedness and Response\n\nGiven the potential dangers posed by malicious use of AI, a number of analysts have proposed crisis monitoring, early warning, and response mechanisms, plus contingency planning based on institutional analogies from international finance and global health.182\n\nSuch mechanisms have often emerged in the wake of crises. The Financial Stability Board was created among G20 countries (who remain its members) in April 2009, during the global financial crisis.183 The board’s purpose is to develop and strengthen common standards for major cross-border financial institutions—including with respect to capital, liquidity, and risk management. Among other functions, it works closely with the IMF on early warning exercises related to financial instability.184 Unlike other global economic institutions like the IMF, World Bank, and World Trade Organization, it remains an informal arrangement, which is housed and funded by the Bank for International Settlements and is reliant on a memorandum of understanding among its members. The HLAB has invoked the Financial Stability Board’s “macro-prudential framework” as a promising prototype for a “techno-prudential model” with respect to AI.185\n\nA different global crisis, namely the COVID-19 pandemic, underscored the urgent need for a new global health surveillance system capable of rapidly detecting emerging (or reemerging) infectious diseases that could pose pandemic threats, a system accompanied by expanded WHO capabilities to orchestrate a quick response to global public health emergencies.186 In September 2021, the WHO launched the Hub for Pandemic and Epidemic Intelligence to support collaborative monitoring of disease outbreaks; that hub now has a presence in more than 150 countries.187 To assist with incident response, the WHO also maintains the Global Outbreak Alert and Response Network, comprising “over 250 technical institutions and networks” around the world.188\n\nDeveloping a similar global crisis-response mechanism for AI may be a more feasible avenue, in the short term, for cooperation on collective security. For now, the OECD tracks broadly defined AI incidents but is working on a direct reporting framework.189 Soon though, a more robust approach that goes beyond incident reporting will be needed, one that involves the participation of more countries and takes action in response to threats.\n\nOne conundrum for this crisis preparation and response model is how to address the question of existential risk. In May 2023, hundreds of scientists and industry leaders released a statement, declaring, “mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.”190 To be sure, expert opinion varies wildly on the credibility and nature of the risk posed by rogue AI, as well as the time frame in which any dangers might emerge. However, even a small probability of catastrophe would presumably warrant an insurance policy.\n\nOne potential model for early warning and monitoring of a far-out threat is international cooperation on planetary defense. This refers to the capabilities and systems needed to detect and warn of threats to Earth posed by impacts from near-Earth objects—asteroids and comets that orbit the sun—and if possible, efforts to prevent or mitigate such a low-probability but literally high-impact disaster. According to the U.S. government, it is believed that there are as many as 1,000 near-Earth objects larger than 1 kilometer in size capable of devastating the globe; 95 percent of those have been found, and none is on a trajectory to collide with Earth. Another 25,000 larger than 140 meters are believed to exist, but only 42 percent of them have been identified and tracked—and any one of them could destroy a city.191\n\nThe United States has been at the forefront of efforts to prepare for this threat, leading multilateral diplomacy, supporting technical information sharing and observations, and organizing scenario planning.192 One could imagine similar multilateral efforts for the existential risk posed by AI as a part of a broader multilateral crisis-preparedness and response framework.\n\nConclusion\n\nRather than a single, tidy, institutional solution to govern AI, the world will likely see the emergence of something less elegant: a regime complex, comprising multiple institutions within and across several functional areas. The messy structure of global AI governance will reflect the distinct functional imperatives of AI regulation, the diversity and incentives of relevant public and private actors, and the absence of a single international political authority with the capacity and legitimacy to orchestrate cooperation across multiple domains.\n\nTo promote effective AI governance, the emerging regime complex must advance several imperatives:\n\nprovide the world with authoritative, up-to-date knowledge on the state of AI science;\n\nfacilitate the negotiation of common standards and harmonization of AI regulations;\n\nadvance equitable access and benefit sharing, particularly for LMICs; and\n\npromote collective security by mitigating dual-use dangers, encouraging arms control, and reducing risks from AI itself.\n\nFor each of these functions, experts and officials have invoked a plethora of institutional analogies from other global challenges. Yet the direct relevance of these comparisons varies widely. As a general-purpose technology with geopolitically salient implications, and whose development is primarily occurring within the private sector, AI eludes simple comparisons and analogies. Insights from existing institutional models are instructive, but policymakers must still move forward with designing innovative governance approaches to manage AI’s unique opportunities, risks, and cooperation dilemmas. As they proceed, they should expect to confront three main challenges.\n\nFirst, a generic challenge in any regime complex is the danger of incoherence across functional areas. For AI, the objective of encouraging equitable access stands in tension with the goal of reducing the risk of malevolent use or unintended consequences. However, a more daunting challenge will be preventing fragmentation within individual issue areas. Major powers, notably the United States and China, are competing fiercely to dominate AI technologies and applications and shape the principles and rules surrounding their governance. Like-minded Western governments can expect to find themselves repeatedly torn between pursuing AI governance within encompassing UN frameworks versus narrower coalitions of the like-minded. Such dynamics, if replicated by China, could well encourage competitive multilateralism, featuring rival minilateral arrangements among subsets of countries committed to competing values or interests. A potential wild card in this geopolitical game will be the postures on AI governance that LMICs in the Global South adopt.\n\nSecond, creating new structures for AI global governance—or even increasing the AI competence of existing institutions like the WHO and IAEA—will take time. In terms of the four core functions identified in this paper, the immediate priority task, and the objective most likely to be achieved in the short term given its relatively technical (as opposed to political) nature, is the first. The world will likely move briskly to create an authoritative scientific body, building on the first report on the state of AI science.\n\nThe second core function—creating common standards and harmonizing national (or in the case of the EU, regional) regulations—will inevitably take longer, due to major differences in the political cultures, values, and institutions of the major players. This heterogeneity, particularly differences between open and closed societies, may result in multispeed governance, with some (perhaps Western) countries adopting more demanding standards than those that emerge at the UN level.\n\nThe question of how to improve access and benefit sharing for developing nations—the third AI governance function—is likely to be fraught, amid growing alienation of Global South nations from the Global North, as well as the intellectual property concerns of private sector platforms. Nevertheless, the current geopolitical context could provide LMICs with leverage to insist on a more inclusive approach to cooperation on AI. At a time of intense competition pitting the West against China and Russia, wealthy nations perceive a strategic need to demonstrate solidarity with developing nations that have been disinclined to align with Western nations, which many regard as indifferent to their plight.193\n\nThe most difficult governance hurdle is likely to be forging broad multilateral agreement on collective security measures to prevent malevolent applications of advanced AI capabilities and mitigate related dual-use dangers. Given the impediments to universal, treaty-based approaches, countries may begin with narrower, less formal CBMs, codes of conduct, and voluntary export control arrangements. More encompassing approaches may be easier to achieve for crisis preparedness efforts, including early warning systems against potential existential risks that endanger all humanity.\n\nFinally, a recurrent dilemma spanning each of these functional imperatives will be to determine the appropriate role of the private sector as a subject and object of AI global governance. Technology companies are the driving force behind AI breakthroughs and control the lion’s share of global AI capabilities. Their participation and cooperation in governance initiatives is critical, but they are also not subject to the same accountability mechanisms as national governments. The question for public authorities is how best to regulate these private actors in a manner that advances the public good, both domestic and global. Compounding this challenge, leading AI labs are themselves divided by stark philosophical differences. Addressing the role of technology companies and other private sector actors across functional areas will require policymakers to diverge from many existing institutional models and pursue novel approaches to global governance.\n\nAcknowledgments\n\nThe authors are grateful to Carnegie President Mariano-Florentino (Tino) Cuéllar and to their colleagues Jon Bateman, Fiona Brauer, Frances Brown, and Steven Feldstein for constructive and insightful comments on previous drafts of this working paper. They also thank Carnegie’s Communications team for their meticulous work throughout the publications process and Carnegie India for hosting the Global Technology Summit, whose discussions informed aspects of this paper. F"
    }
}
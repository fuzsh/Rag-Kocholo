{
    "id": "dbpedia_3610_2",
    "rank": 3,
    "data": {
        "url": "https://www.cambridge.org/core/journals/experimental-results/article/validating-the-behavioral-defining-issues-test-across-different-genders-political-and-religious-affiliations/DD85320416F9C8A886500821829C76CC",
        "read_more_link": "",
        "language": "en",
        "title": "Validating the behavioral Defining Issues Test across different genders, political, and religious affiliations",
        "top_image": "https://static.cambridge.org/covers/EXP_0_0_0/experimental_results.jpg?send-full-size-image=true",
        "meta_img": "https://static.cambridge.org/covers/EXP_0_0_0/experimental_results.jpg?send-full-size-image=true",
        "images": [
            "https://www.cambridge.org/core/cambridge-core/public/images/icn_circle__btn_close_white.svg",
            "https://www.cambridge.org/core/cambridge-core/public/images/logo_core.png",
            "https://www.cambridge.org/core/cambridge-core/public/images/logo_core.svg",
            "https://www.cambridge.org/core/cambridge-core/public/images/logo_core.svg",
            "https://www.cambridge.org/core/cambridge-core/public/images/logo_core.svg",
            "https://static.cambridge.org/covers/EXP_0_0_0/experimental-results.jpg",
            "https://www.cambridge.org/core/page-component/img/save-pdf-icon.080470e.svg",
            "https://www.cambridge.org/core/page-component/img/pdf-download-icon.c7fb40c.svg",
            "https://www.cambridge.org/core/page-component/img/pdf-download-icon.c7fb40c.svg",
            "https://www.cambridge.org/core/page-component/img/dropbox-icon.3d57046.svg",
            "https://www.cambridge.org/core/page-component/img/google-drive-icon.a50193b.svg",
            "https://www.cambridge.org/core/page-component/img/close-icon.194b28a.svg",
            "https://www.cambridge.org/core/page-component/img/share-icon.cbcfad8.svg",
            "https://www.cambridge.org/core/page-component/img/close-icon.194b28a.svg",
            "https://www.cambridge.org/core/page-component/img/cite-icon.44eaaa4.svg",
            "https://www.cambridge.org/core/page-component/img/license-cc-icon.e3a74ed.svg",
            "https://www.cambridge.org/core/page-component/img/license-by-icon.33e212c.svg",
            "https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20230316130603880-0715:S2516712X23000060:S2516712X23000060_tab1.png?pub-status=live",
            "https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20230316130603880-0715:S2516712X23000060:S2516712X23000060_eqnu1.png?pub-status=live",
            "https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20230316130603880-0715:S2516712X23000060:S2516712X23000060_fig1.png?pub-status=live",
            "https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20230316130603880-0715:S2516712X23000060:S2516712X23000060_tab2.png?pub-status=live",
            "https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20230316130626-11474-mediumThumb-S2516712X23000060_tab1.jpg",
            "https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20230316130603880-0715:S2516712X23000060:S2516712X23000060_fig1.png",
            "https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20230316130626-07170-mediumThumb-S2516712X23000060_tab2.jpg",
            "https://www.cambridge.org/core/page-component/img/cite-icon.44eaaa4.svg",
            "https://www.cambridge.org/core/page-component/img/cite-icon.44eaaa4.svg",
            "https://assets.crossref.org/logo/crossref-logo-100.png",
            "https://upload.wikimedia.org/wikipedia/commons/a/a9/Google_Scholar_logo_2015.PNG",
            "https://assets.crossref.org/logo/crossref-logo-100.png",
            "https://www.cambridge.org/core/cambridge-core/public/images/cambridge_logo.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Hyemin Han",
            "Teresa Ober"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "Validating the behavioral Defining Issues Test across different genders, political, and religious affiliations - Volume 4",
        "meta_lang": "en",
        "meta_favicon": "/core/cambridge-core/public/images/favicon.ico",
        "meta_site_name": "Cambridge Core",
        "canonical_link": "https://www.cambridge.org/core/journals/experimental-results/article/validating-the-behavioral-defining-issues-test-across-different-genders-political-and-religious-affiliations/DD85320416F9C8A886500821829C76CC",
        "text": "Introduction\n\nThe Defining Issues Test (DIT) is a widely used tool in the fields of moral psychology and education for evaluating the development of moral reasoning. Its primary purpose is to measure one’s ability to apply postconventional moral reasoning when faced with moral dilemmas (Thoma, Reference Thoma, Killen and Smetana2006). The DIT generates a P-score, a postconventional reasoning score, which quantifies one’s level of postconventional reasoning development. The score reflects their likelihood of utilizing postconventional reasoning, which involves the ability to re-evaluate existing social norms and laws based on moral principles, rather than personal interests or social norms, across different situations (Rest et al., Reference Rest, Narvaez, Bebeau and Thoma1999).\n\nDespite being a widely used tool in the field, concerns have been raised regarding the potential bias of the DIT toward individuals with varying gender, political, and religious affiliations. For instance, Gilligan (Reference Gilligan1982) argued that the model based on postconventional reasoning development might favor men versus women because women are more likely to be assessed to focus on personal interests, social relations in fact, in solving moral dilemmas from the DIT’s perspective. In addition, some argue that the postconventional reasoning presented in the measure is liberal-biased, so conservative populations, including both politically and religiously conservative ones who value traditions and conventions, are also likely to be unfairly penalized due to their political and religious views, not by their actual developmental level (Crowson & DeBacker, Reference Crowson and DeBacker2008).\n\nSeveral moral psychologists have suggested that people affiliated with different political and religious groups are likely to endorse different moral foundations and, thus, they are likely to render different moral decisions based on different moral philosophical rationales (Graham et al., Reference Graham, Haidt and Nosek2009). For instance, research on the moral foundations theory has demonstrated that liberals tend to endorse foundations for individualizing, whereas conservatives tend to focus on foundations for social binding (Graham et al., Reference Graham, Haidt and Nosek2009). Hence, without examining whether a test for moral reasoning, the DIT, is capable of assessing one’s moral reasoning in an unbiased manner across people with different moral views, it is impossible to assure that the test can generate reliable and valid outcomes across such people (Han et al., Reference Han, Dawson and Choi2022b).\n\nPrevious studies have addressed these concerns by demonstrating that there have not been significant differences in the mean P-scores across the different groups (Thoma, Reference Thoma1986; Thoma et al., Reference Thoma, Narvaez, Rest and Derryberry1999), or P-scores significantly predict socio-moral judgment even after controlling for political and religious affiliations and views (Crowson & DeBacker, Reference Crowson and DeBacker2008). However, such score-based comparisons cannot address concerns regarding whether the test per se or its items are biased. To be able to address the concern, it is necessary to conduct: first, a measurement invariance (MI) test, which examines whether a test measures a construct of interest consistently across different groups (Putnick & Bornstein, Reference Putnick and Bornstein2016); and second, a differential item functioning (DIF) test based on the item response theory, which examines whether a specific item favors a specific group, while the latent scores are the same (Zumbo, Reference Zumbo1999).\n\nOnce MI is supported and no item demonstrates a significant DIF across different groups, then it is possible to conclude that the items in the test do not measure one’s latent ability unequally. Of course, there have been a few previous studies employing such methods to evaluate the cross-group validity of the DIT (Choi et al., Reference Choi, Han, Dawson, Thoma and Glenn2019; Richards & Davison, Reference Richards and Davison1992; Winder, Reference Winder2009). However, their sample size was small, or they focused solely on specific groups (e.g., Mormons). Furthermore, the traditional DIT presented a technical challenge for conducting MI or DIF tests due to its scoring method, which uses rank-ordered responses instead of individual item ratings.\n\nIn the present study, to address the abovementioned limitations in the previous studies that examined the validity of the DIT across different groups, I tested the MI and DIF of the behavioral DIT (bDIT) with a large dataset collected from more than 1,400 participants. The bDIT, which is a simplified version of the traditional DIT, uses individual item responses to calculate one’s P-score instead of rank-ordered responses (Han et al., Reference Han, Dawson, Thoma and Glenn2020). Consequently, conducting MI and DIF tests with the bDIT is more straightforward. In contrast to previous studies, the present study analyzed a more extensive dataset collected from participants with diverse political and religious affiliations.\n\nMethods\n\nParticipants and data collection\n\nData were acquired from college students (Age mean: 21.93 years; SD: 5.95 years) attending a public university in the Southern United States of America. All data collection procedures and the informed consent form were reviewed and approved by the University of Alabama Institutional Review Board (protocol number: 18-12-1842). Participants were recruited via the educational and psychological research subject pools. They signed up for the study and received a link to a Qualtrics survey form and received a course credit as compensation.\n\nTable 1 summarizes the demographics of the participants in terms of their gender, political, and religious affiliations, which were the main interests of this study. Due to the convergence issue associated with Confirmatory Factor Analysis (CFA), only groups with n ≥ 100 were used for the MI and DIF tests (Han et al., Reference Han, Blackburn, Jeftić, Tran, Stöckli, Reifler and Vestergren2022a). As a result, for political affiliations, Republicans, Democrats, Independents, and Others were analyzed, and, for religious affiliations, Catholics, Evangelical and Non-Evangelical Protestants, Spiritual but not religious, and Others were analyzed.\n\nMeasures\n\nThe bDIT and demographics survey form used in the present study (survey.docx) and the codebook (varlist.xlsx) are available in the Open Science Framework repository at https://osf.io/ybmp6/ for readers’ information.\n\nBehavioral Defining Issues Test\n\nThe bDIT consists of three dilemmas: Heinz Dilemma, Newspaper, and Escaped Prisoner (see survey.docx in the repository for the sample test form and items). For each dilemma, participants were asked to examine whether a presented behavioral option to address the dilemma is morally appropriate or inappropriate. Then, they were presented with eight items per dilemma asking the moral philosophical rationale supporting their decision. For each item, three rationale options were presented. Each of the three options corresponds to one of three schemas of moral reasoning proposed in the Neo-Kohlbergian model of moral development, personal interests, maintaining norms, and postconventional schemas. The participants were requested to choose the most important rationale.\n\nOnce the participants completed the bDIT, I examined how many postconventional options were selected out of 24 items (eight per dilemma × three dilemmas). Then, one’s P-score, which ranges from 0 to 100%, was calculated as follows:\n\n$$ P=\\frac{\\# of\\ selected\\ postconventional\\ options}{24}\\times 100. $$\n\nFor instance, if one selected the postconventional options as the most important rationale for 12 items, then the P-score becomes 50. It means that the likelihood of utilization of the postconventional schema while solving moral dilemmas is 50% in this person’s case.\n\nDemographics survey form\n\nAt the end of the survey, I presented a demographics survey form to collect participants’ demographics for MI and DIF tests across different groups. The collected demographics include gender, political, and religious affiliations (see survey.docx in the repository for the demographics survey form).\n\nStatistical analysis\n\nFirst, I evaluated the MI of the bDIT via multigroup confirmatory factor analysis (MG-CFA) implemented in an R package, lavaan (Rosseel, Reference Rosseel2012). Whether MI is supported was examined by the extent to which model fit indicators, that is, RMSEA, SRMR, and CFI, changed when additional constraints were added to the measurement model (Putnick & Bornstein, Reference Putnick and Bornstein2016). I tested four different levels of invariance: configural, metric, scalar, and residual invariance (refer to the Supplementary Material for additional methodological details). Because scalar invariance is minimally required for cross-group comparisons, I focused on whether this level of invariance was achieved (Savalei et al., Reference Savalei, Bonett and Bentler2015). In this process, I treated a response to each item as a dichotomous variable, that is, a postconventional versus non-postconventional, because that is consistent with how the actual P-score is calculated as described above. Then, I assumed a higher-order model with three latent factors, one latent factor per the presented dilemma (see Figure 1 for the CFA model).\n\nSecond, I also performed the DIF test to investigate whether any item of the test demonstrated a statistically significant preference for a particular group compared to others, even when the latent ability, moral reasoning, was the same. To implement the DIF test, I employed the logistic ordinal regression DIF test with an R package, lordif (Choi et al., Reference Choi, Gibbons and Crane2011) with an R code for the multiprocessing to distribute the tasks to multiple processors that was previously applied in Han et al. (Reference Han, Blackburn, Jeftić, Tran, Stöckli, Reifler and Vestergren2022a; Reference Han, Dawson, Walker, Nguyen and Choi2022c). Once lordif was performed, I tested whether there was any significant uniform or nonuniform DIF for each item to examine whether the item significantly unequally favored one group versus other (see the Supplementary Material for methodological details).\n\nAll data and source code files are available in the Open Science Framework repository at https://osf.io/ybmp6/.\n\nDiscussion\n\nIn the present study, I examined whether the bDIT can measure the development of postconventional moral reasoning across different gender, political, and religious groups consistently without bias. The MI test indicated that the bDIT assessed postconventional moral reasoning consistently across heterogeneous groups at the test level. At the item level, the DIF test reported that no item significantly favored a specific group. These results suggest that the bDIT was not biased across different groups.\n\nGiven that the bDIT did not show any significant non-invariance or DIF across different gender, political, and religious groups, it would be possible to conclude that the test can consistently examine moral reasoning. The results may address the concerns related to the potential gender and liberal biasedness in measuring postconventional moral reasoning. In the United States, in terms of political affiliations, Democrats are supposed to be more liberal and more likely to endorse individualizing moral foundations than Republications (Han et al., Reference Han, Dawson and Choi2022b). In the case of religious affiliations, Evangelical Protestants are generally considered more conservative and more likely to support binding foundations than other religious groups (Sutton et al., Reference Sutton, Kelly and Huver2020). In the present study, I examined the validity evidence of the bDIT among these groups with diverse political and religious views, which are inseparable from moral standpoints. Hence, moral psychologists and educators may employ the bDIT to test participants’ developmental levels of moral reasoning.\n\nHowever, there are several limitations to the present study that should be acknowledged. First, the study was conducted solely within the United States and, thus, further data should be gathered from a more diverse range of cultural contexts while also taking into consideration different political and religious factors that exist within different countries. Second, the study relied on self-reported political and religious affiliations, which may not fully represent participants’ actual political and religious views. Such variables are categorical and may not capture the complexities of an individual’s beliefs accurately. Finally, while the present study tested the cross-group validity of the bDIT and the bDIT is a reliable and valid proxy for the original DIT (e.g., Choi et al., Reference Choi, Han, Dawson, Thoma and Glenn2019; Han et al., Reference Han, Dawson, Thoma and Glenn2020), it is necessary to examine whether the same level of validity can be supported for the original DIT, the DIT-1, and DIT-2. This can be achieved by administering the original DIT with large, diverse samples (e.g., Choi et al., Reference Choi, Han, Bankhead and Thoma2020) and gathering additional demographic information."
    }
}
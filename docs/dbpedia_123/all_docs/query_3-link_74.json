{
    "id": "dbpedia_123_3",
    "rank": 74,
    "data": {
        "url": "https://github.com/jhc13/2x2-cube-solver",
        "read_more_link": "",
        "language": "en",
        "title": "solver: Reinforcement learning agent that solves a 2x2 Rubik's cube",
        "top_image": "https://opengraph.githubassets.com/5a19967eb5a30c088b612c9bd63df2c5b82570984291eccd79d84cc2c962d0df/jhc13/2x2-cube-solver",
        "meta_img": "https://opengraph.githubassets.com/5a19967eb5a30c088b612c9bd63df2c5b82570984291eccd79d84cc2c962d0df/jhc13/2x2-cube-solver",
        "images": [
            "https://github.com/jhc13/2x2-cube-solver/raw/main/training-runs/220504192153/plot.png",
            "https://user-images.githubusercontent.com/39209141/167299726-121506a4-bfcd-41e3-9f2c-2e63e70cbdd2.png",
            "https://user-images.githubusercontent.com/39209141/167299756-af3994e1-7f30-44c6-aaa5-76f833723aa3.png",
            "https://user-images.githubusercontent.com/39209141/167299769-12782ab3-9be0-4597-84d9-dcbc560309fa.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Reinforcement learning agent that solves a 2x2 Rubik's cube - jhc13/2x2-cube-solver",
        "meta_lang": "en",
        "meta_favicon": "https://github.com/fluidicon.png",
        "meta_site_name": "GitHub",
        "canonical_link": "https://github.com/jhc13/2x2-cube-solver",
        "text": "A reinforcement learning agent that solves a 2x2 Rubik's cube, implemented with a Dueling Double Deep Q-Network.\n\nInside the 2x2-cube-solver directory, cube.py and cube_env.py define the 2x2 Rubik's cube and its Gym environment.\n\ndueling_dqn.py is a simple 4-layer Dueling DQN model implemented in PyTorch, with separate outputs for the state value and the action advantages.\n\nTraining the model is done using train.py, and the trained model can be used to solve cubes through solve.py.\n\nTo train a model, set the configuration variables in config.py and run train.py. The trained model as well as the training configuration and a plot of the training run are saved in the training-runs directory.\n\nResuming a previous training run can be done by setting the RESUME_RUN_ID configuration variable.\n\nTwo ways of solving cubes using a trained model are available in solve.py.\n\nThe first method uses the evaluate_model function and solves a large number of randomly scrambled cubes. Solve results and a plot of the solution length distribution are displayed.\n\nThe second method uses the solve_scramble function and solves a single cube scrambled according to a given scramble. This can be used to examine a model's performance for a specific scramble of interest.\n\nFor solving, the configuration variables must be set inside the respective functions before calling them in solve.py.\n\nInitially, the scramble length was fixed for the entire training run. Because a reward was only given when the cube was solved, and the chance of reaching the solved state through nearly random moves from a sufficiently well-scrambled initial state was extremely low, the model learned almost nothing.\n\nTo fix this, the training process was changed so that the scramble length was initially set to a small number and was increased by 1 each time the model reached a certain solution rate threshold during evaluation. This solved the previous problem of the model not learning, but another problem that emerged was catastrophic forgetting. Whenever the scramble length was increased, there was a good chance that the model would rapidly forget most of its previously learned knowledge.\n\nAn attempt to mitigate this problem was to ensure that some scrambles of the previous length were included along with the current length scrambles, but this was unsuccessful. What finally worked was to introduce a concept of average scramble length and to increase this value in increments of 0.1. The average scramble length determined the proportion of scramble lengths given to the model. For example, if the average scramble length was 5.2, 80% of the training scrambles would be of length 5, and the remaining 20% would be of length 6. This allowed a much more gradual change in the model's training data and resulted in stable training.\n\nThe training runs for the final model can be found in the training-runs directory. It was trained over 7 training runs and 12 million steps, starting at an average scramble length of 3.0. The plot for the first training run is shown below.\n\nPlot of the first training run for the final model\n\nAs can be seen in the plot, the rate of increase of the scramble length decreased as the training progressed. Training was ultimately stopped at an average scramble length of 7.1 as progress at that point was getting stagnant.\n\nThe final model is able to solve the 2x2 Rubik's cube from a variety of scrambled states. Shown below is an example of a randomly generated scramble and the model's solution. The moves are written in standard Rubik's cube notation.\n\nScrambled cube Solved cube Scramble: R2 F' R2 U' F R2 U' R' F R F' Solution: U R F' U2 R' F' R\n\nThe performance of the model was evaluated with sets of 10,000 randomly scrambled cubes. The results are summarized in the following table. The first column is the length of the scramble in quarter turn metric (QTM), and the second column is the maximum number of moves that was allowed for the model to solve the cube, also in QTM. The solution rate in the third column is the proportion of cubes that were successfully solved within the allowed move count. Scrambles longer than 14 moves were not tested because a 2x2 cube in any state can be solved in a maximum of 14 quarter turns (the God's number).\n\nScramble length Maximum allowed solution length Solution rate 4 4 100.0% 5 5 98.0% 6 6 92.6% 7 7 82.3% 7 20 88.6% 8 8 54.8% 9 9 38.0% 14 14 14.0% 14 20 18.6% 14 500 100.0%\n\nThe solution rate decreased as the scramble length increased, but increased when the maximum allowed solution length was increased. Although the model was never trained on scrambles longer than 8 moves, it solved many of them successfully. In fact, it was able to solve all 10,000 of the length 14 scrambles when the maximum allowed solution length was set to 500. The solution length distribution is shown below.\n\nSolution length distribution for 10,000 scrambles of length 14\n\nPart of this solving ability can be attributed to the fact that moves leading to a repeated state were not allowed. For difficult scrambles, the model presumably makes nearly random moves until it reaches a state that it is familiar with. For comparison, a completely untrained model was given the same task. It managed to solve none of the 10,000 scrambles within 1000 moves, even with the restriction of no repeated states. This result demonstrates that the model did indeed achieve some degree of solving ability.\n\nThere exist several potential avenues of improvement, including the following:\n\nMore experimentation with hyperparameter tuning\n\nUsing a different network architecture, such as a residual neural network\n\nUsing a different reinforcement learning algorithm, possibly one that takes advantage of multiprocessing, such as proximal policy optimization"
    }
}
{
    "id": "dbpedia_1546_0",
    "rank": 7,
    "data": {
        "url": "https://www.linkedin.com/company/goldensource",
        "read_more_link": "",
        "language": "en",
        "title": "GoldenSource",
        "top_image": "https://media.licdn.com/dms/image/v2/C4D0BAQFjU2jr-SRBtQ/company-logo_200_200/company-logo_200_200/0/1631325274496?e=2147483647&v=beta&t=4M_pYXdP0dalv_zWmE4jGExgPjL7uu7wAq2gM1oOprg",
        "meta_img": "https://media.licdn.com/dms/image/v2/C4D0BAQFjU2jr-SRBtQ/company-logo_200_200/company-logo_200_200/0/1631325274496?e=2147483647&v=beta&t=4M_pYXdP0dalv_zWmE4jGExgPjL7uu7wAq2gM1oOprg",
        "images": [
            "https://media.licdn.com/dms/image/v2/D563DAQGmXC1zrO5CPw/image-scale_191_1128/image-scale_191_1128/0/1701386253764/goldensource_cover?e=2147483647&v=beta&t=-Ih2aYSr6WvSCPdV5_1zy-UqvtZqu7Dt_pIAvkGBg_g"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "GoldenSource | 23,590 followers on LinkedIn. Enterprise Data Management | THE GOLD STANDARD OF DATA MANAGEMENT\nBy the end of the decade, GoldenSource will be the world‚Äôs leading data management SaaS solution, solving data mastering, reporting and analysis for 250 of the globe‚Äôs most important financial institutions.\n\nGoldenSource makes it easy to manage critical reference, market, risk and ESG data. We offer integrated Enterprise Data Management (EDM) software solutions for the securities and investment management community.",
        "meta_lang": "en",
        "meta_favicon": "https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca",
        "meta_site_name": "",
        "canonical_link": "https://www.linkedin.com/company/goldensource",
        "text": "The growing demand for climate risk data An ESG Cloud Warehouse Hub for climate risk data provides insights into how a company (as the owner of its properties) or an entire portfolio of investments is exposed to the consequences of climate change. This data ‚Äì generated by various agencies, some of which are also very proficient in conducting credit risk analysis ‚Äì is used to perform risk projections and assessments vis a vis the economic outlook of an investee or entire portfolio. It is possible to make risk assessments based on low, medium, medium-high, or high climate change scenarios. Composite risk assumptions are based on projections for the different climate events ‚Äì for example, floods, extreme temperatures, storms, wildfires, and droughts ‚Äì and how combinations of them can disrupt a company overall. The task is to determine mid-term and long-term adverse effects climate conditions may have on a business, and whether there are investment alternatives which would increase the likelihood a portfolio will meet its performance target. For example, if you‚Äôre looking at an agricultural company and determine it has farmland that is heavily susceptible to drought in five to ten years, this will influence your ‚Äì and others‚Äô ‚Äì investment decisions. Something similar applies to a company with e.g. a manufacturing facility in an increasingly high-risk region where workforces are becoming harder to recruit. The fact is, many climate changes are happening even faster than initially projected. They are having a profound effect on the way portfolio managers are assessing potential investments and complying with industry recommendations and their custom base‚Äôs expectations concerning climate-related disclosures. As a result, we are observing strongly increased interest in accessing the most comprehensive and reliable climate risk data possible.\n\n‚öî A tug-of-war between banks and regulators over new capital rules is intensifying, with the industry pushing for a softening of how capital charges are calculated under the commonly used ‚Äòinternal model approach‚Äô. üôÖ‚ôÄÔ∏è Experts say the world‚Äôs biggest banks plan to shun the IMA at scale in favour of the so-called ‚Äòstandardised approach‚Äô. This is because the cost and complexity of using it under incoming rules ‚Äî the Basel IV‚Äôs Fundamental Review of the Trading Book ‚Äî outweigh any capital benefit. ü§∑‚ôÇÔ∏è ‚ÄúThe assumption by the regulators was that as many banks as possible would go [for] the IMA, but the industry turned around and said: ‚ÄòWell, no, we‚Äôre going to go with the standardised approach‚Äô,‚Äù says Charlie Browne of GoldenSource. \"It‚Äôs a kind of a ‚Äòbe careful what you wish for‚Äô message from the banking industry back to the regulators.‚Äù üëá Story by James King. Read more below. https://lnkd.in/e-n2DSfQ More insights from Tim Clarsen at Murex, Panayiotis Dionysopoulos, CFA, at ISDA and David Kelly of BIP.\n\nLognormality, GBM and Black Scholes Many data sets when arranged from fewest occurrences to largest and plotted on a histogram display roughly as a bell shape known as the normal distribution (ND). In finance, daily stock price changes are typically ND. Lognormally distributed (LND) data such as stock prices (i.e., not stock price changes), on the other hand, cannot go below zero and are therefore not symmetric and bell-shaped. ND and LND observed data can be simulated mathematically using probability density functions (PDFs) which generate smooth curves that fit the observed distributions. The mean, ¬µ, and standard deviation, œÉ, are parameters of the PDFs. Models in quantitative finance use PDFs when estimating probabilities that cash flows will occur. Models that assume the ND PDF are easier to find solutions for than models that assume data is LND. The model used for stock price changes is geometric Brownian motion (GBM). It generates data that is LND. It assumes that stock price changes, dS, are generated using a deterministic term, ¬µSdt, +/- increments from a random term, œÉSdW. Because the deterministic term continuously multiplies ¬µ, the growth rate, by S, the stock price, the resulting stock price will compound and increase over time. The increasing stock price will result in larger and larger stock price changes that will lead to those changes exhibiting an LND rather than a ND shape. Can GBM-generated daily price changes be converted from LND to ND? GBM is an example of a stochastic differential equation (SDE) that is an Ito process. It models the change over time in S, dS. Ito‚Äôs lemma states that any function of S will result in another Ito process after its Ito transformation. The diagram below shows GBM on the LHS. The RHS contains three examples of GBM being Ito-transformed. dS is transformed into d(3S), d(forward price of S) and d(Ln S). The problem with the first two examples is that post-transformation, their terms still contain S - the transformed processes will still be geometric and LND. Example 3, however, is different. It is Ln S, the natural log of S. When GBM is transformed into d(Ln S), it results in an Ito process that no longer contains S in its terms. This removes the compounding effect of S, the ‚ÄúG‚Äù from GBM, and converts data to LND. So, the answer to Q above is, yes, GBM can be converted into an SDE that generates price changes that are ND - but only when the Ito-transform is d(Ln S). Once transformed, it is easier to use in option pricing models like the Black-Scholes-Merton (BSM) model. The BSM equation is famously derived using no-arbitrage arguments that remove the troublesome growth rate, Œº, parameter from GBM. The removal of S from the terms of the Ito-transformed GBM, however, is equally important. Without it, the transformed price changes would not be ND, and solutions to the BSM PDE, i.e., the BSM formulas, would not be possible.\n\nRisk Neutral Valuations A 10% change in price, e.g., a 10% daily return, for a blue-chip stock is a more unusual event than a 10% change on a high-risk small-cap stock. Investment managers use the Sharpe ratio, Œª, to compare these two price changes. The ratio, which divides the return minus the risk-free rate (¬µ-r) by the standard deviation of the stock, œÉ, allows the price-changes of the two stocks to be compared from a risk-adjusted perspective. Another term for the Sharpe ratio is the market price of risk (MPR). The core approach in option pricing mathematics is the use of Ito‚Äôs lemma to convert a process for stock price movements, (visualise a stock price chart), into a process for option price movements, (visualise a chart showing the price of an option on that stock). For example, the Black-Scholes-Merton (BSM) model converts the geometric Brownian motion (GBM) stock price process into an option price process that is described by the BSM partial differential equation (PDE). The Heston model converts a random-volatility stock price process into a Heston option price PDE, and the Vasicek model converts a mean-reverting random interest rate process into a Vasicek PDE. The standard deviation parameter, œÉ, aka implied volatility, underlies both the MPR equation and option pricing models. Re-arranging the MPR equation, we get r = ¬µ - ŒªœÉ, so that when there is no market price risk in the stock, i.e., ŒªœÉ=0, we are left with r=¬µ, i.e., the return of the stock = the risk-free rate. This r=¬µ assumption is central to the BSM model. It can be made because of no-arbitrage arguments that underlie the model. If the stock price movement is assumed to be perfectly hedged by the option price movement, it means there is no market risk in the portfolio, which can be assumed then to grow at the risk-free rate r. This makes the model much simpler to use because stock price growth, Œº, does not have to be estimated. The concept of converting real-world growth rates into risk-free rates using no-arbitrage arguments is referred to as risk-neutral valuations. The MPR equation also comes into derivative pricing models that evolved from BSM. In Heston, like BSM, the MPR is assumed to be zero for its PDE terms that overlap with BSM. The non-overlapping terms that control how Heston volatility changes over time, however, do need to be adjusted for the MPR. The option price PDEs in term structure models such as the Vasicek model also include adjustments for the MPR. The MPR, like all model parameters, needs to be estimated before running any model that includes it. Where possible this is done by calibrating to observable market data.\n\nIt‚Äôs clear that the data market is moving towards solutions that integrate data from multiple and diverse sources. Snowflake and Databricks are good examples. These companies are at the forefront of this movement, as this article explains. One of the key factors in this shifting landscape is, unsurprisingly, data, given its role in today‚Äôs business actions. Data has never been as important as today, as it‚Äôs become a necessary part of each business workflow across sectors, informing operations and making them more efficient than ever before. Moreover, the implementation of AI has further improved data models, allowing them to provide clear and simple requests-response outcomes. The idea of unifying platforms into comprehensive solutions that can cater to users‚Äô needs more efficiently is something we at GoldenSource are very passionate about, with all our efforts driving us further down that road. After all, it‚Äôs becoming clearer by day that‚Äôs the future of data management platforms. Do you agree?"
    }
}
{
    "id": "wrong_mix_domainrange_subsidiary_00109_0",
    "rank": 9,
    "data": {
        "url": "http://www.dlib.org/dlib/july98/gladney/07gladney.html",
        "read_more_link": "",
        "language": "en",
        "title": "Safeguarding Digital Library Contents and Users: Interim Retrospect and Prospects",
        "top_image": "",
        "meta_img": "",
        "images": [
            "http://www.dlib.org/dlib/july98/images/story_bar1.gif",
            "http://www.dlib.org/dlib/july98/images/d-line2.gif",
            "http://www.dlib.org/dlib/july98/gladney/Dlworld2.gif",
            "http://www.dlib.org/dlib/july98/gladney/Lib22-thumbnail2.gif",
            "http://www.dlib.org/dlib/july98/gladney/Chasm2.gif",
            "http://www.dlib.org/dlib/july98/images/blue-dot.gif",
            "http://www.dlib.org/dlib/july98/images/blue-dot.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "D-Lib Magazine\n\nJuly/August 1998\n\nISSN 1082-9873\n\nSafeguarding Digital Library Contents and Users\n\nInterim Retrospect and Prospects\n\nHenry M. Gladney\n\nIBM Almaden Research Center\n\nSan Jose, California 95120-6099\n\ngladney@almaden.ibm.com\n\nAbstract\n\nThe Safeguarding ... series in D-Lib Magazine is intended to suggest technology to help manage digital intellectual property. That technology can contribute only in a complex of administrative, legal, contractual, and social practices is broadly accepted; we now pause to examine how efforts to fit in the technological component are progressing and what next needs attention.\n\nAmong concerns for responsive and responsible management of intellectual property, technical aspects are surely secondary to prominent issues of public policy, law, and ethics. The latter are beginning to be addressed both in legislative processes and also by academic investigators. For the technical community, we assert that we can design offerings with sufficient flexibility that we need not wait for policy decisions which might affect software to administer rules chosen or to hinder unacceptable behavior. The current article projects technical directions without designing solutions. It emphasizes managing the data -- how it is stored, protected, and communicated.\n\nFor just over a year, the Safeguarding ... articles in D-Lib Magazine have discussed software assistance for protecting digital library (DL) contents. The current article presents a stock-taking, a pause to inspect what has been accomplished so far, to see how it falls short of needs which are rapidly becoming better understood, and to consider what should be done next. Our emphasis will be seen to be that of an industrial research team, extending from basics through software designs to questions of practical deployment, responsible usage, and pleasing customers. We are striving to devise software helpful within whatever policies the governing organizations choose, i.e., to avoid implicit policy-setting through errors in software design.\n\nThe technical tools which can help mitigate risks attending digital copies of intellectual property are based on digital computing security tools which have been developed for other purposes over a period of 30 years. This is a large field, with many topics and arcana. For practical reasons, digital library service must mostly ride on elements developed for other applications, limiting its own technical development to certain \"middleware\" components; this article is limited to the few components we believe we can and must enhance for digital library. Even these few are represented in a large literature to which we can indicate at most entry points. For brevity in this article, we assume that the reader has either looked at prior D-Lib articles to which we refer, or will do so as needed.\n\nSeveral trends, some well known and some less so, present themselves:\n\nMany users each want more rapid access to each of many collections. Many collection managers want to satisfy this demand and to extend their offerings to more distant and more varied communities than they currently serve. The numbers of users, objects, and access events are so large that manual administration of contracts, laws, human relationships, and social agreements is becoming unaffordable or even impossible.\n\nThe ease with which digital representations are copied is raising a realistic concern among rights-holders: that their statuatory rights and commercial interests are at risk. The retail value of pirated works -- particularly software, music, and motion picture performances -- is estimated as larger than $10B. In a recent presentation, former U.S. Member of Congress the Honorable Pat Schroeder reminded us that many livelihoods depended on managing intellectual property prudently and that we were putting this at risk by failing to teach our youth that, \"Intellectual property is property\".\n\nParts of the software technology to address these concerns have been evolving for 20 years or longer, and other parts have started to evolve rapidly, stimulated by immense interest in e-business and its most visible component, e-commerce. Even well-understood parts of this technology are not yet effectively applied.\n\nProgress in at least one tool class, database for recording and managing contract obligations for copyright materials, is stalled because the next stage of development needs production pilots in enterprises willing to take the risks associated with being deployment leaders. We have come about as far as \"toy\" pilots can take us; for detailed design which realistically responds to enterprise needs, we need test beds of sufficient scale to be important to the executive management of businesses and universities. We will return to this issue below.\n\nManaging to rules for copyright works has been treated independently of access control, but these domains are now seen to be converging.\n\nThe technical means are the easy part of a large set of challenges. The hard part is a complex of policy issues and induced social behavior. An example occurs in the universities, which are beginning to face a revolution over intellectual property as technology blurs the lines between good business and good education; this includes the issue of who owns on-line course materials, the institutions or the faculty members who prepared them; see [Woody 1998]. Another helpful source is a report commissioned by The U.S. Copyright Office: Sketching the Future of Copyright in a Networked World is just now becoming available.\n\nLong lists of detailed requirements are available in many sources; see, for example, what is said about the access control component [Gladney 1997]. Even more recently, we are presented with statements of requirements, from CNI on authentication and access management for cross-organizational use of information resources [Lynch 1998] and from the museum community on user confidence in the authenticity of digital resources [Bearman 1998]. From the many writings illustrated by these and from conversations with members of the IBM development and marketing communities, we select some particular requirements that we feel have not received the attention called for by their compelling nature.\n\nThe means of access management must be scalable to massive numbers of low unit cost interactions, large numbers of users, immense collections, and constantly changing human relationships.\n\nEach user wants to connect to services only once in a network session, rather than as separate steps for each of many services drawn on.\n\nInterruptions to provide information needed by access management are a distraction for end users -- a distraction which can be and should be avoided; similarly, service administrators will not be able to deal with individual grants of authorization for each of many thousands of users, and our manifest ability to automate this must be delivered in software tools rooted in databases.\n\nDifferent resource custodians want different access rule schemes; we can and should permit all possible rules of access, without exposing the inherent complexity to end users or administrators who are not interested in the \"plumbing\".\n\nLibraries differ from piles of books and papers because librarians oversee collection development, selecting only holdings within their institutional missions and of known authenticity and provenance, and organize what they select with catalogs and other means; access management systems must help administer the achievement of such values, which cumulatively make for the quality of the collections.\n\nAuthorization for library services must fit within other administrative processes, such as university student cards for all privileges and as data backup for networked personal computers.\n\nWe demand continuity between delivery from research libraries, scientific databases, and collections of clerical paperwork.\n\nSuch requirements are additional to more detailed functional requirements typically emphasized in the literature, and include aspects essential to durable software.\n\nProviding the protections sought has underpinnings which we need to mention, but not further discuss. These include physical security (e.g., network cables should not be exposed to terrorists), operating system security, and other broadly useful measures, as suggested in [IBM 1998]. They further include correct and responsible management control of how the serving computers are administered and independent audit of the same from time to time [Rosen 1970]. Highly respected guides for these topics were published in 1970 and renewed recently.\n\nFor stable library service, in addition to having the right functionality, the technology base must have \"industrial strength\", viz., handle all sorts of failures gracefully, be extensively tested in the environments in which it will be used, have good user and technical documentation, and be accompanied by long-term commitments for functional and platform upgrades and ready service for unanticipated interruptions. This is affordable only with a sufficient customer base and application breadth; for digital library services, the obvious base includes document imaging applications which are gradually broadening to include multimedia content. This application is sometimes called Enterprise Document Management (EDM); IBM's digital library strategy includes reusing as much as possible of EDM offerings for storing, protecting, and delivering digital documents, and as much electronic commerce technology as applies, recognizing that commerce in physical goods might extend into commerce for digital documents, although the latter prospect is developing more slowly than many people anticipated in 1995. (This fits well with the intention of some research libraries to cover part of their running costs by fee services.)\n\nIf one accepts this, one must consider DL information protection tools as extensions of more broadly applicable tools. What follows implicitly draws on encryption, key management, and certificate technology for which there exist good textbooks and evaluations, e.g., [NRC 1996]; it explicitly discusses access control which assumes personal authentication, with the latter not being further mentioned.\n\nAny information system has a boundary within which its custodians can enforce organization policies. Figure 1 suggests this boundary for the case of a digital collection which includes its own metadata and access control information. We can conveniently separate measures into those effective within this administrative boundary, those which extend the effective administrative boundary, and those far beyond the boundaries; the last encourage proper behavior and hinder unauthorized actions without being able to prevent them. Within the administrative boundary, it is often possible to constrain what software is used to process digital objects and/or to ensure that employees or other institutional members follow defined rules. In administrative boundary extensions of the type discussed below, it is possible to negotiate terms and conditions of document release. Beyond the administrative boundaries, technical means are less effective, so that we must rely on legal and social measures, and develop these beyond what exists today.\n\nFigure 1. A protected resource and the rest of the world: The black portions describe any library or object store; the green portions are specific to access control.\n\nWhat the Safeguarding Series and Other Sources Already Teach\n\nEach article in our Safeguarding ... series has presented a technology without connecting it carefully to other technologies needed to realize \"complete\" digital library services. These and other articles have sketched what is to be protected [Gladney 1997a], [Gladney 1997c]; what is available in digital watermarking for protection outside administrative control boundaries [Mintzer 1997]; how users might inspect and edit protection rules [Walker 1998]; how protection rules can durably record the terms and conditions for each property [Alrashid 1998]; transmitting rules from where they are generated to where they are needed; efficient payment mechanisms [Herzberg 1998]; trustworthy identification of who is generating a rule set (authentication), providing a document, or requesting one; unambiguous identification of the things being protected [Gladney 1998a]; how administrative data can be bundled with content for distribution [Lotspiech 1997]; and so on.\n\nWe do not agree that \"trusted systems\" [Stefik 1997] is a promising concept for document protection, at least not in the next few years and perhaps never because there seem to be fundamental flaws (see [Gladney 1998b]).\n\nIn the early 1990's, when attention was focused on \"open distributed systems\" and \"object orientation\", a popular model articulated what was wanted in building blocks from which loosely coupled services could be marshalled for tasks whose purposes were decided late in design progressions. Curiously, this compelling model no longer figures strongly in discussions, even though it effectively communicates design principles for resources that are sprinkled around the network and invoke each other dynamically. The model centers on the concepts of protected resources and related resource managers; suggested in Figure 2, a protected object is the combination of some data resource (which might be either persistent or ephemeral), and some server and client software which together constitute the resource manager.\n\nFigure 2. Client/server structure for a protected resource: being one way of providing isolation demanded by Figure 1.\n\nThe only access paths to the data are the API's of the client portion of the resource manager.\n\nThe server component confers all the functionality and quality (concurrency, serialization, recovery, integrity, security, ...) properties of the resource.\n\nTo avoid redundant software, any resource manager can call other resource managers (more or less as subroutines).\n\nFor performance when the client portion and the server portion are co-located, this can be detected automatically when (sub)systems are generated or loaded, followed by choice of optimal communications, e.g., when they are in the same minicomputer and suitable memory protection is provided by the hardware, copying in memory could be used.\n\nThe meaning of \"open\" is that the client-server protocol is sufficiently defined and publicly communicated so that any software supplier can supply either a client instance or a server instance with confidence that it will interoperate with complying components supplied by others.\n\nThis protected resource model is implicit in some thinking about digital library, e.g., [Arms 1995] even though the authors do not explicitly acknowledge this.\n\nInside Administrative Control Boundaries: Document Access Control and Permissions\n\nWe know of an academic collection of photographic slides, in use for many years as a tool for undergraduates, for which the managers doubt they comply properly with rights-holders' constraints. As part of considering creating a digital version of this collection, they have decided to invest in careful compliance. Since the collection has several hundred thousand slides, representing the rules and commitments in an on-line database is being considered. This typifies a developing sense in many enterprises that the onset of digital documents demands better compliance with accepted intellectual property rules and that digital databases are part of what is needed to do this at acceptable cost. Recall that copyright privileges apply for 70 years or longer after the death of authors; institutional memory is at risk without more mechanized means than are commonly employed today.\n\nWe should refine prototype database schema and tools to record for decades or longer the terms and conditions of access to copyright-encumbered materials, making this information accessible to administrators and to end users in ways that allow negotiation of access to differentiated communities (members of the university, members of other universities, local citizenry, alumni, ...) with payment for access in some cases. University and IBM projects have such prototype technology in hand and are investigating lacunae, but we currently have too few early deployment opportunities. Many enterprises have urged, and continue to urge, that they need the kind of software that Case Western Reserve University (CWRU) has developed, but none apart from CWRU has yet been willing to risk being a deployment leader. The CWRU prototype is successful in its domain, but needs to be integrated with other digital library components, to have replaced certain components now understood to make untenable business assumptions, and to be tested in a publisher's environment and to scales beyond what CWRU can bring under control. We'll return to this issue of deployment later in the article.\n\nIn layered software, lower layers (those close to the hardware) tend to be more generic (useful to more applications) than higher layers (those close to what users see). To the extent that we can share functionality by pushing it into lower layers, we make this functionality less expensive and better tested. For digital library we certainly do this by using generic file systems, database management tools, and communication services and have to some extent achieved this for the storage subsystem [Gladney 1993], which embeds access control services [Gladney 1997]. We now see it possible and desirable to push into this layer the database core of authorization management by identifying the similarities and differences of permissions management and access control, and representing the similar things once only for applications which range from enterprise document management to digital library.\n\nAcademic applications of document collections differ from clerical applications more in higher layer functions than in lower layer. In each case, library services need to blend into whatever front end applications the end user wants to use for most of his computing workload. For the internal revenue clerk, this is typically a work-flow management for rapidly executing similar checking for hundreds of tax returns; for the scholar, it is searching, reading, and extraction as an adjunct to analysis and writing. For some topics, such as environmental studies and public health investigations, researchers partly draw on the same documents as clerical users. Shared lower level tools thus become mandatory, rather than merely a cost-saving tactic.\n\nThe similarities between access control databases and permissions databases jump out at you if you inspect the schema sketched in [Gladney 1998b] with those in [Gladney 1997]. As authorization decisions begin to depend on users' organizational roles [Ferraiolo 1995] and other user attributes, such as commitment to pay, and document dependent values [Sloman 1994], we are further motivated in this direction. We seek further similarities, because the cost of incorporating more complex decision-making requires larger amortization bases, and because using organizations will neither tolerate nor accept different mechanisms for their administrative systems than for their document management systems. A final push comes from recent understanding that library services must bridge administrative domains (IBM Research and Stanford University are different administrative domains).\n\nAccess control software developed separately between 1970 and today for different operating systems and for different services within some systems (e.g., OS/MVS files and DB2 relational databases are protected by different software, even though access control functionality is similar for these two resource classes). The access control needs of different subsystems are converging as applications use them together. The complexity presented by marginally different access control solutions for the several services within any single computing complex is not acceptable to administrators; such differences extended to differences among service centers will not be acceptable to end users. We find scores of scholarly papers, touching on:\n\nRequirements in databases, in office applications, in LANs, in more general networks, within file systems, and recently also in information systems\n\naccess control in object-oriented databases, and in distributed object services;\n\ndistinctions between military and commercial systems;\n\ndelegation and control based on organizational roles rather than on user identities and group memberships; and\n\naccess control as a component of larger security services.\n\nNotwithstanding all this work, the access control methods used by widely deployed software subsystems is only marginally changed from what was designed in the early 1970s. The new work has mostly had no effect.\n\nOur Document Access Control Method (DACM) [Gladney 1997] is already well poised to implement what the prior paragraphs of this section call for. It is structured into a model-independent base within which each protected object selects an access control object which includes both access control data (this could be the kind of access control list that DCE file systems provide) and a pointer to some interpreter or permission function. Each of several interpreters would implement an access control model; this could be a role-based model, an object-oriented model, the model [Gladney 1997] recommends as particularly suited to office applications, or some entirely new model. As suggested by Figure 1, several such permission managers can be part of and invoked by a document storage subsystem (the data resource manager portion of a digital library).\n\nWe emphasize that thes software structure of the prior paragraph contributes to essential flexibility. Specifically, we already know that different applications favor different authorization models. We further anticipate that different jurisdictions will want different policies (e.g., the French government, in contrast to the University of California). Whenever a new policy is asked for by custodians of a class of collections, we need only create a new permission function (Figure 1). Typically the cost of this will be about two person months. Moreover, adding a permission function has no effect whatever on the existing content of a library; existing objects will continue to point to existing access control objects which will continue to point at prior permission functions. The new permission function will come into effect for those objects whose owners choose to use the new function. This treatment of the core of authorization management illustrates why we are confident of being able to implement whatever policy is chosen by the authorities for each collection, and do not need to wait until policies are chosen before we make available the authorization management. In fact, most of what is described in [Gladney 1997] is embedded in the IBM VisualInfo product, whose pertinent portions are reused in the IBM Digital Library offering, and has been in use for 4 years.\n\nThis dichotomy between supposed needs with suggested solutions and a change pace not suited the pace of introduction of other technologies suggests that we should consider stark alternatives and their consequences.\n\nWe might accept that we must make do with current access control services for another 20 years; to software engineers and computer sciences, this notion is emotionally unacceptable.\n\nWe might build a \"federating\" layer of application level software that intervenes between existing services and users to present interfaces which \"paper over\" the accidental differences; this approach is being followed by Tivoli Systems (an IBM subsidiary), and is commercially successful, but we cannot help wondering whether this is an interim solution or can be given a sufficient architectural basis to be durable.\n\nWe might build modular access control services which can easily be incorporated into various middle software (\"middleware\") offerings and which have sufficient appeal to be adopted instead of custom solutions; we have such software within IBM VisualInfo, but have not yet seen a practical way to promulgate it in the marketplace. The hurdles are in marketing and investment, not technical.\n\nWe might design to replace the current access control mechanisms, knowing the barriers to replacing what exists and works fairly well. For this approach to succeed, the proposed solution must be compellingly better than what is currently in use and must emulate every current method used in applications in which change seen by users is unacceptable. Finally, we must be poised with ready code and persuasive arguments when current methods \"break\", e.g., when centralized human administration breaks down because there are too many objects, too many users, and too many organizational changes for any central group to manage well.\n\nThe last suggests yet another observation: both user administration and access control services have been built starting from designs for monolithic systems (\"big iron\"), with extensions to accommodate the fact of distributed services. For example, LAN security service offerings are still in fact centralized, with a single data collection for each LAN-wide service. Perhaps it is time to design and build from a model with many uncoupled resource pools (Figure 2) to which coupling is added. We find this last prospect so compelling that we address it in a separate section below.\n\nEnlarging Administrative Control Boundaries: Cryptographic Envelopes\n\nNetwork technologies enable delivery directly from publishers, or even authors, to end users. Nevertheless we believe that the fundamental values of library organization along current lines carry into digital library services with little modification beyond their implementation mechanisms. We realize these values in a three-tier architecture in which the middle tier accumulates content (or pointers to content) from each of several publishers and directly from many authors (e.g., the faculty of the university), organizes these accumulations as deemed helpful for its most active user communities, and isolates those users from publishers for privacy, for performance, and for administrative convenience [Choy 1996].\n\nThe widely-known network security technologies, Secure Sockets Layer (SSL) and Secure HyperText Transport Protocol (SHTTP), are not as helpful with such a middle role as cryptographic envelopes, realized by IBM in a design called CryptolopeTM . From the end user's point of view, SSL/SHTTP do well, with good protection against eavesdroppers and guaranteed authenticity of information (if the server is trusted). Durability of access terms and conditions and continued access to information once delivered are, however, problematical. But they serve neither publishers nor librarians very well because they require continuous running of secure servers, which can be expensive, and little mechanism for administering licence restrictions [Lotspiech 1997].\n\nEach cryptolope wraps a collection of related files, administrative information, and cryptographic keys into a package which includes cryptographic signatures to ensure authenticity, povenance, and completeness and encrypts secret portions under a secret key. We do require a network of clearing centers to administer compliance with owners' terms and conditions and provide keys for released portions to end users. The network protocols for encryption keys and other confidential information are fully worked out. Very pretty properties emerge:\n\nCryptolopes may be transmitted by any means at all, without further care for their security.\n\nDocument authenticity and provenance can be checked by end users, without assistance from librarians. Well-known certificate mechanisms and hierarchies of trust can be used for insurance.\n\nAuthorizations can be checked without either publishers or librarians managing user identification.\n\nDescriptions of protected content and protected administrative data can be included in the clear, so that patrons can have information to decide whether to buy access.\n\nPortions of related content can be released separately, so that publishers can package to enable customers to avoid paying for portions of no interest.\n\nAdministration can be extended to information transmitted by different channels, e.g., encryption keys for cable video delivery.\n\nAt this point, the reader might wonder, \"Since this technology is advertised so glowingly, why is it not available commercially?\" Part of the answer is that, although in 1995 it was conventional wisdom that information would be \"sold on the click\", no such market has emerged. IBM's first Cryptolope product was optimized for this market, and misses features needed for digital library applications. We have portions needed for digital library in hand, but have not yet integrated them into a market offering partly for reasons which will become clear below.\n\nAlong the way, we learned that we need to represent terms and conditions in at least three domains: on screens in a style that administrators and end users can edit, understand, and analyze with a minimum of prior training or \"help\" text; in databases made reliably durable for survival over decades and longer; and for transmission among heterogeneous computing systems, i.e., supporting \"open\" systems so that software consumers have the benefit of multiple technology sources. The best storage representation is one that allows the administrative data to be reliably preserved for many years; this can best be done with relational database technology [Alrashid 1998]. The transmission format must be linear; XML is fast evolving as the favored choice. The external language should be whatever is best for human comprehension and convenience; the author feels the best choice to be Walker's IKM.\n\nProtection Beyond Feasible Administrative Control Boundaries: Marking Documents\n\nWhen a valuable digital object is in the clear (not encrypted) and outside the sphere of control of administrators committed to protecting it, the only technical means of hindering misappropriation or other misuse is to distort it to discourage and/or detect wrong-doing. This can be either by releasing only versions of insufficient quality for the suspected sins or by adding a signal that includes provenance information."
    }
}
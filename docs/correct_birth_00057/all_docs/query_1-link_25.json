{
    "id": "correct_birth_00057_1",
    "rank": 25,
    "data": {
        "url": "https://groups.oist.jp/ncu/event/wcci-forum",
        "read_more_link": "",
        "language": "en",
        "title": "WCCI 2024 Open Forum on AI Governance",
        "top_image": "https://groups.oist.jp/sites/default/files/imce/u194/WCCI/BENGIO_Yoshua.jpg",
        "meta_img": "",
        "images": [
            "https://groups.oist.jp/sites/all/themes/oistgroups2016/images/logo-print-en.png",
            "https://groups.oist.jp/sites/all/themes/oistgroups2016/images/logo-print-ja.png",
            "https://groups.oist.jp/sites/all/themes/oistgroups2016/images/oist-header-en.png",
            "https://groups.oist.jp/sites/all/themes/oistgroups2016/images/oist-header-en-mobile.png",
            "https://groups.oist.jp/sites/default/files/styles/group_image/public/eventimg/2406/Open%20Forum%20on%20AI%20Governance%20400.png?itok=hlwidqW-",
            "https://groups.oist.jp/sites/default/files/imce/u194/WCCI/WCCI2024OpenForumFlyer300.jpg",
            "https://groups.oist.jp/sites/default/files/imce/u194/WCCI/BENGIO_Yoshua.jpg",
            "https://groups.oist.jp/sites/default/files/imce/u191/takahashi.png",
            "https://groups.oist.jp/sites/default/files/imce/u194/WCCI/Russell_Stuart.jpg",
            "https://groups.oist.jp/sites/default/files/imce/u191/kurihara.jpg",
            "https://groups.oist.jp/sites/default/files/imce/u191/Prof.NUROCOK.jpg",
            "https://groups.oist.jp/sites/default/files/imce/u191/Lee.jpg",
            "https://groups.oist.jp/sites/default/files/imce/u191/Yoichi%20Iida.jpg",
            "https://groups.oist.jp/sites/default/files/imce/u191/murakami.jpg",
            "https://groups.oist.jp/sites/default/files/imce/u191/Jaan%20Tallinn.jpg",
            "https://groups.oist.jp/sites/all/modules/custom/oist_rsn_client/img/oist-subscription.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-05-02T17:50:34+09:00",
        "summary": "",
        "meta_description": "WCCI 2024 Open Forum on AI Governance How to harness evolving AI: Dialogue of developers, users, and policy makers",
        "meta_lang": "",
        "meta_favicon": "https://groups.oist.jp/sites/default/files/favicon.ico",
        "meta_site_name": "OIST Groups",
        "canonical_link": "https://groups.oist.jp/ncu/event/wcci-forum",
        "text": "How to harness evolving AI\n\n– Dialogue of developers, users, and policy makers –\n\nWorld Congress on Computational Intelligence (WCCI) 2024 in Yokohama is going to be the largest conference on AI in Asia in 2024. At this opportunity, given the public interests in the risks of AI and rapid legislative motions, we hold an open forum to bring together AI researchers, lay users, and policy makers.\n\nThe forum is on-site/online hybrid format and open to the public with pre-registration (Closed). Talks will be in English, and we plan to provide Japanese translation by AI. We expect participation of not only AI experts but also broad citizens and students.\n\nDate & Time: 9:00 – 18:00 JST (UTC+9), Sunday 30th, June 2024\n\nPlace: Pacifico Yokohama Conference Center 5th floor, Room 503\n\nProgram\n\n9:00 Room and Connections Open\n\n9:20 Opening Remarks\n\nSession 1: Future prospects and dangers of AI\n\n9:30 Yoshua Bengio, U Montreal (online)\n\n10:00 Koichi Takahashi, RIKEN, AI Alignment Network\n\n10:30 Stuart Russell, UC Berkeley (online)\n\n11:00 break\n\n11:10 Yi Zeng, Chinese Academy of Science (online)\n\n11:40 Satoshi Kurihara, Keio U, JSAI\n\n12:10 Discussion (coordinator: Kenji Doya, Joichi Ito)\n\n12:40 Lunch break\n\nSession2: How to align AI research and applications to societal values\n\n14:10 Vanessa Nurock, U Côte d'Azur, UNESCO\n\n14:40 Hiroaki Kitano, SONY CSL, OIST\n\n15:10 Lee Hickin, Microsoft Asia (online)\n\n15:40 break\n\n15:50 Yoichi Iida, Ministry of Internal Affairs and Communications, Japan\n\n16:20 Akiko Murakami, Sompo Japan, Japan AI Safety Institute\n\n16:50 Jaan Tallinn, Future of Life Institute (online)\n\n17:20 Discussion (coordinator: Arisa Ema, Satoshi Kurihara)\n\n18:00 Closing\n\nRegistration page (required for participants not registered for WCCI 2024)\n\nRegistration deadline: 25th June, 2024\n\nSpeakers\n\nYoshua Bengio\n\nProfessor, Department of Computer Science, University of Montreal\n\nScientific Director, Mila\n\nCanada CIFAR AI Chair\n\nScientific advisor, UK AI Safety Institute\n\nCatastrophic AI risks and the governance of AGI projects\n\nWe are on a path towards human-level AI, also called AGI, with uncertain timeline sand uncertain risks, ranging from threats to democracy and national security to existential risks due to loss of control to a rogue AI. In spite of these risks, corporations are competing fiercely to build AGI and racing ahead. To make sure they do not cut dangerous corners and to make sure the power of future AIs is not abused or destabilizes the geo-political order, we will need significant effort in how to manage these projects, ranging from organization-level governance to national regulation and international treaties, including to avoid dangerous proliferation, harmonization of policies and a move towards AI for the public good, at a global level.\n\nBiography: Yoshua Bengio is Full Professor in the Department of Computer Science and Operations Research at U. Montreal, as well as the Founder and Scientific Director of Mila. Considered one of the world’s leaders in artificial intelligence and deep learning, he is the recipient of the 2018 A.M. Turing Award with Geoff Hinton and Yann LeCun, known as the Nobel prize of computing, and holds a Canada CIFAR AI Chair. Besides, he is a Fellow of both the Royal Society of London and Canada, ACM Fellow, Officer of the Order of Canada, recipient of the Gerhard Herzberg Canada Gold Medal for Science and Engineering and Scientific advisor for the UK AI Safety Institute.\n\nKoichi Takahashi\n\nChair, AI Alignment Network\n\nProject Professor, Graduate School of Media and Governance, Keio University\n\nBuilding an AI Safety Research Ecosystem in Japan\n\nJapan has seen active discussion on AI governance since before generative AI. In 2014, academia-led initiatives such as 'AI and Society meetings' were formed. In 2017, the Ministry of Internal Affairs and Communications published 'Draft AI R&D Guidelines for International Discussions'. After the advent of generative AI, the Hiroshima AI Process was agreed upon in 2023, and the world's third AI Safety Institute was established in 2024. Although the main focus has been on immediate risks, risks from future AI with more advanced capabilities remain a major concern. Nevertheless, we have seen relatively little attention towards such long-term risks of AI in Japan, and there's only a minor presence of Japanese researchers in this field thus far. To address this missed opportunity, we established a new non-profit organization called AI Alignment Network (ALIGN) in 2023 to pave the way for a hopeful future where AI is harmoniously implemented in society. In this talk, I will introduce ALIGN's aims, objectives, and plans for creating an ecosystem of researchers and practitioners.\n\nBiography: Koichi Takahashi, Ph.D., is the Chair of AI Alignment Network (ALIGN). As a Project Professor at Graduate School of Media and Governance, Keio University, he has been a key advocate of AI safety and alignment in Japan, and also an expert committee member of SIG-AGI at the Japanese Society of Artificial Intelligence.\n\nCatastrophic AI risks and the governance of AGI projects\n\nWe are on a path towards human-level AI, also called AGI, with uncertain timeline sand uncertain risks, ranging from threats to democracy\n\nStuart Russell\n\nProfessor, Department of Computer Science, University of California Berkeley\n\nDirector, Center for Human-Compatible AI\n\nDirector, Kavli Center for Ethics, Science, and the Public\n\nAI: What if we succeed?\n\nThe media are agog with claims that recent advances in AI put artificial general intelligence (AGI) within reach. Is this true? If so, is that a good thing? Alan Turing predicted that AGI would result\n\nin the machines taking control. I will argue that Turing was right to express concern but wrong to think that doom is inevitable. Instead, we need to develop a new kind of AI that is provably beneficial to humans. Unfortunately, we are heading in the opposite direction. Regulation may be required to correct this mistake.\n\nBiography: Stuart Russell is a Professor of Computer Science at the University of California at Berkeley, holder of the Smith-Zadeh Chair in Engineering, and Director of the Center for Human-Compatible AI and the Kavli Center for Ethics, Science, and the Public. He is a recipient of the IJCAI Computers and Thought Award, the IJCAI Research Excellence Award, and the ACM Allen Newell Award. From 2012-14 he held the Chaire Blaise Pascal in Paris. In 2021 he received the OBE from Her Majesty Queen Elizabeth and gave the BBC Reith Lectures. He is an Honorary Fellow of Wadham College, Oxford, an Andrew Carnegie Fellow, an AI2050 Senior Fellow, and a Fellow of AAAI, ACM, and AAAS. His book \"Artificial Intelligence: A Modern Approach\" (with Peter Norvig) is the standard text in AI, used in over 1500 universities in 135 countries. His research covers a wide range of topics in artificial intelligence, with a current emphasis on the long-term future of artificial intelligence and its relation to humanity. He has developed a new global seismic monitoring system for the nuclear-test-ban treaty and is currently working to ban lethal autonomous weapons.\n\nSatoshi Kurihara\n\nProfessor, Faculty of Science and Technology, Keio University\n\nPresident, Japanese Society for Artificial Intelligence (JSAI)\n\nAlignment Difficulty of Scaling Swarm AI\n\nLarge-scale LLMs such as ChatGPT achieved high performance because of the quality changes with deploying scaled resources. One of the typical scaling problems is so-called flash crashes, and there are concerns about unexpected behavior in many autonomous drone formations. The possibility of controlling scaling AI will be discussed.\n\nBiography: Professor of the Faculty of Science and Technology, Keio University. Director of Center of Advanced Research for Human-AI Symbiosis Society, Keio Univ. President of the Japanese Society for Artificial Intelligence (JSAI), Director of JST PRESTO “Social Transformation Platform” (JST: Japan Science and Technology Agency, PRESTO: Promoting Individual Research to Nurture the Seeds of Future Innovation and Organizing Unique, Innovative Network.\n\nVanessa Nurock\n\nProfessor in Philosophy, Université Côte d’Azur, France\n\nUNESCO EVA Chair\n\nTowards an “Ethics by Design for AI”?\n\nThe idea that AI ethics should not be only considered as only top-down, or bottom-up approach is now commonly challenged by the possibility of an “Ethics by Design” for AI. This talk aims at explaining the main characteristics of Ethics by Design, especially regarding the issue of ‘deskilling’. I will argue that the concept of ‘catastrophe’ is particularly fruitful to frame this Ethics by Design for AI and to help us design an ethical AI for future generations.\n\nBiography:Vanessa Nurock is a Professor in Philosophy and Deputy Director of the Centre de Recherche en Histoire des Idées (CRHI) at Université de Côte d’Azur (France). She also holds the UNESCO EVA Chair in the Ethics of the Living and the Artificial (https://univ-cotedazur.fr/the-unesco-chair-eva/chair-eva) and is the head of the Program Committee on AI and Ethics of the International Research Center on Artificial Intelligence (https://ircai.org/project/ai-and-ethics/)\n\nHer research is positioned at the interface between ethics, politics and emerging science & technologies. She has published numerous papers and several books on topics such as justice and care, gender, animal ethics, nanotechnologies, cybergenetics, and neuroethics. Her current research, developed in her next book Care Ethics and New Technologies (Peeters Publishers, 2024 forthcoming), focuses on the ethical and political problems raised by Nanotechnologies, Cybergenetics and Artificial Intelligence.\n\nLee Hickin\n\nAI Technology and Policy Lead, Microsoft Asia\n\nBuilding a model for AI Governance and global coherence\n\nThere have been few moments like that we find ourselves in today, AI stands poised to transform our world, lives, and society and as we build the technology, so to must we build the governance to ensure it does so responsibly. Microsoft has been developing its Office of Responsible AI since 2019 as a mechanism to bring that governance to our business, people, and processes. This session will unpack what we have learned, what we have developed and how we work to ensure our AI technology aligns to societal values. We will also offer a look to what the future might hold for Global governance of AI leaning on examples from civil aviation and the nuclear energy sector.\n\nBiography:Lee has over 30 years’ experience in the technology industry, having been in technical sales, business development roles and most recently as the CTO for Microsoft ANZ.\n\nLee has worked across the UK, Asia, and Australia, before joining Microsoft, Lee led the Internet of Things team for Amazon Web Services in Asia Pacific. Earlier in his career, he was a CISSP security architect with RSA Security and worked at both Tivoli and IBM.\n\nHis work today spans Asia engaging with Government, regulators, policy makers and think tanks on strengthening the understanding of Artificial Intelligence, Large Language models & Machine Learning technology and the responsible ways in which it can be governed and operated.\n\nHe has supported Governments around the world as an independent contributor; being a member of the Singapore AI-Verify committee and a founding member of the NSW AI Advisory Committee. In addition, Lee has supported the Australian Federal Government’s AI Action plan and the National Quantum Strategy working directly with the Department of Industry and the Office of the Chief Scientist. He continues to work in collaboration with the National AI Centre in the Responsible AI Think Tank.\n\nLee believes in the potential of technology to help build a brighter and more inclusive future and is a regular speaker on innovation and Artificial Intelligence.\n\nYoichi Iida\n\nAssistant Vice Minister, Ministry of Internal Affairs and Communications (MIC), Japan\n\nJapan's Efforts toward Global Governance of AI\n\nSince 2016, Japanese government has been taking a leadership in international discussions at various policy fora such as G7, G20 and OECD, on AI governance toward coherent and inclusive global framework. In 2023, Japan launched Hiroshima AI Process with G7 Members and achieved first successful international framework comprising of guiding principles and a code of conduct to address the impact of advanced AI systems on our societies and economies. Yoichi IIDA looks back at what was discussed and agreed and looks to what would come next.\n\nBiography: Yoichi Iida is Assistant Vice Minister for International Affairs at the Japanese Ministry of Internal Affairs and Communications.\n\nHe chaired G7 working group meeting on ICT policy, when Japan took G7 Presidency in 2016. In this WG, Japan proposed starting international discussion on AI principles, which was supported by G7 ICT Ministers, and succeeded by following G7 Presidencies of Italy and Canada. Yoichi Iida also chaired G20 Digital Economy Task Force (DETF) that discussed G20 AI principles which were endorsed by the Leaders in OSAKA Summit in 2019. Since January 2020, he has been serving as the chair of OECD Committee on Digital Economy Policy (CDEP), which is discussing broader aspects of digital economy including AI governance and other digital policies.\n\nIn 2023, he once again chaired G7 Digital and Tech WG covering global AI governance, which led to the launch of Hiroshima AI Process at Hiroshima Summit. He serves as the chair of Hiroshima AI Process WG, and the chair of Executive Committee at Global Partnership on AI (GPAI).\n\nAkiko Murakami\n\nExecutive director, Japan AI Safety Institute\n\nThe Future of AI Safety: The Role of Japan AI Safety Institute\n\nEnsuring AI safety is essential for promoting the widespread adoption and utilization of AI technologies. Considering the rapid evolution of AI, hard regulation for AI safety is not appropriate; therefore, creating guidelines and providing information are more crucial. The AI Safety Institute (AISI) was established for these purposes, not only to create guidelines but also to formulate red teaming and test-bed frameworks. Additionally, AISI collaborates with similar emerging organizations worldwide, such as the US and UK AISIs. This presentation will discuss AISI’s contributions to date and outline our future initiatives.\n\nBiography: Akiko Murakami is the Executive Director of the Japan AI Safety Institute (AISI) since 2024 and also serves as the Chief Data Officer at Sompo Japan Insurance Inc. In her role at Sompo Japan Insurance, she leads the digital transformation of P&C insurance by leveraging digital technologies and AI to drive innovation.\n\nBefore joining Sompo Japan in 2021, Akiko was a researcher in Natural Language Processing, social analysis, and text mining at IBM Research for 16 years. She then spent 5 years as an architect leading Watson development at IBM. She is actively involved in \"resilient engineering\" for disaster recovery, mitigation, and risk management, and founded the IT Disaster Assistance and Response Team (IT DART) in 2015, where she continues to serve as a board member. Additionally, she has been a board member of The Association for Natural Language Processing since April 2022. Akiko also served as a part-time lecturer at the Disaster Prevention Research Institute, Kyoto University, until March 2024.\n\nJaan Tallinn\n\nMember of Board of Directors, Future of Life Institute\n\n‘Fireside Chat’ with Ryota Kanai\n\nBiography: Jaan Tallinn is a founding engineer of Skype and Kazaa. He is a co-founder of the Cambridge Centre for the Study of Existential Risk (cser.org), the Future of Life Institute (futureoflife.org), and philanthropically supports other organisations tackling existential and catastropic risk. Jaan serves on the AI Advisory Body at the United Nations, on the Board of the Center for AI Safety (safe.ai), and the Board of Sponsors of the Bulletin of the Atomic Scientists (thebulletin.org). He has previously served on the High-Level Expert Group on AI at the European Commission, as well as on the Estonian President’s Academic Advisory Board. He is also an active angel investor (metaplanet.com), a partner at Ambient Sound Investments (asi.ee), and a former investor director of the AI company DeepMind (deepmind.google).\n\nCo-organizers\n\nMinoru Asada, Osaka U, RSJ\n\nKenji Doya, OIST, INNS, JNNS, APNNS\n\nArisa Ema, U. Tokyo, RIKEN AIP\n\nJoichi Ito, Chiba Institute of Technology\n\nRyota Kanai, ARAYA/Moonshot\n\nSatoshi Kurihara, Keio U., JSAI\n\nMasashi Sugiyama, U. Tokyo, RIKEN AIP\n\nKoichi Takahashi, RIKEN BDR, ALIGN\n\nSponsors\n\nAI Alignment Network (ALIGN)\n\nAsia Pacific Neural Network Society (APNNS)\n\nChiba Institute of Technology (CIT)\n\nInternational Neural Network Society (INNS)\n\nJapanese Neural Network Society (JNNS)\n\nJapan Society for Artificial Intelligence (JSAI)\n\nOkinawa Institute of Science and Technology (OIST)\n\nRIKEN Center for Advanced Intelligence Project (AIP)\n\nRobotics Society of Japan (RSJ)\n\nSupport by\n\nInformation-technology Promotion Agency, Japan (IPA)\n\nNoeon Research\n\nPokeTalk Co. for AI Translation\n\nCity of Yokohama\n\nRelated Events"
    }
}
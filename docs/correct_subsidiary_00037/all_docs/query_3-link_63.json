{
    "id": "correct_subsidiary_00037_3",
    "rank": 63,
    "data": {
        "url": "https://www.snia.org/news-events/sdc-india/agenda-more/2022-sdc-india-abstracts",
        "read_more_link": "",
        "language": "en",
        "title": "2022 SDC India Abstracts",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.snia.org/sites/default/files/SDCIndia22logo_Stacked%20small.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Day One Plenary Day One Track 1 Day One Track 2 Day Two Plenary Day Two Track 1 Day Two Track 2 Day One Plenary Abstracts Evolution of NVMeoF Controllers in combining decentralized Edge with centralized public/private Cloud Sridhar Sabesan, Director, Platform Engineering, Western Digital Abstract Edge Datacenters and devices/compute/storage are starting to get more",
        "meta_lang": "",
        "meta_favicon": "https://www.snia.org/sites/all/themes/snia_event2017/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://www.snia.org/news-events/sdc-india/agenda-more/2022-sdc-india-abstracts",
        "text": "An SSD with high-end cellular technology provides high storage durability but suffers significant performance degradation due to multiple retrieval functions. However, the re-reading method is essential to ensure the integrity of the SSD memory. It can significantly increase SSD reading delays by introducing multiple re-read steps that re-read the target page with adjusted reading reference values. To reduce re-read delays, two advanced features are widely accepted in NAND Flash-based SSDs: 1) CACHE READ command and 2) ECC robust engine. First, we can minimize the delay in retrying using the advanced CACHE READ command that allows the NAND flash chip to perform sequential readings in a pipeline. Second, a large ECC competent margin exists in the final attempt and can be used to reduce chip-level learning delays. Based on new findings, we discuss two new strategies to reduce re-reading delays effectively: 1) Pipelined Read-Retry (PR²) and 2) and Adaptive Read-Retry (AR²). PR² minimizes the delay of the experimental task by installing the sequence retrieval steps using the CACHE READ command. AR² minimizes the delay of each re-step step by flexibly reducing the chip level reading delay depending on current operating conditions that determine the ECC power margin. These strategies improve SSD response time by up to 31.5% (17% on average) over a modern basis with only a slight change in SSD controller.\n\nEffective device thermal management based on dynamic ranking of device cooling needs\n\nHemant Gaikwad, Test Senior Principal Engineer and Shelesh Chopra, Senior Director, Dell Technologies & Rahul Vishwakarma, Graduate Student, California State University\n\nAbstract\n\nA constant cooling is provided to the systems for the data center cooling perspective, while the fans provided on the devices try to locally cool the device components by spinning faster whenever required. The fans spin faster, in turn, resulting in higher power consumption. Every device in the data center is cooled by the centrally conditioned air at a constant set temperature irrespective of the heat dissipated by the device; however, what is needed is a context-aware cooling per device or a group of devices. Devices heating at a much higher rate than the cooling offered negatively impact the device performance, where CPUs, DIMMs, Disks, NICs, etc. under-perform due to high temperatures. Within the Datacenter racks too, devices higher in the rack get more heated compared to the lower placed devices, since the hot air moves upwards. Moreover, the inlet and outlet temperature control are difficult to achieve, and this inefficient thermal management increases the cost and reduces device availability.\n\nAlso, on the other side, overcooling is another problem for the data center leading to high power bills where all devices are provided with the same amount of cooling irrespective of whether any device needs it or not.\n\nWe present a mechanism to provide recommendations with respect to effective device thermal management based on a-priori predictions for the future device statistics.\n\n1. The solution can autonomously identify devices needing specialized cooling in a particular range and group those together and enforce business continuity for such devices.\n\n2. Dynamic ranking of device cooling needs is done based on the forecast with continuous accuracy and detailed root-cause analysis using causal relationships.\n\n3. Fourier Time Series to find next n-step ahead forecast of the device thermal metrics along with device load & failure and devices depicting prolonged periods of thermally high state are dynamically ranked for special cooling need.\n\nLearning Objectives\n\nChallenges while deploying a solution for device cooling\n\nMachine learning implementation for device thermal management\n\nRecommendations with respect to effective device thermal management based on a-priori predictions\n\nFuture of Long term Archival Data storage - DNA\n\nKannadasan Palani, Senior Test Manager, MSys Technologies\n\nAbstract\n\nThis session will demonstrate the methodology and principle of using DNA for long-term data storage. The audience will learn how to store data in SSD, HDD, or Tape Drive and discuss its cost, reliability & longevity. The session will help understand the pros and cons of each drive type. We will learn What Is DNA storage and how it works.\n\nDNA data storage is the process of encoding and decoding binary data onto and from synthesized strands of DNA. We will learn how the original digital data is encoded, written (synthesized using chemical/biological processes), and stored. We will also understand how DNA molecules are sequenced to reveal each individual A, C, G, or T in order and remapped from DNA bases back to 1s and 0s.” When the stored data is needed again. Also, We will see the pros and cons of DNA storage and why we are calling it as \"Future of Long-term Archival Data storage\"\n\nLearning Objectives\n\nMethodology and principle of using DNA for long-term data storage\n\nProcess of encoding and decoding the binary data\n\nPros and cons of each drive type - SSD, HDD, Tape Drive\n\nMemory Profiling of Java Based Microservice Architecture\n\nGururaj Kulkarni, Distinguished Member of Technical Staff, & Cherami Liu, Architect, Dell Technologies\n\nAbstract\n\nLearning Objectives\n\nUnderstanding of Monolithic and Microservices Architecture\n\nMemory model and sizing consideration for Java based microservices\n\nPotential tools to Monitor and measure the microservice memory usage\n\nHigh Speed Magnetic recording without energy dissipation\n\nBoris Tankhilevich, CEO, Magtera Inc.\n\nAbstract\n\nWhile recent developments in photonics enable lossless data transfer with speeds exceeding 1 Tb/s, current magnetic data storage cannot keep up with these data-flow rates nor decrease energy dissipations. Consequently, already now data centers are becoming the biggest consumers of electricity world-wide.\n\nThe ultrafast writing of bits at the speed up to THz that does not involve the dissipation of energy in the recording medium is a monumental challenge.\n\nMagtera invented an ultrafast writing of bits at the speed up to THz that does not involve any usage of electrical current thus avoiding the dissipation of energy in the recording medium.\n\nMore specifically the apparatus for novel technique of high-speed magnetic recording is based on manipulating pinning layer in magnetic tunnel junction-based memory by using terahertz magnon aser. The apparatus comprises a terahertz writing head configured to generate a tunable terahertz writing signal and a memory cell including a spacer that comprises a thickness configured based on Ruderman-Kittel-Kasuya-Yosida (RKKY) interaction. The memory cell comprises two separate memory states: a first binary state and a second binary state; wherein the first binary memory state corresponds to a ferromagnetic sign of the Ruderman-Kittel-Kasuya-Yosida (RKKY) interaction corresponding to a first thickness value of the spacer; and wherein the second binary memory state corresponds to an antiferromagnetic sign of the Ruderman-Kittel-Kasuya-Yosida (RKKY) interaction corresponding to a second thickness value of the spacer. The thickness of the spacer is manipulated by the tunable terahertz writing signal.\n\nLearning Objectives\n\nTerahertz Magnon Laser\n\nRKKY interaction\n\nUltra-fast magnetic recording without energy dissipation\n\nAugmenting SPDK with xNVMe BDEV\n\nKrishna Kanth Reddy, Associate Director & Kanchan Joshi, Associate Director, Samsung Semiconductor India Research (SSIR)\n\nAbstract\n\nSPDK creates block-device abstraction layer, referred as SPDK BDEV, on top of various low-level backends that are called BDEV modules. Linux aio, io_uring and NVMe are few examples of such modules.\n\nThis talk presents a new BDEV module, xNVMe BDEV, which has recently made its way to upstream and will be available as part of SPDK 22.09 release.\n\nxNVMe BDEV not only provides a way to use existing aio and io_uring interface, but also allows using asynchronous NVMe-passthrough interface that guarantees availability and scalability. We touch upon the new passthrough interface, and elaborate how SPDK block layer can utilize it through the implementation of xNVMe BDEV model. With the xNVMe BDEV module, applications such as RocksDB, CEPH that use SPDK, leverage the high performance Linux Asynchronous IOCTL feature without any change to their software interfaces.\n\nThe talk also showcases performance of the xNVMe BDEV module with the io_uring and the io_uring pass-through backends.\n\nLearning Objectives\n\nUnderstand SPDK block layer\n\nUnderstand xNVMe and xNVMe BDEV\n\nUnderstand io_uring passthrough for NVMe\n\nData Analytics with Computational Storage and PostgreSQL\n\nAjay Joshi, Technologist, Western Digital\n\nAbstract\n\nThe data in the cloud is ever-evolving and rapidly expanding, leading to an explosion in storage and network infrastructure requirements. Unfortunately, data analytics on this disaggregated stored data leads to high latencies and increased IO load.\n\nOur presentation shows how integrating Computational Storage with PostgreSQL's query planner allows it to utilize Computational Storage seamlessly to reduce the network & IO load.\n\nLearning Objectives\n\nUnderstand Computational Storage\n\nOffloading of query planner to hardware\n\nUnderstanding of advantages of the offload mechanism\n\nS3select: Computational Storage in S3\n\nGirjesh Rajoria, Software Engineer, Red Hat\n\nAbstract\n\nS3select is an S3 operation (introduced by Amazon in 2018) that implements a pushdown paradigm that pulls out only the data you need from an object, which can dramatically improve the performance and reduce the cost of applications that need to access data in S3. The talk will introduce s3select operation and architecture. It will describe what the pushdown technique is, why and where it is beneficial for the user. It will cover s3select supported features and their integration with analytic applications. It will discuss the main differences between columnar and non-columnar formats (CSV vs Parquet). We’ll also discuss recent developments for ceph/s3select. The presentation will show how easy it is to use ceph/s3select.\n\nLearning Objectives\n\nExplains how s3select improves the performance and reduces the cost of applications that need to access data in S3\n\nExplaining the official S3 operation (Introduced by AWS a few years ago) and how it is implemented in Ceph/s3select\n\nHelps in understanding how s3select can be used and where it is beneficial\n\nFile System Acceleration using Computational Storage for Efficient Data Storage\n\nSrija Malyala, Software Developer, & Vaishnavi SG, Software Developer, AMD\n\nAbstract\n\nWe examine the benefits of using computational storage devices like Xilinx SmartSSD to offload the compression to achieve an ideal compression scheme where higher compression ratios are achieved with lower CPU resources. This offloading of compute intensive task of compression frees up the CPU to cater to real customer applications. The scheme proposed in this paper comprises of Xilinx Storage Services (XSS) with Xilinx Runtime (XRT) software and HLS based GZIP compression kernel that runs on the FPGA. The hardware platform chosen is Xilinx SmartSSD which also has a unique feature of P2P data transfer where the data input/output to/from the FPGA is directly moved from/to the storage device without moving it back to the host system (x86) memory. This further helps in improving the overall system efficiency by reducing the DDR memory traffic by moving computation closer to where data resides. There are different places in the application/OS software stack where data compression can be offloaded to hardware. We have chosen to do this at the file system level because this will enable all the applications using the filesystem to benefit without necessarily making any changes to the application itself. We have selected the Linux ZFS filesystem as this is the most widely used and popular file system today.\n\nLearning Objectives\n\nWe examine the benefits of using computational storage devices like Xilinx SmartSSD to offload the compression\n\nPeer to Peer data flow and it's advantages\n\nAdvantages of offloading at File System\n\nDay Two Plenary Abstracts\n\nStorage architecture challenges in the Fintech sector – A call for innovation\n\nMurali Brahmadesam, CTO & Head of Engineering, Razorpay\n\nAbstract\n\nRansomware attacks? Data protection holds the key!\n\nVijay Mhaskar, VP Engineering and Pune Center Site Lead, Veritas\n\nAbstract\n\nData has become the Oil of this century. It’s not just large enterprises, but even consumers are not able to conduct their day-to-day chores without access to data. So, no wonder hackers have a field day. It’s not if - but it’s when! 80% of the organizations have publicly accepted that they have been attacked by ransomware, while the rest 20% are just not vocal about it.\n\nData protection is being seen as the last defense against any security threat. So, what is the expectation from customers and what are the possibilities?\n\nStreaming data platform\n\nKarnendu Pattanaik, Senior Manager, & Somesh Joshi, Software Engineer, Dell Technologies\n\nAbstract\n\nStreaming data is data that is generated continuously by thousands of data sources, which typically send in the data records simultaneously, and in small sizes (order of Kilobytes). Streaming data includes a wide variety of data such as log files generated by customers using your mobile or web applications, ecommerce purchases, in-game player activity, information from social networks, financial trading floors, or geospatial services, and telemetry from connected devices or instrumentation in data centers. Streaming data processing is beneficial in most scenarios where new, dynamic data is generated on a continual basis. The presentation will talk about storage of streaming data and aspects of platforms that enable data processing capabilities on this streaming data.\n\nCyberstorage - A security first storage approach\n\nSanjeev Kumar, Lead, Storage Center of Excellence, Tata Consultancy Services\n\nAbstract\n\nLiving in this modern era, losing information in physical mode is an outdated trend now. We have created a better solution for this i.e., Digital Data storage. Evolution of security also evolved the fluidity of attack on our unstructured data storage. Cyber-attack, a familiar term which resists storage organizations from providing a privacy consolidate experience to their users. Organizations have shown maturity in protecting end points for data centre products but the centralized storage is still vulnerable to most of the cyber-attack. Unstructured data platforms provide inadequate protection from malicious deletion, encryption and data exfiltration, making it an easy-to-attack target.\n\nIn this presentation, we will discuss about Cyber Storage phenomenon which uses active defence mechanism to identify, protect, detect, respond and recover cyber- attack on un-structured data storage solutions based on NIST Framework. We will draw an end to end solution approach, tools and best practices for unstructured data platform active protection from cyber-attack.\n\nThe new, catch-all, fast path to NVMe in Linux\n\nKanchan Joshi, Associate Director, & Anuj Gupta, Software Engineer, Samsung Semiconductor India Research (SSIR)\n\nAbstract\n\nWhile the Linux kernel has elevated the efficiency of the I/O stack with constructs such as io_uring, the NVMe storage continues to evolve with new features and command sets.\n\nMany new NVMe features either do not fit well within the Linux abstraction layers and/or face adoption challenges due to a lack of appropriate syscall interface. The existing NVMe passthrough interface does not help as it is tied to synchronous ioctls.\n\nTo address these challenges, we have added a new NVMe passthrough interface that will be available in the mainline kernel from 5.19 onwards.\n\nThis interface guarantees:\n\n(a) availability - any existing/future NVMe command-set/feature remains usable with this path\n\n(b) performance efficiency - as it is paired with io_uring\n\nThe talk presents the specifics, and how applications can go about using this new interface.\n\nWe also present the tooling/testing enhancements and provide the evaluation comparing this path against regular block and passthrough IO\n\nLearning Objectives\n\nUnderstand the limitation of existing path (block IO and sync passthrough)\n\nUnderstand how to use the the new path (async passthrough)\n\nUnderstand io_uring based async programming\n\nPoseidonOS: An Innovative Open Source Storage System and its exploration using Trident\n\nBadarinarayan Joshi, Associate Technical Director, & Arun V Pillai, Staff Engineer, Samsung Semiconductor India Research (SSIR)\n\nAbstract\n\nWith the popularity of NVMe SSDs, the NVMe-oF interface is becoming important in disaggregated datacenters. However, it is non-trivial to fully utilize large numbers of high-performance and high-capacity NVMe SSDs. In this talk, we will introduce PoseidonOS, which is an NVMe-oF reference solution that was open-sourced last year. In particular, we will introduce its essential features, and our choices for an NVMe-oF storage system as a building block of a disaggregated datacenter. We will also address the challenges in developing test libraries for the POS software stack, and the design principles applied in order to build libraries which are powerful but also simple to use. We will help audience explore Poseidon OS using a test tool for NVMe-oF systems called Trident. Trident enables using simple test scripts to test complex scenarios, including use cases wherein the details are handled by configuration-based core libraries\n\nLearning Objectives\n\nNVMeOF based storage systems\n\nTest library design for complex storage system test\n\nPoseidon OS and its Introduction\n\nStream processing and storage system : how streams help to make real time insights.\n\nKrishan Rai, Senior Manager & Abhin Balur, Senior Principal Engineer, Dell Technologies\n\nAbstract\n\nThe evolution of technology has enabled the continuous generation of massive data (e.g., from connected devices, sensors etc. ) and Stream processing has been an active research field for more than 20 years. As, more data becomes available, organizations are using cutting-edge tools and techniques to extract useful insight from the data immediately once they are generated.\n\nWe would like to talk about streaming storage system and how streaming can help to make real time insight from streaming data and how open source Pravega can help to achieve this objective.\n\nLearning Objectives\n\nChallenges while performing analytics over historical data\n\nIntroduction to Streaming World\n\nPravega's contribution to stream processing and storage system\n\nCXL Vs Computational Storage: Revving up the Compute in Data centers\n\nAbhilash Nag, Principal Engineer-II & Shiva Pahwa, Senior Manager, Micron Technologies\n\nAbstract\n\nAs Moore's law catches up with the CPU's technology enablement, vendors are finding it hard to reduce the cost to performance ratio of newer nodes to meet the ever evolving industry needs.\n\nAs micro service architecture evolves, more and more applications are being parallelly executed on the same compute node, making CPU the bottleneck.\n\nToday's industry is moving away from CPU bound operations to offloading significant amount of compute tasks to devices that are better capable of handling such operations. This has generated significant amount of interest at industry wide horizon to design future standards in compute disaggregation.\n\nThe Storage and compute technology enablers have come together to define the next generation of standards and technologies.\n\nThis digital transformation of accelerators is being led by two different industry consortiums - CXL and Computational storage standards.\n\nIn our presentation, we would compare both the computational paradigms and the use cases of each by providing insights into the following:\n\n1. CXL and Computational storage's driving force\n\n2. Commonality and differences between both the standards.\n\n3. Use cases and fitment of each standards.\n\n4. Advantages of each over the other.\n\nLearning Objectives\n\nCXL and Computational storage's driving force\n\nCommonality and differences between both the standards.\n\nUse cases and fitment of each standards.\n\nBreaking The Barrier – How To Apply Dynamic Machine Learning Modelling At Scale In Production Storage Systems\n\nSupriya Kannery, Software Senior Principal Engineer, & Prajwala B Patil, Software Quality Senior Engineer, Dell Technologies\n\nAbstract\n\nHow automated machine learning can be used for applying dynamic machine learning at scale in production systems\n\nFundamentals of hyperparameter tuning for a resource-constrained environment\n\nEnhanced AutoML approach for dynamic modelling to get required speed and accuracy\n\nDemystifying Edge devices cloud native storage services for different data sources\n\nUmang Kumar, Master Technologist, Subhadip Das, Expert Technologist, & Chaitra Shankar, Sr Cloud Developer, HPE\n\nAbstract\n\nUnderstanding cloud native operating model at edge\n\nUnderstanding Cloud native storage services at far edge ,Enterprise edge and network edge\n\nUnderstanding Different data sources(block,object,streaming etc) at edge and its associated storage services\n\nBuilding an Object based STaaS solution with Poseidon Storage\n\nSwati Chawdhary, Senior Manager, & Abdul Ahad Amir, Senior Staff Engineer, Samsung\n\nAbstract\n\nPoseidon Project\n\nSignificance of Object Storage in Cloud\n\nHow to build Object based STaaS with Kubernetes\n\nScale Out Cloud Storage Testing Challenges, Tools and Techniques\n\nPranav Sahasrabudhe, Senior Staff Engineer, & Sarang Sawant, Senior Staff Engineer, Seagate Technology\n\nAbstract\n\nScale Out Cloud Storage\n\nCloud Storage Testing challenges\n\nStorage Testing Tools and Techniques\n\nAzure SMB multichannel using an improved Linux client\n\nShyam Prasad, Senior Software Engineer, Microsoft\n\nAbstract\n\nWhat is SMB multichannel feature?\n\nHow to get best perf out of Azure files?\n\nHow does Linux do SMB multichannel?\n\nDIRL: An Innovative Fabric-Centric SAN Congestion Solution\n\nFausto Vaninetti, Technical Solutions Architect, Cisco Systems\n\nAbstract\n\nIllustrate the different congestion scenarios in a SAN\n\nDescribe the working principles of the DIRL solution\n\nPresent the benefits of DIRL in contrast to alternative approaches\n\nDay Two Track 2 Abstracts\n\nZero Trust Security And Air Gapping For Secure Long Term Data Archival\n\nMohamed Ashraf Kottilungal, Senior Solutions Architect, MSys Technologies\n\nAbstract\n\nKnow the existing challenges for long-term data archival and current cybersecurity trends\n\nLearn the fundamental needs for data archive protection that led to Zero Trust Security measure\n\nUnderstand the intrusion defence and data manipulation protection that can be achieved by logically air gapping offered by Zero Trust\n\nIntelligent Data Centre health monitoring and Remediation\n\nNibu Habel, Principal Engineer, NetApp\n\nAbstract\n\nChecking validity of configurations\n\nAI driven methods that could classify systems with issues\n\nAuto remediation of issues\n\nSecuring APIs: Threats and security best practices\n\nAnupam Chomal, Software Security Expert, CARIAD\n\nAbstract\n\nAPI Security issues\n\nRecent attacks against APIs\n\nLearning the OWASP API Security top 10\n\nPerformance Study between On Wire Encryption through gRPC and Secure TCP interfaces for Data Replication\n\nSravan Kumar Reddy Bhavanam, Member Technical Staff, Nutanix\n\nAbstract\n\nTLS equipped TCP interfaces\n\nComparative study between gRPC and secure TCP interfaces\n\nPerformance benefits of secure TCP over gRPC interfaces\n\nSecure P2P File Sharing in Distributed Cloud Storage\n\nVishwas Saxena, Senior Technologist, Firmware Engineering, Western Digital\n\nAbstract\n\nLearning state of the art secure Peer to Peer File sharing method\n\nLearning state of the art distributed cloud storage architecture and algorithms\n\nFeedback from storage developers developing distributed cloud storage\n\nChallenges to Container storage from perspective of data protection\n\nSushantha Kumar, Lead Architect, & Pravin Ranjan, Senior System Architect, Huawei Technologies\n\nAbstract\n\nSome learning on container Data protection\n\nContainerised storage usage patterns\n\nContainerised application deployment view\n\nCrash consistent Backup of Complex Containerised Application with multiple PVCs\n\nDeepak Ghuge, Software Architect and Master Inventor, Rahul Nema, Lead Architect, & Rasika Vyawahare, Quality Engineer, IBM\n\nAbstract\n\nUnderstand real life containerised application\n\nUnderstand current state of backup and restore in container environment\n\nUnderstand Consistent Backup and restore of Containerised application\n\nTesting Resiliency and Fault Tolerance in Containerized Environment using Chaos Tools\n\nAparna Bindage, Software Quality Assurance Engineer & Sheryl Francis, Software Quality Assurance Engineer, Veritas\n\nAbstract\n\nPerformance of applications run in containerized environment in unpredictable situations\n\nEvaluate the stability and reliability of storage resources by applications\n\nEvaluate fault tolerance, resiliency and recovery of applications\n\nBuild Kubernetes Native Storage Observability\n\nSanil Kumar D, TOC Co-Chair, Arch Lead, SODA Foundation & Chief Architect, Huawei & Joseph Vazhappilly, Lead Architect, Huawei/SODA Foundation\n\nAbstract\n\nIntroduce Storage Monitoring and Observability\n\nWhat is available in Kubernetes for Storage monitoring\n\nHeterogeneous container storage monitoring\n\nArchitecting Multi-Cloud Storage Solutions\n\nKaustubh Katruwar, Senior Software Engineer & Sasikanth Eda, Cloud Storage Engineer, IBM\n\nAbstract"
    }
}
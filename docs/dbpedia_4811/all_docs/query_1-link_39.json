{
    "id": "dbpedia_4811_1",
    "rank": 39,
    "data": {
        "url": "https://meta.wikimedia.org/wiki/Help_talk:Export",
        "read_more_link": "",
        "language": "en",
        "title": "Help talk:Export",
        "top_image": "https://meta.wikimedia.org/static/favicon/community.ico",
        "meta_img": "https://meta.wikimedia.org/static/favicon/community.ico",
        "images": [
            "https://meta.wikimedia.org/static/images/icons/metawiki.svg",
            "https://meta.wikimedia.org/static/images/mobile/copyright/metawiki-wordmark.svg",
            "https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1",
            "https://meta.wikimedia.org/static/images/footer/wikimedia-button.svg",
            "https://meta.wikimedia.org/static/images/footer/poweredby_mediawiki.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/static/favicon/community.ico",
        "meta_site_name": "",
        "canonical_link": "https://meta.wikimedia.org/wiki/Help_talk:Export",
        "text": "first line\n\nsecond line\n\nnew paragraph\n\na\n\nb\n\nc\n\nend of list\n\nsome more\n\nAfter creating this page and one more edit the XML-source was copied to the wikitext of this talk page:\n\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?> <mediawiki version=\"0.1\" xml:lang=\"en\">\n\n<page> <title>Help talk:Export</title> <revision> <timestamp>2005-05-06T11:59:48Z</timestamp> <contributor><username>Patrick</username></contributor> <comment>for demo</comment> <text>For demo:\n\nfirst line<br>second line\n\nnew paragraph\n\na\n\nb\n\nc\n\nend of list</text>\n\n</revision> <revision> <timestamp>2005-05-06T12:00:09Z</timestamp> <contributor><username>Patrick</username></contributor> <text>For demo:\n\nfirst line<br>second line\n\nnew paragraph\n\na\n\nb\n\nc\n\nend of list\n\nsome more</text>\n\n</revision> </page>\n\n</mediawiki>\n\nand also the text rendered by the browser:\n\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n\n- <mediawiki version=\"0.1\" xml:lang=\"en\"> - <page>\n\n<title>Help talk:Export</title>\n\n- <revision>\n\n<timestamp>2005-05-06T11:59:48Z</timestamp>\n\n- <contributor>\n\n<username>Patrick</username> </contributor> <comment>for demo</comment> <text>For demo: first line\n\nsecond line new paragraph *a *b *c end of list</text> </revision>\n\n- <revision>\n\n<timestamp>2005-05-06T12:00:09Z</timestamp>\n\n- <contributor>\n\n<username>Patrick</username> </contributor> <text>For demo: first line\n\nsecond line new paragraph *a *b *c end of list some more</text> </revision> </page> </mediawiki>\n\nAfter copying the first result to the edit box, we get:\n\nFor demo: first line\n\nsecond line\n\nnew paragraph\n\na b c end of list\n\nsome more\n\nCaveats: should namespaces be the text, or symbolic names? Or should we leave them out entirely and let the parser deal with such a thing?\n\nThe parser needs to know the namespace's prefixes of the article's language anyway to parse the article content so it does not matter. BTW cur_counter is missing. -- Nichtich 18:09, 1 Dec 2003 (UTC)\n\nI'm sorry, but [I] still can't get how can I e.g. export all the pages? What should I write in query window at Special:Export?\n\nI am having the same problem. How does it work? --Donrob 08:40, 3 Jan 2005 (UTC)\n\nSeems like instruction for import is needed too.\n\nYes! It is definetely needed --65.94.224.235 17:14, 20 Jan 2005 (UTC)\n\nTo start with an explanation on how to export all pages would be nice. --146.50.205.252 00:54, 1 Feb 2005 (UTC)\n\nFirst, write a script that expports all pages. --brion 01:37, 2 Feb 2005 (UTC)\n\nTo my knowledge exporting all wiki pages involves several manual steps\n\nget a list of all pages with specialAllPages\n\nsave that list into a text file\n\nreplace tab characters with newline (e.g. with sed) giving a list of all wiki pages, each page in a separate line\n\npaste that list into special:export\n\nensure you save the XML, not the HTML representation\n\nSupport for import in another wiki seems to be on the list of tasks for mediawiki 1.5.\n\nRegarding the .xsd: MediaWikiType needs an element or attribute encoding\n\nCan you elaborate?\n\nWithin the text section there seems to be an escaping conflict: If every \"<\" will be replaced with \"&lt;\", real \"&lt;\" would be lost during the decoding. --Vlado\n\nNot so: every literal ampersand is escaped as well. This is just straightforward standard everyday escaping, see http://php.net/htmlspecialchars --brion\n\nSorry, these were two stupid thoughts. What about this: How can I access the \"oldid\" of a revision? It would be nice to have it for linking revisions to the Wikipedia like it is done in \"history\". --Vlado\n\nI think the oldid on the other system is not necessarily the same, it should be assigned by the importing system.--Patrick\n\nOr perhaps you mean importing the current versions, plus the history pages, but not the old version pages themselves. That I don't know.--Patrick\n\nThere was code for including the page and revision ID numbers in the export file, but it wasn't normally enabled before. It will be on by default in 1.5, see eg [1]. (Also every revision, including the current, will have an id number. This wasn't previously the case.) Of course if you were importing pages from one wiki to a different one, the final id number would be whatever gets assigned there rather than the original wiki's id number. --brion\n\nIf an error happens during export, it quietly inserts an error message in your XML, which you might not necessarily notice. Not sure if there's an easy way to improve that.\n\nOn my mediawiki installation such an error ocurrs if you have any blank lines in the text box, but I notice that's been improved on this wiki for example -- Harry Wood\n\nYou can also try Victor Stinner script to download all pages:\n\nwget http://svn.berlios.de/viewvc/*checkout*/happyboom/haypo/mediawiki/import_pages.py?revision=260&pathrev=468 python import_pages.py http://www.example.com/wiki/\n\n(where svn is SubVersion)\n\nIt will create a subdirectory containing files like \"namespace_0.xml\". TODO:\n\nSupport page list (Special:Allpages) on several subpages.\n\nSupport split into small files (for fast re-import).\n\nSupport \"only last version\" import.\n\nThis script doesn't seem to work any more, are there any later revisions, it seems like that source tree is gone? 85.229.120.9\n\nThere's another page called 'How to move a MediaWiki Database' which describes using the mysqldump tool to move the whole lot. This should be mentioned or linked on this page somehow. I would do it myself, but having not tried it, there's some things I'm not clear on. Is the mysqldump approach better, easier, and more complete, than doing XML export then XML import? Obviously it's not always possible (e.g. if you don't have system admin access). Using the XML export/import approach, can you do other tricks such as merging two wikis? -- Harry Wood\n\nAre there any tools which convert MediaWiki's export XML to a PDF e.g.? WikiPDF See also WikiPDF wiki and also pdf_export\n\nThere is one for LaTeX and thus PDF. MediaWiki to LaTeX\n\nHow can i dump them? I don't have access to mysqldump!\n\n32.59.2.18\n\nNeed them JWatley7089k (talk)\n\nHi i export a wiki to my local wiki with dumpBackup.php and importDump.php. I have alle pages, which were createt by an user on my new local wiki. but i am missing all user accounts and all user pages. How can I export/ import these?\n\nHi. I just tried to export the full history of \"Allegations of Israeli apartheid\" but I keep getting a blank result. It seems I can export other pages. Is there a restriction on exporting pages with long edit histories? If you respond, can you respond to my Wikipedia user page User:Bhouston. --64.230.121.208\n\nI also cannot get this to work. I really want to get the template \"Otheruses4\" off wikipedia and tried all kinds of different ways( on the wikipedia site). I tried the latest versions of firefox and internet explorer. I have tried random articles on this site as well and it does nothing. The page essentially does nothing, it acts as if the page just reloaded, with no change what so ever. A bug in the latest mediawiki? 221.143.3.14\n\nI am attempting to use Special:export on my wiki, and I get this error message:\n\nThe XML page cannot be displayed\n\nCannot view XML input using style sheet. Please correct the error and then click the Refresh button, or try again later.\n\nInvalid at the top level of the document. Error processing resource\n\nIn firefox:\n\nXML Parsing Error: not well-formed\n\nLocation: http://www.***.com/wiki/index.php?title=Special:Export&action=submit\n\nLine Number 2, Column 1: <mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd\" version=\"0.3\" xml:lang=\"en\">\n\n^\n\nI am running mediawiki 1.5.7 siteground.com is hosting my mediawiki, and i can't upgrade to 1.7\n\nAny suggestions? Odessaukrain\n\nResponse from Byon on mIRC #mediawiki :\n\nodess: check for extra whitespace at the beginning or end of LocalSettings.php, any extension files, or any other modified .php files\n\nI figured it out!\n\nThere was a large space between this code:\n\nrequire_once( \"extensions/****.php\" ); # ****\n\nIf PHP's memory limit is very low, some operations may fail.\n\nini_set( 'memory_limit', '20M' );\n\nWhen I changed it to:\n\nrequire_once( \"extensions/****.php\" ); # ****\n\nIf PHP's memory limit is very low, some operations may fail.\n\nini_set( 'memory_limit', '20M' );\n\nProblem solved! Odessaukrain\n\nNo way to export to plain XHTML? Just for save or copy-paste the article to a simple html file, without several stylesheets (and hidden edit links). --80.104.165.175\n\nI'm trying to transwiki a page from wikipedia: to StrategyWiki:. However, the page has more than 100 revisions meaning that Export doesn't give me everything. This article says that the pywikipediabot can be used to export files from wikipedia, but I haven't been able to find any mention of it anywhere. Any pointers would be greatly appreciated (or another way to export complete history rather than going 100 edits at a time). -- Prod\n\nHow do I export articels, that are named like \"Müllabfuhr\"? Export does not find the article.\n\nLooking at the export format, Help:Export#Export_format, I can't see any specification of the namespace for that article. From what I can tell, that is simply taken from the page title such as Help:Contents. Is that correct? Reason asking is are generating text that is to be imported into a wiki using 3rd party software and want to ensure that it is imported into a custom namespace. So, are I correct in assuming that there doesn't have to be any ID number or something associated with the namespace in the XML file that is imported? --Dr DBW\n\nTo answer my own question, yes you just specify that at the start of the page title and it is all taken care of. Of course, the namespace has to be defined and all, but you don't need the ID number etc. --Dr DBW\n\nWould be handy to know which fields in the export format is required for it to work, i.e. what I am interested in is the bare minimum required to get a valid XML file that will get imported into the wiki. --Dr DBW\n\nI have reduced it to the following tags, and it appears to work fine: --Dr DBW\n\n<mediawiki></mediawiki>\n\n<page></page>\n\n<title></title>\n\n<revision></revision>\n\n<timestamp></timestamp>\n\n<text></text>\n\nHi,\n\nI would like to export only the pages from the main name space from a wikipedia project to use for a school assignment (information retrieval and machine learning).\n\nWhat is the easiest/fastest way to do this?\n\nI already downloaded a xml page dump but this contains all namespaces.\n\n--GrandiJoos\n\nI've listed the instructions for how to export a full version of an article using Special:Export.\n\nUse http://en.wikipedia.org/w/index.php?title=Special:Export&pages=ARTICLE_NAME&history=1&action=submit\n\nSave the file as something.xml\n\nUse a find/replace feature of a text editor and find all \"</username>\" and replace it with \"@en.wikipedia.org</username>\"\n\nSave. You should now be ready to import the file via Special:Import on another MediaWiki wiki.\n\nFrom what I've seen this is pretty much not documented anywhere. I'll take a crack at updating the main page in a bit. There's a few things that need to be updated here. -- Ned Scott\n\nToday I see \"Exporting the full history of pages through this form has been disabled due to performance reasons\" when exporting. I wonder why.--Jusjih\n\nMy mentioned problem seems to be gone now. I just wonder why the problem once happened.--Jusjih\n\nI don't think this *always* exports a full version. It clips exports at 1000 history entries. The main page here says 100, but that may be old or a type-o? I'm leery of updating it, but it contradicts the Special:Export page information - \"Full history exports are limited to 1000 revisions.\" (at http://en.wikipedia.org/wiki/Special:Export). Does anyone know a way around that limit without resorting to curl (which I can't get to work, likely a U53R error - or however that joke goes). --Nobboddoddy\n\nany clues? --Anshuk\n\nKann mal jemand den Link erneuern? --194.114.62.70\n\nhttp://cvs.sourceforge.net/viewcvs.py/wikipedia/phase3/maintenance/dumpBackup.php\n\nis there a way to get dumpBackup.php to *NOT* dump IDs?\n\nAs the export function allows to export content and history from pages and even allows the export templates. We are wondering why the export function does not have an option to export category content and history (as it only exports the link objects) through XML.\n\nAs the category is an taxonomy/ontology object, it could (in our case we have) have descriptive text about how and why to use this category and support a common understanding within the wiki about this entity.\n\nEven with version 1.16beta1 it is still not possible to export content from categories. Does anyone have a solution, besides making a sqldump. -- 25 March 2010 / 213.163.84.215\n\nI'm searching for this feature too :(\n\nHi, I am using History Flow (IBM) to see the history of editing article. It designed to get data from [[2]]. However the special export page made the \"only current revision \" as automatic choice.\n\nHow can I get the entire history of article on this stage. Can I change the set of special export?\n\nThanks for help! Zeyi\n\nIs there a way to export logs? I'd like to export one of the specific logs (rather than the entire page log datadump), but am not sure if there's currently a way to do this. Thanks. --Dfinzer —Preceding undated comment added ‎.\n\nI second this. The Special:Log page specifies some events using obscure units—such as “months”—and there is no way to get the precise meaning other than to examine the source code. Incnis Mrsi (talk)\n\nmw:API:Logevents resolved my problem. Unsure what exactly did Dfinzer want to achieve. Incnis Mrsi (talk)\n\nI use special:export a lot and I am appealed at how terrible it is to get page names, these steps are complex:\n\n# You can achieve that relatively quickly if you paste the names into say MS Word - use paste special as unformatted text - then open the replace function (CTRL+h), entering ^t in Find what, entering ^p in Replace with and then hitting Replace All button. (This doesn't seem to work - there are no tabs between the page names.) # The text editor Vim also allows for a quick way to fix line breaks: after pasting the whole list, run the command :1,$s/\\t/\\r/g to replace all tabs by carriage returns and then :1,$s/^\\n//g to remove every line containing only a newline character. # Another approach is to copy the formated text into any editor exposing the html. Remove all <tr> and </tr> tags and replace all <td> tags to <tr><td> and </td> tags to </td></tr>. the html will then be parsed into the needed format.\n\nDoes anyone know an extension which gives all results in ONE COLUMN, on ONE PAGE and with the namespace name? For example Template:Dirt, instead of Dirt. Adamtheclown\n\nAny reason not to move this to MediaWiki.org? --Varnent\n\nAccording to the page for this, and the download page, dumpBackup.php is supposed to be compatible with MW ver 1.5 and later. Using MW ver 1.7.1, I got endless errors, using php5 dumpBackup.php fixed a lot of that. Altering the backup.inc & commandline.inc directories within dumpBackup.php so it could be run from the root MW directory was necessary. Trying to run it from either maintenance or includes produces directory errors in Localsettings.php. If I fix those in LocalSettings, it breaks the wiki.\n\nEventually it boiled down /includes/Export.php not having the proper line for \"Class Constant 'TEXT'\". Specific message was \"Fatal error: Undefined class constant 'TEXT' in /xxx/xxx/xxx/dumpBackup.php on line 72\" This was the hardest to solve, and the real crux of the issue. The version of Export.php included with 1.7.1, and probably prior versions does not include the right code to run dumpBackup.php. Replacing it with Export.php from the most recent version of MW allowed dumpBackup to execute, but I am not certain if the output is exactly what it is supposed to be since I am mixing versions of files at this point. There is an error \"Cannot modify header information\" or something at the top of the dump. Skimming through the output seems to have produced the contents of the wiki, however. Toastysoul (talk)\n\nIf you want to, say, export all of Wikipedia's templates, and need the page titles, this PHP code could do the trick. It polls the API (mw:API:Allpages) to get that data, and then puts it in output.txt.\n\nIf anyone knows of a more elegant way to parse the XML, feel free to provide some alternative code. This worked pretty well, but the parsing methodology is obviously not ideal. Leucosticte (talk)\n\nHi there, I worte a little script in PHP that allows to export all pages of a category including all pages of subcategories into a single PDF-file. The script is called \"mwc2pdf.php\".\n\nmwc2pdf uses MediaWiki's \"api.php\" to collect all data and create a \"pagetree\". mwc2pdf prints out every item of that pagetree into a single pdf-file using \"wkhtmltopdf\". It than combines all single pdf-files into one single pdf-file called \"MWC2PDF.pdf\" using \"pdftk\".\n\nThe code is available on github: https://github.com/produnis/myscripts/blob/master/PHP/mwc2pdf.php"
    }
}
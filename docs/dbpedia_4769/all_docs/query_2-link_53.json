{
    "id": "dbpedia_4769_2",
    "rank": 53,
    "data": {
        "url": "https://craftofcoding.wordpress.com/tag/programming-languages/",
        "read_more_link": "",
        "language": "en",
        "title": "programming languages – The Craft of Coding",
        "top_image": "https://s0.wp.com/i/blank.jpg",
        "meta_img": "https://s0.wp.com/i/blank.jpg",
        "images": [
            "https://craftofcoding.wordpress.com/wp-content/uploads/2021/10/algorithmver.jpg?w=785",
            "https://craftofcoding.wordpress.com/wp-content/uploads/2021/10/algorithmeqn.jpg?w=785",
            "https://craftofcoding.wordpress.com/wp-content/uploads/2021/10/algorithmtree.jpg?w=543",
            "https://craftofcoding.wordpress.com/wp-content/uploads/2021/10/algorithmflow.jpg?w=544",
            "https://craftofcoding.wordpress.com/wp-content/uploads/2019/10/image-1.png?w=220",
            "https://craftofcoding.wordpress.com/wp-content/uploads/2019/10/image.png?w=26",
            "https://s2.wp.com/i/logo/wpcom-gray-white.png",
            "https://s2.wp.com/i/logo/wpcom-gray-white.png",
            "https://pixel.wp.com/b.gif?v=noscript"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-01-22T15:34:14+00:00",
        "summary": "",
        "meta_description": "Posts about programming languages written by spqr",
        "meta_lang": "en",
        "meta_favicon": "https://s1.wp.com/i/favicon.ico",
        "meta_site_name": "The Craft of Coding",
        "canonical_link": "https://craftofcoding.wordpress.com/tag/programming-languages/",
        "text": "Oh right, there aren’t any. But why not? Mainly because this isn’t the 1980s when it was still possible to design a language that looked a little different from everything else around it. There is also the issue of why? Why do we need new programming languages? The ones we currently have work just fine. They do, go and have a close look at them. The monolithic Cobol does a fine job at finance… why would we want to change that? Why would we take the risk? C is fantastic at systems programming. Others like Python do a good job at dealing with data. Too many articles talk about “dead” programming languages with little or context to real-world use.\n\nEvery year there are the ubiquitous posts, postulating the “new programming languages” that are paving the way in the frontiers of programming. One I saw recently touted Kotlin as a great “new” language. I mean it first appeared in 2011, and yes it is one of the official Android development languages… but it isn’t anywhere near new or revolutionary. In fact if you look at the syntax of many of these new languages, they all in some way are derived from Java, or C++, or some other language in the long line of C-based languages. Nothing exciting here, move along.\n\nWe keep re-inventing the wheel with a slightly different spin on it. Mostly these languages appear as augmentations for software development on mobile platforms, or for data analysis. Many are super-bloated with features, which makes you long for the simplicity of C or Fortran, where you could memorize the syntactic structure in your head. None of these new languages are saving the world, yet people love jumping onto whatever new bandwagon appears.\n\nLook, I’m not against new languages, but you can’t sell a language purely on the idea that it runs faster. Great. Is it at all usable? Is it bloated? Does it do anything some other language doesn’t already do? Truly revolutionary programming languages will come when people step away from the the current C-based language family and come up with something completely different – more than just a new spin on a for loop. Actually what I would like to see is people actually do some large scale studies on the learnability and usability of existing programming languages before we attempt to create new languages. Will it happen? It did once, but the last of these type of studies happened in the 1980’s when people actually still gave a damn about the languages they were developing.\n\nFrom my perspective programming languages have become like iOS. Many of them have a new releases multiple times a year, yet none of these updates really change the landscape of the language, except perhaps killing backwards compatibility, or adding some new feature that likely isn’t needed. Good languages are simple to design, and simple to use. Yes, it’s a nice concept to fit every stupid feature possible in a language, but it isn’t necessary – that’s what libraries are for. Remember the idea of “reuse”? It was one of the big selling points of OO, but never really led to anything tangible. New programming languages? No thanks, I’ll stick with the tried and true.\n\nPrograms are created to solve problems, and without problems to solve there would be little or no use for programming languages, let alone computers. The first thing to do with a problem is to try and derive a solution, one which can subsequently be implemented in a programming language. For example consider the problem of trying to identify a tree based on its features, e.g. leaves. There are a bunch of apps available for mobile devices that do just that, e.g. PlantSnap, LeafSnap, and even web-based apps like TreeBee.\n\nDeriving a solution requires defining the steps involved in identifying a tree from a leaf. This series of steps is called an algorithm. Some algorithms are simple, some are complex. For example an app which identifies a tree based on taking a picture of a leaf is complex because it involves extracting information like leaf shape, size, colour from the image. A web-based app which is guided by human observations of leaves is much simpler.\n\nHere are some simple questions for an algorithm based on human observation (based on TreeBee, which identifies trees in Ontario). In this case the algorithm is basically in the form of a decision tree – the algorithm for identifying a tree is based on the information provided through observations.\n\nIs the tree deciduous or coniferous?\n\nConiferous → does the tree have needles or scales?\n\nNeedles → does it have 1, 2, 3 or 5+ needles in a bundle?\n\n1 needle → Balsam Fir, Eastern Hemlock, etc.\n\n2 needles → Scots Pine, Red pine, etc.\n\n3 needles → Pitch Pine\n\n5+ needles → short (<5cm) or long (>5cm)\n\nShort → European Larch, Tamarack\n\nLong → Eastern White Pine\n\nScales → Eastern Red Cedar or Eastern White Cedar\n\nDeciduous → simple or compound leaf?\n\netc.\n\nFor example if the tree is coniferous, and has 3 needles in a bundle then the tree is a Pitch Pine. But this algorithm is somewhat complex to begin with, so we’ll go back to deriving an algorithm to calculate simple interest. What steps are needed in order to calculate simple interest?\n\nObtain the values for the principal amount, the interest rate (per year), and the time period (number of years).\n\nCalculate the simple interest.\n\nUse the formula: interest = principal × rate × time\n\nOutput the interest calculated.\n\nOnce we have an algorithm it is not that difficult to implement the algorithm in a programming language. Of course there are some algorithms that are theoretically valid, but cannot be implemented due to limitations in technology. Some algorithms don’t work for every situation either. I tried PlantSnap on the small maple in from of my house and it identified it as Acer pectinatum, an Asian species of maple, yet I know it is an Amur maple. Complex algorithms lend themselves to not always producing the right result. In the case of PlantSnap it involves extracting information from a picture, and may have issues identifying trees with similar leaves, or image not containing enough information. TreeBee on the other hand provides a narrowed series of options, which the user can then visually identify.\n\nAlgorithms are the building block of programming. Taking the time to create a well conceived algorithm will greatly reduce the effort needed to implement it. Algorithms can be represented as a list of steps, visually as a diagram or decision tree, verbally, or as a set of equations. Below are some examples:\n\nLearning to program is not that intrinsically difficult, sometimes boring, but not hard. Programming exists to solve problems. Take a problem you wish a computer to process, derive a solution, implement the solution in a programming language, and create a program. Simple, right? Well, not all problems are trivial to solve, but the elements of programming can be learned by anyone.\n\nThe first thing you need is a programming language. It doesn’t matter what type of programming language is used – it is merely a tool used to intrepret a series of English-language instructions into machine code (that the computer can understand). A program to calculate compound interest will function in the same way whether it is written in C, Fortran, Cobol, or Python. The only real difference is the syntax of a language, or rather how it expresses things. Some languages such as C or C++ may have syntax which is a little more obtuse and challenging to understand than say Python.\n\nThere are two basic types of programming language: compiled and interpreted. A language which is compiled produces a physical program, like an app, which can then be activated to perform whatever task it is designed to perform (activating the program is often called running it). Fortran and C are examples of compiled languages. An interpreted language is one which does not produce a program, but rather provides an environment in which the program can run. Examples of interpreted languages are Python and Julia. The other difference is that compiled programs are often more formal than interpreted ones.\n\nConsider the following programs for calculating simple interest implemented in Fortran, Python, Julia, and C. Simple interest is just principal×rate×time. For example $1000 at 10% per year for 3 years gives $300. For this exercise, look at the programs below, and consider which ones are easiest to interpret from the perspective of using clear English language terms.\n\nFortran\n\nprogram simple real :: p, r, t, i write(*,*) \"principal, rate, time:\" read(*,*) p, r, t i = p * r * t write(*,*) \"Interest = $\", i end program simple\n\nFortran is one of the easiest languages to decipher, as its syntax is pretty much in plain English. You can see the phrases program and end program are used to enclose the program. The statements read and write are used for input and output, and the equation is fairly self evident.\n\nC\n\n#include <stdio.h> int main(void) { float p, r, t, i; printf(\"principal, rate, time: \"); scanf(\"%f%f%f\",&p, &r, &t); i = p * r * t; printf(\"Interest = $%.2f\\n\",i); return 0; }\n\nC is sometimes considered an obfuscated language, which just means confusing. The programs tend to contain things that aren’t clear to the novice programmer. For example scanf() is a function that reads input from the keyboard, but is more confusing than just calling it read(). The only thing that might not be confusing is the interest calculation.\n\nPython\n\np = float(input('principal: ')) r = float(input('rate: ')) t = float(input('time: ')) i = p * r * t print('Interest = $',i)\n\nPython is also quite simple, it even integrates the prompts with the function that reads in the input from the keyboard, which is aptly named input.\n\nJulia\n\nprintln(\"principal: \") p = parse(Float64, chomp(readline())) println(\"rate: \") r = parse(Float64, chomp(readline())) println(\"time: \") t = parse(Float64, chomp(readline())) i = p * r * t println(\"Interest = \\$\",i)\n\nJulia is also quite simple, but tends to have a messy way of inputting numbers.\n\nAs you can see, every language has its pros and cons, but those best suited to learning programming have a syntax which is easy to interpret. In this series, we will use Fortran and Python to illustrate concepts.\n\nNow to make programs work, you have to have a compiler or interpreter, which translates the human readable program into machine code. To make it easy, tthere are a number of online web-based interfaces for creating small programs:\n\nFortran at Tutorialspoint\n\nFortran at Online GDB\n\nMany languages at OneCompiler\n\nIn the 1970s and 80s there were a bunch of programming books that taught programming by introducing programming language concepts and illustrating them in 3-5 different languages. Usually the languages were old school: Fortran, Algol, BASIC, maybe Cobol. The idea is that once you understand a structure it should be easy to replicate it in any language. Doing this means that you effectively learn a number of languages at once. Now it may be hard to envision this because we are so tied to learning one language at a time, but it is possible, and over time it becomes easier to remember the difference between the syntax of differing languages. You can obviously start with data, because any discussion of programming languages should start with data.\n\nAn integer is a data type associated with whole numbers, e.g 5, 13, 99, 19763758. A number with a decimal point in it, i.e. not a whole number is called a real or a floating-point number. Once you know those two things, you know the basics of data – in any language. Now to use data in a program you have to create a container to hold the data in memory, this is most often called a variable. In most programming languages variables have a name (the identifier). and a data type associated with it. For example, in Fortran to create an integer variable named thor, is as simple as:\n\ninteger :: thor\n\nThe variable thor is now an integer variable, which incidentally has no value yet, because we haven’t given it a value. How is this done in other languages? Here is the Ada version:\n\nthor : integer;\n\nOr maybe the C version (which shortens the datatype name to int):\n\nint thor;\n\nNow values can be assigned to them:\n\nFortran → thor = 27 Ada → thor := 27; C → thor = 27;\n\nThe syntax is a little different in each language, but the concept is the same – the variable thor is given the value 27 to store.\n\nIn more “dynamic” languages such as Python and Julia, variables don’t need to be pre-declared. A datatype is associated with a name of a variable when a value is assigned to it. For example (in Python or Julia):\n\nthor = 27\n\nThe variable gets the value 27, and is deemed an “integer”. If the variable is given a new value, say 64.7, then it is deemed a “float”. Next we’ll look at some basic programming. In one fell swoop we have learned how to create integers in five languages. The syntax can be trivial, if you have an understanding of the concept that underpins it.\n\nProgramming languages are languages. When it comes to the mechanics of the task, using a programming language is in many ways like learning to speak a human verbal or sign language. In both kinds of language, you must learn new vocabulary, syntax, and semantics (new words, sentence structures, and meanings). Moreover, both kinds of language require considerable practice to make perfect. There is, however, at least one important general difference that may strike you forcibly as you learn:\n\nProgramming languages lack ambiguity and vagueness.\n\nQuestions raised in English by sentences such as “I saw the man with a telescope” (Who had the telescope?) or “Take a pinch of salt” (How much is a pinch?) simply do not arise. In a programming language a sentence either means one thing or it means nothing!\n\nProgramming is fundamentally the same in any language. The constructs of programming such as decision and repetitive statements are conceptually the same. Only the syntax of the particular language makes it different. The syntax of a language is the set of allowed lexical elements and the set of rules used to combine the lexical elements into legitimate constructs. These lexical elements include operators (+, =), reserved words, function names, and variable names. In spoken languages, syntax is concerned with building valid sentences and paragraphs. In a programming language the syntax is concerned with building elements such as expressions and functions. The following sentence contains numerous spelling and punctuations errors, but its meaning can still be deciphered:\n\nTha kat saton, tha matt:\n\nA compiler or interpreter is very strict about syntax. A single missing semicolon in a C program could cause a compiler failure. For instance in mathematics, the equation for the area of a circle might be expressed as:\n\nIf you wrote this in most programming languages, the compiler would most likely reject it for a number of reasons. First, it might consider the assignment symbol “←” to be incorrect (i.e., it’s not a part of the standard character set). It also has no idea what Greek letters are, so π would be a mystery (unless you are using Julia). Super-scripts aren’t allowed. But if you get something wrong, the compiler will usually produce a syntax error. More about these later. The structure of English is much more flexible, but rules about the structure of sentences do exist. For example consider the process of calculating the area of a circle, which can be described in C in the following manner:\n\narea = 3.14159 * radius * radius;\n\nIf we leave the semicolon out at the end of the statement, the compiler will complain that there is a problem with the syntax, because it expects a semicolon at the end of the statement to terminate it. If it doesn’t find things the way it expects to see them, the compiler will balk. The semantics of a language is the meaning given to its elements. Consider again the area of a circle. The equation for the area of a circle is πr^2. If the statement in the program were to become the equivalent of πr^3, or\n\narea = 3.14159 * radius * radius * radius;\n\nthen this becomes a gaffe in the semantics. The compiler will compile the program with no problems, as there are no problems with the syntax. It’s sort of equivalent to writing:\n\nThe cat sat on the hot tin roof.\n\nMaybe it did, and in terms of syntax, it looks good. It has lost its meaning, though.\n\nLanguages are often designed with a specific use in mind, and some are better than others for dealing with certain problems. Each language provides slightly different func- tionality, so the language should match the problem. There are many different programming languages: Julia, Fortran, Pascal, Java, Python, Perl, Oberon, Modula-2, Lisp, Eiffel, Ada, ALGOL, Smalltalk, HTML, and Prolog. Consider the problem of calculating the area of a circle. It can be expressed in a similar fashion in a number of languages:\n\nALGOL area := 3.14159*r*r; FORTRAN area = 3.14159*r**2 C/C++ area = 3.14159*r*r; MATLAB area = pi*(r^2); Ada area := 3.14159*(r)**2; Pascal area := 3.14159*r*r Python area = 3.14159*r**2 Java area = 3.14159*r*r; Ruby area = 3.14159*r**2 Julia area = pi*r^2\n\nAs you can see, there isn’t a great deal of difference between the nine languages.\n\nWe interact with machines in many ways, but the most primeval means is by writing programs. Programs become the soul of the machine, without them, the machine is useless, soulless. The programming language is the medium between the world of humans and the world of 1’s and 0’s. Yet despite 60 odd years of programming language design, we are yet to design one which is easy for everyone to use. This is partially due to the intricacies of the machine, but also due to the fact that we don’t design for language usability per se. The easiest language for people to learn programming language concepts is a simple one – very little syntax, easy to decipher. The problem of course is more complex than that because programming does not only involve writing code.\n\nThere is the task of solving a problem, then developing an algorithm, and converting that algorithm into a programming language. Nothing is trivial. For example it is easy to state a problem of the form: “find all faces in a photograph“. The problem *seems* simple because to humans it is. Developing an algorithm is considerably more complex.\n\nThere are also differences between natural language and the programming language, what some would term linguistic transfer. Programming is difficult for novices because of how humans interpret a language. A case in point is the use of while to indicate a loop.\n\nwhile (n < 1000) -- do something constructive end\n\nThe term while could be construed as a period of time, but more often it is misinterpreted as being applied continuously (i.e. a continuously active test), versus being evaluated once per iteration of the loop. In English one could say “while the sun is shining, play outside“, which is a loop of sorts, but not one we actively think about. We don’t look at our environment every minute to determine whether the sun is still shining. A novice programmer may misconstrue a while loop to terminate the minute the condition is satisfied. It is an instinctive thing. Ada may actually have had a good idea with the concept of a loop where the exit is introduced within the loop itself.\n\nloop exit when (n < 1000); -- do something end loop;\n\nWe know a physical loop is continuous, be it in the form of a rope loop, or a loop in a road (or even the new Apple HQ building at Apple Park). Leaving the loop in the road involves making a conscious choice to leave while in the loop. This actually makes sense in many ways because there is one generic loop to deal with most situations. It could also deal with the case where one wants to do something n times, but that may itself be better expressed using something like a repeat statement:\n\nrepeat n times -- do something end\n\nOr what about this?\n\ndo repeat i with values 1 to n -- do something end do\n\nToo wordy? Likely. The same issues are encountered by the if statement. Novice programmers might not yet understand the sequential life of a program, and so they might consider an if statement to be “waiting” continuously, instead of being executed in sequence. Something like “if it starts raining, open an umbrella“. It might seem easy to the experienced programmer, but not so to the novice.\n\nIs there a definitive solution? Unlikely. The use of the existing programming vernacular is ingrained into programmers. We have never really delved into the terms used for loops since the for loop appeared in Algol, and the while loop shortly afterwards. It might be possible to make small changes over time… but computer science rarely does things in small ways. We likely have to find better ways of teaching people to program.\n\nThere is a lot of literature in the CS community about the reasons students drop-out of introductory programming courses. There are two streams of students: those who will go on to immersive careers in computer science (e.g. software engineering), and those who will use programming as a tool in their discipline, maybe to analyze process data. The latter includes fields like finance, physics, and bioinformatics. They are two vastly different groups of people, and it is almost impossible to create one course that suits everyone. Teaching a course based on a language such as C or C++, which requires knowledge of low level concepts such as memory management, will alienate non-CS students. Spend too much time on high-level use of a language, and the CS-majors will miss critical information on low-level concepts.\n\nOften it comes down to difficulties associated with the programming language being used to teach programming concepts. For example, non-CS students don’t really need to know about object-oriented programming – it often just complicates things from the perspective of building programs. Game developer John Carmack once said “Sometimes, the elegant implementation is just a function. Not a method. Not a class. Not a framework. Just a function“. They also don’t need to know that much about memory, and certainly not to the complexities of dynamic memory, or indeed complex ways of coding things to make a program more efficient, e.g. vectorization.\n\nSo what to teach a non-CS student in the way of learning to program?\n\nHow programs can be used to solve real-world problems. This often involves the use of real-data, e.g. digital images. Computer-centric examples are often irrelevant for non-CS individuals.\n\nFocus on problem solving.\n\nHow to formulate a problem as an algorithm. For example if the problem is removing random noise from a monochromatic photograph – how do we do this?\n\nLearning the basic constructs of programming – making decisions, repeating things, how to store data (from a high-level perspective).\n\nWriting the programs to implement the the algorithms\n\nHow to test programs.\n\nChoice of language is extremely important, and from that perspective Julia is a perfect language to teach novice programmers who don’t want to make a career of it, but want the skills to help them in their disciplines. Teaching programming can also be moved to a more experiential process, whereby languages syntax is introduced as they are used in the problem solving context.\n\nBy the early 1970s, the discussion on structured programming, and indeed the utility of goto was in full swing. With the growing complexity of programs, it was no longer appropriate to create them in an ad hoc manner. The 1960s heralded great break-throughs in programming language design, the most notable being the transition from structureless code to structured code, modularization, recursion, and efficient control structures.\n\nIn 1972, Niklaus Wirth produced the language Pascal, and named in honor of mathematician and philosopher Blaise Pascal. Pascal evolved from the likes of Algol, and was grounded in simplicity, making it an excellent teaching language. Wirth later went on to develop Modula-2 (1978) and Oberon, both based on Pascal. An outlier to this trend was BASIC. BASIC was born in 1964, but became to prominence in the late 1970s, largely due to the rise of the personal computer. The personal computer is what drove one side of the programming language evolution in the 1970s, with both Pascal and BASIC vying for dominance. This was likely jarring for the established languages (Cobol, Fortran, even Algol). Edsger Dijkstra’s is famously quoted as saying:\n\n“It is practically impossible to teach good programming to students that have had a prior exposure to BASIC: as potential programmers they are mentally mutilated beyond hope of regeneration.”\n\nThe Pascal/BASIC clashes would almost morph into a form of holy war. The other side of the programming language development was spurned by system programming. In 1972 Ken Thompson and Dennis Ritchie introduced C, in part to help in the evolution of the Unix OS. They probably didn’t quite fathom the effect C would have on the design of future programming languages. C allowed for programming at a lower level than was possible with many programming languages, in many respects C was a structured assembly language.\n\nSome languages, such as Fortran and Cobol evolved – Fortran into F77, Cobol into Cobol74. For others, it was a less than happy time. Algol all but disappeared during this decade, displaced by Pascal. But with the 1980s, the dawn of the personal computer ushered in one new player (OO), and an older one, BASIC, whose time had just begun.\n\nAfter the freewheeling fifties, programming languages entered a more bohemian time, with many languages appearing, and those 1950s languages evolving, as new programming ideologies evolved. First out of the gate was ALGOL 60 which evolved from the fledgling Algol 58. It was common practice to “re-design” a language quite considerably, rather than make tweaks to an existing language. As such languages such as Algol 58 often fell out of favour as people adopted newer versions of a language. Algol was to spawn what would eventually become quite an issue in the computing community: augmentations, extensions, and derivations, leading to numerous dialects of a language. ALGOL 68 appeared just before its contemporaries C and Pascal, however due to inherent complexities, never really hit it off. (ALGOL 68 was used by European defense agencies, however the US decided to hedge its bets with Ada.) There was some controversy over the design of a successor to Algol 60, with Wirth and Hoare going on to develop their own successor, Algol-W. Algol 68 was a major revision to Algol 60, whereas Algol-W included more subtle changes. The 1960’s also saw a number of updates to both Fortran and Cobol.\n\nLanguages had also begun to diversify into differing realms. In 1964, Kenneth E. Iverson introduced APL, (A Programming Language) a concise symbol-based language adept at dealing with arrays. Early versions of the language contained no control structures, and whilst Cobol may have strayed too far into “English-language” syntax, APL may have gone too far with its mathematical syntax. APL was popular with those doing computer graphics, but its use declined in the 1980s, due in part to the advent of languages such as MATLAB and GNU Octave. The 1960s also saw the second generation of programming languages evolve, those influenced by the likes of Fortran or Algol. In 1964 PL/I (Programming Language One) made its appearance, supposedly for use in data processing, numerical computation, scientific computing, and systems programming. However it was a more complex language than either Fortran or Cobol, from which it had evolved, and was not extremely successful. Algol also influenced the design of Simula, the first OO language which appeared in 1967. In an educational context, Logo appeared in 1967, famous for its use of “turtle graphics”, influenced by Lisp.\n\nP.S. If you want to have some fun with a Logo interpreter, check this out.\n\nA language is a language is a language – or so they say.\n\nIn the 1950s, when Fortran evolved from the depths of IBM, there were no computer scientists. Let’s not mince words, by todays standards John Backus (who developed Fortran) was a computer scientist, but the reality is that early “computer scientists” were mathematicians. So began the long road of programming language development, with many of the most popular developed by industry (including languages designed for scientific use):\n\nFortran (IBM, John Backus)\n\nCobol (Conference on Data Systems Languages, US DOD)\n\nC (Bell Labs, Dennis Ritchie)\n\nC++ (Bell Labs, Bjarne Stroustrup)\n\nAda (CII Honeywell Bull, Jean Ichbiah, US DOD)\n\nJava (Sun Microsystem, James Gosling)\n\nPython (Guido van Rossum)\n\nRuby (Yukihiro Matsumoto)\n\nThere are of course good languages that had their birth in academe to the likes of ALGOL, Pascal, Modula-2, and Lisp. Many of these languages contributed to the evolution of others – but very few were successful in industry. Languages used in introductory programming courses generally tend to follow one of two paths: (i) a commercial language, or (ii) an “academic” language. The former are dominated by the likes of Java and Python. The latter can be scoped into functional languages such as “Scheme”, and Haskell, which still have limited industrial scope.\n\nSo which is better – industrial or academic? There is of course a good argument to be made for industrial languages – best to use what industry advocates, or at least obtain a grounding in it. One can go too industry specific too early, i.e. Objective C, are end up having learnt a language which is being supplanted (by Swift in the case of iOS). There is also the case for using simple languages which advocate the teaching of programming concepts, rather than syntax. For the purpose of teaching the craft of programming, I would advocate a language which has some industry force, but in which complexity such as pointers can be minimized, and language idiosyncrasies such as dangling-else can be avoided. It would also be nice to dodge compiler issues like lack of array-out-of-bounds checking. Foremost a language used in introductory programming is used to illustrate programming concepts. The syntax of a language should not overwhelm novice programmers.\n\nGood vis-à-vis bad choices for a first programming language, based on the idea of teaching the craft of structured programming?\n\nGOOD: Pascal, Ada, Python, Julia, Fortran\n\nBAD: Scheme (and anything based on Lisp), industry specific languages, Java, Cobol, SmallTalk (and anything too OO specific)\n\nMAYBE?: C, C++"
    }
}
{
    "id": "correct_subsidiary_00004_2",
    "rank": 90,
    "data": {
        "url": "http://www.cafeconleche.org/quotes2005.html",
        "read_more_link": "",
        "language": "en",
        "title": "XML Quotes in 2005",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "XML quotes",
            "Internet quotes",
            "web quotes",
            "XML"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "The Quote Archive at Cafe con Leche --\rXML and other notable quotes from January 1, 2005 through today.",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Saturday, December 31, 2005\n\nFrom personal experience, I'd have to say that complex DTDs are slightly more penetrable than XSDs. As a user, I'm usually just trying to find out one or two things and I can do this by chasing entities through the DTD with a text editor. I give up completely when faced with a complex XSD document.\n\n--Ronald Bourret on the xml-dev mailing list, Monday, 01 Nov 2004\n\nFriday, December 30, 2005\n\n\"View Source\" is terribly important. I know a huge number of people who know HTML at varying degrees of expertise and for every one *without exception*, \"view source\" provided a large part of their education.\n\n--Tim Bray on the WWW-Tag mailing list, Friday, 04 Oct 2002\n\nThursday, December 29, 2005\n\nSince SGML and XML provide no semantic primitives in terms of which documents can be interpreted, they are frequently described as being “just syntax.” One may wonder whether a purely syntactic notation constitutes a real step forward. Does defining a labeled bracketing, a tree structure, and a formalism for document grammars really suffice to make XML interesting or important? In some ways, of course, the answer is no; none of these is a particularly complex problem. What graduate student in computer science would regard the problem of developing a notation for serializing trees as requiring more than a weekend’s worth of work? Any competent programmer can write a program to parse any reasonably clean notation. Constraint checking is a bit more difficult, but if constraints are checked in a Turing-complete programming language, we may have a better chance of actually expressing all the constraints we would like to express.\n\nIn many other ways, however, the answer is yes; purely syntactic notation is a big step forward. XML is interesting to people who wish to exploit their data, because it provides enough commonality among documents to make possible the construction of generic tools instead of tools specialized for single notations. For those who want to spend their time working with their data, rather than building new tools, such generic tools are a great step forward. Generic syntax-directed editors, guided by nothing more complicated than a schema or DTD, make it easier to create good data. Generic tools for data validation make checking for many errors possible, and allow programmers to spend more time on processing the data and less time on checking that the input is clean. Generic browsers and display engines, supported by good style-sheet languages, make it possible to display data in multiple styles tailored for different users or processes.\n\n--C. M. Sperberg-Mcqueen\n\nRead the rest in ACM Queue - XML and Semi-Structured Data - What role can XML play in solving the semi\n\nWednesday, December 28, 2005\n\nOf course, we're advocating OpenOffice.org, not because it's free in the sense of gratis, although that's certainly an argument, but free in the sense that they can configure it now and in the future. So even if they don't necessarily want to make their own modifications now, maybe in the future they might want to make a better spell checker or make it an “on demand” software or re-brand it however they like. It's all possible. Thus, someone in Brazil has just done a grammar checker using Java. It's quite good. And at the same time, one of the Summer of Code students worked on his own grammar checker, using quite different strategies, I believe. Who benefits? Everyone. Choice is good.\n\nThe thing about open source is that it's perfectly unscripted in this way. We like to let the invisible hand of the market actually work. Open source generally works best in that way for certain areas, where you let the invisible hand determine what people need and want. But at the same time, governments are interested in predictability, and want to have a very visible hand in the process. So if governments want to participate, they certainly can put up money for this process. They can say, “Okay, I want this or that feature, and I'm willing to give a developer X amount of money to let them work on it.” That's fine. That's what Japan Inc, did, and it worked wonderfully. Japan Inc. is now doing this with some open-source projects, and other governments are certainly encouraged to do the same.\n\n--Luis Suarez-Potts\n\nRead the rest in :: Interviews : OpenOffice.org 2.0: An Office Suite With No Horizons\n\nTuesday, December 27, 2005\n\nthere is no such thing as a CDATA node. CDATA is just a nice way to write something so that you don't have to worry about escaping all the < and other special characters. There's only text nodes.\n\n--Jon Gorman on the xsl-list mailing list, Tuesday, 13 Dec 2005 09:17:52\n\nMonday, December 26, 2005\n\nOur brains aren't wired to think in terms of statistics and probability. We want to know whether an encyclopedia entry is right or wrong. We want to know that there's a wise hand (ideally human) guiding Google's results. We want to trust what we read. When professionals--editors, academics, journalists--are running the show, we at least know that it's someone's job to look out for such things as accuracy. But now we're depending more and more on systems where nobody's in charge; the intelligence is simply emergent. These probabilistic systems aren't perfect, but they are statistically optimized to excel over time and large numbers. They're designed to scale, and to improve with size. And a little slop at the microscale is the price of such efficiency at the macroscale.\n\n--Chris Anderson\n\nRead the rest in The Long Tail: The Probabilistic Age\n\nSaturday, December 24, 2005\n\nIt took the markup and then the web community a long time to come to the realization that binding types strongly to a syntax standard is a bad design for reach and scale even if it works for a particular product.\n\n--Claude L (Len) Bullard, on the xml-dev mailing list, Wednesday, 12 Oct 2005 13:17:38\n\nFriday, December 23, 2005\n\nFor all kinds of application design, the most common mistakes is to jump right in and start adding features to whatever it is you already have (or have copied from someone else). For various reasons this doesn’t work: Big piles of hit and miss features, chosen thoughtlessly, is less desirable that small piles of good features, chosen carefully. Consider this: what makes a good feature? It’s not an abstract quality: goodness means a problem is solved for a user. If you don’t spend some time considering who these people are and what they’re doing, odds are slim you’ll find features that matter much:\n\n--Scott Berkun\n\nRead the rest in How to build a better browser\n\nThursday, December 22, 2005\n\nWe need architectural forms painfully. SGMLers sold us out by making them very unnecessarily gnomic. OK just kidding, SGMLers, but when even Mike Kay admits he doesn't grok them, someone feel asleep at the tutorial wheel.\n\n--Uche Ogbuji on the xml-dev mailing list, Friday, 02 Dec 2005 08:42:55\n\nWednesday, December 21, 2005\n\nThe progression, IMHO, is really between the generally high skill set of your average XForms author and the comparatively low skill set of your average HTML author. It is quite possible to write highly accessible HTML (even if it uses scripting) and quite possible to write highly _in_accessible XForms. In both cases, the author is violating either best practice guidelines or actual rules of the language.\n\n--Ian Hickson on the www-forms mailing list, Friday, 11 Mar 2005 16:13:18 +0000\n\nTuesday, December 20, 2005\n\nGoogle Earth is not acquiring new imagery. They are simply repurposing imagery that somebody else had already acquired. So if there was any harm that was going to be done by the imagery, it would already be done.\n\n--John Pike, Globalsecurity.org\n\nRead the rest in Google Offers a Bird's-Eye View, and Some Governments Tremble\n\nMonday, December 19, 2005\n\nPay attention to physics. As a simple rule of thumb, an individual server can normally handle perhaps 100 requests a second (I’ll say within one order of magnitude up if simple ones and down if very hard ones). If the goal is for each server to support 1,000 concurrent users, therefore, try to avoid an event model in which each user pings the server more than once per 10 seconds, let alone one. In short, avoid a fine-grained model of code on the server responding to events at the level of mouse moves or keys typed or each time the scrollbar is dragged three millimeters because if you do, you’ll be generating about 10 events per second or two orders of magnitude too many. By the way, even if supporting only 10 concurrent users per server is acceptable, communications are often fragile, and it isn’t a great idea to be extremely fine-grained because a small rare glitch in communications can much more easily break the system.\n\n--Adam Bosworth\n\nRead the rest in ACM Queue - Learning from THE WEB\n\nThursday, December 15, 2005\n\nchanging a schema (DTD, or whatever structure representation) and associated documents is quite common. We keep doing this in ActiveMath. Changing the POJOs to match is a bit more delicate (and typically has influence much deeper into the code than just the beans). Allow me to add that XPath (which can be used in XOM, DOM, JDOM, XSLT, DOM4j and many others) is the best flexibility and readability you can afford. The performance is smaller, indeed (but not enormous), but the manageability is much greater!\n\n--Paul Libbrecht on the xml-dev mailing list, Monday, 05 Dec 2005 10:50:40\n\nSunday, December 11, 2005\n\nThe unquestioning regurgitation of administration spin through the use of anonymous sources is the fault line of modern American journalism. You'd think that after all we've seen -- from the horrific reporting on WMD to Judy Miller and Plamegate (to say nothing of all the endless navel-gazing media panel discussions analyzing the issue) -- these guys would finally get a clue and stop making the Journalism 101 mistake of granting anonymity to administration sources using them to smear their opponents.\n\nThe Washington Post, for example, citing an anonymous \"senior Bush official,\" reported on Sept. 4 that, as of Saturday, Sept. 3, Louisiana Gov. Kathleen Babineaux Blanco \"still had not declared a state of emergency\" ... when, in fact, the declaration had been made on Friday, Aug. 26 -- more than two days before Katrina hit Louisiana. This claim was so demonstrably false that the paper was forced to issue a correction just hours after the original story appeared. It's time for the media to get back to doing their job and stop being a principal weapon in Team Bush's damage-control arsenal.\n\n--Ariana Huffington\n\nRead the rest in Wired News: Arianna Learns to Love the Blog\n\nSaturday, December 10, 2005\n\nthere are at least five individuals on the XQuery WG who could have produced a perfectly usable spec two years ago if the other four individuals had not been present.\n\n--Michael Kay on the xml-dev mailing list, Thu, 19 Aug 2004\n\nFriday, December 9, 2005\n\nAtom. It can be tricky to explain why it was needed when there is RSS to anyone that hasn’t spent time coding around the stuff. As I see it Atom has three key benefits over RSS 2.0: a clear, community-consensus spec; mandates the identifiers of the Web (URIs); the content model isn’t broken (apart from its general opaque messiness, escaped HTML in content is fundamentally flawed - check silent data loss).\n\n--Danny Ayers\n\nRead the rest in Danny Ayers, Raw Blog : » Presenting syndication\n\nThursday, December 8, 2005\n\nTraditional tools require the data schema to be developed prior to the creation of the data. Unfortunately, sometimes the data schema emerges only after the software is already in use—and the schema often changes as the information grows. A typical example is the information contained in the item descriptions on eBay. It seems impossible for the eBay developers to define an a priori schema for the information contained in such descriptions. Today, all of this information is stored in raw text and searched using only keywords, significantly limiting its usability. The problem is that the content of item descriptions is known only after new item descriptions are entered into the eBay database. EBay has some standard entities (e.g., buyer, date, ask, bid...), but the meat of the information—the item descriptions—has a rich and evolving structure that isn’t captured.\n\nTraditional software design methodology does not work in such cases. One cannot rigidly follow the steps:\n\nGather knowledge about the data to be manipulated by the software components being designed.\n\nDesign a schema to model this information.\n\nPopulate the schema with data.\n\nWe need software and methodologies that allow a more flexible process in which the steps are interleaved freely, while at the same time allowing us to process this information automatically.\n\n--Daniela Florescu\n\nRead the rest in ACM Queue - Managing Semi-Structured Data\n\nWednesday, December 7, 2005\n\nIt seems that everyone using CSS eventually resorts to using absolute positioning with widths specified in pixels, to get everything to work. How, exactly, is this better than using tables?\n\n--Phillip Pearson\n\nRead the rest in Second p0st: I hate CSS | Generating Motorola S19 checksums in Python\n\nTuesday, December 6, 2005\n\nAnother important question is the extent to which the Open Document file format itself supports or fails to support accessibility. This comes up for things like storing the alternate text tag for an image, or noting the relationships of labels with the objects they label in on-line forms. While a thorough examination of the file format specifically for these issues still needs to be done, much of ODF is based on standard web technologies like SMIL for audio and multimedia, and SVG for vector graphics, which have and continue to be vetted by the World Wide Web Accessibility Initiative processes. We also know that two of the existing applications that currently read/write ODF can export Tagged PDF files in support of PDF accessibility, and Adobe has already conducted some tests to verify that accessibility across that translation is preserved (and thus must exist in the original ODF file). Finally, at this very moment the OASIS Technical Committee that created ODF is looking into forming a specific subcommittee to examine ODF for just these accessibility issues and address any shortcomings found.\n\nThis is in stark contrast to proprietary file formats like those used by Microsoft Office. Those formats are totally opaque, with no peer review of accessibility issues possible. Thus we cannot objectively tell how well the Microsoft Office file format supports accessibility, or say whether it does a better or worse job than ODF.\n\n--Peter Korn\n\nRead the rest in Peter Korn's Weblog\n\nMonday, December 5, 2005\n\nIn the general case, REST is XML over HTTP. In the general case, SOAP is XML over HTTP. Anything you can do with one, you can do with the other. Using SOAP standardizes some things, you'd have to reinvent if you go with REST. Using SOAP+WSDL does so even more.\n\n--Dare Obasanjo on the xml-dev mailing list, Wednesday, 30 Mar 2005 08:44:34\n\nSunday, December 4, 2005\n\nDon't make premature demands on users who aren't ready to buy. For example, don't require registration to read whitepapers or see a demo. If you do, you'll scare away many users who otherwise might have converted at a later date.\n\n--Jakob Nielsen\n\nRead the rest in The Slow Tail: Time Lag Between Visiting and Buying (Jakob Nielsen's Alertbox)\n\nSaturday, December 3, 2005\n\nAn example of where an inappropriate extension has been made to the protocol to support features that contradict the desired properties of the generic interface is the introduction of site-wide state information in the form of HTTP cookies [73]. Cookie interaction fails to match REST's model of application state, often resulting in confusion for the typical browser application.\n\nAn HTTP cookie is opaque data that can be assigned by the origin server to a user agent by including it within a Set-Cookie response header field, with the intention being that the user agent should include the same cookie on all future requests to that server until it is replaced or expires. Such cookies typically contain an array of user-specific configuration choices, or a token to be matched against the server's database on future requests. The problem is that a cookie is defined as being attached to any future requests for a given set of resource identifiers, usually encompassing an entire site, rather than being associated with the particular application state (the set of currently rendered representations) on the browser. When the browser's history functionality (the \"Back\" button) is subsequently used to back-up to a view prior to that reflected by the cookie, the browser's application state no longer matches the stored state represented within the cookie. Therefore, the next request sent to the same server will contain a cookie that misrepresents the current application context, leading to confusion on both sides.\n\nCookies also violate REST because they allow data to be passed without sufficiently identifying its semantics, thus becoming a concern for both security and privacy. The combination of cookies with the Referer [sic] header field makes it possible to track a user as they browse between sites.\n\nAs a result, cookie-based applications on the Web will never be reliable. The same functionality should have been accomplished via anonymous authentication and true client-side state. A state mechanism that involves preferences can be more efficiently implemented using judicious use of context-setting URI rather than cookies, where judicious means one URI per state rather than an unbounded number of URI due to the embedding of a user-id. Likewise, the use of cookies to identify a user-specific \"shopping basket\" within a server-side database could be more efficiently implemented by defining the semantics of shopping items within the hypermedia data formats, allowing the user agent to select and store those items within their own client-side shopping basket, complete with a URI to be used for check-out when the client is ready to purchase.\n\n--Roy T. Fielding\n\nRead the rest in Fielding Dissertation: CHAPTER 6: Experience and Evaluation\n\nFriday, December 2, 2005\n\nWhen someone says \"my code doesn't work in your browser, which you claim to be DOM-compliant\", but it worked in Safari, I am much happier to tell them \"that is because Safari is not DOM compliant\" rather than telling them \"that is because DOM isn't interoperable between two implementations in this feature, even though you complied with the standard\". That is why making exceptions optional in this sort of case is a bad idea\n\n--Ray Whitmer on the www-dom mailing list, Sunday, 1 Dec 2005 07:46:16\n\nThursday, December 1, 2005\n\nSimplicity takes effort-- genius, even. The average programmer seems to produce UI designs that are almost willfully bad. I was trying to use the stove at my mother's house a couple weeks ago. It was a new one, and instead of physical knobs it had buttons and an LED display. I tried pressing some buttons I thought would cause it to get hot, and you know what it said? \"Err.\" Not even \"Error.\" \"Err.\" You can't just say \"Err\" to the user of a stove. You should design the UI so that errors are impossible. And the boneheads who designed this stove even had an example of such a UI to work from: the old one. You turn one knob to set the temperature and another to set the timer. What was wrong with that? It just worked.\n\n--Paul Graham\n\nRead the rest in Ideas for Startups\n\nWednesday, November 30, 2005\n\ngiven that search engines are pretty much the only reason that the Web works as well as it does (HTML & HTTP are nice, but I remember the Web before search engines, and it was rather grim), I don't see how the Semantic Web can ever work either without search engines. In practical terms, nobody has a clue how to make the Web work without search engines; at least, nobody has ever demonstrated such a thing that I am aware of. Which is to say that getting Google and other search engines on board with a common and workable approach to Semantic Web content has to be a #1 priority, because it's really the only game in town.\n\n--Anthony B. Coates on the www-tag mailing list, Monday, 28 Nov 2005 08:44:14\n\nTuesday, November 29, 2005\n\nXML does not have null values. It just has character content (#PCDATA) and nested element content (and attributes). If you are wanting to model a database entry that may be null, there are several ways that you may do this, by allowing the element to be empty, or by having an explict child element, say <null/>, or having an explict attribute, it is your choice how to model this in teh XML, and you must then code your application to understand this model. However you chose to model the null value, the DTD declaration will reflect that model, you can't declare \"null\" in a DTD.\n\n--David Carlisle on the xml-dev mailing list, Tuesday, 29 Nov 2005 11:58:00\n\nMonday, November 28, 2005\n\nFrequently updated web sites perform better in organic search engine results. This enables the business to attract customers to its web site without paying for sponsored links.\n\n--Robert J. Miller\n\nRead the rest in java.net: Fitnesse Testing for Fast\n\nSunday, November 27, 2005\n\nsome usability issues are different for users with disabilities than for those without, but the overlap is remarkably large. Also, it's an oversimplification to distinguish between users with and without disabilities as if that were a dichotomy. It's really a continuum of people with more or less severe disabilities. For example, most users over the age of 45 have somewhat reduced vision and need resizable fonts, even if they don't qualify under the official definition as \"low-vision users.\"\n\n--Jakob Nielsen\n\nRead the rest in Accessibility Is Not Enough (Jakob Nielsen's Alertbox)\n\nSaturday, November 26, 2005\n\nWith very few exceptions, any attempt to devise a suitable upper bound for any 'maxOccurs' value is bound to involve wild-ass-guessery. How many paragraphs should one allow in an HTML document? You can take this from a business logic standpoint (\"what's the longest web page anyone is ever going to want to produce\"), or a processing standpoint (how many <p> elements can Mozilla cope with? What about MSIE? Does your answer change depending on how old the user's computer is?), but you'll never be able to come up with a satisfactory number. Whatever number you choose will either be too large as a meaningful resource constraint, or it will be too small for some existing or future document.\n\n--Joe English\n\nRead the rest in xml-dev - Re: xml-dev] Constrain the Number of Occurrences of Elements in your XML\n\nFriday, November 25, 2005\n\nBuy Nothing Day has become this huge phenomenon around the world. It's sort of like an edgy Earth Day and people are doing all sorts of things including blogs. But I have to tell you that there is also a downside to blogs. There are a number of people who think they are activists if they start a blog and talk sustainability.\n\nI think there is more to it than that. The downside of the internet is that it has spawned a generation of activists who are actually very passive, who basically forward an e-mail to a friend and they think they are being some kind of an activist, and to me that is not the sort of activism that is effective.\n\n--Kalle Lasn\n\nRead the rest in Wired News: Put Your Money Where Your Mind Is\n\nThursday, November 24, 2005\n\nWhen XSLT is a good fit for a problem, it is a phenomenally good fit. When it's a bad fit it's incredibly frustrating. Unfortunately it's totally possible to take an approach to a problem that could be a good fit, but find that the approach makes it a very bad fit.\n\nThe fact that it's so different from procedural and object oriented languages only exacerbates this for newcomers, who are vastly more likely to choose bad fits than good ones. But the great strengths XSL has are inextricably tied to these differences.\n\n--Nathan Young on the xsl-list mailing list, Sunday, 8 Sep 2005 14:43:21\n\nWednesday, November 23, 2005\n\nSounds like it might be interesting but you know, I'm tired, and just not up to the aggravation of clicking on something that ends in \".pdf\". And I bet I'm not the only one\n\n--Tim Bray on the xml-dev mailing list, Monday, 25 Aug 2003\n\nTuesday, November 22, 2005\n\nNever make users register, unless you need to in order to store something for them. If you do make users register, never make them wait for a confirmation link in an email; in fact, don't even ask for their email address unless you need it for some reason. Don't ask them any unnecessary questions. Never send them email unless they explicitly ask for it. Never frame pages you link to, or open them in new windows. If you have a free version and a pay version, don't make the free version too restricted. And if you find yourself asking \"should we allow users to do x?\" just answer \"yes\" whenever you're unsure. Err on the side of generosity.\n\n--Paul Graham\n\nRead the rest in Web 2.0\n\nMonday, November 21, 2005\n\nAll the alternatives to MS Office suck eggs, and they have all tried to copy all the worst things about the MS tools. I have news for the ODF fans - an open document format doesn't mean anything if the base tool is horrid - and Open Office is, quite simply, horrid.\n\n--James Robertson\n\nRead the rest in Microsoft and the Mountain\n\nSaturday, November 19, 2005\n\nPoorly designed XSLT is hard to maintain like poorly designed code of any type. Well designed XSLT is a treat to work with.\n\n--Charles Knell on the xsl-list mailing list, Tuesday, 02 Aug 2005 10:21:59\n\nFriday, November 18, 2005\n\nData integrity is one of those idealistic formal criteria that sound and look great on the drawing board, but tend to lose their street appeal as soon as they get implemented and the rubber meets the road, so to speak. Like that shopping mall that looks fabulous on the computer-generated screen, but gives us the chills of repulsion once it gets built and is twice as hideous as anyone could’ve imagined, the concept of data integrity offers a rather laughable sense of false security that only people who spend nights at the computer screen and never go out can buy into.\n\n--Alex Bunardzic\n\nRead the rest in Ethical Software by Alex Bunardzic » The Myth Of Data Integrity\n\nThursday, November 17, 2005\n\nSometimes a rather thin, syntax-oriented, semantically vacuous layer of commonality is all that is needed to simplify things dramatically.\n\n--C. M. Sperberg-Mcqueen\n\nRead the rest in ACM Queue - XML and Semi-Structured Data - What role can XML play in solving the semi\n\nFriday, November 11, 2005\n\nWS has long ago degenerated into a joke to all but a few marketing professionals, industry analysts and committed developers. Originally it was supposed to be an improvement over the likes of COM and CORBA, this improvement coming because somehow the use of XML would work a salubrious magic. So much for that fantasy. WS-*, as Robertson points out, and many others have before, is now a far more complex \"stack\" than the entire OMA (of which CORBA is but a part) and with much less grounding in practice. Too bad for WS folk. XML folk don't care. Why? Because just as XML was never going to magically save an under-architected system from itself, XML was never likely to be substantively damaged by the fact that it was considered the keystone of said under-architected system.\n\n--Uche Ogbuji\n\nRead the rest in More proof that Web services are poison to XML\n\nThursday, November 10, 2005\n\nYou can tell Java developers who understand XML from those that don't. Those who don't often tend to have overspecified object model hierarchies and like to boast of using hundreds of classes - the ones who do typically have far simpler interfaces, prefer to put most of the hierarchical complexities into the XML, and usually can be found at the beach during the summer.\n\n--Kurt Cagle on the xml-dev mailing list, Friday, 28 Jan 2005 10:54:18\n\nWednesday, November 9, 2005\n\nAnother common problem in managing today’s information is the lack of agreement on vocabularies and schemas. Existing information-processing methodologies require that all the communities involved in generating, processing, or consuming the same information agree to a given schema and vocabulary. Unfortunately, different people, organizations, and communities have inherently different ways of modeling the same information. This is independent of the domain to be modeled or the target abstract model being used (e.g., relations, Cobol structures, object classes, XML elements, or RDF [Resource Description Framework] graphs). Reaching schema agreements among different communities is one of the most expensive steps in software design. Database views have been designed to alleviate this problem, yet views do not solve the schema heterogeneity problem in general. We need to be able to process information without requiring such a priori schema and vocabulary agreements among the participants.\n\n--Daniela Florescu\n\nRead the rest in ACM Queue - Managing Semi-Structured Data\n\nTuesday, November 8, 2005\n\nif you have 5GBs of data, you probably should not be keeping it in one XML file.\n\n--Jason Robbins on the jdom-interest mailing list, Sunday, Mar 2005 14:33:12\n\nMonday, November 7, 2005\n\nThe wisdom of crowds works amazingly well. Successful systems on the Web are bottom-up. They don’t mandate much in a top-down way. Instead, they control themselves through tipping points. For example, Flickr doesn’t tell its users what tags to use for photos. Far from it. Any user can tag any photo with anything (well, I don’t think you can use spaces). But, and this is a key but, Flickr does provide feedback about the most popular tags, and people seeking attention for their photos, or photos that they like, quickly learn to use that lexicon if it makes sense. It turns out to be amazingly stable. Del.icio.us does the same for links (and did it first, actually). Google’s success in making a more relevant search was based on leveraging the wisdom of crowds (PageRank). RSS 2.0 is taking off because there is a critical mass of people reading it and it is easy to read/write, so people have decided to leverage that when publishing content. It isn’t that it is a good or bad format for things other than syndicated content (for which I think it is very good). Rather, it works well enough.\n\n--Adam Bosworth\n\nRead the rest in ACM Queue - Learning from THE WEB\n\nSunday, November 6, 2005\n\nThou shalt remember the customer's phone number. This means you, computer and cellphone companies. We call for help; we're asked to type in our 10-digit phone numbers or 20-digit customer numbers; then when an agent picks up, we're asked for that number again.\n\nWhat - did you think we actually moved and changed our identities since placing the call?\n\nIf they can write software that sends a man to the moon, they can surely write call-center software that passes on to the agent the information we've already typed in.\n\n--David Pogue\n\nRead the rest in 10 Ways to Please Us, the Customers\n\nFriday, November 4, 2005\n\nIn my experience working with developers and users, this is the biggie: There is a certain hard core of SGML/XML people who just 'grok' the XSLT development style and can use if to great advantage. But there are a lot of people (and I count myself among them, to my shame) that just can't get anything non-trivial done in XSLT without an example to work from and a reference manual in hand. Many of those same people can grok the basics of XQuery pretty quickly (\"oh, it's a lot like SQL except ....\"). My rough estimate from talking to XML users (as opposed to geeks) over the years is that SQL/XQuery grokkers outnumber XSLT grokkers by something like 10:1.\n\n--Michael Champion on the xml-dev mailing list, Monday, 8 Nov 2004\n\nThursday, November 3, 2005\n\nmany stylesheets consist of two-thirds data to be copied into the result tree, and one-third instructions to extract data from the source document. An XML-based syntax is beneficial for the two thirds that is data, because it means the code in the stylesheet is a template for the final result. This also facilitates a development approach that starts with graphical designers producing a mock-up of the target HTML or XSL-FO page, and then handing it over to a programmer to add the control logic. (XQuery has recognized this by using an XML-like syntax for element constructors, but there's a lot of difference between being XML-like and being XML.)\n\n--Michael Kay on the xml-dev mailing list, Wednesday, 8 Dec 2004\n\nWednesday, November 2, 2005\n\nCharsets are easy to accomodate with forethought, but hell to deal with if you just assume (or don't think of it at all, which is an implicit assumption). Nor is it a new problem. Just one that's ignored too often.\n\n--Greg Guerin on the java-dev mailing list, Saturday, 3 Sep 2005 19:53:04\n\nMonday, October 31, 2005\n\nThere all right but let me add one more little thing. XSLT is a template language and when written in its more functional/template-based nature XSLT is really quite easy and doesnt require complexity to get proper results. The problem as I see it (and Colin pointed this out to a nice example of using XPath... people tend to force XPath to do TONS of heavy lifting when in reality you just have to let the XML fall gracefully until targeted by the best matching template and then captured and processed accordingly) is that our core focus for so many years has been to have COMPLETE and TOTAL control at every moment using procedural coding that we all tend to force the issue with XSLT (you will do this and do this now! kind of thing) instead of letting the issue happen and then dealing with it when it does... laaaaazzzzyyyy style ;) When you step back from that mentality and simply design your XML and XSLT in such a way that there is no need to climb up and down the tree to process the data then XSLT is never complex. It may take a little getting used to the style and as such seem a bit complex but after you just \"let go and let God\" as the saying goes then your mind adjusts and it simply makes sense. It seems to me that just about the time you find yourself having to write complex code to make something work in XSLT is right about the time you need to rethink things and realize you are doing things the hard way (the procedural side of your brain if you will) and that there is a much simpler approach that will work twice as efficient if you block the procedural line of thinking and embrace the template-based functional side of your brain\n\n--M. David Peterson on the xsl-list mailing list, Sunday, 17 Apr 2005 12:50:54\n\nFriday, October 21, 2005\n\nOnce you've fundamentally handicapped yourself by tying your data model/programming language to XML Schema then you are already broken. It's like saying \"I did the best job I could painting my apartment with a tootthbrush\". The thing to do isn't asking for better ways to paint apartments with a toothbrush but instead realizing you picked the wrong tool for the job.\n\n--Dare Obasanjo on the xml-dev mailing list, Friday, 3 Dec 2004\n\nThursday, October 20, 2005\n\n\"Make the right thing easy and the wrong thing hard.\" If designers followed that one clear principle, there'd be a lot more happy users. I'd get a lot more work done instead of struggling with a counterintuitive interface. Writing software would be easier because APIs would simply make sense, with less chance of blowing up at runtime. I could use my car stereo.\n\n--Kathy Sierra\n\nRead the rest in Creating Passionate Users: Making happy users\n\nWednesday, October 19, 2005\n\nMany big companies, such as Microsoft and Nortel, in their quest to gain shares of the large Internet market in China, transform China into an information prison by collaborating with the Chinese regime on questions of censorship. They should not forget all moral principles under the temptation of financial gain.\n\n--Guo Guoting\n\nRead the rest in Battle blogging for profit\n\nTuesday, October 18, 2005\n\nClassic network file systems are designed around the idea of mapping system file procedures into network transactions. A lot of naive designs that use XML do much the same thing: they wrap a relatively small amount of data up in a call, and get a relatively small amount of data in return (fopen, fseek, fread equivalents).\n\nXML enables larger granularity. Rather than sending \"commands\" and getting \"return values\", you can send and receive larger documents. This corresponds to the way that WebDAV works, for instance. Instead of \"opening, seeking, reading\", you just get the document/file. Fewer, somewhat larger network transactions; in many cases one can rely on low-level infrastructure to speed the operations. Compression is also more effective in this scenario. Parser start up times contribute overhead, if the documents are larger, the overhead starts to recede to insignificance.\n\nFor increasing performance, increasing the message size (that is, increasing the content, increasing the granularity of operations) is often far more effective than attempts to bum a few cycles from fine-grained operations.\n\n--Amelia A Lewis on the xml-dev mailing list, Thu, 11 Nov 2004\n\nMonday, October 17, 2005\n\nthe reason most current XForms documents are of a higher markup quality (more conformant, more accessible) than the average HTML document is that the average XForms author today is more competent than the average HTML author. It has nothing to do with whether XForms is easy or hard.\n\n--Ian Hickson on the www-forms mailing list, Sunday, 13 Mar 2005 17:22:13 +0000\n\nSaturday, October 15, 2005\n\nThe internet is not an accident. The internet was not bound to happen. There was no guarantee that the internet would reach its current state as a side effect of emerging digital processing and communications capabilities. We did not recover complex alien technology.\n\nThe internet, that place where all eventual business will be transacted, all content and media will be distributed, all correspondence will be exchanged, all history will be recorded, and all pornography will be is being admired, has a design - and its meant for exactly these purposes.\n\nMany of the principles that led to this design are still with us today, although I would challenge you to ascertain them by observing the mainstream technologies being peddled by leading vendors, publications, and analyst firms. Those who rose to power in a much different environment, where the short-term profits of disconnected, dead-end business software was deemed more important than laying a fertile ground where millions of new ideas (and hence new profits) could bloom.\n\nBut the dead-end has long been reached and so these industry leaders have turned their attention to this new place, built on principles and values very different from their own, and have somehow reached the conclusion that this thriving ecosystem must be re-arranged such that they have somewhere to place their baggage. Instead of embracing the people, principals, and technologies that gave rise to this phenomenon they have chosen to subvert its history and to implant the ridiculous notion that it is “incapable of meeting the stringent demands of the business community.”\n\nNot only have these business radicals claimed the internet as their own but they have also somehow gained the confidence of all the worlds industry in their ability to deliver a new and sparkling internet, one no doubt capable of reproducing the complexities and flaws that plague existing mediums so as to make it feel more like home. They’ve brought their own principles and agendas, asserting them as obvious and correct while ignoring the wisdom we’ve gained and shared and gained and shared over years of collaborative practice and observation of working systems at this scale.\n\n--Ryan Tomayko\n\nRead the rest in Motherhood and Apple Pie [@lesscode.org]\n\nFriday, October 14, 2005\n\nXLink is neat conceptually, but in practical use, there are easy ways to do the same thing without using XLinks. Without a compelling use case shared by a near universal set in the user community, it gets very little play outside implementations of link data bases\n\n--Bullard, Claude L (Len) on the xml-dev mailing list, Friday, 14 Jan 2005 12:12:34\n\nWednesday, October 12, 2005\n\nThere comes a point where some of these functionalities, such as the seamless interoperation of the Internet, are too important to leave to the private interest of businesses. We like to think that people won't do antisocial things, but when push comes to shove they will defend their economic interests even at the expense of the public.\n\n--Mark Cooper, Consumer Federation of America\n\nRead the rest in Net blackout sparks talk of new rules\n\nTuesday, October 11, 2005\n\nWhen you are using a platform-specific data binding tool, it will generate good quality code for serializing the data into XML/SOAP and then recreating the objects at the other end fine. And it will also generate an XML Schema, if it didn't use a custom annotated one for its template.\n\nBut that XML schema may be suspect or unacceptable to other tools. But no worries, the other end probably does not validate with the XML Schema or use its typing, anyway.\n\nThe paradox? The way to use XML Schemas successfully in multi-vendor web services is to just pretend to use them. The schema and/or WSDL become just documentation, and should not be considered translatable specifications.\n\n--Rick Jelliffe on the xml-dev mailing list, Sunday, 10 Jul 2005 05:15:29 +1000\n\nSunday, October 9, 2005\n\nPeople are already calling the XQuery data model the XML data model. They point to how vendors rallying behind SQL made relational DBMS valuable to the mainstream. The problem with this comparison is that SQL is not an omni-tool; it is designed to work on very highly normalized data sets. XML, by contrast, derives much of its power from denormalization; as a result, there is no way to have a single catchall data model for XML that is suitable for all purposes.\n\n--Uche Ogbuji\n\nRead the rest in Is XQuery an omni-tool?- ADTmag.com\n\nSaturday, October 8, 2005\n\nNetscape 4 was a really crappy product. We had built this really nice entry-level mail reader in Netscape 2.0, and it was a smashing success. Our punishment for that success was that management saw this general-purpose mail reader and said, \"since this mail reader is popular with normal people, we must now pimp it out to `The Enterprise', call it Groupware, and try to compete with Lotus Notes!\"\n\nTo do this, they bought a company called Collabra who had tried (and, mostly, failed) to do something similar to what we had accomplished. They bought this company and spliced 4 layers of management in above us. Somehow, Collabra managed to completely take control of Netscape: it was like Netscape had gotten acquired instead of the other way around.\n\nAnd then they went off into the weeds so badly that the Collabra-driven \"3.0\" release was obviously going to be so mind-blowingly late that \"2.1\" became \"3.0\" and \"3.0\" became \"4.0\". (So yeah, 3.0 didn't just seem like the bugfix patch-release for 2.0: it was.)\n\n--Jamie Zawinkski\n\nRead the rest in Groupware Bad\n\nFriday, October 7, 2005\n\nPerhaps I am missing the point, but it seems to me that the answer to the question \"Xforms or Web Forms\" is an emphatic yes. There is room and need in the Web universe for both the evolutionary Web Forms approach, to support backwards HTML compatibility, and the revolutionary XForms approach, to support a new XML based paradigm.\n\n--Eric S. Fisher on the www-forms mailing list, Monday, 14 Mar 2005 17:00:47\n\nThursday, October 6, 2005\n\nFlash should not be used to jazz up a page. If your content is boring, rewrite text to make it more compelling and hire a professional photographer to shoot better photos. Don't make your pages move. It doesn't increase users' attention, it drives them away; most people equate animated content with useless content.\n\nUsing Flash for navigation is almost as bad. People prefer predictable navigation and static menus.\n\n--Jakob Nielsen\n\nRead the rest in Top Ten Web Design Mistakes of 2005 (Jakob Nielsen's Alertbox)\n\nTuesday, October 4, 2005\n\nA person from the UK makes an online purchase from a US supplier. The online supplier requires entry of a two-letter code in the \"State\" box and a numeric value in the \"postal code\" box, despite the fact that the person entered UK as the country. So, the person entered \"ZZ\" as the state and 12345 as the postal code. Does validation result in forcing people to supply incorrect information?\n\n--Roger L. Costello on the xml-dev mailing list, Tuesday, 24 Aug 2004\n\nMonday, October 3, 2005\n\nWe need XML Schema so that other WGs know what not to do. It is a signpost of pain warning each and every specifier of standards gone astray.\n\n--Robin Berjon on the xml-dev mailing list, Tuesday, 16 Aug 2005 14:52:34\n\nMonday, October 3, 2005\n\nOne of the ironies of technological evolution is that even when we *intend* there to be a Darwinian weed-out, sometimes the opposite happens -- we get more variety and speciation -- just as sometimes when we hope/expect many alternatives to flourish, along comes something and dominates, and almost everyone switches to that. XML has seen both kinds of wrong guesses in its short history. For example, way back when, we thought there might be lots of different document formats, but it turns out that to the extent this is the case, the \"bespoke\" formats are fairly private, and in public one sees pretty much the same thing over and over (Docbook, TEI, OpenOffice XML, etc.). Yet on the other hand, the expectation that we'd have *a single* schema language was met by the proliferations of DTD, W3C Schema, RelaxNG and Schematron (not that there hasn't been any Darwinian weeding here), plus a smattering of ingenious more local solutions -- so the opposite has happened, and to be an expert on \"schemas in/for XML\" you have to be conversant with all these.\n\n--Wendell Piez on the xsl-list mailing list, Wednesday, 24 Aug 2005 12:42:31\n\nSaturday, October 1, 2005\n\nI don't recommend Real Media to anyone these days because the free player has become such a nuisance for end-users to install. It endlessly tries to force you to register, trick you into paying for a premium player, and then once they do get it installed, the video display is tiny because the pane is crowded with their \"content\", little mini browsers and so-called \"channels\". Bleah.\n\n--David Kaufman on the WWWAC mailing list, Sunday, 8 Sep 2005 17:11:32\n\nFriday, September 30, 2005\n\nBut comma-delimited ASCII doesn't work just as well.\n\nFirst: comma-delimited. What if the fields contain commas? Or newlines? They need to be quoted (and the developers need to know that they need to be quoted), which means you've already got an interop problem, namely, which of the half-dozen flavors of CSV are you going to use?\n\nSecond: ASCII. 'Nuf said.\n\nThird, and most important, is the shape of the data. Not everything fits in a list of homogeneous records, which is CSV's the natural shape. Of course you can wedge data that isn't shaped like an N by M table into a CSV file, but then you have to devise your own encoding scheme.\n\nXML's basic building blocks of elements, attributes, and text are flexible enough to accomodate a much broader range of data. Only a few things fit in a regular table, but a lot of things can fit in a tree.\n\n--Claude L (Len) Bullard, on the xml-dev mailing list, Wednesday, 1 Jun 2005 15:41:15 -0500\n\nWednesday, September 28, 2005\n\nOne of the ironies of technological evolution is that even when we *intend* there to be a Darwinian weed-out, sometimes the opposite happens -- we get more variety and speciation -- just as sometimes when we hope/expect many alternatives to flourish, along comes something and dominates, and almost everyone switches to that. XML has seen both kinds of wrong guesses in its short history. For example, way back when, we thought there might be lots of different document formats, but it turns out that to the extent this is the case, the \"bespoke\" formats are fairly private, and in public one sees pretty much the same thing over and over (Docbook, TEI, OpenOffice XML, etc.). Yet on the other hand, the expectation that we'd have *a single* schema language was met by the proliferations of DTD, W3C Schema, RelaxNG and Schematron (not that there hasn't been any Darwinian weeding here), plus a smattering of ingenious more local solutions -- so the opposite has happened, and to be an expert on \"schemas in/for XML\" you have to be conversant with all these.\n\n--Wendell Piez on the xsl-list mailing list, Wednesday, 24 Aug 2005 12:42:31\n\nTuesday, September 27, 2005\n\nmany customers have XML files in the 5 gig size range and XML content sets exceeding 5 terabytes. I think we'll see larger XML files and XML content sets as people get familiar with the advanced tools capable of handling them. As a comparison, probably no one has a 100 million row Excel table, but a 100 million row relational database is common.\n\n--Jason Hunter on the jdom-interest mailing list, Monday, 14 Mar 2005 21:47:19\n\nMonday, September 26, 2005\n\nI have been studying the SVG specs when I happened upon the SVG path tag. Much to my dismay, the W3C decided to stuff a micro-language within a single parameter! That is one of the most awful hacks I have ever seen. This completely defeats the purpose of XML! Well so much for the original goals of simplicity and genericity of XML representations. Apparently this is old news. It's a shame the spirit of XML is being ignored in such a blatant manner.\n\n--Christopher Diggins\n\nRead the rest in XML down a slippery slope\n\nSunday, September 25, 2005\n\nK-12 teachers tend to simplify English style into rules. The idea is benevolent enough - by following these so-called rules, the children will produce writing that works reasonably well AND that the teacher can grade without much mental effort, leaving time and brain cells for the rest of the multitude of required chores. But as a result, many people grow up believing in the linguistic equivalent of Santa Claus: Do things according to \"the rules\" (say, don't use parentheses, or don't begin a sentence with \"but\"), and your writing will be good writing. And any writing that doesn't follow these rules will be bad writing - even if you like it.\n\n--Hilary Powers on the cbp mailing list, Saturday, 09 Jul 2005 10:19:55\n\nFriday, September 23, 2005\n\nThe Binary Infoset issue is small compared to the larger one that has crippled fundamental standards at the W3C: the chaotic development of DTD-replacing layers (xml:include, xml:base, xml:id, xlink, XML Schemas) without having a corresponding dependable processing sequence like that of DTDs.\n\n--Rick Jelliffe on the xml-dev mailing list, Tuesday, 12 Apr 2005 04:05:06 +1000\n\nThursday, September 22, 2005\n\nThe Times has just placed all of their \"name\" op/ed columnists behind a pay wall ($49.95 per year). The question I have to ask is, why would anyone pay for that? Forget the political persuasion of any of the writers - there are tons of voices on the net that cover the opinion spectrum. The vast majority of them are free, and can be read quickly and easily with a news aggregator (and the masses will be using syndication, once IE7 ships).\n\nSo why would you pay for the privilege of reading the Times' stable of writers? Are they really better than the free pundits? I don't think so, and I think that the Times is in for a rude awakening. The reality is, they just opted out of the political conversation. Up until now, bloggers of all stripes linked to the Times writers, either to agree or disagree. That's not going to happen now - even if a given blogger subscribes, he'll know that most of his readers won't\n\n--James Robertson\n\nRead the rest in Smalltalk Tidbits, Industry Rants: The New York Times vs. Free Content\n\nTuesday, September 20, 2005\n\nIt often takes some time to internalize the design of XSLT, especially if your background is in procedural programming. When you are accustomed to writing programs that say \"Do this, and then do that until this is true.\", writing and understanding XSLT can be a challenge.\n\n--Charles Knell on the xsl-list mailing list, Tuesday, 02 Aug 2005 10:21:59\n\nMonday, September 19, 2005\n\nit's just not clear to me that XQuery really does what XSLT does in a developer-friendly way. It's certainly a lot more approachable as a way to query databases than XSLT, but is it dramatically more convenient as a way to filter, merge, and transform XML data and services? Is anyone even marketing it that way any more? Most of the commercial interest in XQuery these days is as a database interface,\n\n--Michael Champion on the xml-dev mailing list, Sunday, 12 Dec 2004\n\nSunday, September 18, 2005\n\nThe problem isn't that the stories I care about aren't being covered, it's that they aren't being covered in the obsessive way that breaks through the din of our 500-channel universe. Because those 500 channels don't mean we get 500 times the examination and investigation of worthy news stories. It often means we get the same narrow, conventional-wisdom wrap-ups repeated 500 times. Paradoxically, in these days of instant communication and 24-hour news channels, it's actually easier to miss information we might otherwise pay attention to. That's why we need stories to be covered and re-covered and re-re-covered and covered again -- until they filter up enough to become part of the cultural bloodstream.\n\n--Ariana Huffington\n\nRead the rest in Wired News: Arianna Learns to Love the Blog\n\nFriday, September 16, 2005\n\nMicrosoft hasn't really improved on their browser for five years. That's a long time not to update a product and especially when it's the most used product in the world.\n\n--John Von Tetzchner, Opera Software\n\nRead the rest in Gussying up for the Opera | Newsmakers | CNET News.com\n\nWednesday, September 14, 2005\n\nI use Word for book creation (don't bother commenting about that; I've heard all the \"you should use this\" comments, and every one of them would require much more work to use than Word does. And OpenOffice, much as I love the concept, won't handle a book. When I find a truly better solution, I'll be the first to switch.).\n\n--Bruce Eckel\n\nRead the rest in Java Threads\n\nTuesday, September 13, 2005\n\nI have been a web developer for ten years. It's not 1997 anymore. I cannot think of any misguided \"feature\" that would require a particular browser, except testing guidelines. That is: THIS IS NOT A TECHNICAL PROBLEM. THIS IS A BUREAUCRACY PROBLEM.\n\n--Boyd Waters\n\nRead the rest in MacInTouch: timely news and tips about the Apple Macintosh\n\nMonday, September 12, 2005\n\nXOM has the best API ever.\n\nIn my app we churn business objects into XHTML then XSL:FO and finally PDF. XOM makes it super easy to build the XHTML tree. And if I play my cards right, I might be able to turn that XHTML into FO without serializing it to bytes first. Amazing.\n\nXOM makes XML fun again! Get rid of SAX, DOM and hardcoded \"<html>\". Get XOM, be happy.\n\n--Jesse Wilson\n\nRead the rest in Public Object: Get XOM\n\nSunday, September 11, 2005\n\nI am a Web developer for a non-profit that is trying to help families with special health needs affected by Hurricane Katrina. Why in the world would FEMA, a federal agency subject to Section 508 provisions, build a web site that is only accessible via Internet Explorer for Windows? I know of no *good* technical reason to do so. In fact, if your web team is worth its salt, they should be developing to W3C standards anyway, which would mean that your web resources would be available to anyone with a web browser, even an old one on an old system! As a Macintosh user, I'm used to this kind of marginalization, but I find it outrageous that a resource as critical to people in desperate straits as this one would exclude millions of people for no reason other than ignorance of best practices.\n\n--Andrew Hedges\n\nRead the rest in MacInTouch: timely news and tips about the Apple Macintosh\n\nSaturday, September 10, 2005\n\nThe XML Spec says:\n\n\"Names beginning with the string \"xml\", or with any string which would match (('X'|'x') ('M'|'m') ('L'|'l')), are reserved for standardization in this or future versions of this specification.\"\n\nhttp://w3.org/TR/2004/REC-xml-20040204/#sec-common-syn\n\nReserved does not meen invalid or forbidden. The meaning is: \"You can still use such names but you have been warned. Some nice day you'll wake up to find that the nice name \"xmlns2\" you used has been given exclusively to \"XML Namespaces -- 2\" and your source xml documents have suddenly changed their meaning\".\n\n--Dimitre Novatchev on the xsl-list mailing list, Friday, 11 Feb 2005 23:07:16\n\nFriday, September 9, 2005\n\nDTDs are not an option for browsers. Browsers have to say \"No!\" to DTDs for performance reasons. None of Mozilla, Opera and Safari actually load any of the XHTML 1.x DTDs. The reason why the XML spec made external DTD loading optional was that the writers of the spec considered external entities incompatible with browsers.\n\n--Henri Sivonen on the xml-dev mailing list, Saturday, 3 Sep 2005 01:46:51\n\nThursday, September 8, 2005\n\nA few years ago, I did a major restructuring of the Open Source FlightGear simulator, moving as much of the configuration as possible out of the C++ and into XML (the physics models for aircraft were already using a pseudo-XML, but everything else was hard-coded). A short while after that, the contributor base grew enormously, drawing in people with little or no programming experience but other useful skills, such as 3D modelling, aerodynamics, etc.\n\nSo, in this case, the majority of *coders* know C++ better than XML, but the majority of *contributors* know only XML. They don't know it all that well, but XML doesn't have to be hard -- just make sure the tags match, escape the special characters, quote the attributes, and remember that names are case sensitive.\n\n--David Megginson on the xml-dev mailing list, Friday, 28 Jan 2005 08:53:51\n\nWednesday, September 7, 2005\n\nelementFormDefault is quite obviously the worst design decision ever made in a W3C specification\n\n--Robin Berjon on the xml-dev mailing list, Sunday, 30 Jun 2005 19:04:05\n\nTuesday, September 6, 2005\n\nPersonally I'm no great fan of data binding tools: it's much better to write your application in a language that has XML Schema as its native type system (i.e., XSLT or XQuery) than to mess around converting your data between two very different representations.\n\n--Michael Kay on the xml-dev mailing list, Tuesday, 30 Aug 2005 11:55:40\n\nMonday, September 5, 2005\n\nMany W3C Working Groups try very hard to minimize any work in response to reviews. Good strategies here are\n\nignoring comments alltogether\n\nwaiting several months or years to get back to reviewers\n\nadressing feedback by explaining things in mails but not updating deliverables as if only the very reviewer is too dumb to understand it in the first place\n\nauthoring documents such that they are inaccessible to large parts of the intended audience\n\nmaintaining comments-only mailing lists\n\nmoving important illustrations and clarifications to out of band deliverables such as primers and FAQs\n\nmaking it impossible for reviewers to schedule reviews by maintaining impossible schedules about the working group's expected progress\n\nusing misleading maturity levels (Fourth Last Call, this time we /really/ mean it!)\n\ninfrequent publication\n\nSome strategies are less common, for example, applying twisted logic to the design process like developing a solution first and then trying to find use cases that fit to the solution, or simply plain ignorance, \"Its our language and we can do that.\"\n\n--Bjoern Hoehrmann on the xml-dev mailing list, Friday, 19 Aug 2005 01:16:05\n\nSunday, September 4, 2005\n\nDesktop software that supports OpenDocument and PDF in the future is acceptable; Microsoft's proprietary XML formats are not. during an telephone interview Friday.\n\n--Eric Kriss, Secretary of Administration & Finance for the Commonwealth of Massachusetts,\n\nRead the rest in InformationWeek > XML > Microsoft Blasts Massachusetts' New XML Policy > September 2, 2005\n\nFriday, September 2, 2005\n\nThe modern day H1b program in the United States of America is no better than slave labor. It is bad for the Alien, and it is bad for the US Denizen. It is however good for a lot of enterprising \"body shops\" - which I prefer to call as \"pimps\" who are able to make a quick buck by bringing fresh meat to the united states to sell in the software industry.\n\n--Sahil Malik\n\nRead the rest in Sahil Malik [MVP C#] : 21st Century Slave Labor\n\nThursday, September 1, 2005\n\nIf you want to do something that's going to change the world, build software that people want to use instead of software that managers want to buy.\n\nWhen words like \"groupware\" and \"enterprise\" start getting tossed around, you're doing the latter. You start adding features to satisfy line-items on some checklist that was constructed by interminable committee meetings among bureaucrats, and you're coding toward an externally-dictated product specification that maybe some company will want to buy a hundred \"seats\" of, but that nobody will ever love. With that kind of motivation, nobody will ever find it sexy. It won't make anyone happy.\n\n--Jamie Zawinkski\n\nRead the rest in Groupware Bad\n\nTuesday, August 30, 2005\n\nLet’s say you buy a new computer and use it for three years. That’s about 1,000 days. Your first-run experience – the experience you encounter the first time you boot the machine after taking it out of the box – therefore constitutes about one-thousandth of your entire experience with the machine. I think that’s the sort of logic that has driven most companies not to put that much effort into designing the first-run UI – it’s only going to happen once, and if it isn’t smooth, so what?\n\nWhereas I think Jobs looks at the first-run experience and thinks, it may only be one-thousandth of a user’s overall experience with the machine, but it’s the most important one-thousandth, because it’s the first one-thousandth, and it sets their expectations and initial impression.\n\nThe first-run experience with a new Mac – or a new installation of Mac OS X – is much better now than it ever was before. The music is nice, the animation looks cool, and the new Migration Assistant makes it easy for anyone to move their important files from an old machine to a new one.\n\n--John Gruber\n\nRead the rest in GUIdebook > Articles > Interview with John Gruber\n\nMonday, August 29, 2005\n\nit takes six years for software to become mature; so, for libraries and applications, XML is mature, XML Schemas still has 2 years to go, and XQuery has maybe 7 years to go. Caveat emptor: when you use semi-mature technologies you will have problems that you wouldn't (or shouldn't) have with mature technologies.\n\n--Rick Jelliffe on the xml-dev mailing list, Friday, 29 Apr 2005 17:06:32\n\nSunday, August 28, 2005\n\nCDATA is just an authoring convenience, within the marked region < acts like &lt; and & acts like &amp; so if you have a large chunk of XML that you want to quote as data rather than as part of the XML tree, you can do\n\n<x><![[CDATA[<p>zzz <span>dddd ...</span></p>]]></x>\n\nbut XSLT will see the same input as if you had gone\n\n<x><p>zzz <span>dddd &nbsp; ...</span></p></x>\n\n--David Carlisle on the xsl-list mailing list, Friday, 1 Apr 2005 10:18:17\n\nSaturday, August 27, 2005\n\nPart of the value of XML is its nearly universal interoperability. XML data can be repurposed over and over again, sometimes for uses not originally anticipated. You can take most any XML and read it into Excel, import it into a variety of databases, transform it with widely available XSL tools, etc. While in principle one could re-release all the software that's already out there to include new drivers for binary XML, in practice there will for years be software that only understands the text form. Even if binary is successful, we will bear for the indefinite future the cost of conversion between the two, e.g. when editing in Emacs is desired.\n\n--Noah Mendelsohn on the www-tag mailing list, Sunday, 7 Apr 2005 12:59:58\n\nFriday, August 26, 2005\n\nWXS hasn't got a type *system*. It contains a collection of \"simple\" types, which includes, for example, nine different unrelated types associated with dates, times, and durations, three numeric types (four if you include boolean) which are unrelated, two encoded byte-stream types which are not related to one another, the obscure NOTATION type, string, two types (qname and anyuri) which arguably ought to be subtypes of string (unless they're separately typed because they act as pointers within XML, in which case one could wish for some relation between them, somehow), no expanded name type, no types associated with currency, no consistency, rhyme, reason, or evident method.\n\nDespite this inconsistency and incompleteness, the collection is closed to extension on the ur-type (where one might reasonably wish to create a type for, say, geographic position, or a rich enough \"number\" type that it could include rational numbers (fractions) or imaginary and irrational numbers). Realms that need *those* as basic types are effectively excluded, unless they can plead, cajole, beg or threaten the committee into including *their* pet types.\n\nAs a counter-example, and much as I hate Lisp in general, the Lisp type system encompasses, or can encompass, all of the WXS types, systematically. And more. And is extensible. Partly *because* it's a system.\n\n--Amelia A Lewis on the xml-dev mailing list, Saturday, 1 Jan 2005 23:28:38\n\nThursday, August 25, 2005\n\nBlogs scare the MBAs witless. Blogs out branding, they out attempts to use language to hide the truth, they out the con artists and the fakirs who can push a stock price into a bubble by gutting the employees's benefits without doing anything to get product out the door. The blogs scare the ownership society. They force them to confront the customers face on without the protection of their spin meisters and closed door meet-and-greets. They can co-opt blogs, but they can't change the essential openness. Vampires don't like bay windows on their crypts.\n\n--Claude L (Len) Bullard, on the xml-dev mailing list, Friday, 12 Aug 2005 10:42:44\n\nWednesday, August 24, 2005\n\nWith hindsight we can say XML could have been simpler, and so could XML Schema. This isn't unique to the W3C: FTP could be simpler, and TCP (as in TCP/IP) had some features that turned out not to scale well in all cases, such as \"Slow Start\". The local building codes here in Ontario aren't the simplest either. But I'd rather have all of these things than not have them at all, and in practice that's usually the choice.\n\n--Liam Quin on the xml-dev mailing list, Friday, 19 Aug 2005 18:11:03\n\nTuesday, August 23, 2005\n\nThe whole Web Services space is one that is based more on hope than any real evidence of success. There are dozens of companies that have invested heavily in this space and thousands of individuals who have tied their hopes to the idea of Web Services. The mere fact that nothing useful seems to be coming out of the expenditure of all this energy doesn't seem to have dimmed the desire to make it happen.\n\n--Bob Wyman on the xml-dev mailing list, Friday, 2 Apr 2004\n\nMonday, August 22, 2005\n\nImplicitness fails us I think. There's a correlation between bozo technology and implicit or axiomatic approaches to specification. Personally speaking, I think I have wasted a lot of time since coming into this industry in having to decipher the consequences of specs or interpret things through to some form of conclusion that may or may not be what was expected. And then to go back to the source or the community only to be told that's not the preferred interpretation. And then watch the bugs and interop reports come in. At that point the typical response is that it that it's too late to fix now. When I think of a spec written in implicit form I tend to think of XML Namespaces.\n\n--Bill de hÓra on the xml-dev mailing list, Saturday, 20 Aug 2005 18:17:44\n\nSunday, August 21, 2005\n\nEffectively, the Web services architecture is a separate architecture from the Web architecture. There are almost no services that are generally considered Web services (that is use SOAP and/or WSDL) that are on the Web. Further, Web services clients rarely see Web applications. The attempt to unify the architectures in SOAP 1.2 with the SOAP-response MEP simply have not been deployed. This could be because the community has not moved to SOAP 1.2, perhaps partially because of the lack of WSDL 2.0 deployment. However, I think it is much more that Web services authors perceive little benefit in offering their services on the Web. For example, the WS-ResourceFramework[1] and WS-Transfer[2] provide generic operations in SOAP for achieving state transfer, despite proposal such as WS-REST [3] that could have integrated the resource framework with the Web.\n\n--David Orchard on the www-tag mailing list, Monday, 13 Jun 2005 11:32:51\n\nFriday, August 19, 2005\n\nIt's not surprising that XForms is raising hackles and ruffling feathers since it has the potential to displace a language that a lot of people have spent a lot of time learning and building businesses around. But just as I gave up assembler programming to use C, so I now gladly drop spaghetti-script for XForms. It's not perfect and there are plenty of new features that I would like to see introduced, but its most important role has been to demonstrate that we don't need to be wedded to the old approaches for ever, and that new solutions are possible, and here today.\n\n--Mark Birbeck\n\nRead the rest in Internet Applications: The \"XForms Myth\" Myth\n\nThursday, August 18, 2005\n\nThe problem is that a lot of customers don't really know what they want when they first start out using a new technology. They haven’t had enough experience with the technology and their use-cases to know all the ins and outs. XML has demonstrated itself to be unfortunately bad in this. Customers think they only need to worry about X (some simplified subset of XML) until 2 weeks before ship, when someone points out that they have been completely ignoring xml-namespaces / whitespace / mixed-content / processing-instructions / etc... I have seen this time and time again.\n\n--Derek Denny-Brown\n\nRead the rest in only this, and nothing more: Serving Many Masters\n\nWednesday, August 17, 2005\n\nI wrote an XML Schema for SVG Full 1.1, and another for SVG Tiny 1.1. Doing so taught me a number of things:\n\n85% of XML Schema is thoroughly useless and without value;\n\nthe few useful features are weak and without honour;\n\ncreating a modularized XML Schema is easier than with DTDs, but nowhere near as simple as with RNG;\n\nwhile a zillion useless features have been included in the spec, anything useful such as making attributes part of the content model has obviously been weeded out with great care, basically leaving one with DTDs supporting namespaces, a few cardinality bits, no entities, and loads of cruft;\n\ntools like XML Spy that are supposed to help one write schemata will produce very obviously wrong instances, meanwhile the syntax of XML Schema was obviously produced by someone who grew up at the bottom of a deep well in the middle of a dark, wasteful moor where he was tortured daily by abusive giant squirrels and wishes to share his pain with the world;\n\nthe resulting schema is mostly useless anyway as there is no tool available that will process it correctly.\n\n--Robin Berjon on the xml-dev mailing list, Sunday, 09 Jun 2005 11:59:45\n\nTuesday, August 16, 2005\n\nDifferent applications have different processing requirements, so having the syntax specification define any one model would be wrong. For example, editing applications could well require a different API/InfoSet than XML databases. IMHO. This was one of the most significant issues in the early DOM specification process: the tension between different application domain requirements.\n\n--Gavin Thomas Nicol on the xml-dev mailing list, Friday, 6 May 2005 14:54:36\n\nMonday, August 15, 2005\n\nstandardisation of 'object mapping', even within a set of today's best of breed technologies such as Java/C#/Python is a little dangerous given XML is about exchanging documents, or at least interoperating with those who want to work with XML directly. What goes on behind the XML curtain is very much a per-implementation concern.\n\n--Paul Downey on the xml-dev mailing list, Wednesday, 13 Jul 2005 16:48:14\n\nSunday, August 14, 2005\n\nXML is not the best format but it's at least one everyone seemed to agree on.\n\n--Didier PH Martin on the xml-dev mailing list, Wednesday, 01 Jun 2005 21:08:41\n\nFriday, August 12, 2005\n\nUnless there were compelling arguments in favor of using XSD, I'd use RNG. Even if there were compelling arguments, I might very well use RNG and create the XSD with trang.\n\n--Norman Walsh on the docbook mailing list, Wednesday, 02 Feb 2005 18:43:16\n\nThursday, August 11, 2005\n\nif the domain model behind the service changes (due to integration or evolution) and this (naturally) propagates to the service API conversation with former clients becomes impossible because they do not know the new semantics (and API). With REST you at least *can* have a conversation and the server can for example provide the client with the neccessary information to adapt (e.g. transformation code can be send accross to transform the new schema into the old one).\n\nIMHO, this capability of REST to handle domain model evolution without having to re-program existing clients makes it a very interesting choice not only for internet-scale/outside-firewall situations but also for common enterprise IT (behind the firewall) systems.\n\n--Jan Algermissen on the xml-dev mailing list, Tuesday, 05 Apr 2005 19:07:54\n\nWednesday, August 10, 2005\n\nWe wound up purchasing the codezoo.com domain name from someone who owned it already, and in the process, I had a brutal introduction to the world of domain name registrar transfers. In the name of openness and deregulation, ICANN allows many companies to register a domain name, and then manage the registration of that domain, for you or your company. The past owner of codezoo.com had registered it with a registrar named Omnis, and O'Reilly manages its domains with another firm. We had to transfer the name from Omnis to our registrar in order to use it. Of course, that doesn't benefit Omnis at all, so they did everything possible to prevent the transfer from occurring: holding requests for as long as they could, directly lying to us on the phone about it, day after day, and eventually refusing to take our calls at all unless we wanted to sign up for services from them. Towards the end of this process, when we had launched the site as codezoo.net since Omnis had prevented us from launching with the name we'd bought, I realized: \"Wait a minute! I'm dealing with the phone company, that's what this is!\" Ma Bell is gone, and in her place, the new locus of bureaucratic metastasization is domain registries. I guess the Internet really has grown up! Don't do business with Omnis, and if you plan to transfer a domain name, allow more than a month for the whole thing to happen.\n\n--Marc Hedlund\n\nRead the rest in Some notes on the building of CodeZoo\n\nTuesday, August 9, 2005\n\nWhat amazes me when I study the work of the truly great programmers such as Dimitre and his FXSL Functional XSLT library is that there tends to be one very common trait among them -- simplicity.\n\n--M. David Peterson on the xsl-list mailing list, Sunday, 17 Apr 2005 15:52:04\n\nMonday, August 8, 2005\n\nOne of the oddities of the open-source world is that I don't know very much about what my users are doing. I know that there are around 250 downloads of Saxon a day, of which around half are the 7.x version, but I have very little idea who is downloading it and what they are doing with it (if anything). Most of the feedback I get is either from a small group of experts who know the technology inside out and are stretching its boundaries, or from beginners who don't know where to start. There's a silent majority in between that I never hear from.\n\n--Michael Kay\n\nRead the rest in An Interview with Michael Kay\n\nThursday, August 4, 2005\n\nAnyone who can’t make a syndication feed that’s well-formed XML is an incompetent fool.\n\n--Tim Bray\n\nRead the rest in ongoing - On Postel, Again\n\nWednesday, August 3, 2005\n\nI am a great believer in the bogosity of authorial intent.\n\n--Walter Perry, Extreme Markup Languages 2005, August 3, 2005\n\nTuesday, August 2, 2005\n\nMixed content, entites, PIs, etc. are undeniably central to document apps, but an annoyance to data-centric apps. Types, the PSVI, and nil are central to data apps but an annoyance to pure document folks. But that doesn't mean we can all just go our separate ways: there is a huge middle ground (e.g. most InfoPath documents) that have features from both worlds, because real business documents have both semi-structured text and strongly typed data in them.\n\n--Michael Champion on the xml-dev mailing list, Sunday, 14 Jul 2005 13:07:02\n\nMonday, August 1, 2005\n\nwe're only just scratching the surface with what can be done with xml. Getting the ideas and selling them is the easy part. Writing them is a little harder. Debugging them harder again. Production testing is time-consuming.\n\n--David Lyon on the xml-dev mailing list, Tuesday, 3 May 2005 09:18:18\n\nSunday, July 31, 2005\n\nIn the web platform team that I lead, our top priority is (and will likely always be) security – not just mechanical “fix buffer overruns” type stuff, but innovative stuff like the anti-phishing work and low-rights IE. For IE7 in particular, our next major priority is removing the biggest causes of difficulty for web developers. To that end, we’ve dug through a lot of sites detailing IE bugs that cause pain for web developers, like PositionIsEverything and Quirksmode, and categorized and investigated those issues; we’ve taken feedback from you directly (yes, we do read the responses to our blog posts) on what bugs affect you the most and what features you’d most like to see, and we’ve planned out what we can and can’t do in IE7.\n\nIn IE7, we will fix as many of the worst bugs that web developers hit as we can, and we will add the critical most-requested features from the standards as well.\n\n--Chris Wilson\n\nRead the rest in IEBlog : Standards and CSS in IE\n\nSaturday, July 30, 2005\n\nframes may be useful, if used in the right way, in intranets and certain web applications. For a public website, however, frames have too many accessibility and usability problems. Bookmarking problems, printing difficulties, trouble with deep linking, and having to do search engine workarounds are a few of the drawbacks to using frames.\n\n--Roger Johansson\n\nRead the rest in Web development mistakes, redux | Archive | 456 Berea Street\n\nFriday, July 29, 2005\n\nyou cannot write a normative DTD for a namespaced language. That's an issue that crops up regularly in document-based communities where people have tried to learn from HTML's errors and recommend validation against a DTD. Unfortunately, since it produces false-negatives, you then end up with content producers wondering why their content doesn't validate when according to every knowledge of the spec they can muster it is perfectly fine.\n\n--Robin Berjon on the www-tag mailing list, Monday, 04 Apr 2005 01:13:16\n\nThursday, July 28, 2005\n\nWe use XML for messaging so we can stop talking about what format to use whenever we want to exchange something. That lets us get our work done efficiently, leaving us lots of time to hang out on XML-Dev and discuss what other formats might have been better.\n\n--Jonathan Robie on the xml-dev mailing list, Wednesday, 01 Jun 2005 16:52:33\n\nWednesday, July 27, 2005\n\nXerces has many wonderful characteristics that make it the right choice for many purposes, but it is nowhere near the fastest parser you can write for many important high-performance applications.\n\n--Noah Mendelsohn on the www-tag mailing list, Sunday, 7 Apr 2005 10:31:18\n\nTuesday, July 26, 2005\n\nThe OFWeb (Old-Fashioned Web) primarily uses http: URIs as a means to an end, namely retrieving encoded character streams with some kind of rendering semantics, the encoding and semantics typically being signalled in the http header. The SemWeb primarily uses http: URIs as an end in themselves, as the constituents of RDF triples i.e. as names for things and relationships between them.\n\n--Henry S. Thompson on the www-tag mailing list, Sunday, 28 Apr 2005 16:34:52\n\nMonday, July 25, 2005\n\nThere IS an art to working with XML, just as there is with any other language, a perceptual understanding of XML trees in a set mode, just as the way you approach programming using SQL differs from the way you approach C++. Linux programmers in general usually tend to come up the ranks as either shell coders (pure proceduralists) or C/C++ devs, and getting them to shift their thinking to XML can be as hard as it is on the Windows side (I note that most highly proficient XML developers still come from the web/eCommerce side, where declarative programming structures are far more common).\n\n--Kurt Cagle on the xml-dev mailing list, Friday, 28 Jan 2005 11:17:32\n\nSunday, July 24, 2005\n\nOver the years I have worked with a good number of XML developers, ranging in skill from occasional user to expert. In almost every case I have found a lack of understanding of namespaces -- or, in the presence of understanding, hands-on confusion in working with and debugging namespace-related issues. XML namespaces, as defined by the current specification, are a departure from the perl-hacker-should-be-able-to-create-an-XML-parser-in-two-weeks credo; it takes more than two weeks just to understand the nuances of XML namespaces. The XML namespace FAQ gives the flavor of this confusion\n\n--Parand Darugar\n\nRead the rest in Abolish XML namespaces?\n\nFriday, July 22, 2005\n\nEvil design is where they stop you from doing what you are trying to do, like putting an advert over the top of the page. That's the wrong way to do it. Google has made billions by putting the ads where people do want them, rather than where they don't want them.\n\n--Jakob Nielsen\n\nRead the rest in Guardian Unlimited | Online | Lazy, stupid and evil design\n\nThursday, July 21, 2005\n\nThe early adopters of Firefox includes a lot of people in IT departments. Those are the people you want to have, as they will be the ones who can convince their management to migrate to Firefox. We have high hopes that we'll do better and better in that space with Windows 2000 users. If users don't upgrade to Windows XP they won't get IE 7, but 50 percent of businesses are still using Windows 2000. We're excited about Microsoft launching IE 7 — it will remind a lot of people that if they want better features they have to spend hundreds of dollars upgrading. Even if we stopped supporting Windows 98, a company can support themselves as it is open source. This is one of the advantages of open source — you can avoid the forced update cycle.\n\n--Asa Dotzler\n\nRead the rest in Firefox: Doing it for love\n\nWednesday, July 20, 2005\n\nHmmmm, what Does SOAP/WS Do that A REST System Can't?\n\nLots:\n\n1) Provides big vendors with new vendor lock-in and marketing spin opportunities.\n\n2) Adds additional complexity and obfuscation that are unnecessary in many circumstances.\n\n3) Adds a lot of overhead.\n\n4) Provides a marked reduction in scalability, especially when used by the unwashed masses as an RPC mechanism.\n\n5) Highlights the beauty of KISS-based approaches.\n\n6) Lets you create an acronym that has no formal expansion, which sounds like an oxymoron.\n\n7) Gives the XML-Dev group, and the industry as a whole, lots to discuss/argue about.\n\n8) Opens up the door to silly questions.\n\n--Andrzej Jan Taramina on the xml-dev mailing list, Sunday, 07 Apr 2005 12:30:55\n\nTuesday, July 19, 2005\n\nIf you are going to make the switch to XML, you need to understand that, to be truly effective, you probably will have to read a number of books.\n\nYou are learning a new programming paradigm. You will need to combine technologies, parsers, schemas, namespaces, xpath, xquery, and transformations. In the short time I've been working with XML (3-6 months, off and on) I have had to study all of these things. From my experience,\n\nlearning \"XML\" is really learning a family of technologies. Certainly, this complicates the communication; it is not a trivial, pick-it-up-and-go family of technologies. XML, in my opinion, is a *transport* mechanism that, by itself, is of limited use; what makes XML really powerful is all those other manipulators you can bring to bear on the base document - learning those takes time.\n\n--Robert A. Jacobs on the xom-interest mailing list, Friday, 6 May 2005 11:03:39\n\nMonday, July 18, 2005\n\nAnother great advantage of an XML-based syntax is that it's so easy to automate the manipulation of resources expressed in that syntax. Outside of XSLT, I can't think of another language in which it's so easy to write programs that read and manipulate other programs written in the same language, and that includes LISP and Scheme (\")))))))....\"). Many, many production applications out there have automated the reading and writing of XSLT stylesheets as part of their workflow.\n\n--Bob DuCharme on the xml-dev mailing list, Tuesday, 9 Nov 2004\n\nSaturday, July 16, 2005\n\nAbout 5-6 years ago, there was an interesting comparison of the size of a binary EDI (CEFACT) message versus the same transaction using an XML/EDI document. The size difference was about 1K (EDI) versus 11K (XML/EDI). For a single transaction, the \"XML overhead\" was about 10K. Bandwidth and disk storage were comparatively more expensive than MIPS, so economics favored using cheap CPU cycles to compress documents.\n\nDisk and bandwidth costs are lower today than in 1998. Compression seems less important, but securing documents has become more important because we're using them as the basis for critical services. We may not need to invest in MIPS for compressing documents, but (in some cases) we need to do so to secure them using digital signatures and encryption. Changes in the regulatory environment (Sarbanes-Oxley, HIPAA, Basel II, SEC/NASD) are trending us towards securing more data and document exchanges.\n\n--Ken North on the xml-dev mailing list, Thu, 19 Aug 2004\n\nFriday, July 15, 2005\n\nWhat I really wanted from XQuery was its data manipulation language and I'm very disappointed that it doesn't even appear to be on the horizon. Instead I've had to devise my own XML update language as an extension of XUpdate.\n\n--John Watson on the xml-dev mailing list, Thu, 11 Nov 2004\n\nWednesday, July 13, 2005\n\nAs a consumer and something of a student of history, I always question people that are highly motivated to protect their jobs and money. Did big tobacco say their products were safe long after they knew it wasn't true? Might Microsoft be inclined to say that their products provide better total cost of ownership (TCO) and security than another product despite knowing it wasn't true?\n\n--Chris Spencer\n\nRead the rest in Linux Opinion: An Open Letter to a Digital World (LinuxWorld)\n\nTuesday, July 12, 2005\n\nIn most XML applications that actually do anything significant with the parse events, parsing overhead is a tiny fraction of total processing time, say 1% of the total. In other words, making the XML parser twice as fast might reduce processing time by 1/200.\n\n--David Megginson on the xml-dev mailing list, Saturday, 1 Jan 2005\n\nSaturday, July 9, 2005\n\nXLinks go beyond identification or the resource and enable typed links. Given that a link for the web is the URI, the rest of that information is annotative, that is, a link processor of some kind might use it. For example, you often want the links in the bibliography, the links in the table of contents, and the links in the index at the back of a book to be displayed differently, and certainly, to behave differently. A click on a table of contents should take you to the location of that resource. A click on the inverted index might display all, none or one of the resources.\n\nSo linking as a concept seems simple, but it has overtones of identity, display and control over navigation. Not everyone agrees or can agree on how that fits into a single set of concepts or implementations. Also, most anything one can do with a multiway link, one can do with multiple simple links given some control to display it in. The classic example is a popup menu.\n\n-- Claude L (Len) Bullard on the xml-dev mailing list, Friday, 14 Jan 2005 12:12:34\n\nFriday, July 8, 2005\n\nMy definition of a mainstream language is that Computer Weekly has job ads asking for people with three years' experience. XSLT passes that test with no trouble.\n\n--Michael Kay on the xml-dev mailing list, Sunday, 2 Dec 2004\n\nThursday, July 7, 2005\n\nIt strikes me that most of my complaint vis-a-vis XML are not so much with *XML* as the things built around or on top of it.\n\n--Gavin Thomas Nicol on the xml-dev mailing list, Tuesday, 26 Oct 2004\n\nWednesday, July 6, 2005\n\nwe ought to have learned by now that standards are terrific when applied to proven industry practice but high-risk in the domain of theory and science. SQL and XML were both exercises in writing down something that had already been proven to work.\n\nWhen committees get together either in an informal cabal or an official standards process, and go about inventing new technologies, the results are usually pretty bad. ODA (Never heard of it? Exactly); OSI Networking; W3C XML Schemas. The list goes on and on.\n\n--Tim Bray\n\nRead the rest in ongoing · Web Services Theory and Practice\n\nTuesday, July 5, 2005\n\nNo one wants a repeat of the mess that is W3C XML Schema.\n\n--Dare Obasanjo on the xml-dev mailing list, Thu, 21 Oct 2004\n\nSunday, July 3, 2005\n\nYour data is going to change, that's why you're using XML to begin with. Do NOT lead your clients down a path that leads them to overly coupling their systems to your changing data. I.e. do not encourage them to use some kind of XML data binding tool that's strongly coupled to a particular schema. This also means you need to be very selective in what you consider required data. Adding additional required data to an existing operation is going to cause breakage. If data is not absolutely 100% for certain required then just make it optional. Adding additional optional data should never break the system. Another reason XML Schema based systems are so horribly brittle is they tend to break on any schema change, not just required data changes.\n\n--Kimbro Staken\n\nRead the rest in Inspirational Technology: 10 things to change in your thinking when building REST XML Protocols\n\nSaturday, July 2, 2005\n\nproperly used a PI does not change the data semantics, but as with everything else, XML Doesn't Care. and PIs give people rope with when they have cheerfully hanged themselves. Of course, they also provide plenty of rope to tie up loose ends, e.g. letting an instance offer hints about what stylesheet to use to display it.\n\n--Michael Champion on the xml-dev mailing list, Sunday, 6 Feb 2005 23:09:58\n\nFriday, July 1, 2005\n\nWhat a lot of people have not figured out yet--though the W3C and IETF are moving in this direction--is that open standards depend on open source reference implementations. It is extremely difficult to specify everything that needs to be specified in a technical standard; in some notorious cases, such as the way Microsoft perverted Kerberos, vendors have used that underspecification as a loophole to create implementations that locked in their customers. Open source reference implementations are the only known way to prevent such abuse.\n\n--Eric S. Raymond\n\nRead the rest in ONLamp.com: ESR: \"We Don't Need the GPL Anymore\"\n\nThursday, June 30, 2005\n\nWhenever a vendor tries to do a new OSS license (c.f IBM, Apple, Sun, Mozilla), their corporate lawyers keep trying to retain control of things (as good lawyers should do), and the effect is to introduce a new license with controversy and integration uncertainty. Picking something well known means you say \"LGPL\" when you are asked what license you use, and nobody needs to discuss details.\n\n--Steve Loughran on the xom-interest mailing list, Wednesday, 9 Feb 2005 12:30:47\n\nWednesday, June 29, 2005\n\nPutting XML inside other XML is still too hard. Part of that happens because envelope metaphors are taken too literally. Part of it happens because XML grammar efforts, especially container formats, are usually chartered to think up a storm about what might go on inside their format, but are remarkably thought-free about what might go on when their format is inside someone else's. This is why the default namespace is broken as designed - it enables container markup to happily trash what's being contained. I'm amazed it hasn't been deprecated.\n\n--Bill de hóra on the xml-dev mailing list, Tuesday, 08 Feb 2005 01:06:22\n\nTuesday, June 28, 2005\n\nI won't argue that XSLT 2.0 doesn't bring useful features and I must admit I am using it punctually, but I consider that the flaws of the PSVI based architecture promoted by XSLT 2.0 generally outweigh the benefit of these features and that's what prevent me from using it on a large scale.\n\n--Eric van der Vlist on the xsl-list mailing list, Tuesday, 19 Apr 2005 12:48:41\n\nMonday, June 27, 2005\n\nI remember the networking people complaining just as much a decade and a half ago about the inefficiencies of IP, TCP, etc. I had to fight hard as late as 1993-94 to get a university lab to use TCP/IP because the administration had been convinced by their vendor (Novell) that TCP/IP was too slow for serious work.\n\nI think that we might be at the same place right now with XML. The self-annointed analysts are telling the networking people that they'll have to handle enormous volumes of XML network traffic in a few years, and the networking people are freaking out over hypothetical future problems like verbosity and parsing time. I don't think that we have much of a clue yet whether (a) there actually will be much XML network traffic over the next few years, or (b) what it might look like, so any optimization is waaaay premature.\n\n--David Megginson on the XML Developers mailing list, Monday, 19 Apr 2004\n\nSunday, June 26, 2005\n\nThis isn't really a question of data loss, data protection or data safeguarding. That, my friends, is a red herring. The real question is why corporations need to store all of this personal data in the first place. Why does my credit card company need to store my social security number? Why does Amazon need to store my credit card number? Why shouldn't every company store only what I tell them they can store? And why shouldn't the data that they store be as little as they possibly need to conduct business?\n\n--Eric Norlan\n\nRead the rest in The red herring of data protection | Perspectives | CNET News.com\n\nSaturday, June 25, 2005\n\nWhile it is possible to serialize a Xerces2 DOM using Java object serialization, we recommend that a DOM be serialized as XML wherever possible instead of using object serialization. The Xerces2 DOM implementation does not guarantee DOM Java object serialization interoperability between different versions of Xerces. In addition, some rough measurements have shown that XML serialization performs better than Java object serialization, and that XML instance documents require less storage space than object-serialized DOMs.\n\n--Elena Litani and Michael Glavassevich, IBM\n\nRead the rest in Improve performance in your XML applications, Part 2\n\nFriday, June 24, 2005\n\nSurprisingly, after all these years, users of standard-compliant browsers are still faced with sites that do not support their browser or with a link suggesting they download Internet Explorer.\n\n--Deri Jones, SciVisum\n\nRead the rest in BBC NEWS | Technology | Websites alienate Firefox users\n\nThursday, June 23, 2005\n\nThe Soviet Union collapsed not because of communism or central planning, but because of corrupt accounting. They couldn't organize the means of production because everybody was lying about everything. It was a game of fake numbers, and when you do that, you get crap for answers.\n\n--Bill Joy\n\nRead the rest in Fortune.com - Technology - Joy After Sun\n\nMonday, June 20, 2005\n\nOne interesting things about client/server XML is that the computing load is asymmetric. It's a heck of a lot easier to generate XML than it is to consume and process it.\n\n--Rich Salz on the xml-dev mailing list, Friday, 12 Nov 2004\n\nSunday, June 19, 2005\n\nMany developers and publishing systems use query strings to send variables from one document to another within a website, and the ampersand is the most widely used query string separator. The problem is that the ampersand is also used in HTML and XHTML to start a character entity. If a query string contains an unencoded ampersand which is followed by a sequence of characters that resemble a character entity, browsers will convert the ampersand and those characters to the character represented by the entity. That will obviously break the query string, and can make things stop working. Having been bitten by this bug myself, I make sure any ampersands under my control are properly encoded.\n\n--Roger Johansson\n\nRead the rest in Web Standards Group - Ten Questions for Roger Johansson\n\nSaturday, June 18, 2005\n\na CDATA section is just a syntactic convenience, <x><![CDATA[aaa]]></x> means exactly the same as <x>aaa</x> so schema languages do not allow any significance to be placed on CDATA sections, just as they do not allow significance to be placed on whether \" or ' is used to surround an attribute value: <a b=\"2\"/> means the same as <a b='2'/>.\n\n--David Carlisle on the xml-dev mailing list, Friday, 22 Apr 2005 11:59:07\n\nFriday, June 17, 2005\n\nI personally like XForms. However, I think it is a lost cause on the popular Web for three reasons:\n\nIt's not backwards compatible with existing Web content and existing Web browsers,\n\nIt uses too many levels of abstraction to be understood by most authors.\n\nIt requires the use of many namespace prefixes.\n\nNone of these problems really have anything to do with XForms features per se. It is IMHO possible to bring many of XForms features (especially the declarative constraints ideas) to authors without introducing the other problems. For instance, by just extending HTML's form features instead of making a totally new language.\n\n--Ian Hickson\n\nRead the rest in Internet Applications: The \"XForms Myth\" Myth\n\nThursday, June 16, 2005\n\nFor the data you receive just take what you need and pass through everything else. If there's extra data in the stream it is NOT an error as long as all the other data you need is there as well. Just ignore the extra data. This is one reason why XML Schema based systems are so fragile, they take too literal of a view about the shape of the data.\n\n--Kimbro Staken\n\nRead the rest in Inspirational Technology: 10 things to change in your thinking when building REST XML Protocols\n\nWednesday, June 15, 2005\n\nThe hacks that authors use are not to get around browser bugs -- if only that were the case. The reason authors are forced to carry out hacks is because they are having to do things that HTML and JavaScript were never designed to do. Every day we see sites having to validate data and create clever pop-up solutions for help text and calendar widgets. But whilst the HTML programmer is messing around with mouseover events and scripting, the XForms author writes the following (declarative) mark-up:\n\n<input ref=\"sn\">\n\n<label>Surname:</label>\n\n<hint>Please enter your surname</hint>\n\n</input>\n\nNote that whilst the JavaScript hack has to rely on platform-specific featu"
    }
}
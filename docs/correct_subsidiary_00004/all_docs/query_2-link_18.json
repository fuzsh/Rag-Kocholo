{
    "id": "correct_subsidiary_00004_2",
    "rank": 18,
    "data": {
        "url": "http://www.mcmullon.com/reports/web/netspeak.htm",
        "read_more_link": "",
        "language": "en",
        "title": "",
        "top_image": "",
        "meta_img": "",
        "images": [
            "http://www.mcmullon.com/myicons/mapworld.gif",
            "http://www.mcmullon.com/homepage.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "NetSpeak\n\nArticles and definitions relating to the Internet.\n\nArticles\n\nThe World Wide Web\n\nWorld Wide Web The World-Wide Web (WWW) project began in 1992 at the Geneva-based European Centre for Nuclear Research (CERN) and its commercial possibilities were quickly recognised.\n\nWWW, or Web, can be thought of as a collection of documents resident on thousands of servers around the world. Each document, written in the HyperText Mark-up Language (HTML) can obtain text, images, sounds and video. More importantly, it can contain links to documents held on other machines accessible through the Internet, creating what is usually called \"hypertext\".\n\nThese links appear as hotspots in the document, and can be a word, phrase or even an image. Selecting the hotspot automatically connects your computer with the one referenced in the underlying hypertext document, and loads a new hypertext document held there on your machine. As the user, you need know nothing about where the machines are, or how the connection is made.\n\nSince these hypertext links often take you to other documents rich in hotspots, the ensemble can be thought of as a huge hypertext system spanning the world, a kind of global version of a Microsoft Windows Help file.\n\nTo access WWW you need a browser: these programs let you read hypertext documents, view any built-in images and activate hotspots.\n\nTwo of the most important browsers are Netscape and Microsoft Internet Explorer.\n\nBrowsers can also be used to link to file transfer protocol (FTP) servers (where files are stored), gophers (where information can found using a menu structure) and wide area information servers (WAISs, which allow free-text searches).\n\nBusinesses can benefit from the World Wide Web by providing: -\n\nE-mail\n\nThis avoids the game of telephone-tag that is so common in business today. Yet, it is surprising how few computer users use E-mail when taking messages for colleagues and still resort to post-it notes stuck around the edge of person's monitor.\n\nAdvanced E-mail features include attachments; which enables spreadsheets, documents, images and presentations to be distributed with your message.\n\nFTP\n\nFor larger transmissions the File Transfer Protocol facility allows huge amounts of data to be sent anywhere in the world very quickly and very cheaply. This could be used, for example, to transmit complicated documents such as colour brochures for cheaper printing in the Far East.\n\nFTP can also be used to obtain free updates from leading software companies.\n\nTelnet\n\nThis allows technicians control computers from anywhere in the world, allowing a single, centralised support department to correct software faults throughout an organisation.\n\nWeb Pages\n\nThe Internet can be used to promote a company's products and services.\n\nHow it all works and why nobody runs it\n\nThe Internet is not online service, but a collaborative collection of networks that adhere to certain basic standards when exchanging information among themselves.\n\nEach Internet service provider pays for the cost of the connections it runs, and makes profit from the charges to its subscribers for their use of them. Economics of scale and continuing advances in technology make these cheap, even for intercontinental traffic.\n\nExtensive connectivity among major Internet providers means that information is sent around the Internet efficiently, traversing only a few separate networks.\n\nUnsurprisingly no one body controls it. Although various standards need to be adhered to by Internet service providers, there are no Internet police who check and enforce them. The system is self-policing; if any organisation strays from collective standards, it loses the benefits of universal connectivity - which is the whole point of becoming part of the Internet in the first place.\n\nThere are bodies that carry out central functions for the Internet such as the InterNIC ( http://www.internic.net ) which, among other things, registers companies that are connected to the Internet, and the Internet Society (gopher,isoc.org). The society has various engineering committees that help make technical recommendations for the future development of the Internet, but none of these has the power to force a particular direction or action on the Internet community.\n\nMuch of the information available over the Internet is held on university computers and is managed by public-spirited individuals. Similarly, many of the Internet indexing and cataloguing services available - Archie, Gophers, World Wide Web servers - have been set up and run by university departments. Manufacturers, who effectively sponsor an Internet site, often provide the computers themselves and their huge storage requirements free. For example the important UK FTP site at Imperial College (src.doc.ic.ac.uk) is sponsored by Sun Microsystems, which receives due acknowledgement each time you log on.\n\nInternet Providers\n\nThere are hundreds of Internet providers and it is difficult choosing between them. Most however rely upon the services of a selected few. These, including Demon Internet Services, EUnet, GB, Pipex (owned by UUnet, which itself is partly owned by Microsoft) and BT. These companies, together with Ukerna (the UK Education and Research Networking Association - the body that runs the UK academic network Janet) formed Linx; the London Internet Exchange. This is a neutral point of interconnect where they can exchange data among themselves directly.\n\nHops\n\nThe structure of the Internet consists of many joined links. Cyber road maps show the information as a hierarchical tree, with local Internet service providers connecting to national connectivity providers. For users a more natural way of conceiving the Internet is as a series of hops between your computer and the site you are trying to reach. Different parts of the same message - that is different packets - may take different paths that vary from moment to moment according to the status of the intervening network.\n\nWhatever path is taken, it consists of sections between computers that decide how to pass on the packets (the routers). These sections are commonly called hops. How many hops there are between computer and destination depends upon the end-points, the time of day and other factors. It can be as low as three of four, or may extend into the 20s or even 30s.\n\nThe number of hops required to reach the destination matters: the more there are, the longer the transit time, and the more chance that the packet will get lost. Indeed, if the number of hops is too large, the system may simply give up, since for reasons of efficiency packets are generally discarded automatically once they have passed through a predetermined number of hops.\n\nHow best to get your company name on the Net\n\nAn early problem of registering Internet names for companies was that sharp individuals in the US were registering the names of major corporations. These companies then found themselves unable to use their main brand online. There is now a formal requirement in the US that the requested name to be used on the Internet does not infringe the intellectual property of any third party and will not be used for any unlawful purpose (full details at ftp://rs.internic.net/policy/internic/internic-domain-4.txt).\n\nIn the UK, things have always been simpler. A considerable amount of material has appeared online that will prove useful for companies considering making an application for their first or subsequent Internet names.\n\nThe main .UK country domain is divided up into various sub-domains: .ac.uk (the academic world); .gov.uk (for government bodies); .mod.uk (Ministry of Defence): .net.uk (network suppliers); .org.uk (general organisations that do not fall into any other category); .nhs.uk (for NHS organisations); .sch.uk (for schools); and .co.uk as the main commercial domain. See the URL http://www.nic.uk/for details.\n\nThere are formal sub-domains within these (called neutral sub-domains), such as .music.co.uk, .law.co.uk, .tv.co.uk, .radio.co.uk and .internet. co.uk (for Internet suppliers), see http://www.britain.eu.net/naming-co/other-domains.html.\n\nEUNET GB administers the .co.uk sub-domain. However, the final decision as to whether a name will be accepted or not is down to an informal committee, called the naming-co list. It has voting and non-voting members. The former are essentially the UK Internet suppliers who have their own international connections. See http://www.britain.eu.net/naming-co/members.html.\n\nThe main page with information on how to register company names in this sub-domain is at http://www.britain.eu.net/naming-co/. Perhaps the most important information here is the hints and tips on what names will be accepted by the naming committee (at http://www.britain.eu.net/naming -co/tips.html).\n\nRemember that the name should reflect the company applying for it, and that two- and three-letter names will not in general be accepted (unless the company is very well known, like BT or HP).\n\nNames that are already registered will obviously not be given out again but neither will new names be given to a company that does not intend to stop using its old name. Requests for a large number of similar names for related organisations are frowned on and should be created at the next level down. One easy way of finding out if a name has already been registered is to use the search engine at http://www.britain.eu.net/naming-co/whois-form.html which will look through all the registered names in the .co.uk sub-domain. Also useful is the list at http://www.hensa.ac.uk/uksites/co/index.html, which lets you look through a consolidated list of company names in this sub-domain.\n\nFinally, to keep up-to-date with who has applied recently, see http://http.demon.net/external/networks/ncprov.html, which shows the various company names that have been applied for by each of the Internet suppliers.\n\nThe normal procedure is to go via your Internet provider in order to register your name, although you can also apply to EUnet GB directly using the form at http://www.britain.eu.net/naming-co/user-form.html.\n\nIf all this sounds too complicated to bother with, you might want to visit the URLs at http://www.britain.eu.net/naming-co/count-couk/count.html and http://www.britain.eu.net/naming-co/count-couk/history.txt. These show how the growth of the .co.uk sub-domain is more or less exponential. In other words, if you don't register soon, you may well find yourself left behind in the great business stampede to the Internet.\n\nCreating A Web Site\n\nQuite how you establish a World Wide Web site is perhaps the least important aspect of the whole process. A company might choose to develop the skills of people in-house (for example in the marketing and IT departments), exploiting the fact that setting up HTML pages is extremely simple (though hard to do well). The pages will then be placed on a server run as part of the company's network and connected to the Internet through a firewall, or isolated from it completely for total security and corporate peace of mind.\n\nAlternatively, the work can be farmed out to specialists who will write pages (rather as advertising agencies produce copy for marketing purposes) and then arrange for them to be held on a Web server where space can be rented. This has the advantage that there are no security worries, and design is left to experts. The downside is that you will obviously pay a premium for these services and there is no opportunity to learn from the process of creating pages - which means forgoing the chance to develop skills that in the future all companies will need to understand if not practise.\n\nMore crucial to the success of a Web site is the content. It is content that draws people to pages and holds them there. Design can help or hinder, but never acts as a substitute.\n\nThe three most important aspects of Web site content are that it needs to be appropriate to its intended audience, genuinely valuable (not marketing fluff) and constantly changing. You cannot just place anything on a Web page and hope that somebody finds it interesting: you need to have a clear idea of what kind of visitor you want to attract (generally this will be the same as your typical customer profile). In fact, even more than with conventional marketing, online activities need to be very highly targeted so that visitors can tell at a glance whether it is worth their while lingering and exploring further.\n\nAssuming that they decide to do so, it will then be the richness of the online content that determines whether they stay long and what their overall impressions will be. There is nothing worse than a site that promises much and delivers little. Word will soon get out on the Internet grapevine (and in a sense the Internet is all grapevine) that the pages in question are not worth visiting, and the site will languish.\n\nFinally, assuming that visitors find your pages of genuine interest, you need to ensure that there is at least some element that changes on a regular basis so as to draw people back. There is so much competition online that Web sites must fight for their audiences every day.\n\nAssuming this battle is won, and visitors find the site interesting and worth returning to, you will have created a powerful online marketing tool. All the time that people view your site they are imbibing your corporate messages (either implicit or explicit). If what they have seen there is useful, impressive or entertaining, they will leave with an enhanced opinion of the site's brand and owner.\n\nBut there is another, rather novel benefit. In creating a site which meets the criteria mentioned above, you will effectively be putting together an online periodical, targeted at a particular sector (in fact the same as that served by your company). What you gain is an online readership - readership, moreover, that if big enough might even be sold to online advertisers in the form of links to their Web pages. Every company setting up a WWW site adds the second business of Internet publishing to its traditional activities.\n\nHow companies can set up stalls on the Internet\n\nOnce a company has decided to use the Internet for promotional or sales purposes by creating some kind of publicly-accessible site - whether FTP, Gopher, telnet or World Wide Web - it is faced with the problem of letting potential visitors know about its existence. For one of the novel aspects of the Internet is that its users are active rather than passive: it is they who decide to go to the company rather waiting for the company to come to them as with conventional marketing.\n\nAn obvious place to start in the process of informing people about a new site is to join the great list of commercial sites at Yahoo (http://www.yahoo.com/Business/). This offers perhaps the most comprehensive list of companies on the Internet, and has an extremely large number of visitors who use it as a jumping-off point.\n\nThe entries in Yahoo are strictly informational, with little scope for imaginative design, subtle approaches or more comprehensive material. To meet this clear need for a forum where companies on the Internet can inform and attract potential visitors to their sites a new kind of Web site has evolved, generally called an Internet shopping mall by analogy with the physical equivalents that are such a feature of the US.\n\nThe pre-eminent source of information in this area is called, appropriately enough, The Internet Mall. It was begun in February last year as an E-mail document with just 34 companies offering various items for sale over the Internet. Today, it has over 1000 cybershops, with tens joining each day, and the main document is almost a megabyte in size; if your E-mail system can cope, it can be retrieved by sending the message send fullmail to the address taylor@netcom.com. It might be more advisable is to retrieve instead one of the fortnightly updates: use the message send mall passed to the same address. The full list can also be retrieved by FTP from ftp://ftp.netcom.com/pub/Gu/Guides/Internet.Mall.\n\nBut perhaps the best way to access the information is through its hypertext incarnation at the URL http://www.mecklerweb.com/imall/. This very impressive site offers various ways of approaching the material: a thematic organisation (based around mall 'floors') and via a search engine. There is also information (at http://www.mecklerweb.com/imall/howto.htm) on how to add your own company to the list.\n\nIt costs nothing to join the Internet Mall since the Internet Shopping Network sponsors the project. The latter is part of the US cable TV company Home Shopping Network, which had sales of $1.2 billion in 1993. The Internet Shopping Network can be found at the URL http://shop.internet.net/, and claims to be the world's largest Internet shopping mall with 600 companies (presumably the original Internet Mall is not included since it is more of an information source than a commercial venture).\n\nNor is the Internet Shopping Network the only Internet mall to be owned by a major company with plenty of experience in online selling. NetMarket (at http://www.netmarket.com/) is part of CUC International, which sells goods at discount to its 30 million members using conventional means, and obviously hopes to do the same in cyberspace. Particularly interesting are the pilots of the advanced interactive shopping services (follow the Business Solutions link on the home page above).\n\nAlongside these giants there are many other smaller Internet malls. A comprehensive list can be found at the URL http://www.yahoo.com/Business/Corporations/Shopping_Centers/. In the UK, for example, there are sites at Apollo UK (http://apollo.co.uk/) and MarketNet (http://mkn.co.uk/). Also of note is Downtown Anywhere (http://www.awa.com/) which offers both commercial and other information in a form that extends the basic metaphor of the US mall to include additional areas in a virtual city.\n\nA Web page has been set up to aid the submission of new Web sites to the main global listings and search engines such as Yahoo, EInet Galaxy, Lycos, Web Crawler and so on. It allows you to cut and paste your URL to the various submission forms. It can be found at http://www.cen.uiuc.edu/~banister/submit-it/. or http://submit-it.permalink.com/submit-it/.\n\nA comprehensive list of those pages willing to consider new URLs for inclusion in their holdings can be found at http://www.homecom.com/global/pointers.html. The list includes HomeCom global Village (http://www.homecom.com/global/gc_entry.html); a graphically based \"Starting Point\" for new site listings (http://www.stpt.com/); The World Wide Yellow Pages (http://www.yellow.com/); Open Market (http://www.directory.net/dir/submit.cgi); Entrepreneurs on the Web (http://sashimi.wwa.com/~notime/eotw/EOTW.html); The Centre of the 'Comp-Uni-Verse' for Net Surfers (http://netcenter.com/yellows.html); BizWeb (http://www.bizweb.com/InfoForm/infoform.html); and Net Search (http://www.ais.net/netsearch/).\n\nWhy everyone on the Net should know about HTML\n\nAlthough most of the essential components of the Internet have been in existence for nearly 25 years, it is only in the last two or three that the Net's use in business has become widespread.\n\nIn part this has been caused by the increasing appreciation of how E-mail can simplify and extend all kinds of business contacts. However, this corporate awakening has been largely due to the introduction and extraordinarily rapid take-up of the World Wide Web. Even before the exciting latest developments of Java and VRML, the Web offered a business medium with immediate impact, interactivity and a sense of immersion hitherto lacking in Internet services. Its application for marketing and sales purposes, particularly the former, have been so obvious that there has been surprisingly little resistance within corporate structures to at least experimenting in this new arena.\n\nAnd so it is that some sense of how Web pages are put together - what is and is not possible - is increasingly becoming a prerequisite for modern managers.\n\nIf you use a Web browser that offers a bookmark/ hotlist feature to store your favourite Internet sites, this is effectively your own home page, although represented on screen in a slightly unconventional form. In fact it is a trivial matter to export a set of bookmarks and turn them into a fully-fledged and even more accessible home page that can be loaded as the default every time you run your browser.\n\nFor those who remain sceptical about the relevance of Web page creation in the ordinary business context, there is now another reason why some knowledge of HTML is likely to become an indispensable skill for the modern manager.\n\nThe use of internal TCP/IP networks - the so-called \"intranets\" - is already a major trend, especially in the US. There, such internal E-mail, news and Web systems are fast turning the long-promised groupware/ Executive Information Systems into reality. Where intranets flourish, personal pages are usually encouraged as a way of allowing staff to participate and to add a human touch.\n\nA by-product of this is that those unable to write their own HTML pages will be denying themselves the opportunity to stake a personal claim in this new, very public corporate space. Adding your home page gives you the chance to present to both peers and superiors a honed self-image that can act as a kind of online CV in permanent application for that next promotion.\n\nIt is surely not too fanciful to predict that in the future the ability to knock up a solid Web page will be just as useful as good memo or report writing skills are today.\n\nHow to get the best from your Net site pages\n\nWriting World Wide Web pages is very easy, but as with so much else on the Internet, understanding the underlying principles can be an enormous help in getting the most from the tools available and avoiding the various pitfalls. For example, few are aware that the HyperText Markup Language (HTML) used for creating Web pages derives part of its name and much of its underlying philosophy from the world of the Standard Generalised Markup Language (SGML).\n\nSGML is about defining logical structures of documents in a formal and self-consistent way (for an excellent introduction see Readme.1st, SGML for Writers and Editors, £30.47, ISBN 0-13-432717-9). Although this might appear to be a fairly dry, academic exercise, it has considerable benefits. It allows you to take a SGML document and use it in many different contexts. For example, it may be displayed on a screen, printed on a dot matrix or laser printer, or even converted into Braille. Because SGML codifies the structure of the document, it allows each logical element of the document to adjust itself according to the final medium, without the need for further user intervention. HTML is what is known as an application of SGML. This means it uses the conventions and ideas of SGML to define the basic structural elements of its documents. HTML is an extremely simple application of SGML, but there are two very important properties that must always be remembered when using it to create Web pages.\n\nFirst, HTML is not a language for describing the appearance of a page, however much it might seem to be. Instead, it describes the underlying structure of that page. In this it differs radically from the more familiar desktop publishing programs. These explicitly define the size and position of all the components of a page.\n\nBecause HTML describes the structure, not the appearance, when creating a Web page you do not know how exactly it will appear on the screen of the person viewing it. To understand why this is so, it is worth examining what happens when a Web page is requested and retrieved.\n\nWhen you use a browser such as Netscape to access a Web page, say the one at http://www.gm.com/index.htm, a request is sent across the Internet to the Web server at that address. Assuming that the requested page exists and is freely available (some require passwords before they are sent), the server returns that page to the browser over the Internet.\n\nThe page itself consists of nothing but a text file, written according to the rules of HTML. Within this document there may be references to multimedia elements (graphics, sounds etc), in which case these are sent separately. When the main HTML document arrives at the browser, it is processed in a simple but important way. The HTML structural markers are located (following SGML conventions, they are all written between angled brackets <>), and then converted to an on-screen representation. However - and this is crucial to remember - it is the browser that chooses what form this will take.\n\nFor example, some of the commonest structural elements within an HTML document are various levels of headings. When creating the HTML file, you simply specify that certain phrases are first or second level headings, etc. You are not able to specify the size of headings or a certain typeface, since these are determined by the settings of the browser that processes your HTML document. These setting can often be altered by the user.\n\nThe fact that HTML is an SGML application therefore changes the way you must think about the Web pages you create. Remember that what you see on your screen is not what other users will necessarily get, and that special and \"clever\" effects will almost certainly be lost on some browsers. For HTML documents, logical structure, not dramatic layout, is paramount, and simple but effective design is the order of the day.\n\nHow to survive the data deluge from the Internet\n\nThe culmination of the incredible progress the various Web search engines has been the recent appearance of services offering full-text searches of a very large proportion of the Internet, with all of the Web promised for some time in the near future.\n\nThe more you use these search engines, the more you become aware of the yawning gap between what is available globally and what is available locally. The problem is exacerbated by the sheer richness of the Internet and the fact that most of what you find is free. The temptation to download a file or a page is almost irresistible.\n\nThe result is plain to see on anyone's hard disc: tens if not hundreds of Mbytes of Internet-derived data and programs. For anyone who uses the Internet regularly the challenge is to manage this local data flood in the same way that the search engines are helping people cope on the global scale.\n\nFortunately there are solutions available that give you much of the power of an Alta Vista or Open Text search through the contents of your hard disc. Although these have not been designed specifically for Internet users, they lend themselves very readily to the task.\n\nThere are two programs for PCs running Windows: AskSam, which costs £99.95 from Guildsoft (01752) 895100, and ZyIndex, costing £395 from ZyLab UK (01235) 861681.\n\nBoth allow you to carry out full-text searches of groups of documents and other text files that have previously been indexed using this software. Both programs offer a good range of features, including Boolean searches (using AND, NOT, OR, etc), proximity searches (finding two words within a certain distance of each other) and fuzzy searching (where near-equivalents can be found for a given search word).\n\nResponse times are good for both: just a few seconds to search through several Mbytes of data. However, the approach taken is quite different in each case, and which you choose depends to an extent on your working environment.\n\nFor example, AskSam creates an entirely new file containing the data and the index, roughly the same size as the files themselves. This has the virtue that you can carry copies of this indexed version to other machines running AskSam.\n\nZyIndex, in contrast, creates an index that is separate from the files themselves. This has the advantage that the index is smaller than that of AskSam, and allows data spread across a network to be indexed more efficiently.\n\nThe down side is that once words and phrases have been located, you need to load the file's native application (e.g. Microsoft Word) to view it in its original form. (ZyIndex can only display rather crude ASCII excerpts.)\n\nFor the Macintosh there is the program On Location, &#163;99 from ESP (01628) 23453. This offers fewer text retrieval facilities than the PC programs, but also serves as a more general file-search tool.\n\nThese free-text search engines solve one part of the problem of the data deluge - how to find things - but leave untouched another aspect. Using a 28.8Kbyte modem it is quite possible to download tens of Mbytes of files from the Internet in a day, and with leased lines much more can be obtained.\n\nEven with Gbyte hard discs, such a stream of files can soon fill up the most ample storage. Moreover, many programs (such as the latest versions of Netscape Navigator) are so large they will not fit on a floppy disc (and dividing programs across floppies is not simple).\n\nHappily, a new generation of low-cost removable discs has arrived to meet this need. Products such as Iomega's Zip drive (available for the PC and Macintosh, priced at about £108+VAT), makes it possible to store 100Mbytes of data on a disc costing under £10. And if that isn't enough, the Iomega Jazz drive (£229+VAT) offers no less than one Gbyte on a removable medium (£52), which should keep even the most voracious of downloaders happy. Iomega is on (0800) 973194.\n\nCreating a World Wide Web for every tongue\n\nFew people have tried to add multilingual capabilities to their browsers. A good reference (http://wwli.com/library/localize.html) point for users interested in this area is the site for multi-lingualism and the Internet.\n\nPerhaps not surprisingly, Microsoft's Internet Explorer is well thought-out as far as international use is concerned. For all its faults, Microsoft is a company that is keenly aware of the importance of localisation for software. Internet Explorer is available in 9 languages (that is, with menus changed appropriately), and it is also easy to add extra characters sets to view multilingual pages.\n\nThis is done by downloading the appropriate font file (http://www.microsoft.com/msdownload/ieadd/03.htm), (the choices are Simplified Chinese, Traditional Chinese, Japanese, Korean and Pan-european). Running the file causes it to be installed and the relevant changes for Internet Explorer made automatically. Thereafter, there is a pop-up list of available character sets available in the bottom right-hand corner of the browser window. When you encounter a page using a character set other than Latin-1 you can simply select from this list to refresh the page.\n\nAdding this capability to Netscape is much harder, and reflects this young company's relative inexperience in dealing with international markets. First, you need to find the relevant fonts yourself (in practice, the simplest solution is to use those provided by Microsoft). Once these have been installed, you must then activate them in Netscape. This is done from the Options menu, choosing General Preferences and Fonts. For each of the encodings you specify the font that you have added. Then, to use this encoding for a Web page, you will need to go to the drop-down list available on the Document Encoding entry on the Options menu.\n\nNone of this is very intuitive; worse is the fact that for Japanese font capabilities you have to edit an entry in the Windows registry - the software equivalent of brain surgery, and about as risky. If you want full foreign language capabilities for Netscape, it may be easier to buy the plug-in (http://www.accentsoft.com/) called Navigate with an Accent from Accent Software. This adds a new drop-down list of character sets alongside the main menu buttons. An evaluation copy (http://www.accentsoft.com/download/dleng.htm) is available. Unfortunately the add-in disables important features such as plug-ins, frames and Java.\n\nAccent produces its own standalone browser (based on the original Mosaic). This too adopts a drop-down list of language options, though strangely Chinese is absent. Accent does, however, offer both Arabic and Hebrew, something that neither Internet Explorer nor Netscape is capable of. An evaluation copy is available from the URL given above. Another product in the Accent range that can be downloaded from there in a trial version is Accent Publisher. This addresses the other side of the multilingual problem: creating Web pages with character sets other than Latin-1.\n\nWith Accent Publisher, you can design a page in most European languages plus Arabic and Hebrew (floating keymaps let you use a QWERTY pad to enter non-Latin characters) and then to convert them to HTML files automatically. More advanced features such as tables are supported. Also notable is the ability to swap among 21 languages (including Arabic, Greek, Hebrew, Russian and Turkish) for the menus.\n\nAnother browser product based on the original Mosaic is Tango, from Alis Technologies (http://www.alis.com/), whose site was mentioned last week as a useful starting point for exploring Internet multilingual issues. An evaluation copy (http://www.alis.com/internet_products/try_form.en.html) can be downloaded. Tango can display no less than 90 languages, including Arabic, Chinese, Greek, Hebrew, Korean, Russian and Thai. The interface can be switched to any of 19 languages. The corresponding creation software called Tango Creator lets you compose HTML pages in 90 languages using character sets other than Latin-1, and supports tables and frames.\n\nWhy the Web will be the font of all corporate data\n\nSeveral surveys in the US have indicated that already the majority of larger companies there have or are implementing intranets. The UK is still a little behind in this area, but as with the Internet in general it is probably further advanced than any other country outside North America.\n\nSince even the most basic of intranets - one where you replace costly and inefficient paper-based communication systems within companies by equivalent TCP/IP network technologies - is so simple to grasp and so compelling as an idea, it is easy to forget its intrinsic limitations.\n\nMuch of the intranet's enormous potential derives from the use of Web browsers as the common front-end to corporate information. These are easy to use and platform-independent, making rollout across a company straightforward both in terms of training and development. But where simple intranets fall down is at the back-end.\n\nNormally an internal Web system will run off one or more Web servers; this will therefore mean that all of the information to be made available on the intranet must first be transferred (and possibly translated) to the store of HTML documents that are served across the network. This is relatively straightforward for text documents, but for anything more complex - in particular for the kind of information held in corporate databases or financial systems - the simple intranet approach is not enough.\n\nAs a result, there is now a growing interest in the marriage of the conceptual simplicity of Web front-ends with the rich complexity of corporate databases, mediated by the TCP/IP-based intranets.\n\nThe vision driving the very many disparate approaches being developed is to employ the Web browser as the universal client that will allow anyone to access any information held on a company's heterogeneous array of data servers. This would avoid the need for new, proprietary software on every desktop or costly re-training.\n\nIn many respects, this coming together of Web and database represents the second generation of the Internet and intranets in business. On the external Web such systems are paramount for electronic commerce, and still thin on the ground as a consequence of the work they require in setting them up.\n\nThere, typically, a customer will access a database of product information via a Web browser, and place an order. The customer and order details would then be entered into another database and fed from there into the vendor's fulfilment system. Both sides of the equation therefore require tight integration of the Web front-end and the database back-end.\n\nThe importance of this integration of Web and database is even more crucial for intranets.\n\nCorporate databases in all their forms represent a unique and therefore highly valuable store of information about customers, markets, divisions and future trends. Enabling people to get at these easily, and to drill down in myriad ways to find other, possibly unsuspected kinds of data deposits could offer major benefits in terms of a company's day-to-day running and longer-term planning.\n\nBecause the Web, intranets and their associated standards are so new, and because databases are so much an established part of corporate computing, there remain many thorny problems to be resolved before this golden age of internal information retrieval dawns. Although it is quite possible to lash up quick fixes using more or less any of the many development tools that are currently available, the strategic importance of this area means these must be superseded by solutions that are properly thought-out, reliable and fully scalable.\n\nWhy small is beautiful on an intranet\n\nScalability is one of the main reasons for why the TCP/IP protocol is used for business intranets. The same basic elements can be used for a multi-national company employing hundreds of thousands of people as for a small office network: at no point is it necessary to switch technologies as the number of users increases, and there is no need for bridges between different networks.\n\nAnother great advantage of intranets is that they are very easy to set up, although maintenance may be another matter. This means that they can be rolled out not just centrally, but locally too, with each department, office or even worker able to create and run their own personal servers, which will typically be Web-based.\n\nClearly, money is an important issue: if many sites are being set up, the overall expense can soon mount up. This means that traditional heavy-duty Internet or corporate intranet solutions such as Sun Sparc systems running Solaris are ruled out for widespread local use. One solution that might well find favour is to use a PC running the Linux operating system and the Apache Web server, both of which can be downloaded for free.\n\nAlthough some managers may be nervous about entrusting their data to free software, they can be comforted by the knowledge that more than 40% of all Internet Web servers use this software, according to the definitive Netcraft guide. Another obvious candidate is Windows 95. Microsoft has brought out its aptly named Personal Web Server, which is free, while Netscape has its Fast Track Server (£220) and many other freeware and shareware programs for Windows 95.\n\nHowever, as anyone who has used the platform for a while will know, Windows 95 is hardly robust enough for this kind of role. Windows NT is far more appropriate, albeit more expensive. This is particularly the case since Microsoft has chosen to impose an arbitrary limit in terms of maximum simultaneous TCP/IP connections on its cheaper Windows NT Client product, which more or less forces you to use Windows NT Server if you are likely to exceed this limit. The benefit of moving up to NT Server is that you automatically get Microsoft's Internet Information Server (IIS) for free. This is an impressive piece of software, now in its third release, and may well be justification enough for swallowing the price of NT Server.\n\nAn alternative is Netscape's Fast Track server, which also runs under NT. A particular benefit of this product is the way it employs a Web browser as the main administration tool, rather than a standalone program as with Microsoft's IIS. Indeed, this is likely to become the standard way of administering many functions, and Netscape deserves much of the credit for pioneering this approach. Alongside these high-profile products, there are two others that are worth noting: Purveyor Encrypt (£599 from Process Software) and Website (which costs £365 from O'Reilly & Associates). Both are notable for the excellence of their documentation.\n\nNor do they suffer in comparison with Microsoft's and Netscape's products. Like them, Purveyor and Website offer secure transactions (increasingly important even for small intranets), as well as easy access to programming interfaces. Where Microsoft and Netscape have ISapi and NSapi, Purveyor has ISapi too (which Process helped Microsoft develop) and Website has WSapi. All four products have sophisticated ways of integrating with back-end databases.\n\nVersion 2.0 of the Wingate program is now available. This software allows entire networks to be connected to the Internet through a single Windows 95/NT machine, which acts as a proxy server.\n\nHow to make money from your Web site\n\nOne of the initial promises of commercial Web sites was to generate revenues from the huge and global audiences the Internet offers. Three revenue models were available: subscriptions, advertising and commerce. Subscriptions, almost without exception, have failed miserably: Internet users seem unwilling to pay for something they have by-now come to think of as free. E-commerce is still very small-scale, though recent forecasts have become very optimistic in the light of technological developments (the agreement of SET etc.).\n\nThis has left advertising as the primary means of making money with Web sites, notably through the use of banner ads. These are block-shaped advertisements, typically placed at the top and bottom of Web pages, with links to the advertiser's site. Practically all of the most popular sites employ them, and earn millions of dollars as a result. For those companies that have established a Web site and built up a more modest readership, it has hitherto not proved possible to convert this audience into money. The main problem is creating a viable sales infrastructure: setting up a Web site is one thing, managing an ad sales force to go with it, quite another.\n\nThis has led to the rise of services, which sell advertising space on others' Web sites. There is much to be said for this idea. For the Web site owner, it obviates the problem of running an ad sales team, and even small sites can gain revenues by being bundled with others to form attractive packages of sites - advertisement networks, as they are called. For the advertiser, there is the advantage of a single purchase, along with better targeting: with large advertising networks it is possible to put together tailor-made groupings of sites.\n\nMoreover, the technique employed by most services - whereby the advertisements are held on a central server and accessed by URL references to them in the participating sites' Web pages - means that obtaining statistical information about visitors is much easier. Although this is a very new area, there are already hundreds of companies offering these services. The best resource is the excellent collection of links at Web Site Banner Advertising (http://www.ca-probate.com/comm_net.htm). As well as companies offering to sell advertising space on a company's Web site on a commission basis, listed here are also those that hold ads centrally and pay according to hits. Other variants include companies that use an auction technique (for example AdBot http://www.adbot.com/) and even host the entire site themselves (Intercity Oz Network http://interoz.com/network/).\n\nThere are a few relatively well-established services such as DoubleClick (http://www.doubleclick.net/), SIMWeb ( http://www.simweb.com/, part of Softbank) and WebRep (http://www.webrep.com/). The quality of their current clients is perhaps the best guarantee that they are offering a serious service generating real money. However, they only deal with very large sites. Of the newer entrants, it is probably safe to assume that schemes involving well-known companies such as Infoseek (http://info.infoseek.com/network/info.html), British Sky Broadcasting and the UK publisher EMAP (http://www.webwidemedia.com/ ) will be less problematic than working with unheard-of start-ups.\n\nBut in general the watchword for this whole new area is caution. Few of these companies have any track-record that can be examined, and it will take many months - perhaps years - before this interesting idea has been refined into something that will allow ordinary corporate Web sites to generate money in this way. Until then, you may prefer to adopt a rather safer strategy. Rather than worrying about whether the people selling space on your pages are just making money out of you, you can simply exchange banner ads with other sites. The relatively well-established Link Exchange (http://www.linkexchange.com/) has been organising this for some time, and uses a system of ad banner credits. It costs nothing to join and there are no charges. There is a UK-based operation called LinkSwap UK (http://www.ukwebmarketing.com/linkswap-uk/) that is also free, and even simpler.\n\nNew tools to create the perfect Web site\n\nNetObjects Fusion is notable for the power of its design tool; you can place elements on a Web page with pixel-level accuracy. This means that designers can fine-tune their pages and be reasonably sure that what appears in the visitor's browser will look very similar to their intentions. However, there is a price to be paid for this very unWeb-like behaviour: the huge number of tables and small graphical elements the program uses to pad out space.\n\nHTML purists will probably be shocked if they examine the HTML code generated, and certainly they will find it very hard to edit it by hand. However, if such matters do not worry you, and design considerations are paramount, then Fusion is probably the best tool. It also offers standard site management facilities such as link checking, and a very visual though fairly limited approach to hooking up with back-end databases\n\nMicrosoft's FrontPage 97 is more orthodox, and it is much easier to edit the HTML code directly, not least because FrontPage produces code that is automatically indented (examples of this can be seen at the site http://dialspace.dial.pipex.com/glyn.moody/, created using FrontPage).\n\nMicrosoft also offers other, complementary tools for the Web site development process. For example you can use Visual Source Safe to manage simultaneous development, while the new Visual InterDev (see) is a powerful tool for linking Web pages to back-end databases, and creating Active Server Pages, which generate HTML on the fly.\n\nCorel's WebMaster Suite provides a similar range of tools for editing pages (Web.Designer), managing a site (Web.SiteManager) and database integration (Web.Data - which employs a very simple nine-step approach), and trial versions can be downloaded from. In some respects Corel's tools are even better than Microsoft's. For example, as well as indenting HTML source, it also colour-codes the links within it according to whether they have been checked, are broken, etc.\n\nSoftQuad's Web site tools represent in many respects the antipodes to NetObjects' Fusion. Where the latter is particularly concerned with appearance, the former concentrates on content. One consequence of this is that HoTMetal Pro 3, SoftQuad's Web page editor, is now looking distinctly old-fashioned with its non-WYSIWYG approach and its limited design capabilities.\n\nHowever, its site tool, called Information Manager, offers a number of interesting features. For example, unlike Fusion's simple hierarchical representation, or the more flexible approaches of FrontPage and WebMaster, Information Manager uses what it calls a Cyberbolic display of elements employing a spherical geometry to give you an ingeniously compact view of your site.\n\nPerhaps even more impressive is SoftQuad's intranet product, HoTMetal Intranet Publisher (HiP). This offers the same basic features as the Internet product, but adds the extremely powerful ability to add user-defined extensions to HTML to allow different views to be produced from the same basic HTML code (so that accounts and production can pull out different relevant information from the same document, for example).\n\nIt does this by employing cascading stylesheets, the only Web site tool to support this new standard. Also provided is a server-side tool for monitoring Web site usage and sending user notifications of changes to pages of interest. However, HiP's lack of database tools is a serious weakness in the current product - one that is apparently being addressed in the next release.\n\nHow the Web is going to turn up everywhere\n\nMicrosoft's fascinating attempt to integrate Internet functionality directly into the Windows operating system, in effect, the Web browser becomes the interface to the entire computing environment, although bold - not least for the rigour with which it has been carried out - this move is by no means unprecedented.\n\nFor example, last year Microsoft introduced NT Web Admin to allow Windows NT environments to be administered using a Web interface. Similarly, it has been possible for some time to control both Microsoft's Information Internet Server and the cut-down Personal Web Server employing just a Web browser.\n\nThere are two big advantages of this approach. First, and most obvious, is the ability to exercise control at distance: if the target system is connected to a TCP/IP network it can be manipulated from anywhere. More subtly, Web interfaces draw on the intuitive nature of the browser. One of the reasons why the World Wide Web has taken off so dramatically, particularly in business, is because it requires only the most minimal training. As some cynics have put it, a browser is software even a Chief Executive can use.\n\nThe credit for this shift towards using the Web for general interface purposes must be given to Netscape. When it launched its Web servers it adopted the then-new technique of administering them using a standard Web browser (which access the server on a non-standard port number). Microsoft's adoption of this technique sets the seal on the idea. The beauty of the Web approach, along with its essential simplicity, is that it can be applied to almost any field. For example, Pipex has created a service whereby its Internet subscribers can read their e-mail using a Web interface - which means that it can be read anywhere in the world that an Internet connection is available.\n\nMore ambitious is the attempt by a consortium including Microsoft, Cisco, Compaq and Intel to create a complete Web-based approach to managing all aspects of networks, and from any location. An exemplary site devoted to the initiative has been created, with details of the basic ideas and the elements involved, including an illuminating Web-based demo and full details of the new HyperMedia Management Protocol.\n\nNor is this all promises; BMC has already added support for this proposed standard to its Patrolwatch Management Suite. Similarly, Cisco has employed a Web interface for its ClickStart management software for some time. Other network hardware devices are also being drawn into this approach. For example, Hewlett-Packard has developed Web JetAdmin for managing its JetDirect printers, while IBM has created a Java-based Network Printer Manager. Even CD-ROM units can now be controlled from a distance through a Web browser.\n\nThe Web interface can also be applied to software, as the server administration tools described above show. But so powerful is the approach that it can be used with any kind of application. For example, both Softquad and Netiva have come up with databases that are accessed purely through Web interfaces. And the use of Java means that potentially any kind of functionality can be added to a Web page while retaining the basic metaphor.\n\nThis Web-based approach could become even more central to computing if the ambitious vision underlying the next version of the TCP/IP protocols, IPv6, is realised. IPv6 was designed in part to allow IP addresses to be allocated to just about every electrical object on the planet - from light bulbs and toasters up; what could be more natural than accessing them from a distance via a Web interface?\n\nMake a spectacle of your Web site\n\nWeb site development is no longer a job for amateurs and enthusiasts, writes Brian Clegg. As delegates at the Web 98 Design and Development Conference conference in Boston, US, September can vouch, to do a professional job requires knowledge of strategy, usability, information and visual design, and programming.\n\nThe first consideration in setting up a Web site is the server software needed to host it. A Web site is, in effect, a simple database. This database, the server, uses hypertext transfer protocol (HTTP) to deliver the specific Web page requested by many browsers simultaneously.\n\nBut delivering pages efficiently is no longer enough: a Web server must be able to execute programs to retrieve data, format pages on request and give a page life.\n\nDepending on the server's abilities, this external program might be written in languages such as Perl, C++ and Visual Basic. In these an application communicates with the Web server using Web technology called the common gateway interface (CGI).\n\nMicrosoft has provided an extra twist by adding ISapi - an interface that allows external programs to plug more directly into the server.\n\nOperating system\n\nThe next thing to consider is the operating system on which the Web server will run. If the platform is Windows NT, the obvious choice is Microsoft's Internet Information Server, which is free, fast and feature-rich.\n\nPage functionality can be added by writing direct to ISapi, using any development environment that can build a dynamic linked library; or by scripting Active Server Pages using Perl or the popular browser scripting languages. If you want to provide access to a wide range of data try OLE-DB, Microsoft's latest object interface for databases.\n\nEven more development options are available with O'Reilly's Website Professional, which adds CGI and its own proprietary interface. However, it costs about £500, has less general functionality and is probably better suited to smaller enterprises.\n\nIf the platform is Unix, the most powerful choice of operating system is Netscape's Enterprise Server. Here, development interfaces are provided for NSapi - Netscape's equivalent of ISapi, for CGI and for the Java language. Enterprise Server also has a huge range of connectivity options, making it an excellent choice for fronting up a database. But it isn't cheap - about $1,200 (£750).\n\nOne budget option is Apache. It is limited to CGI but has add-on modules to extend it - and it's free. If you have a free choice, Internet Information Server probably has the edge at the moment, with Enterprise Server coming a close second.\n\nOnce the server is chosen, the site needs to be built. hypertext markup language (HTML), which is used to describe Web pages, is plain text.\n\nHowever, developing a site with a text editor such as Notepad in Windows is tedious and risky. Most developers use a visual design tool, which act like a word processor for the Web.\n\nSimple pages can be created using actual word processors such as Microsoft Word and Lotus Wordpro, which generate HTML from an ordinary document. Developers can also use free Web editors such as Netscape's Composer which is included in the Communicator Suite, or Microsoft's Frontpage Express which comes with Windows 98 or Internet Explorer 4.\n\nFor professional sites, though, products such as Softquad Hotmetal Pro, Microsoft Frontpage, Adobe Pagemill and the unusual free-format Netobjects Fusion offer a much wider range of features.\n\nThese typically allow Web designers to preview pages in multiple browsers. They offer high-end facilities such as style sheets, and support for scripting. There are also software packages that manage the Web site, map its layout and check for dead ends, and upload pages to the live site from a test environment.\n\nStyle sheets\n\nAs the tools have matured, so has the underlying language. HTML has been revised and extended. Most notably, it now offers cascading style sheets, providing a mechanism to fix items on a Web page and set standard styles across a site.\n\nAt the same time, other extensions enhance the presentation of data within Web pages (XML), and make possible pages that can be tailored to an individual or change on the fly.\n\nTraditionally, scripting ran on the server. This meant it was independent of the functionality that the end-user's Web browser had to offer. But increasingly, scripting has moved to the browser.\n\nBoth major manufacturers support the de facto standard Javascript, a cut-down version of the Java language, while VBScript, derived from Visual Basic, is available in Microsoft's browser, or through a Netscape plug-in.\n\nOne development that has been around for a while without achieving the penetration initially expected is \"push\". This technology enables users to subscribe to \"channels\" - special subsets of sites with active information. By contrast, another advance - hybrids of the Web and TV - is already under test.\n\nFew environments change more quickly than the Web, and consequently Web site development remains a frantically moving world.\n\nDesign trends\n\nFactors influencing development are server choice, site design tools, language enhancements and browser facilities\n\nWysiwyg tools and sophisticated scripting now used to develop and manage Web sites\n\nMore links to data, more tailored pages and more active information\n\nAccept that whatever you do will look dated within weeks of going live\n\nDefinitions\n\nActive-X\n\nThe surprisingly wide support that Java has generated derives in part from manufacturers excited by the possibility of software-on-demand, perhaps sold on a per-use basis and delivered directly to your machine over the Internet.\n\nMicrosoft has responded vigorously to this and has come up with it's own Java-like approach. Recognising that there was a demand for 'componentware' Microsoft has plucked a fairly technical aspect of its programming products from obscurity and promoted it to linchpin of its new Internet strategy.\n\nActive-X is the latest incarnation of OCX, which itself derives from OLE Component Object and evolved from VBXs (Visual Basic custom controls) found in Microsoft Visual Basic. These are software elements - components - that can be used in a variety of projects.\n\nActive-X is an outgrowth of this software recycling. They add extra features, the ability to work in both 16- and 32-bit environments, and a greater portability than that offered by VBXs. This last property has allowed Microsoft to recast them as its platform independent Java-killer, with the added bonus that this builds on the highly popular and well-understood technology of VBXs.\n\nThe distinction between Active-X components and Java is that the former is compiled and therefore the appropriate binary has to be downloaded when required for the target computer. Java is transmitted as code and compiled upon demand.\n\nVisual Java mirrors Microsoft Visual C++ tools and includes a way of turning Java applets into ActiveX controls. Using Visual Basic scripts as the glue that binds all these ActiveX elements together, Microsoft has managed to extend the functionality offered by Java.\n\nMicrosoft has made available a free software development kit (SDK) for use with its new Visual InterDev Web application development system. The Design-Time ActiveX Control SDK aids the creation of server-side components for use in Active Server Pages.\n\nWhy Microsoft has made ActiveX take a back seat\n\nThe most interesting aspect of the UK launch of Internet Explorer 4.0 was not what was said but what was omitted. During the surprisingly amateurish two-hour presentation the ActiveX approach was referred to neither directly or indirectly. ActiveX was introduced back in December 1995 as Microsoft's answer to the then relatively new Java applets. Rather than taking a chance on new and untried technology, so Microsoft's argument went, far better to go with ActiveX, a new incarnation of OCXes.\n\nAccording to Microsoft, ActiveX could match all the exciting new interactive and dynamic features offered by downloaded Java applets in Web browsers, and required no investment in new languages or techniques. The fact that OCXes were strictly for the Windows platform would be addressed by extending the technology to the Macintosh and Unix at some point in the future.\n\nWith its usual superb marketing, Microsoft was able to convince the Internet world that there was therefore a real rival to Java applets, and that Java's apparent raison d'être - to extend Web browsers - had disappeared. Having come up with an approach that was clearly reactive and tactical, the company built on the initial success of the idea and made it central, to its Internet and later to its entire computing strategy.\n\nActiveX controls migrated from the client side to the server, notably with the Active Server Pages approach, and formed the basis of Microsoft's DCom component technology and fledgling transaction processing model. But while these important shifts were going on behind the scenes, the most visible manifestation of ActiveX remained on the client side.\n\nThis makes Microsoft's retreat all the more dramatic: dramatic, but perhaps inevitable. ActiveX's security model is fundamentally flawed. Whereas Java applets are restricted in terms of the operations they can carry out once they have been downloaded to a client, ActiveX controls can do anything their creator's desire.\n\nMicrosoft's response to this unacceptable situation is Authenticode. This employs digital certificates to ensure that ActiveX controls are not tampered with as they pass across the network, and to provide a sure way of establishing who wrote them. The idea is that if a control comes certainly from a trusted source - a major software house, say - then it can be left to operate freely on the user's system. But the trouble with Authenticode is that it places the burden on the user: he or she must decide whether a digital certificate provides enough assurance to allow the corresponding control full access to a system.\n\nThis is of course unrealistic for most users who are not experts in digital certificates and simply want to get on with their work. Moreover, if rogue ActiveX controls are accepted and cause damage, there is no guarantee that the certified software publisher still exists, much less is within any useful jurisdiction where it can be sued or prosecuted.\n\nIn other words, for all practical purposes, ActiveX controls are useless on the open Internet, since they simply cannot be trusted. Microsoft has tacitly recognised this with the introduction of security zones in Internet Explorer 4.0. Using these it is possible to set defaults in terms of accepting or rejecting ActiveX controls, according to whether they originate on the Internet or from within a intranet.\n\nSecurity zones mean that ActiveX controls can still be employed within a corporate intranet, since their origin and capabilities are presumably known. And this in its turn means that Microsoft's server-side ActiveX strategy - and with it Active Server Pages, DCom and Transaction Server - is still viable. But it does mean that ActiveX controls are unlikely to be used for public Web design.\n\nInstead, Microsoft is pinning its hopes on Dynamic HTML, which made an appearance at the Internet Explorer 4.0 launch, and which will presumably now take over as Microsoft's latest anti-applet technology.\n\nActiveX Web Database Programming (£27.49, ISBN 1-861000-46-4) presents an excellent practical introduction to the Microsoft's alternative middleware technologies.\n\nAddress classes/IP address\n\nThe internet uses a 32-bit address scheme, called IPv4 (Internet Protocol version 4) to define hosts on the global network. This 32-bit address is usually written as four eight-bit numbers in the decimal form 123.45.67.89, where each of the four elements is less than 256. These Internet addresses, of which there are theoretically 4,294,967,296 (though in practice there are fewer because blocks of numbers are reserved for special purposes), are split up into various classes, each of which has important characteristics. Moreover, the way in which blocks were allocated to users (especially in the early days of the Internet) means that there are now relatively few of these addresses left.\n\nClass A IP addresses are those whose first element runs from 1 to 127. Because the other 24 bits can be freely assigned by the holder of Class A address, this gives a network with a potential 16,777,216 Internet addresses. Examples of these lucky organisations include IBM, which has the Class A address 9.0.0.0, and MIT, which has 18.0.0.0.\n\nIt was quickly realised that the entire address space would soon be exhausted if many Class A addresses were handed out, and so Class B numbers became the default.\n\nClass B numbers run from 128,0,0,0 through 128.1.0.0 to 191.255.0.0 giving 65,536 addresses for each of the 16,000 or so networks available.\n\nHowever, such has been the growth of the Internet that even Class B addresses are in short supply. It is now the policy to give out Class C numbers, which run from 192.0.0.0 through 192.0.1.0 to 235.255.255.0, and each of which has 256 addresses.\n\nOrganisations requiring more than this number may need to take several Class C networks.\n\nThe remaining IP addresses, those from 224.0.0.0 up to 255.255.255.255 are divided into two more classes: D and E. However these are reserved for special purposes, and are not allocated to individual networks on the Internet.\n\nThe solution is relatively simple: to increase the address size and with it the number of possible Internet nodes. This is the approached adopted with what is called IPv6 or IPng (for Next Generation). What is surprising is the scale of the extension that has been adopted. Instead of the current 32-bit system, IPng will use no less than 128 bits for addresses. This does not give a mere four-fold increase: in fact, according to the excellent introduction to the whole subject of IPng at the URL http://playground.sun.com/pub/ipng/html/INET-IPng-Paper.html, an address-length of 128-bits implies 340,282,366,920,938,463,463,374,607,431,768,211,456 (3.4x10^26) nodes. Since such large numbers are difficult to grasp, the same source puts things in context by point out that this figure represents 665,570,793,348,866,943,898,599 (6.7x10^23) possible nodes for every square metre of the planet.\n\nThis extraordinary number does not simply represent some rather excessive caution on the part of the IETF working group that drew up the new standard embodied in RFC1752, which is available from ftp://ds.internic.net/rfc/rfc1752.txt . It hints at something altogether grander and more exciting.\n\nFor with such huge numbers available it is possible to look beyond allocating Internet addresses to every computer. IP numbers could be given to every computer peripheral; to every piece of business equipment in an office - fax machines, photocopiers, telephones (assuming they had some kind of networkable digital control element that could become part of this super-Internet).\n\nAnd beyond this lies the integration of an even broader range of digitally-controlled electrical devices into a massive total network girdling the world. Included could be transport systems (most cars already have as many chips inside them as a desktop PC) and even semi-intelligent domestic appliances. The day when individual light bulbs can be accessed and controlled over the Internet is perhaps not so far off - and already implicit in IPng.\n\nAlthough the physical infrastructure can expand almost endlessly, some of the logical elements have limits in their current incarnations. Perhaps the most serious of these has to do with routing tables.\n\nThese determine how the individual data packets should be routed over the multiply-connected Internet. As the latter grows and changes on a daily basis, so these tables need to be updated - sometimes several times a day.\n\nAs well as resolving issues of address space - finding enough Internet addresses for new users is now becoming a problem - IPv6 adopts a more efficient approach to routing that avoids the need for massive tables and their frequent update.\n\nLeaving aside the fact that IPv6 has not yet been implemented yet - and will require some years before it is fully rolled out - there remain broader challenges for the Internet. For example, the Internet phones, along with other kinds of multimedia traffic, generate huge quantities of data packets. The current Internet infrastructure and pricing system is simply not designed to cope with this flood which is likely to clog the system increasingly in the future.\n\nIt may, for example, be necessary to implement new pricing schemes for Internet connections whereby you pay for the volume of data transmitted, rather than a flat rate. Another possibility is that Internet users, particularly companies who are beginning to depend on their connections, will demand Quality of Service (QoS) guarantees from their ISPs - and be prepared to pay for them.\n\nHere, too, IPv6 should help. It supports the new ReSerVation Protocol (RSVP) that attempts to negotiate a certain QoS on the Internet. Once this is in place users will be able to expect the same kind of reliability from their Internet suppliers that they do from telecom companies or energy utilities. In a few years' time it will be unthinkable for companies to work with an ISP unable to offer these kinds of guarantees. Although this is still a way off, now is the time to start talking to Internet connectivity suppliers about their plans in this area.\n\nFurther information can be found at Digital http://www.digital.com/info/ipvt/ with a noteworthy main white paper at http://www.digital.com/info/ipv6/white-paper.html, one of the best non-technical introductions to the subject.\n\nADSL\n\nThe recent features on ISDN described the advantages of this by-now rather old technology. Interestingly, the potential throughput - 64Kbit/s on each of two channels that can be combined to give 128Kbit/s without compression - is beginning to look less impressive. The new generation of 56Kbit/s modems running over ordinary telephone lines are not so far behind, though ISDN does have other advantages.\n\nIndependently of these modem technologies, ISDN is unlikely to remain the speed champion for long. As a previous Net Speak explained, a new kind of modem working with cable TV networks offers the promise of Mbit/s download speeds in the not-too-distant future. Trials are already underway in the UK, and there is talk of launching cable modem services next year.\n\nNeedless to say this cable modem challenge is not being taken lying down by the telecom companies of the world. Almost as if by magic, their engineers have discovered that they can push data down a telephone line at not just the current 28.8 Kbit/s, or the coming 56 Kbit/s, but at a stunning and rather convenient multi-Mbit/s speed.\n\nThis is accomplished by using higher frequency transmissions over conventional telephone lines. There are limitations of distance from user to exchange - typically 10-18 kilofeet, as the jargon has it - and not every exchange or area will be able to offer the technology. But the new service - generally known as Asymmetric Digital Subscriber Line (ADSL) - promises cable modem speeds down an ordinary phone line. It is asymmetric because upload speeds are \"only\" a few hundred Kbit/s - more than enough for most people's needs.\n\nMicrosoft has a white paper about ADSL\n\nAgents\n\nThere are more than one million Web sites according to the Netcraft survey, with many more being added each day. As well as the new sites that are cropping up, established locations are being updated, often on shorter and shorter time-scales as constantly refreshed content and form becomes a paramount factor in attracting and keeping visitors.\n\nThe rise of the Internet search engines has been one response to this data deluge. Initially they seemed almost unbelievably powerful tools that placed millions of Web pages within the user's grasp, and allowed unimaginably large quantities of data to be sifted in seconds.\n\nBut now, as the Net continues to expand, even search engine results have become unmanageable. Instead of reducing data to information, the listings of hundreds or even thousands of hits across the Internet provide simply a first winnowing.\n\nIt is clear that two new elements are needed to help users in their struggle against this flood of facts. First, a way of conducting searches automatically, without needing to specify every time what you are looking for. And, secondly, more intelligence applied to the filtering process to produce usable results.\n\nThe new class of programs designed to offer these two elements is called agent software. Although grandiose claims have been made for agents, so far their incarnations have been simple and disappointing. But these will undoubtedly change and probably soon - not least because the Internet would otherwise soon drown in its own content.\n\nThe AgentNews Webletter, (http://www.cs.umbc.edu/agents/agentnews/) devoted to the fashionable subject of software agents can be found.\n\nAgents are not limited to simple browsing but can help to find the best prices on offer by online merchants. One of the leaders in this field was Jango from Netbot at http://www.netbot.com/ , now owned by the search engine Excite at http://www.excite.com/. In the shopping section at http://jango.excite.com/cf/index.html are now offered Jango-enhanced searches for various categories including Computers and Software, Movies and Games & Toys.\n\nAlexa\n\nIf you hate the way browsers lead you around the Internet by the nose, you'll welcome a new helper which aims not only to take you where you want to go but also to make things more interesting along the way.\n\nAnyone who has spent any time browsing through the sections of netspeak and articles in magazines and books online will no doubt have come across links that no longer lead to the Web pages cited. Many of the resources referred to have not just moved, but disappeared completely.\n\nThis is a fundamental problem with the Internet: it is not like a traditional library gradually gaining more titles, but a huge, organic entity that changes constantly with time, losing sites and pages as well as gaining them. The idea of trying to capture and save these previous incarnations may seem utterly unrealistic, and yet this is precisely what Internet Archive is trying to do.\n\nThe man behind the Archive is Brewster Kahle, who developed the Wide Area Information Server search system. He has set up an ancillary company called Alexa which aims to use the Internet Archive (which it generates, largely) for commercial purposes. (For more information; the company, Brewster Kahle).\n\nAlexa is also the name of a product, (for Windows 95/NT currently). It is a kind of browser helper: it runs alongside Navigator or Internet Explorer, and monitors which sites you visit. On the basis of where you are and where you have been, it offers suggestions about other sites that you might find relevant and interesting.\n\nIt does this in part by drawing on its database of where other people who have visited similar sites have gone: what it calls usage paths. A corollary of this is that it keeps a record of where you have been, though the company insists this information is kept private.\n\nNovel navigation\n\nThere are full details of the technology, and a good article on some of the information that has been gleaned about the Web in general\n\nThe aim of Alexa is twofold: to offer a novel way to navigate through the World Wide Web, and to create a kind of consensus about which sites are worth visiting on the basis of what Internet users think and do, not according to some self-appointed arbiter's judgements. Alexa offers other benefits. As well as providing suggested locations to visit, it tells you about the site you are currently viewing: who runs it, how big it is etc.\n\nGet into the archive\n\nMore interestingly, Alexa allows you to hook into the Internet Archive: when older material relevant to the site you are viewing is available, this is signalled on the Alexa toolbar that appears with your browser. You can then view previous versions of Web pages. There is also a facility to search through the electronic version of the Encyclopaedia Britannica (or at least some of it) and an online dictionary and thesaurus.\n\nAlexa is free: the company aims to generate revenue by a new kind of online advertising. In the pop-up menus offering suggestions of where to go next small banner ads (http://www.alexa.com/company/advertise.html) can appear. I found the suggestions offered by Alexa stimulating, not least because they were frequently unexpected, and nearly always worthwhile. Too often search engines take you to pages that are irrelevant or dull. If enough people start using Alexa seriously - and so feeding in their use patterns to the system - it could represent a genuinely innovative way of using the Internet.\n\nAmerican Online (AOL)\n\nAOL's dramatic acquisition of Netscape for $4.2bn (£2.6bn) is deeply ironic. For the start of Netscape's decline can perhaps be traced to the moment in March 1996 when AOL chose Microsoft's Internet Explorer, rather than Netscape's Navigator, as its default browser.\n\nAs Netscape lost market share in the browser sector, it also began to lose focus, proclaiming first the central importance of its server products, and then of its Netcenter portal.\n\nIt was this corporate schizophrenia that allowed AOL to step in with an offer that effectively split off the portal from the software, with Sun taking over the latter.\n\nThis \"strategic alliance\" will allow Sun to sell and co-develop the Netscape line of software, and guarantees the company $500m of business from AOL in terms of systems and services. America Online will receive more than $350m from Sun over the next three years.\n\nAOL's acquisition of Netcenter certainly makes sense. It turns AOL into the undisputed leader in terms of Internet advertising and gives it a large number of new visitors at a stroke (since AOL's core market is outside business, where Netcenter's strengths lie).\n\nWith AOL's backing, Netcenter is now in a position to challenge even Yahoo, something that seemed unlikely before the deal. Similarly, Microsoft will have to work even harder to establish its own portal.\n\nWhether AOL will switch to Netscape Navigator as its bundled browser is less clear. Although this might seem an obvious move, doing so would cost AOL the preferential placing it receives in Windows - Microsoft's trump card, which was the key to gaining the AOL deal. AOL may prefer to wait - not least because it will be easier to integrate later versions of Navigator that can be designed with this in mind.\n\nIf the browser is not bundled immediately, and the Netscape servers are being farmed out to Sun, the entire software side of the deal reveals itself as rather pointless for AOL - and problematic. Quite how the agreement with Sun will pan out remains to be seen; in particular, the product overlap between Sun's and Netscape's server lines will not be easy to manage.\n\nCollateral benefit\n\nHowever, irrespective of how things unravel on the software side, the deal will certainly change the Internet landscape dramatically and have a considerable impact on most players and sectors.\n\nMicrosoft is clearly affected adversely in terms of competitive pressures, since the deal represents the formation of a new AOL-Sun axis, with Netscape as the glue. None the less, there may be a collateral benefit in that the formation of a strong rival would seem to weaken the threat that Microsoft will operate as a monopoly (though it leaves open the question of whether it has already acted as one).\n\nSun does well out of the deal. It picks up guaranteed business, gets to control - and so neutralise - an erstwhile competitor, and cosies up to an emerging power in the online world. Perhaps even more important is the boost the deal will give to Java.\n\nAOL is committed to support Java, including the imminent Java 2 and PersonalJava. The latter is very interesting, because it opens up the possibility of AOL-branded devices such as mobile phones, pagers and personal digital assistants.\n\nAlternative approach\n\nClear losers in the deal are the other portals, especially the second-tier ones. Yahoo will doubtless survive (perhaps through alliances or acquisitions), but there may not be room for many smaller players. Another group that may not be too happy is the open source movement. Netscape was one of the most important evangelists for this alternative approach, and although the Mozilla project is safe (since open source code cannot be revoked), AOL may well be less committed ideologically.\n\nThe other main loser is Netscape, its employees and users. Although the name may linger on, the company now straddles uncomfortably two markets and two masters.\n\nTo see the first Internet company, once such an exciting and innovative player, disappear in this way is sad. Its passing represents if not the beginning of the end, certainly the end of the beginning for the Internet world.\n\nApache\n\nBesides the browser Mosaic at the National Center for Supercomputing Applications (NCSA), NCSA has another claim to fame on the Internet, as the creator of the NCSA httpd Web server.\n\nJust as Mosaic played a crucial role on the client side in popularising the idea of graphical navigation of the World Wide Web, so the NCSA httpd Web server was one of the key programs in providing a practical demonstration on the server side of just what could be achieved. The \"d\" in \"httpd\" in the name refers to the daemon, or continuous Unix process, that runs the HTTP service used by the server to supply Web documents to the client browser.\n\nLike Mosaic, the NCSA was (and still is) free. However, also like Mosaic, it suffered from a number of bugs of varying severity. To solve these problems, and to improve the overall performance, a new Web server was developed, taking as its starting point some of the fixes - patches - to the NCSA httpd software. Because of its origins as a \"patchy\" server it was dubbed Apache, a name it retains to this day.\n\nDespite its less-than-glamorous origins, Apache has been a phenomenal success. According to the Netcraft survey of Web servers, over 40% of sites are on machines that are running Apache, a market share far ahead of any commercial server program. Apache has the advantage of offering full-power encryption - even outside the US. Apache is explicitly designed for Unix platforms, and one of the most popular of these is Linux, not least because like Apache it can be obtained free of charge.\n\nApache's home page is at http://www.apache.org/.a very full FAQ at http://www.apache.org/docs/misc/FAQ.html and main documentation at http://www.apache.org/docs/. There is a version for windows under development at http://www.apache.org/docs/windows.html.\n\nApplication Programming Interfaces\n\nNow that the Internet is becoming more integrated into the rest of corporate computing it may well be that the larger ensemble created begins to lose some of the very qualities that made the Internet such a success in the first place. Platform-independence is a case in point.\n\nOne of the current key areas of Internet development is how to plug the Internet into non-TCP/IP elements - specifically databases, perhaps using some kind of middleware. This means that the platform-independent techniques that lie at the heart of the Internet must be supplemented with others, some of which do depend on the platform in question.\n\nFor example, in the database arena, the use of Application Programming Interfaces is proving to be an important part of linking Internet Web servers with the heterogeneous components of the corporate computing matrix.\n\nAn API is essentially a published set of functions that are available to other third-party programs to enable them to carry out actions. They provide a common standard that can be used by many disparate programs without the need for special patches to be created on a case-by-case basis. However, APIs must be defined - usually by a leading player with enough clout to make them viable in the marketplace - which means that they are arbitrary to a certain extent, and tied to that defining manufacturer.\n\nIn the Internet field, two of the most important APIs that are starting to be widely supported are the Netscape Server API (NSAPI) and Microsoft's Internet Server API ISAPI. Although end-users need not worry about the details of such APIs, it is likely that they will be aware of their presence more in the future.\n\nArchie\n\nArchie was the first attempt to solve the Internet's biggest problem; the lack of a central directory. Begun at McGill University in Canada, the Archie project (the name comes from the fact that it has to do with file archives) provides users with a way to search for files on the Internet. It is indispensable when you are looking for a particular program among the 50 Gbytes of publicly-accessible software held on some sites.\n\nInformation about where files can be found is held on a number of Archie servers located around the world. These update their lists by contacting major FTP sites in turn and retrieving information on the directories and files held. To find a particular file you should ideally use an Archie client program resident on you desktop machine or elsewhere on the company network. There are such clients for all major platforms, including Microsoft Windows. You simply enter the name of the file you are looking for, send the request to an Archie server (the main one for the UK is archie.doc.ic.ac.uk) and wait for the response. This will generally consist of hundreds of locations throughout the world where that particular file is held. You would then use FTP to retrieve the file. If you don't have an Archie client on your system, it is also possible to use the telnet facility to log into an Archie server (enter Archie for log-in and the password) to carry out the searches directly.\n\nThere is an E-mail version of Archie that complements the FTPmailers well. By sending an E-mail message to a special Archie site (e.g. archie@archie.hensa.ac.uk). For example, to locate pkz204g.exe you would send the message find pkz204g.exe to the address archie@archie.hensa.ac.uk. You will eventually receive a list of locations and directory entry for the file. You will then need to extra the name of the site, the relevant directory and the name of the file (already known). You then send an E-mail message to ftpmail@doc.ic.ac.uk such as open teseo.unipg.it binary uuencode chdir pub/stat/jse/software/misc/ get pkz204g.exe end\n\nArchie is a rather crude instrument; it is difficult to refine its searches unless you have a mastery of regular Expressions (and if you don't you might like to see the pages at http://venus.ubishops.ca/course/regex.html which has a good explanation of the subject).\n\nRather better in someway is Shase - the shareware Search Engine. Its home page is at http://www.fagg.uni-lj.si/SHASE/ - which, as the .si domain indicates , is located in Slovenia (at the University of Ljubljana). A UK mirror can be found at http://shase.doc.ic.ac.uk/SHASE.\n\nShase offers the possibility of carrying out searches for files in a more sophisticated way than is possible with Archie. Its index covers over 110,000 files totalling 13.6Gb. An interesting list of these archives (which includes CICA, SIMTEL, Info-Mac and Microsoft) can be found at http://dolphin.doc.ic.ac.uk/DB-SHARE/statistics.html.\n\nUseful too is the list of 100 new files at each site. Selecting one of these files takes you to a page with a list of possible FTP locations: as well as providing hot links to the site, there is a useful statistic that indicates how often a test program was able to access the directory in question. This indicator of how easy it is to log in to leading FTP servers is probably unique.\n\nAsynchronous Transfer Mode (ATM)\n\nAsynchronous Transfer Mode (ATM) in the context of broadband communications is one of the most important technologies for the future. Broadband simply means able to transmit large quantities of data (over 1.5 Mbits per second according to the official definition).\n\nAs anyone who has used the Internet for a while knows, sooner or later you hit against transmission speed limitations - either locally, in the connection to your computer, or in terms of the size of the data pipe that your Internet provider uses to connect to the rest of the Internet (especially the size of the transatlantic connection, often a critical bottleneck).\n\nFor this reason many see ATM as offering one of the best ways of upgrading the global Internet infrastructure to provide bigger data pipes from which faster local feeds can be taken.\n\nATM has nothing to do with the physical side of these connections, but is all about how the data is packaged and transmitted. ATM employs packets of fixed size (rather perversely chosen to be 53 bytes - 48 bytes of data plus five for routing information). Once data has been chopped up into these packets, the latter are then transmitted across the physical network in question asynchronously (hence the name). That is, the sender and receiver do not have to be rigidly synchronised before or during the transmission.\n\nThe great advantage of ATM, apart from its ability to offer high data throughput reliably, is that its very simplicity means that it can cope with any kind and mixture of data and run over any kind of network. This flexibility makes it a good match for the similarly minimalist Internet, which is defined by little and which can be used in almost any situation.\n\nA tutorial on ATM can be found at http://juggler.lanl.gov/lanp/atm.tutorial.html, which introduces all the main concepts. Even more information can be found in the ATM FAQ directory at ftp://ftp.funet.fi/pub/networking/technology/atm/FAQ/. As well as the very detailed three-part FAQ itself (contained in the files ftp://ftp.funet.fi/pub/networking/technology/atm/FAQ/ATM-FAQ1.txt, ATM-FAQ2.txt and ATM-FAQ3.txt) it also contains a file devoted to ATM-acronyms at ftp://ftp.funet.fi/pub/networking/technology/atm/FAQ/ATM-Acronyms.txt and more about the Fibre Distributed Data Interface than you could ever imagine at ftp://ftp.funet.fi/pub/networking/technology/atm/FAQ/FDDI-FAQ1.txt.\n\nA list of ATM sites can be found at http://www.cl.cam.ac.uk/users/cm213/Project/atm.html. It has good links to other locations with ATM information, ATM testbeds, and ATM product vendors.\n\nThe ATM Forum is a non-profit organisation bringing together most of the main players in this market. Its home page can be found at http://www.atmforum.com/. From here there are links to its newsletter (at http://www.atmforum.com/atmforum/atm_newsletter.html), called 53 Bytes, the basic cell-size used for ATM transmission, a list of its members and other useful information. The Internet Engineering Task Force has a group looking at IP protocols running over ATM; details can be found at http://www.ietf.cnri.reston.va.us/html.charters/ipatm-charter.html.\n\nAcceptable Use Policy (AUP)\n\nThe acronym stands for Acceptable Use Policy, and referred originally to a short but important document drawn up by those running the NSFnet, the first backbone of the Internet.\n\nIt defines very basic rules for governing who could and could not join the Internet (which necessarily meant using the NSFnet, since the latter tied everything else together), and for years was the main limiting factor on employment of the Internet for commercial purposes.\n\nIts opening statement is unequivocal: \"NSFnet Backbone services are provided to support open research and education in and between US research and instructional institutions, plus research arms of for-profit firms when engaged in open scholarly communication and research. Use for other purposes is not acceptable.\"\n\nThe full document can be found at ftp://nic.merit.edu/accpetable.use.policies/nsfnet.txt, although this is now only of historical interest. NSFnet has more or less shrivelled away.\n\nThere are other AUPs apart from that of the NSF: for example, the subsidiary academic networks frequently impose constraints similar to the original NSFnet statement. Even commercial suppliers usually have some kind of AUP, usually limiting people to legal activities.\n\nAvatars\n\nOne of the most exciting applications of Virtual Reality Modelling Language (VRML) is in the creation of virtual worlds through which users can move, for example to allow information about data hierarchies and relationships to be conveyed in a simple visual way. A completely different use of VRML involves the fashioning of shared virtual environments. Here the emphasis is on interaction with other users, extending the other forms of online communication currently employed such as Internet chat or Internet telephony.\n\nSince a crucial aspect of these worlds is their three-dimensional nature, it follows that some kind of virtual corporeal presence is required in them if the overall metaphor is to be preserved. This has led to the growth of an entirely new online element: avatars. An avatar is the incarnation or form that you take in one of these virtual worlds. Typically it will have some three-dimensional characteristics, a front and a back, for example, so that other participants in these worlds can move around you. The form might be minimalist - a floating head, or a simple object - or a complex piece of three-dimensional graphics crafting that is a work of digital art.\n\nAlthough the word itself comes ultimately from the Hindu religion, the first use of the term avatar in a computing context is generally traced back to Neal Stephenson's seminal cyberpunk novel Snow Crash. In the world described there, avatars inhabit a huge and rich virtual domain called the Metaverse, where all kinds of activities and transactions are conducted. Current implementations are rather cruder, but may one day evolve into an important business and entertainment medium.\n\nAuctions\n\nFor those interested in auctions, there are more and more sites springing up where you can make your bids over the Internet\n\nA natural approach to a new medium like the Internet is to try to transfer pre-existing business activities online, as well as inventing wholly new ones. A case in point is the auction: since the Internet is essentially about communication, it obviously lends itself very naturally to the process of receiving information about lots and making bids for them against others.\n\nIt should therefore come as no surprise that this field is flourishing, as Yahoo's long list of online auctions sites testifies.\n\nHowever, it is important to distinguish between two quite different kinds of auctions, since the way they work and their relevance for business are quite different.\n\nMerchant\n\nThe first kind might be called the merchant auction. Here the online site auctions goods that come from manufacturers and that are frequently old stock that is being sold off cheaply. The online auction is a good way for both the seller and the buyer to agree a price efficiently.\n\nThe relation of the merchant auction to its conventional counterparts is made clear by the affiliation of some of the leading sites in this area. For example First Auction is part of Internet Shopping Network, which in turn is part of the huge US-based Home Shopping Network.\n\nSimilarly, SurplusAuction is an arm of Egghead stores, U-bid a division of Creative Computers, and Webauction part of Microwarehouse - all companies that sell computer equipment in the ordinary way. For them, such merchant auctions are simply a natural extension of their activities.\n\nOther players in this area include Onsale, Dealdeal and FairMarket. There are also some Europe-based merchant auction houses, like Quixell, (there is also a German version) and Online Auctions UK.\n\nGiven the large and expanding list of auction houses, it is obviously difficult to track them all. An interesting service in this context is Lycos' AuctionConnect, which searches other merchant auctions for certain items then notifies users.\n\nThe other major class of auctions are those between individuals: what might be called personal auctions. Probably the most famous firm here is Ebay which is notable for being one of the few publicly-traded Internet companies to turn a profit. Ebay makes its money from charging sellers an initial insertion fee and commission on sold items.\n\nOf course, personal auctions are inherently more risky than merchant auctions, since buyers are dealing with private individuals, not firms, and it is generally much harder to obtain information about their trustworthiness. This will tend to make personal auctions of less interest to companies. Interestingly, the main way of safeguarding users who engage in personal auctions is through other users' comments on sellers.\n\nProtection\n\nA more formal approach to protecting users is offered by Auction Universe which sells a kind of insurance policy called BidSafe. Auction Universe is a subsidiary of Times Mirror, which owns seven major US newspapers, and these personal auction sites are a natural outgrowth of the classified advertising that sustains local newspapers.\n\nA dedicated Classifieds site, Classifieds2000, has added auctions. Classifieds2000 is a division of Excite, which is not alone in adding this service to its portal. Yahoo has started its own personal auctions area, and Lycos has promised to follow suit.\n\nIt may well be that this re-invention of classified ads online will prove to be as important a source of revenue for Web sites as banner advertising now is.\n\nBanner sizes\n\nThe commonest form of Web advertising is through the use of images with promotional messages placed on a Web page. Given that many people do not scroll all the way through to the end of a document, the prime position is \"above the fold\", in the initial screen displayed to visitors when they reach a site. In particular, ads are frequently found at the top of Web pages to ensure that they are the first thing seen (unless inline images have been turned off).\n\nThese banner ads, as they are generally called, have sprung up in a completely uncontrolled way; not surprisingly, given the Internet's general lack of supervisory bodies. As a result, the ads tend to be designed to fit in with the overall form of the Web page on which they appear. This means that there are currently hundreds of slightly different shapes and sizes employed for banners.\n\nFor the user this is not a problem, but it is for companies such as Microsoft that wish to place the same advertisement in hundreds of sites. Rather than designing the promotional image for one or two standard sizes, it must be tweaked to fit the demands of particular pages.\n\nThis is clearly extremely inefficient from the advertiser's viewpoint, and indicative of the immaturity of the Web advertising market.\n\nTo combat this, the Internet Advertising Bureau and the Coalition for Advertising Supported Information and Entertainment have drawn up some standard sizes for banner advertisements. Unfortunately, to date there has been little effort to enforce them, and so the current banner size anarchy continues.\n\nBinHex\n\nThe file extension .hqx refers to the BinHex format, commonly-encountered in the context of Macintoshes, and occasionally seen elsewhere on the Internet too.\n\nMacintosh files are unusual in that they can consist of two pa"
    }
}
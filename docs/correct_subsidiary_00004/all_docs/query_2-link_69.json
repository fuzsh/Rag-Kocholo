{
    "id": "correct_subsidiary_00004_2",
    "rank": 69,
    "data": {
        "url": "https://www.science.gov/topicpages/h/hewlett%2Bpackard%2Bmodel",
        "read_more_link": "",
        "language": "en",
        "title": "hewlett packard model: Topics by Science.gov",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.science.gov/scigov/desktop/en/images/SciGov_logo.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Actual Operation Simulation of RESSOX Ground Experiments\n\nDTIC Science & Technology\n\n2010-11-01\n\nNote 1289,â (Hewlett-Packard), 60 pp. [5] F. Tappero, A. Dempster, T . Iwata, M. Imae, T . Ikegami , Y. Fukuyama, K. Hagimoto, and A. Iwasaki, 2006...Delay Ionospheric Delay Ku-band Delay Estimation Frequency- Independent part Frequency- Dependent part Elapsed Time with Current Time Set as 0 (s) T ...im e t o b e A d ju s te d 0-6-105 Extrapolated with First- Order Least-Squares Filter Time Adjustment Command 42 nd Annual Precise Time and Time\n\nData Transfers Among the HP-75, HP-86, and HP-9845 Microcomputers.\n\nDTIC Science & Technology\n\n1983-01-01\n\nAD-A139 438 DAT TRANSFERS AMONG THE HP-75 HP-86 AND HP-9845 / MICROCOMPUTENS(U) AIR FORCE INST OF TECH WNIOHT-PATTERSON AFN OH D P CONNOR 1983...hereafter called the ඓ\") and the HP-86 (hereafter called the ඞ\"). The computers are to be used for classroom instruction and research at SOC. On...the main campus another Hewlett-Packard desktop computer, the HP-9845 (hereafter called the 񕚕\"), is already in use; it controls and processes data\n\n76 FR 4725 - Hewlett Packard Company Application Services Division Including Workers Whose Unemployment...\n\nFederal Register 2010, 2011, 2012, 2013, 2014\n\n2011-01-26\n\n... Universal Music Group; Fishers, IN; Amended Certification Regarding Eligibility To Apply for Worker... of Universal Music Group and that some workers separated from employment at the Fishers, Indiana... unemployment insurance (UI) tax account under the name Universal Music Group. Accordingly, the Department is...\n\n78 FR 48466 - Hewlett Packard Company, Printing & Personal System Americas Division, Marketing Services...\n\nFederal Register 2010, 2011, 2012, 2013, 2014\n\n2013-08-08\n\n..., Printing & Personal System Americas Division, Marketing Services, Houston, Texas; Notice of Investigation... Division, Marketing Services, Houston, Texas. On January 25, 2013, the Department issued a Notice of... & Personal System Americas Division, Marketing Services, Houston, Texas) to be filed. Because the later-filed...\n\nGraphics-Printing Program For The HP Paintjet Printer\n\nNASA Technical Reports Server (NTRS)\n\nAtkins, Victor R.\n\n1993-01-01\n\nIMPRINT utility computer program developed to print graphics specified in raster files by use of Hewlett-Packard Paintjet(TM) color printer. Reads bit-mapped images from files on UNIX-based graphics workstation and prints out three different types of images: wire-frame images, solid-color images, and gray-scale images. Wire-frame images are in continuous tone or, in case of low resolution, in random gray scale. In case of color images, IMPRINT also prints by use of default palette of solid colors. Written in C language.\n\nThe Quorum-Sensing Regulon of Vibriofischeri: Novel Components of the Autoinducer/LuxR Regulatory Circuit\n\nDTIC Science & Technology\n\n1999-06-01\n\ncpdP, from the marine symbiotic bacterium Vibrio fische ri 160 Table of abbreviations 30C6-HSL AI-1 AI-2 C8-HSL CHAPS CNP EDTA FMN GFP HPLC ...using a Zorbax C18 1.0 mm by 150 mm reverse-phase column on a Hewlett-Packard 1090 HPLC /1040 diode array detector at the Harvard Microchemistry...separated by reversed-phase HPLC , and sequenced (Table 2; 10-PK12, 10-PK39, and 10-PK51). From two of the three peptide sequences (Materials and\n\nDollar Summary of Prime Contract Awards by State, Place, and Contractor, FY84, Part 4, (Allenville, Missouri-Wyoming, Pennsylvania).\n\nDTIC Science & Technology\n\n1984-01-01\n\nHEATHER DRUG CO INC 182 182 HEATHER DRUG CO INC 50 50 HEATHER DRUG CO INC 81 81 HUSSMANN CORPORATION 76 76 INFORMATION SPECTRUM INC 519 319 74 INFORTRON...PISCATAWAY BEECHAM INC 151 151 BLOCK DRUG COMPANY INC 29 29 DIGITAL EQUIPMENT CORPORATION 692 692 HEWLETT PACKARD CO 206 206 INGERSOLL-RAND COMPANY INC 26...FOOD & DRUG RESEARCH LABS 131 131 VALLEY FORGE FLAG CO INC 1,504 1,504 TOTAL - VERONA 2,126 131 144 35 1,816 VINCENTOWN K I M CONSTRUCTION COMPANY INC\n\nInstantaneous relationship between solar inertial and local vertical local horizontal attitudes\n\nNASA Technical Reports Server (NTRS)\n\nVickery, S. A.\n\n1977-01-01\n\nThe instantaneous relationship between the Solar Inertial (SI) and Local Vertical Local Horizontal (LVLH) coordinate systems is derived. A method is presented for computation of the LVLH to SI rotational transformation matrix as a function of an input LVLH attitude and the corresponding look angles to the sun. Logic is provided for conversion between LVLH and SI attitudes expressed in terms of a pitch, yaw, roll Euler sequence. Documentation is included for a program which implements the logic on the Hewlett-Packard 97 programmable calculator.\n\nWOLF; automatic typing program\n\nUSGS Publications Warehouse\n\nEvenden, G.I.\n\n1982-01-01\n\nA FORTRAN IV program for the Hewlett-Packard 1000 series computer provides for automatic typing operations and can, when employed with manufacturer's text editor, provide a system to greatly facilitate preparation of reports, letters and other text. The input text and imbedded control data can perform nearly all of the functions of a typist. A few of the features available are centering, titles, footnotes, indentation, page numbering (including Roman numerals), automatic paragraphing, and two forms of tab operations. This documentation contains both user and technical description of the program.\n\nProceedings of the Antenna Applications Symposium Held in Hanscom AFB, Massachusetts on 23-25 September 1987. Volume 2\n\nDTIC Science & Technology\n\n1988-07-01\n\niv BROADBAND ANTENNAS Contents 21. \"Performance Characteristics of Notch Array Elements Over a 6/1 263 Frequoncy Band,\" C. J. Monser * 22. \" Broadband ...synthesized RF signal is taken from the output port of the Hewlett-Packard 8408B Microvae Network Alnalyzer to a 20-watt TWT anplifier, and then to...j kr -Sinn H 0 = -jk 47rjk Â° a 2 H r4 0 r 406 4. Helix antenna The cur-ent distribution of a helix antenna as shown in Fig. 2.11 of [33 may be\n\nNASA Future Forum\n\nNASA Image and Video Library\n\n2012-02-21\n\nMichael Donovan, technology consultant, New Services Development, Hewlett-Packard Company talks during the NASA Future Forum panel titled \"Importance of Technology, Science and Innovation for our Economic Future\" at The Ohio State University on Tuesday, Feb. 21, 2012 in Columbus, Ohio. The NASA Future Forum features panel discussions on the importance of education to our nation's future in space, the benefit of commercialized space technology to our economy and lives here on Earth, and the shifting roles for the public, commercial and international communities in space. Photo Credit: (NASA/Bill Ingalls)\n\nAccurate Dynamic Response Predictions of Plug-and-Play Sat I\n\nDTIC Science & Technology\n\n2010-03-01\n\ndamping. The foam pads are necessary to damp out the s ystem b etween s trikes f rom t he s haker . Elevating t he f oam p ads p rovides i ncreased... haker set to a ct as an au tomatic p ing h ammer ( Figure 15) provides impulse like excitiations. A Hewlett Packard 33120A 15MHz/Arbitray...n M B Dynamics C al50 E xciter el ectrodynamic s haker b eing d riven b y a H ewlett P ackard 33120A 15M Hz/Arbitrary waveform generator p\n\nBread: CDC 7600 program that processes Spent Fuel Test Climax data\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nHage, G.L.\n\nBREAD will process a family of files copied from a data tape made by Hewlett-Packard equipment employed for data acquisition on the Spent Fuel Test-Climax at NTS. Tapes are delivered to Livermore approximately monthly. The process at this stage consists of four steps: read the binary files and convert from H-P 16-bit words to CDC 7600 60-bit words; check identification and data ranges; write the data in 6-bit ASCII (BCD) format, one data point per line; then sort the file by identifier and time.\n\n77 FR 4023 - Hewlett-Packard Company, Provisional Acceptance of a Settlement Agreement and Order\n\nFederal Register 2010, 2011, 2012, 2013, 2014\n\n2012-01-26\n\n... December 2004 and July 2006, HP imported approximately 32,000 lithium-ion battery packs (the ``Products... April 2007, HP conducted a study, from which it obtained additional information about the Products. 9...\n\n76 FR 10403 - Hewlett Packard (HP), Global Product Development, Engineering Workstation Refresh Team, Working...\n\nFederal Register 2010, 2011, 2012, 2013, 2014\n\n2011-02-24\n\n...), Global Product Development, Engineering Workstation Refresh Team, Working On-Site at General Motors... groups: The Non-Information Technology Business Development Team, the Engineering Application Support Team, and the Engineering Workstation Refresh Team. On February 2, 2011, the Department issued an...\n\nRELBET 4.0 user's guide\n\nNASA Technical Reports Server (NTRS)\n\nCerbins, F. C.; Huysman, B. P.; Knoedler, J. K.; Kwong, P. S.; Pieniazek, L. A.; Strom, S. W.\n\n1986-01-01\n\nThis manual describes the operation and use of RELBET 4.0 implemented on the Hewlett Packard model 9000. The RELBET System is an integrated collection of computer programs which support the analysis and post-flight reconstruction of vehicle to vehicle relative trajectories of two on-orbit free-flying vehicles: the Space Shuttle Orbiter and some other free-flyer. The manual serves both as a reference and as a training guide. Appendices provide experienced users with details and full explanations of program usage. The body of the manual introduces new users to the system by leading them through a step by step example of a typical production. This should equip the new user both to execute a typical production process and to understand the most significant variables in that process.\n\nThe effects of FreeSurfer version, workstation type, and Macintosh operating system version on anatomical volume and cortical thickness measurements.\n\nPubMed\n\nGronenschild, Ed H B M; Habets, Petra; Jacobs, Heidi I L; Mengelers, Ron; Rozendaal, Nico; van Os, Jim; Marcelis, Machteld\n\n2012-01-01\n\nFreeSurfer is a popular software package to measure cortical thickness and volume of neuroanatomical structures. However, little if any is known about measurement reliability across various data processing conditions. Using a set of 30 anatomical T1-weighted 3T MRI scans, we investigated the effects of data processing variables such as FreeSurfer version (v4.3.1, v4.5.0, and v5.0.0), workstation (Macintosh and Hewlett-Packard), and Macintosh operating system version (OSX 10.5 and OSX 10.6). Significant differences were revealed between FreeSurfer version v5.0.0 and the two earlier versions. These differences were on average 8.8 Â± 6.6% (range 1.3-64.0%) (volume) and 2.8 Â± 1.3% (1.1-7.7%) (cortical thickness). About a factor two smaller differences were detected between Macintosh and Hewlett-Packard workstations and between OSX 10.5 and OSX 10.6. The observed differences are similar in magnitude as effect sizes reported in accuracy evaluations and neurodegenerative studies.The main conclusion is that in the context of an ongoing study, users are discouraged to update to a new major release of either FreeSurfer or operating system or to switch to a different type of workstation without repeating the analysis; results thus give a quantitative support to successive recommendations stated by FreeSurfer developers over the years. Moreover, in view of the large and significant cross-version differences, it is concluded that formal assessment of the accuracy of FreeSurfer is desirable.\n\nThe Effects of FreeSurfer Version, Workstation Type, and Macintosh Operating System Version on Anatomical Volume and Cortical Thickness Measurements\n\nPubMed Central\n\nGronenschild, Ed H. B. M.; Habets, Petra; Jacobs, Heidi I. L.; Mengelers, Ron; Rozendaal, Nico; van Os, Jim; Marcelis, Machteld\n\n2012-01-01\n\nFreeSurfer is a popular software package to measure cortical thickness and volume of neuroanatomical structures. However, little if any is known about measurement reliability across various data processing conditions. Using a set of 30 anatomical T1-weighted 3T MRI scans, we investigated the effects of data processing variables such as FreeSurfer version (v4.3.1, v4.5.0, and v5.0.0), workstation (Macintosh and Hewlett-Packard), and Macintosh operating system version (OSX 10.5 and OSX 10.6). Significant differences were revealed between FreeSurfer version v5.0.0 and the two earlier versions. These differences were on average 8.8Â±6.6% (range 1.3â64.0%) (volume) and 2.8Â±1.3% (1.1â7.7%) (cortical thickness). About a factor two smaller differences were detected between Macintosh and Hewlett-Packard workstations and between OSX 10.5 and OSX 10.6. The observed differences are similar in magnitude as effect sizes reported in accuracy evaluations and neurodegenerative studies. The main conclusion is that in the context of an ongoing study, users are discouraged to update to a new major release of either FreeSurfer or operating system or to switch to a different type of workstation without repeating the analysis; results thus give a quantitative support to successive recommendations stated by FreeSurfer developers over the years. Moreover, in view of the large and significant cross-version differences, it is concluded that formal assessment of the accuracy of FreeSurfer is desirable. PMID:22675527\n\n76 FR 46854 - Hewlett Packard Company, Imaging and Printing Group, World Wide Product Data Management...\n\nFederal Register 2010, 2011, 2012, 2013, 2014\n\n2011-08-03\n\n..., Imaging and Printing Group, World Wide Product Data Management Operations, Including On-Site Leased... Company, Imaging and Printing Group, World Wide Products Data Management Operations, Boise, Idaho and Fort... Management Operations. The Department has determined that these workers were sufficiently under the control...\n\n[Short-term and long-term fetal heart rate variability after amnioinfusion treatment of oligohydramnios complicated pregnancy].\n\nPubMed\n\nMachalski, T; Sikora, J; Bakon, I; Magnucki, J; Grzesiak-Kubica, E; Szkodny, E\n\n2001-12-01\n\nResults of computerised analysis of cardiotocograms obtained in the group of 21 pregnancies complicated by idiopathic oligohydramnios are presented in the study. Amnioinfusion procedures were administered serially in local anesthesia with ultrasound and colour Doppler control on the base of oligohydramnios criteria by Phelan. The analysis was based on KOMPOR software created by ITAM Zabrze based on PC computer connected to Hewlett-Packard Series 50A cardiotocograph. Significant short-term variability increase just after amnioinfusion procedure from 5.55 ms to 8.24 ms and after 24 hours up to 7.25 ms was found, while significant long-term variability values changes were not observed.\n\nColor management in the real world: sRGB, ICM2, ICC, ColorSync, and other attempts to make color management transparent\n\nNASA Astrophysics Data System (ADS)\n\nStokes, Michael\n\n1998-07-01\n\nA uniformly adopted color standards infrastructure has a dramatic impact on any color imaging industry and technology. This presentation begins by framing the current color standards situation in a historical context. A series of similar appearing infrastructure adoptions in color publishing during the last fifty years are reviewed and compared to the current events. This historical review is followed by brief technical, business and marketing reviews of two of the more popular recent color standards proposals, sRGB and ICC, along with their operating system implementations in the Microsoft and Apple operating systems. The paper concludes with a summary of Hewlett- Packard Company's and Microsoft's proposed future direction.\n\nMesure du Deplacement de Frequence au Maximum de la Raie P20 un Laser CO2 a Ondes Guidees (Measurement of the Frequency Shift at the Peak of the P20 Line of a CO2 Waveguide Laser),\n\nDTIC Science & Technology\n\n1986-06-01\n\nVINCENT ET AL . UNCLASSIFIED JUN 86 DREV-4419/86 F/G 28/5 NI iiiimEEmmomEEI 111-25 W1.6 * AD-A170 318 WIAI iLL. f.Pf I)E V jP ’)MEN, fFIlA 4 H l t A 4...triangulaires Hewlett-Packard, modale 3310B (OSC. 1), dont le signal est amplifi6 par un amplificateur haute ten- sion Lansing, modale 80.315 (PZT AL ...CLASSIFICATION 3 PT AL O.G AL CONTROLE * OSC’ I V-~ ----- GAZ 05; C. - - SPEC-. I -_ _ - - 0 G. -ET SYNC 2 REF. - L - BP -- CSPEC 2 - DET. PYR1C - \" VDET\n\nNREL's Building-Integrated Supercomputer Provides Heating and Efficient Computing (Fact Sheet)\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nNot Available\n\n2014-09-01\n\nNREL's Energy Systems Integration Facility (ESIF) is meant to investigate new ways to integrate energy sources so they work together efficiently, and one of the key tools to that investigation, a new supercomputer, is itself a prime example of energy systems integration. NREL teamed with Hewlett-Packard (HP) and Intel to develop the innovative warm-water, liquid-cooled Peregrine supercomputer, which not only operates efficiently but also serves as the primary source of building heat for ESIF offices and laboratories. This innovative high-performance computer (HPC) can perform more than a quadrillion calculations per second as part of the world's most energy-efficient HPC datamoreÂ Â» center.Â«Â less\n\nBLISS: a computer program for the protection of blood donors. Technical report\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nCatsimpoolas, N.; Cooke, C.; Valeri, C.R.\n\n1982-06-28\n\nA BASIC program has been developed for the Hewlett-Packard Model 9845 desk-top computer which allows the creation of blood donor files for subsequent retrieval, update, and correction. A similar modified version was developed for hte HP 9835 Model. This software system has been called BLISS which stands for Blood Information and Security System. In addition to its function as a file management system, BLISS provides warnings before a donation is performed to protect the donor from excessive exposure to radioactivity and DMSO levels, from too frequent of donations of blood, and from adverse reactions. The program can also be usedmoreÂ Â» to select donors who have participated in specific studies and to list the experimental details which have been stored in the file. The BLISS system has been actively utilized at the Naval Blood Research Laboratory in Boston and contains the files of over 750 donors.Â«Â less\n\nA randomized controlled trial of efficacy and ST change following use of the Welch-Allyn MRL PIC biphasic waveform versus damped sine monophasic waveform for external DC cardioversion.\n\nPubMed\n\nAmbler, Jonathan J S; Deakin, Charles D\n\n2006-11-01\n\nBiphasic waveforms have similar or greater efficacy at cardioverting atrial and ventricular arrhythmias at lower energy levels than monophasic waveforms, and cause less ST depression following defibrillation of ventricular fibrillation. No studies have investigated this effect on ST change with atrial arrhythmias. We studied the efficacy of the Welch Allyn-MRL PIC biphasic defibrillator. One hundred and thirty-nine patients undergoing elective DC cardioversion for atrial arrhythmias were randomised to cardioversion by monophasic (Hewlett Packard Codemaster XL; 100, 200, 300, 360 and 360J) or biphasic (Welch Allyn-MRL PIC; 70, 100, 150, 200 and 300J) defibrillator. We analysed success of cardioversion after 0 and 30min, cumulative energy, number of shocks and energy at successful cardioversion. The ST change in the recorded electrocardiogram was measured at 15s after all shocks using electronic callipers. Immediately after cardioversion 59/68 (86.8%) of the monophasic group versus 56/60 (93.3%) of the biphasic group were in sinus rhythm. Of the monophasic group, 55/67 (82.1%) remained in sinus rhythm at 30min versus 53/58 (91.4%) of the biphasic group. These differences were not significant at 0min (P=0.35) or 30min (P=0.21). The biphasic group required significantly fewer shocks (P=0.006), less cumulative energy (P<0.0001) and required lower total energy for successful cardioversion (P<0.0001). Of the 102 patients with electrocardiogram recordings suitable for analysis, ST segment change was greater in the monophasic group (P=0.037). The Welch Allyn-MRL biphasic waveform for DC cardioversion results in fewer shocks, with less cumulative energy delivered and less post shock ST change than with a Hewlett Packard Codemaster XL damped sine wave monophasic waveform.\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nWessol, D.E.; Wheeler, F.J.; Babcock, R.S.\n\nSeveral improvements have been developed for the BNCT radiation treatment planning environment (BNCT-Rtpe) during 1994. These improvements have been incorporated into Version 1.0 of BNCT-Rtpe which is currently installed at the INEL, BNL, Japanese Research Center (JRC), and Finland`s Technical Research Center. Platforms supported by this software include Hewlett-Packard (HP), SUN, International Business Machines (IBM), and Silicon Graphics Incorporated (SGI). A draft version of the BNCT-Rtpe user manual is available. Version 1.1 of BNCT-Rtpe is scheduled for release in March 1995. It is anticipated that Version 2.x of BNCT-Rtpe, which includes the nonproprietary NURBS library and data structures, will bemoreÂ Â» released in September 1995.Â«Â less\n\nA study of workstation computational performance for real-time flight simulation\n\nNASA Technical Reports Server (NTRS)\n\nMaddalon, Jeffrey M.; Cleveland, Jeff I., II\n\n1995-01-01\n\nWith recent advances in microprocessor technology, some have suggested that modern workstations provide enough computational power to properly operate a real-time simulation. This paper presents the results of a computational benchmark, based on actual real-time flight simulation code used at Langley Research Center, which was executed on various workstation-class machines. The benchmark was executed on different machines from several companies including: CONVEX Computer Corporation, Cray Research, Digital Equipment Corporation, Hewlett-Packard, Intel, International Business Machines, Silicon Graphics, and Sun Microsystems. The machines are compared by their execution speed, computational accuracy, and porting effort. The results of this study show that the raw computational power needed for real-time simulation is now offered by workstations.\n\nManifold and method of batch measurement of Hg-196 concentration using a mass spectrometer\n\nDOEpatents\n\nGrossman, M.W.; Evans, R.\n\n1991-11-26\n\nA sample manifold and method of its use has been developed so that milligram quantities of mercury can be analyzed mass spectroscopically to determine the [sup 196]Hg concentration to less than 0.02 atomic percent. Using natural mercury as a standard, accuracy of [+-]0.002 atomic percent can be obtained. The mass spectrometer preferably used is a commercially available GC/MS manufactured by Hewlett Packard. A novel sample manifold is contained within an oven allowing flow rate control of Hg into the MS. Another part of the manifold connects to an auxiliary pumping system which facilitates rapid clean up of residual Hg in the manifold. Sample cycle time is about 1 hour. 8 figures.\n\nOptimization of wide-area ATM and local-area ethernet/FDDI network configurations for high-speed telemedicine communications employing NASA's ACTS.\n\nPubMed\n\nMcDermott, W R; Tri, J L; Mitchell, M P; Levens, S P; Wondrow, M A; Huie, L M; Khandheria, B K; Gilbert, B K\n\n1999-01-01\n\nA high data rate terrestrial and satellite network was implemented to transfer medical images and data. This article describes the a optimization of the workstations and switching equipment incorporated into the network. Topics discussed in this article include tuning of the network software, the configuration of the Sun Microsystems workstations, the FORE Systems asynchronous transfer mode switches, as well as the throughput results of two telemedicine experiments undertaken by Mayo's physician staff. The technical staff was successful in achieving the data throughput needed by the telemedicine software; particularly important was the proper determination of peak throughput and TCP window sizes to ensure optimum use of the resources available on the Sun Microsystems and Hewlett Packard workstations.\n\nManifold and method of batch measurement of Hg-196 concentration using a mass spectrometer\n\nDOEpatents\n\nGrossman, Mark W.; Evans, Roger\n\n1991-01-01\n\nA sample manifold and method of its use has been developed so that milligram quantities of mercury can be analyzed mass spectroscopically to determine the .sup.196 Hg concentration to less than 0.02 atomic percent. Using natural mercury as a standard, accuracy of .+-.0.002 atomic percent can be obtained. The mass spectrometer preferably used is a commercially available GC/MS manufactured by Hewlett Packard. A novel sample manifold is contained within an oven allowing flow rate control of Hg into the MS. Another part of the manifold connects to an auxiliary pumping system which facilitates rapid clean up of residual Hg in the manifold. Sample cycle time is about 1 hour.\n\nEvaluation of space shuttle main engine fluid dynamic frequency response characteristics\n\nNASA Technical Reports Server (NTRS)\n\nGardner, T. G.\n\n1980-01-01\n\nIn order to determine the POGO stability characteristics of the space shuttle main engine liquid oxygen (LOX) system, the fluid dynamic frequency response functions between elements in the SSME LOX system was evaluated, both analytically and experimentally. For the experimental data evaluation, a software package was written for the Hewlett-Packard 5451C Fourier analyzer. The POGO analysis software is documented and consists of five separate segments. Each segment is stored on the 5451C disc as an individual program and performs its own unique function. Two separate data reduction methods, a signal calibration, coherence or pulser signal based frequency response function blanking, and automatic plotting features are included in the program. The 5451C allows variable parameter transfer from program to program. This feature is used to advantage and requires only minimal user interface during the data reduction process. Experimental results are included and compared with the analytical predictions in order to adjust the general model and arrive at a realistic simulation of the POGO characteristics.\n\nInteractive graphics system for IBM 1800 computer\n\nNASA Technical Reports Server (NTRS)\n\nCarleton, T. P.; Howell, D. R.; Mish, W. H.\n\n1972-01-01\n\nA FORTRAN compatible software system that has been developed to provide an interactive graphics capability for the IBM 1800 computer is described. The interactive graphics hardware consists of a Hewlett-Packard 1300A cathode ray tube, Sanders photopen, digital to analog converters, pulse counter, and necessary interface. The hardware is available from IBM as several related RPQ's. The software developed permits the application programmer to use IBM 1800 FORTRAN to develop a display on the cathode ray tube which consists of one or more independent units called pictures. The software permits a great deal of flexibility in the manipulation of these pictures and allows the programmer to use the photopen to interact with the displayed data and make decisions based on information returned by the photopen.\n\nDevelopment of a High Strength Isothermally Heat-Treated Nodular Iron Road Wheel Arm\n\nDTIC Science & Technology\n\n1985-03-31\n\ncapacity load cell was calibrated using a Satec Universal Test System and a Hewlett-Packard X,Y Plotter to record the calibrated curve. The load cell...12e 1,3- 0 s0 3 I I r~ I I Il.1660 119 13. 30 1 3,1Ã½4-514 1 1 o36 1~~ 123 1, d 51 7 4~ 14 ~ 3.â¢ 3I 2k i 7, G 0 se, y 2 es I Q.~ 141/ 14( 13.0130 1B14...LOT BAR QCH YIELD TESS. ELON(. Rc Rc 0HNCARPY LENGTH CaVNT. Nio. No. TIME .000 1O..O % Ã½Ma-crol~licrd IFt Lb INCH~ ES IOU 2-77 â¢,3~-341.4___ 1 79 7,o\n\nCompiling and editing agricultural strata boundaries with remotely sensed imagery and map attribute data using graphics workstations\n\nNASA Technical Reports Server (NTRS)\n\nCheng, Thomas D.; Angelici, Gary L.; Slye, Robert E.; Ma, Matt\n\n1991-01-01\n\nThe USDA presently uses labor-intensive photographic interpretation procedures to delineate large geographical areas into manageable size sampling units for the estimation of domestic crop and livestock production. Computer software to automate the boundary delineation procedure, called the computer-assisted stratification and sampling (CASS) system, was developed using a Hewlett Packard color-graphics workstation. The CASS procedures display Thematic Mapper (TM) satellite digital imagery on a graphics display workstation as the backdrop for the onscreen delineation of sampling units. USGS Digital Line Graph (DLG) data for roads and waterways are displayed over the TM imagery to aid in identifying potential sample unit boundaries. Initial analysis conducted with three Missouri counties indicated that CASS was six times faster than the manual techniques in delineating sampling units.\n\nDetermining blood and plasma volumes using bioelectrical response spectroscopy\n\nNASA Technical Reports Server (NTRS)\n\nSiconolfi, S. F.; Nusynowitz, M. L.; Suire, S. S.; Moore, A. D. Jr; Leig, J.\n\n1996-01-01\n\nWe hypothesized that an electric field (inductance) produced by charged blood components passing through the many branches of arteries and veins could assess total blood volume (TBV) or plasma volume (PV). Individual (N = 29) electrical circuits (inductors, two resistors, and a capacitor) were determined from bioelectrical response spectroscopy (BERS) using a Hewlett Packard 4284A Precision LCR Meter. Inductance, capacitance, and resistance from the circuits of 19 subjects modeled TBV (sum of PV and computed red cell volume) and PV (based on 125I-albumin). Each model (N = 10, cross validation group) had good validity based on 1) mean differences (-2.3 to 1.5%) between the methods that were not significant and less than the propagated errors (+/- 5.2% for TBV and PV), 2) high correlations (r > 0.92) with low SEE (< 7.7%) between dilution and BERS assessments, and 3) Bland-Altman pairwise comparisons that indicated \"clinical equivalency\" between the methods. Given the limitation of this study (10 validity subjects), we concluded that BERS models accurately assessed TBV and PV. Further evaluations of the models' validities are needed before they are used in clinical or research settings.\n\n78 FR 48472 - Hewlett Packard Company; Enterprise Storage Servers and Networking (Tape) Group; Formerly D/B/A...\n\nFederal Register 2010, 2011, 2012, 2013, 2014\n\n2013-08-08\n\n..., as amended, an investigation was initiated on May 20, 2013 in response to a petition filed on behalf... this 9th day of July, 2013. Del Min Amy Chen, Certifying Officer, Office of Trade Adjustment Assistance...\n\nQuantum auctions: Facts and myths\n\nNASA Astrophysics Data System (ADS)\n\nPiotrowski, Edward W.; SÅadkowski, Jan\n\n2008-06-01\n\nQuantum game theory, whatever opinions may be held due to its abstract physical formalism, have already found various applications even outside the orthodox physics domain. In this paper we introduce the concept of a quantum auction, its advantages and drawbacks. Then we describe the models that have already been put forward. A general model involves Wigner formalism and infinite dimensional Hilbert spaces - we envisage that the implementation might not be an easy task. But a restricted model advocated by the Hewlett-Packard group (Hogg et al.) seems to be much easier to implement. We focus on problems related to combinatorial auctions and technical assumptions that are made. Powerful quantum algorithms for finding solutions would extend the range of possible applications. Quantum strategies, being qubits, can be teleported but are immune from cloning - therefore extreme privacy of the agentâs activity could in principle be guaranteed. Then we point out some key problems that have to be solved before commercial use would be possible. With present technology, optical networks, single photon sources and detectors seems to be sufficient for an experimental realization in the near future.\n\nRegion stability analysis and tracking control of memristive recurrent neural network.\n\nPubMed\n\nBao, Gang; Zeng, Zhigang; Shen, Yanjun\n\n2018-02-01\n\nMemristor is firstly postulated by Leon Chua and realized by Hewlett-Packard (HP) laboratory. Research results show that memristor can be used to simulate the synapses of neurons. This paper presents a class of recurrent neural network with HP memristors. Firstly, it shows that memristive recurrent neural network has more compound dynamics than the traditional recurrent neural network by simulations. Then it derives that n dimensional memristive recurrent neural network is composed of [Formula: see text] sub neural networks which do not have a common equilibrium point. By designing the tracking controller, it can make memristive neural network being convergent to the desired sub neural network. At last, two numerical examples are given to verify the validity of our result. Copyright Â© 2017 Elsevier Ltd. All rights reserved.\n\nLinear combination reading program for capture gamma rays\n\nUSGS Publications Warehouse\n\nTanner, Allan B.\n\n1971-01-01\n\nThis program computes a weighting function, Qj, which gives a scalar output value of unity when applied to the spectrum of a desired element and a minimum value (considering statistics) when applied to spectra of materials not containing the desired element. Intermediate values are obtained for materials containing the desired element, in proportion to the amount of the element they contain. The program is written in the BASIC language in a format specific to the Hewlett-Packard 2000A Time-Sharing System, and is an adaptation of an earlier program for linear combination reading for X-ray fluorescence analysis (Tanner and Brinkerhoff, 1971). Following the program is a sample run from a study of the application of the linear combination technique to capture-gamma-ray analysis for calcium (report in preparation).\n\nStudies of ferroelectric and dielectric properties of pure and doped barium titanate prepared by sol-gel method\n\nNASA Astrophysics Data System (ADS)\n\nBisen, Supriya; Mishra, Ashutosh; Jarabana, Kanaka M.\n\n2016-05-01\n\nIn this work, Barium Titanate (BaTiO3) powders were synthesized via Sol-Gel auto combustion method using citric acid as a chelating agent. We study the behavior of ferroelectric and dielectric properties of pure and doped BaTiO3 on different concentration. To understand the phase and structure of the powder calcined at 900Â°C were characterized by X-ray Diffraction shows that tetragonal phase is dominant for pure and doped BTO and data fitted by Rietveld Refinement. Electric and Dielectric properties were characterized by P-E Hysteresis and Dielectric measurement. In P-E measurement ferroelectric loop tracer applied for different voltage. The temperature dependant dielectric constant behavior was observed as a function of frequency recorded on hp-Hewlett Packard 4192A, LF impedance, 5Hz-13Hz analyzer.\n\nImpact of workstations on criticality analyses at ABB combustion engineering\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nTarko, L.B.; Freeman, R.S.; O'Donnell, P.F.\n\n1993-01-01\n\nDuring 1991, ABB Combustion Engineering (ABB C-E) made the transition from a CDC Cyber 990 mainframe for nuclear criticality safety analyses to Hewlett Packard (HP)/Apollo workstations. The primary motivation for this change was improved economics of the workstation and maintaining state-of-the-art technology. The Cyber 990 utilized the NOS operating system with a 60-bit word size. The CPU memory size was limited to 131 100 words of directly addressable memory with an extended 250000 words available. The Apollo workstation environment at ABB consists of HP/Apollo-9000/400 series desktop units used by most application engineers, networked with HP/Apollo DN10000 platforms that use 32-bitmoreÂ Â» word size and function as the computer servers and network administrative CPUS, providing a virtual memory system.Â«Â less\n\nErgonomic Chairs\n\nNASA Technical Reports Server (NTRS)\n\n1997-01-01\n\nFindings published in the NASA Anthropometric Source Book by Johnson Space Center helped BodyBilt, Inc. to fashion controlled comfort chairs that lessen the harmful effects of gravity on seated workers. Crew members living aboard NASA's Skylab noted that in space the human posture differs from the normal posture caused by the tug of one gravity. There has been an alarming increase in back pain and muscle fatigue in workers, along with a dramatic escalation in repetitive stress injuries. BodyBilt's ergonomically-correct line of office chairs are targeted for the average worker that sits for prolonged periods, be it in the classroom or boardroom. Their roster of national clients lists such organizations as IBM, Microsoft, Texas Instruments, Hewlett-Packard, Eastman-Kodak, Boeing, Motorola, and Walt Disney Studios.\n\nA high-pressure liquid chromatographic method for the determination of N-acetyl-p-aminophenol (acetaminophen) in serum or plasma using a direct injection technique.\n\nPubMed\n\nManno, B R; Manno, J E; Dempsey, C A; Wood, M A\n\n1981-01-01\n\nN-Acetyl-p-aminophenol (acetaminophen) is becoming more prevalent as an intoxicant in accidental or intentional overdose, therefore, a direct injection ultra-micro high-pressure liquid chromatographic (HPLC) method has been developed for its quantitation. The HPLC analysis was performed using a Model 110 Solvent Metering Pump equipped with a Model 110-19 Pressure Filter (Altex Scientific, Berkeley, CA), a Model 7120 Rheodyne Injector (Rheodyne, Berkeley, CA) or a Model U6K Injector (Waters Associates, Milford, MA) a Model 440 Absorbance Detector (Water's Associates), and a Model 3380A Recorder Integrator (Hewlett Packard, Avondale, PA). A commercially prepared muBonapak C18 Column (Water's Associates) was used. Acetaminophen was eluted with a mixture of 0.01 mol/L aqueous sodium acetate, pH 4.0: acetonitrile (93:7) and the absorbance detector was operated wih a 254 nm filter. The method, which requires only 2 microL of serum or plasma for analysis, offers several distinct advantages to the analyst. No pre- or post-column extraction or other manipulation of the specimen is required to obtain a quantitative result. Rapid processing of the specimen is possible because both acetaminophen and the internal standard are eluted in less than 10 minutes. The small sample (2 microL) is ideal for use with pediatric patients.\n\nFuel-conservation evaluation of US Army helicopters. Part 6. Performance calculator evaluation. Final report for period ending January 1981\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nDominick, F.; Lockwood, R.A.\n\n1986-07-01\n\nThe US Army Aviation Engineering Flight Activity conducted an evaluation of Flight Management Calculator for the UH-1H. The calculator was a Hewlett-Packard HP-41CV. The performance calculator was evaluated for flight planning and in-flight use during 14 mission flights simulating operational conditions. The calculator was much easier to use in-flight than the operator's manual data. The calculator program needs improvement in the areas of pre-flight planning and execution speed. The mission flights demonstrated a 19% fuel saving using optimum over normal flight profiles in warm temperatures (15/sup 0/C above standard). Savings would be greater at colder temperatures because of increasing compressibilitymoreÂ Â» effects. Acceptable accuracy for individual aircraft under operational conditions may require a regressive analog model in which individual aircraft data are used to update the program. The performance data base for the UH-1H was expanded with level flight and hover data to thrust coefficients and Mach numbers to the practical limits of aircraft operation.Â«Â less\n\nHigh accuracy time transfer synchronization\n\nNASA Technical Reports Server (NTRS)\n\nWheeler, Paul J.; Koppang, Paul A.; Chalmers, David; Davis, Angela; Kubik, Anthony; Powell, William M.\n\n1995-01-01\n\nIn July 1994, the U.S. Naval Observatory (USNO) Time Service System Engineering Division conducted a field test to establish a baseline accuracy for two-way satellite time transfer synchronization. Three Hewlett-Packard model 5071 high performance cesium frequency standards were transported from the USNO in Washington, DC to Los Angeles, California in the USNO's mobile earth station. Two-Way Satellite Time Transfer links between the mobile earth station and the USNO were conducted each day of the trip, using the Naval Research Laboratory(NRL) designed spread spectrum modem, built by Allen Osborne Associates(AOA). A Motorola six channel GPS receiver was used to track the location and altitude of the mobile earth station and to provide coordinates for calculating Sagnac corrections for the two-way measurements, and relativistic corrections for the cesium clocks. This paper will discuss the trip, the measurement systems used and the results from the data collected. We will show the accuracy of using two-way satellite time transfer for synchronization and the performance of the three HP 5071 cesium clocks in an operational environment.\n\nInvestigation into the effects of VHF and UHF band radiation on Hewlett-Packard (HP) Cesium Beam Frequency Standards\n\nNASA Technical Reports Server (NTRS)\n\nDickens, Andrew\n\n1995-01-01\n\nThis paper documents an investigation into reports which have indicated that exposure to VHF and UHF band radiation has adverse effects on the frequency stability of HP cesium beam frequency standards. Tests carried out on the basis of these reports show that sources of VHF and UHF radiation such as two-way hand held police communications devices do cause reproducible adverse effects. This investigation examines reproducible effects and explores possible causes.\n\nStudies of ferroelectric and dielectric properties of pure and doped barium titanate prepared by sol-gel method\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nBisen, Supriya; Mishra, Ashutosh; Jarabana, Kanaka M.\n\n2016-05-23\n\nIn this work, Barium Titanate (BaTiO{sub 3}) powders were synthesized via Sol-Gel auto combustion method using citric acid as a chelating agent. We study the behavior of ferroelectric and dielectric properties of pure and doped BaTiO{sub 3} on different concentration. To understand the phase and structure of the powder calcined at 900Â°C were characterized by X-ray Diffraction shows that tetragonal phase is dominant for pure and doped BTO and data fitted by Rietveld Refinement. Electric and Dielectric properties were characterized by P-E Hysteresis and Dielectric measurement. In P-E measurement ferroelectric loop tracer applied for different voltage. The temperature dependant dielectricmoreÂ Â» constant behavior was observed as a function of frequency recorded on hp-Hewlett Packard 4192A, LF impedance, 5Hz-13Hz analyzer.Â«Â less\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nStrohmaier, Erich; Meuer, Hans W.; Dongarra, Jack\n\n20th Edition of TOP500 List of World's Fastest Supercomputers Released MANNHEIM, Germany; KNOXVILLE, Tenn.;&BERKELEY, Calif. In what has become a much-anticipated event in the world of high-performance computing, the 20th edition of the TOP500 list of the world's fastest supercomputers was released today (November 15, 2002). The Earth Simulator supercomputer installed earlier this year at the Earth Simulator Center in Yokohama, Japan, is with its Linpack benchmark performance of 35.86 Tflop/s (trillions of calculations per second) retains the number one position. The No.2 and No.3 positions are held by two new, identical ASCI Q systems at Los Alamos National LaboratorymoreÂ Â» (7.73Tflop/s each). These systems are built by Hewlett-Packard and based on the Alpha Server SC computer system.Â«Â less\n\nAirplane stability calculations with a card programmable pocket calculator\n\nNASA Technical Reports Server (NTRS)\n\nSherman, W. L.\n\n1978-01-01\n\nPrograms are presented for calculating airplane stability characteristics with a card programmable pocket calculator. These calculations include eigenvalues of the characteristic equations of lateral and longitudinal motion as well as stability parameters such as the time to damp to one-half amplitude or the damping ratio. The effects of wind shear are included. Background information and the equations programmed are given. The programs are written for the International System of Units, the dimensional form of the stability derivatives, and stability axes. In addition to programs for stability calculations, an unusual and short program is included for the Euler transformation of coordinates used in airplane motions. The programs have been written for a Hewlett Packard HP-67 calculator. However, the use of this calculator does not constitute an endorsement of the product by the National Aeronautics and Space Administration.\n\nOptical Design Using Small Dedicated Computers\n\nNASA Astrophysics Data System (ADS)\n\nSinclair, Douglas C.\n\n1980-09-01\n\nSince the time of the 1975 International Lens Design Conference, we have developed a series of optical design programs for Hewlett-Packard desktop computers. The latest programs in the series, OSLO-25G and OSLO-45G, have most of the capabilities of general-purpose optical design programs, including optimization based on exact ray-trace data. The computational techniques used in the programs are similar to ones used in other programs, but the creative environment experienced by a designer working directly with these small dedicated systems is typically much different from that obtained with shared-computer systems. Some of the differences are due to the psychological factors associated with using a system having zero running cost, while others are due to the design of the program, which emphasizes graphical output and ease of use, as opposed to computational speed.\n\nNASA Future Forum\n\nNASA Image and Video Library\n\n2012-02-21\n\nLaurie Leshin, dean of the School of Science, Rensselaer Polytechnic Institute, left, Mason Peck, NASA Chief Technologist, 2nd from left, Ron Sega, Vice president and enterprise executive for Energy and the Environment, The Ohio State University and Colorado State University, Michael Donovan, technology consultant, New Services Development, Hewlett-Packard Company, and, Jordan Hansell, chairman and CEO, NetJets Inc., right, participate in the NASA Future Forum panel titled \"Importance of Technology, Science and Innovation for our Economic Future\" at The Ohio State University on Tuesday, Feb. 21, 2012 in Columbus, Ohio. The NASA Future Forum features panel discussions on the importance of education to our nation's future in space, the benefit of commercialized space technology to our economy and lives here on Earth, and the shifting roles for the public, commercial and international communities in space. Photo Credit: (NASA/Bill Ingalls)\n\nParallelising a molecular dynamics algorithm on a multi-processor workstation\n\nNASA Astrophysics Data System (ADS)\n\nMÃ¼ller-Plathe, Florian\n\n1990-12-01\n\nThe Verlet neighbour-list algorithm is parallelised for a multi-processor Hewlett-Packard/Apollo DN10000 workstation. The implementation makes use of memory shared between the processors. It is a genuine master-slave approach by which most of the computational tasks are kept in the master process and the slaves are only called to do part of the nonbonded forces calculation. The implementation features elements of both fine-grain and coarse-grain parallelism. Apart from three calls to library routines, two of which are standard UNIX calls, and two machine-specific language extensions, the whole code is written in standard Fortran 77. Hence, it may be expected that this parallelisation concept can be transfered in parts or as a whole to other multi-processor shared-memory computers. The parallel code is routinely used in production work.\n\nA Survey of Electronic Color Printer Technologies\n\nNASA Astrophysics Data System (ADS)\n\nStarkweather, Gary K.\n\n1989-08-01\n\nElectronic printing in black and white has now come of age. Both high and low speed laser printers now heavily populate the electronic printing marketplace. On the high end of the market, the Xerox 9700 printer is the market dominator while the Canon LBP-SX and CX engines dominate the low end of the market. Clearly, laser printers are the predominant monochrome electronic printing technology. Ink jet is now beginning to engage the low end printer market but still fails to attain laser printer image quality. As yet, ink jet is not a serious contender for the substantial low end laser printer marketplace served by Apple Computer's LaserWriter II and Hewlett-Packard's LaserJet printers. Laser printing generally dominates because of its cost/performance as well as the reliability of the cartridge serviced low end printers.\n\nBASIC Programming In Water And Wastewater Analysis\n\nNASA Technical Reports Server (NTRS)\n\nDreschel, Thomas\n\n1988-01-01\n\nCollection of computer programs assembled for use in water-analysis laboratories. First program calculates quality-control parameters used in routine water analysis. Second calculates line of best fit for standard concentrations and absorbances entered. Third calculates specific conductance from conductivity measurement and temperature at which measurement taken. Fourth calculates any one of four types of residue measured in water. Fifth, sixth, and seventh calculate results of titrations commonly performed on water samples. Eighth converts measurements, to actual dissolved-oxygen concentration using oxygen-saturation values for fresh and salt water. Ninth and tenth perform calculations of two other common titrimetric analyses. Eleventh calculates oil and grease residue from water sample. Last two use spectro-photometric measurements of absorbance at different wavelengths and residue measurements. Programs included in collection written for Hewlett-Packard 2647F in H-P BASIC.\n\nHigh-Temperature RF Probe Station For Device Characterization Through 500 deg C and 50 GHz\n\nNASA Technical Reports Server (NTRS)\n\nSchwartz, Zachary D.; Downey, Alan N.; Alterovitz, Samuel A.; Ponchak, George E.; Williams, W. D. (Technical Monitor)\n\n2003-01-01\n\nA high-temperature measurement system capable of performing on-wafer microwave testing of semiconductor devices has been developed. This high temperature probe station can characterize active and passive devices and circuits at temperatures ranging from room temperature to above 500 C. The heating system uses a ceramic heater mounted on an insulating block of NASA shuttle tile material. The temperature is adjusted by a graphical computer interface and is controlled by the software-based feedback loop. The system is used with a Hewlett-Packard 8510C Network Analyzer to measure scattering parameters over a frequency range of 1 to 50 GHz. The microwave probes, cables, and inspection microscope are all shielded to protect from heat damage. The high temperature probe station has been successfully used to characterize gold transmission lines on silicon carbide at temperatures up to 540 C.\n\nHair analysis for delta(9)-THC, delta(9)-THC-COOH, CBN and CBD, by GC/MS-EI. Comparison with GC/MS-NCI for delta(9)-THC-COOH.\n\nPubMed\n\nBaptista, Maria JoÃ£o; Monsanto, Paula VerÃ¢ncio; Pinho Marques, Estela Gouveia; Bermejo, Ana; Avila, Sofia; Castanheira, Alice Martelo; Margalho, ClÃ¡udia; Barroso, MÃ¡rio; Vieira, Duarte Nuno\n\n2002-08-14\n\nA sensitive analytical method was developed for quantitative analysis of delta(9)-tetrahydrocannabinol (delta(9)-THC), 11-nor-delta(9)-tetrahydrocannabinol-carboxylic acid (delta(9)-THC-COOH), cannabinol (CBN) and cannabidiol (CBD) in human hair. The identification of delta(9)-THC-COOH in hair would document Cannabis use more effectively than the detection of parent drug (delta(9)-THC) which might have come from environmental exposure. Ketamine was added to hair samples as internal standard for CBN and CBD. Ketoprofen was added to hair samples as internal standard for the other compounds. Samples were hydrolyzed with beta-glucuronidase/arylsulfatase for 2h at 40 degrees C. After cooling, samples were extracted with a liquid-liquid extraction procedure (with chloroform/isopropyl alcohol, after alkalinization, and n-hexane/ethyl acetate, after acidification), which was developed in our laboratory. The extracts were analysed before and after derivatization with pentafluoropropionic anhydride (PFPA) and pentafluoropropanol (PFPOH) using a Hewlett Packard gas chromatographer/mass spectrometer detector, in electron impact mode (GC/MS-EI). Derivatized delta(9)-THC-COOH was also analysed using a Hewlett Packard gas chromatographer/mass spectrometer detector, in negative ion chemical ionization mode (GC/MS-NCI) using methane as the reagent gas. Responses were linear ranging from 0.10 to 5.00 ng/mg hair for delta(9)-THC and CBN, 0.10-10.00 ng/mg hair for CBD, 0.01-5.00 ng/mg for delta(9)-THC-COOH (r(2)>0.99). The intra-assay precisions ranged from <0.01 to 12.40%. Extraction recoveries ranged from 80.9 to 104.0% for delta(9)-THC, 85.9-100.0% for delta(9)-THC-COOH, 76.7-95.8% for CBN and 71.0-94.0% for CBD. The analytical method was applied to 87 human hair samples, obtained from individuals who testified in court of having committed drug related crimes. Quantification of delta(9)-THC-COOH using GC/MS-NCI was found to be more convenient than GC/MS-EI. The latter may give rise\n\n75 FR 57506 - EDS, an HP Company, A Subsidiary of Hewlett-Packard Company Including On-Site Leased Workers from...\n\nFederal Register 2010, 2011, 2012, 2013, 2014\n\n2010-09-21\n\n.... The workers are engaged in activities related to information technology (IT) outsourcing services. New information shows that workers leased from Compuware Corporation were employed on-site at the Detroit...\n\nTOP500 Supercomputers for June 2003\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nStrohmaier, Erich; Meuer, Hans W.; Dongarra, Jack\n\n2003-06-23\n\n21st Edition of TOP500 List of World's Fastest Supercomputers Released MANNHEIM, Germany; KNOXVILLE, Tenn.;&BERKELEY, Calif. In what has become a much-anticipated event in the world of high-performance computing, the 21st edition of the TOP500 list of the world's fastest supercomputers was released today (June 23, 2003). The Earth Simulator supercomputer built by NEC and installed last year at the Earth Simulator Center in Yokohama, Japan, with its Linpack benchmark performance of 35.86 Tflop/s (teraflops or trillions of calculations per second), retains the number one position. The number 2 position is held by the re-measured ASCI Q system at Los AlamosmoreÂ Â» National Laboratory. With 13.88 Tflop/s, it is the second system ever to exceed the 10 Tflop/smark. ASCIQ was built by Hewlett-Packard and is based on the AlphaServerSC computer system.Â«Â less\n\nA VLSI implementation of DCT using pass transistor technology\n\nNASA Technical Reports Server (NTRS)\n\nKamath, S.; Lynn, Douglas; Whitaker, Sterling\n\n1992-01-01\n\nA VLSI design for performing the Discrete Cosine Transform (DCT) operation on image blocks of size 16 x 16 in a real time fashion operating at 34 MHz (worst case) is presented. The process used was Hewlett-Packard's CMOS26--A 3 metal CMOS process with a minimum feature size of 0.75 micron. The design is based on Multiply-Accumulate (MAC) cells which make use of a modified Booth recoding algorithm for performing multiplication. The design of these cells is straight forward, and the layouts are regular with no complex routing. Two versions of these MAC cells were designed and their layouts completed. Both versions were simulated using SPICE to estimate their performance. One version is slightly faster at the cost of larger silicon area and higher power consumption. An improvement in speed of almost 20 percent is achieved after several iterations of simulation and re-sizing.\n\nMixed reaction to science department proposal\n\nNASA Astrophysics Data System (ADS)\n\nThe recommendation last month by a presidential commission that a federal Department of Science and Technology be created to encompass âmajor civilian research and development (R&D) agenciesâ has elicited a mixed reaction from members of the geophysical sciences community.The Commission on Industrial Competitiveness, created by President Ronald Reagan in June 1983 to study ways to strengthen the ability of the United States to compete in a global marketplace, recommended establishment of a Cabinet-level science department âto promote national interest in and policies for research and technological innovation.â The commission, chaired by John A. Young, president of the Hewlett-Packard Company, was composed primarily of presidents and chief executive officers of major technology corporations but also included members of academia and government. Creation of a federal science and technology 'department is one of many suggestions contained in the commission's final report, Global Competition: The New Reality.\n\n[Determination of aristolochic acid A in Guanxinsuhe preparations by RP-HPLC].\n\nPubMed\n\nLi, Lin; Gao, Hui-Min; Wang, Zhi-Min; Wang, Wei-Hao\n\n2006-01-01\n\nTo establish a determination method of aristolochic acid A in Guanxisuhe preparations by RP-HPLC. The instrument used was Hewlett-Packard 1100 HPLC with a Alltech C18 column (4.6 mm x 250 mm, 5 microm). The mobile phase was methanol-water-acetic acid (68: 32:1) and the flow rate was 1.0 mL x min(-1). The UV detection wavelength was 390 nm and the column temperature was at 35 degrees C. The extracted solvent for the preparations was methanol solution contained 10% formic acid. The calibration curve was linear (r = 0.999 9) within the range of 0.119-1.89 microg for aristolochic acid A. The average recovery 99.0%, RSD 0.63%. The method with good linear relationship was convenient, quick, accurate, and suitable for the quality control of the aristolochic acid A in Guanxinsuhe and other traditional Chinese medicines containing aristolochic acid A.\n\nLARCRIM user's guide, version 1.0\n\nNASA Technical Reports Server (NTRS)\n\nDavis, John S.; Heaphy, William J.\n\n1993-01-01\n\nLARCRIM is a relational database management system (RDBMS) which performs the conventional duties of an RDBMS with the added feature that it can store attributes which consist of arrays or matrices. This makes it particularly valuable for scientific data management. It is accessible as a stand-alone system and through an application program interface. The stand-alone system may be executed in two modes: menu or command. The menu mode prompts the user for the input required to create, update, and/or query the database. The command mode requires the direct input of LARCRIM commands. Although LARCRIM is an update of an old database family, its performance on modern computers is quite satisfactory. LARCRIM is written in FORTRAN 77 and runs under the UNIX operating system. Versions have been released for the following computers: SUN (3 & 4), Convex, IRIS, Hewlett-Packard, CRAY 2 & Y-MP.\n\nMagCloud: magazine self-publishing for the long tail\n\nNASA Astrophysics Data System (ADS)\n\nKoh, Kok-Wei; Chatow, Ehud\n\n2010-02-01\n\nIn June of 2008, Hewlett-Packard Labs launched MagCloud, a print-on-demand web service for magazine selfpublishing. MagCloud enables anyone to publish their own magazine by simply uploading a PDF file to the site. There are no setup fees, minimum print runs, storage requirements or waste due to unsold magazines. Magazines are only printed when an order is placed, and are shipped directly to the end customer. In the course of building this web service, a number of technological challenges were encountered. In this paper, we will discuss these challenges and the methods used to overcome them. Perhaps the most important decision in enabling the successful launch of MagCloud was the choice to offer a single product. This simplified the PDF validation phase and streamlined the print fulfillment process such that orders can be printed, folded and trimmed in batches, rather than one-by-one. In a sense, MagCloud adopted the Ford Model T approach to manufacturing, where having just a single model with little or no options allows for efficiencies in the production line, enabling a lower product price and opening the market to a much larger customer base. This platform has resulted in a number of new niche publications - the long tail of publishing.\n\nVisualizing multiattribute Web transactions using a freeze technique\n\nNASA Astrophysics Data System (ADS)\n\nHao, Ming C.; Cotting, Daniel; Dayal, Umeshwar; Machiraju, Vijay; Garg, Pankaj\n\n2003-05-01\n\nWeb transactions are multidimensional and have a number of attributes: client, URL, response times, and numbers of messages. One of the key questions is how to simultaneously lay out in a graph the multiple relationships, such as the relationships between the web client response times and URLs in a web access application. In this paper, we describe a freeze technique to enhance a physics-based visualization system for web transactions. The idea is to freeze one set of objects before laying out the next set of objects during the construction of the graph. As a result, we substantially reduce the force computation time. This technique consists of three steps: automated classification, a freeze operation, and a graph layout. These three steps are iterated until the final graph is generated. This iterated-freeze technique has been prototyped in several e-service applications at Hewlett Packard Laboratories. It has been used to visually analyze large volumes of service and sales transactions at online web sites.\n\nCLARET user's manual: Mainframe Logs. Revision 1\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nFrobose, R.H.\n\n1984-11-12\n\nCLARET (Computer Logging and RETrieval) is a stand-alone PDP 11/23 system that can support 16 terminals. It provides a forms-oriented front end by which operators enter online activity logs for the Lawrence Livermore National Laboratory's OCTOPUS computer network. The logs are stored on the PDP 11/23 disks for later retrieval, and hardcopy reports are generated both automatically and upon request. Online viewing of the current logs is provided to management. As each day's logs are completed, the information is automatically sent to a CRAY and included in an online database system. The terminal used for the CLARET system is amoreÂ Â» dual-port Hewlett Packard 2626 terminal that can be used as either the CLARET logging station or as an independent OCTOPUS terminal. Because this is a stand-alone system, it does not depend on the availability of the OCTOPUS network to run and, in the event of a power failure, can be brought up independently.Â«Â less\n\nHigh accuracy in short ISS missions\n\nNASA Astrophysics Data System (ADS)\n\nRÃ¼eger, J. M.\n\n1986-06-01\n\nTraditionally Inertial Surveying Systems ( ISS) are used for missions of 30 km to 100 km length. Today, a new type of ISS application is emanating from an increased need for survey control densification in urban areas often in connection with land information systems or cadastral surveys. The accuracy requirements of urban surveys are usually high. The loss in accuracy caused by the coordinate transfer between IMU and ground marks is investigated and an offsetting system based on electronic tacheometers is proposed. An offsetting system based on a Hewlett-Packard HP 3820A electronic tacheometer has been tested in Sydney (Australia) in connection with a vehicle mounted LITTON Auto-Surveyor System II. On missions over 750 m ( 8 stations, 25 minutes duration, 3.5 minute ZUPT intervals, mean offset distances 9 metres) accuracies of 37 mm (one sigma) in position and 8 mm in elevation were achieved. Some improvements to the LITTON Auto-Surveyor System II are suggested which would improve the accuracies even further.\n\nProgrammable calculator software for computation of the plasma binding of ligands.\n\nPubMed\n\nConner, D P; Rocci, M L; Larijani, G E\n\n1986-01-01\n\nThe computation of the extent of plasma binding of a ligand to plasma constituents using radiolabeled ligand and equilibrium dialysis is complex and tedious. A computer program for the HP-41C Handheld Computer Series (Hewlett-Packard) was developed to perform these calculations. The first segment of the program constructs a standard curve for quench correction of post-dialysis plasma and buffer samples, using either external standard ratio (ESR) or sample channels ratio (SCR) techniques. The remainder of the program uses the counts per minute, SCR or ESR, and post-dialysis volume of paired plasma and buffer samples generated from the dialysis procedure to compute the extent of binding after correction for background radiation, counting efficiency, and intradialytic shifts of fluid between plasma and buffer compartments during dialysis. This program greatly simplifies the analysis of equilibrium dialysis data and has been employed in the analysis of dexamethasone binding in normal and uremic sera.\n\nIdentification of subsurface microorganisms at Yucca Mountain. Quarterly report, July 1, 1995--September 30, 1995\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nStetzenbach, L.D.\n\n1995-12-01\n\nMore than 1100 bacterial isolates were obtained over a two year period from 31 springs in a region along the southern boarder of California and Nevada. Water samples were collected from 17 springs in Ash Meadows National Wildlife Refuge and 14 springs in Death Valley National Park. Bacteria isolated from these samples were subjected to extraction and gas chromatography to determine the cellular fatty acid profile of each isolate. Fatty acid methyl esters (FAME) extracted from cell membranes were separated and classified using the Hewlett Packard by gas chromatography. The FAME profiles of each isolate were then subjected to clustermoreÂ Â» analysis by the unweighted pair-group method using arithmetic averages. During this quarter the relatedness of FAME patterns of bacterial isolates were examined at the genus level by counting the number of clusters produced in a MIDI dendrogram at a Euclidian distance of 25. This information was then used to determine microbiological relationships among springs.Â«Â less\n\nAnalysis of polar and non-polar VOCs from ambient and source matrices: Development of a new canister autosampler which meets TO-15 QA/QC criteria\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nBurnett, M.L.W.; Neal, D.; Uchtman, R.\n\n1997-12-31\n\nApproximately 108 of the Hazardous Air Pollutants (HAPs) specified in the 1990 Clean Air Act Amendments are classified as volatile organic compounds (VOCs). Of the 108 VOCs, nearly 35% are oxygenated or polar compounds. While more than one sample introduction technique exists for the analysis of these air toxics, SUMMA{reg_sign} canister sampling is suitable for the most complete range of analytes. A broad concentration range of polar and non-polar species can be analyzed from canisters. A new canister autosampler, the Tekmar AUTOCan{trademark} Elite autosampler, has been developed which incorporates the autosampler and concentrator into a single unit. Analysis of polarmoreÂ Â» and non-polar VOCs has been performed. This paper demonstrates adherence to the technical acceptance objectives outlined in the TO-15 methodology including initial calibration, daily calibration, blank analysis, method detection limits and laboratory control samples. The analytical system consists of a Tekmar AUTOCan{trademark} Elite autosampler interfaced to a Hewlett Packard{reg_sign} 5890/5972 MSD.Â«Â less\n\nPhotometric and colorimetric measurements of CRT and TFT monitors for vision research\n\nNASA Astrophysics Data System (ADS)\n\nKlein, Johann; Zlatkova, Margarita; Lauritzen, Jan; Pierscionek, Barbara\n\n2013-08-01\n\nVisual displays have various limitations that can affect the results of vision research experiments. This study compares several characteristics of CRT (Hewlett Packard 7650) and TFT (LG Flatron L227 WT and Samsung 2233 RZ) monitors, including luminance and colour spatial homogeneity, luminance changes with viewing angle, contrast linearity and warm-up characteristics. In addition, the psychophysical performance in grating contrast sensitivity test for both CRT and TFT monitors was compared. The TFT monitors demonstrated spatial non-homogeneity ('mura') with up to 50% of luminance change across the screen and a more significant luminance viewing angle dependence compared with CRT. The chromaticity of the white point showed negligible variation across the screen. Both types of monitors required a warm-up time of the order of 60 min. Despite the physical differences between monitors, visual contrast sensitivity performance measured with the two types of monitors was similar using both static and flickering gratings.\n\nClassics in physical geography revisited. Hewlett, J.D. and Hibbert, A.R. 1967: Factors affecting the response of small watersheds to precipitation in humid areas. In Sopper, W.E. and Lull, H.W., editors, Forest hydrology, New York: Pergamon Press, 275-90.\n\nTreesearch\n\nJeffrey J. McDonnell\n\n2009-01-01\n\nHewlett and HibbertÃ¢ÂÂs (1967) Ã¢ÂÂFactors affecting the response of small watersheds to precipitation in humid areasÃ¢ÂÂ (hereafter referred to as Ã¢ÂÂFactorsÃ¢ÂÂ) is one of the most important papers published in the field of catchment hydrology.\n\nProphylactic transabdominal amnioinfusion in oligohydramnios for preterm premature rupture of membranes: increase of amniotic fluid index during latency period.\n\nPubMed\n\nGarzetti, G G; Ciavattini, A; De Cristofaro, F; La Marca, N; Arduini, D\n\n1997-01-01\n\nThis study was designed to: (i) evaluate the effect of amnioinfusion on the latency period in patients with oligohydramnios for preterm premature rupture of membranes, and (ii) to investigate the relationship between changes in the amniotic fluid index and fetal heart rate short-term variability by computerized Hewlett-Packard cardiotocography, longitudinally estimated before and after prophylactic amnioinfusion. All singleton pregnancies with prolonged premature rupture of membranes after 25 weeks of gestation and seen at the Institute of Obstetrics and Gynecology, University of Ancona (Italy), between January 1994 and June 1995 were included in the study. Transabdominal amnioinfusion with 150-350 ml warmed normal saline (25-50 ml/min) was performed at weekly intervals. Amniotic fluid volume was assessed ultrasonographically by means of the four-quadrant technique on a weekly basis before and after each amnioinfusion, as well as the short-term variability by a Hewlett-Packard computerized cardiotocographic system. 18 women were enrolled and underwent prophylactic transabdominal amnioinfusion at weekly intervals until delivery. Eighteen controls, who did not undergo prophylactic amnioinfusion, were recruited from our 1992-1993 series and included in the study. The median interval between premature rupture of membranes and delivery was 3.0 weeks (range 1-8 weeks), with an average delivery age of 33.0 weeks (range 27-36 weeks). The latency period was significantly longer in patients who underwent prophylactic amnioinfusion (mean +/- SD, 4.1 +/- 1.7 weeks) than in controls(1.7 +/- 1.0 weeks; p < 0.001). An increase in both the weekly amniotic fluid index (linear regression analysis r = 0.8, p = 0.03) and the weekly short-term variability (linear regression analysis r = 0.82, p = 0.02) was observed among patients who underwent prophylactic amnioinfusion. A direct relationship was observed between the amniotic fluid index and short-term variability (linear regression\n\nComputer program for the calculation of grain size statistics by the method of moments\n\nUSGS Publications Warehouse\n\nSawyer, Michael B.\n\n1977-01-01\n\nA computer program is presented for a Hewlett-Packard Model 9830A desk-top calculator (1) which calculates statistics using weight or point count data from a grain-size analysis. The program uses the method of moments in contrast to the more commonly used but less inclusive graphic method of Folk and Ward (1957). The merits of the program are: (1) it is rapid; (2) it can accept data in either grouped or ungrouped format; (3) it allows direct comparison with grain-size data in the literature that have been calculated by the method of moments; (4) it utilizes all of the original data rather than percentiles from the cumulative curve as in the approximation technique used by the graphic method; (5) it is written in the computer language BASIC, which is easily modified and adapted to a wide variety of computers; and (6) when used in the HP-9830A, it does not require punching of data cards. The method of moments should be used only if the entire sample has been measured and the worker defines the measured grain-size range. (1) Use of brand names in this paper does not imply endorsement of these products by the U.S. Geological Survey.\n\nPACER -- A fast running computer code for the calculation of short-term containment/confinement loads following coolant boundary failure. Volume 2: User information\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nSienicki, J.J.\n\nA fast running and simple computer code has been developed to calculate pressure loadings inside light water reactor containments/confinements under loss-of-coolant accident conditions. PACER was originally developed to calculate containment/confinement pressure and temperature time histories for loss-of-coolant accidents in Soviet-designed VVER reactors and is relevant to the activities of the US International Nuclear Safety Center. The code employs a multicompartment representation of the containment volume and is focused upon application to early time containment phenomena during and immediately following blowdown. PACER has been developed for FORTRAN 77 and earlier versions of FORTRAN. The code has been successfully compiled and executedmoreÂ Â» on SUN SPARC and Hewlett-Packard HP-735 workstations provided that appropriate compiler options are specified. The code incorporates both capabilities built around a hardwired default generic VVER-440 Model V230 design as well as fairly general user-defined input. However, array dimensions are hardwired and must be changed by modifying the source code if the number of compartments/cells differs from the default number of nine. Detailed input instructions are provided as well as a description of outputs. Input files and selected output are presented for two sample problems run on both HP-735 and SUN SPARC workstations.Â«Â less\n\nMemristor-based cellular nonlinear/neural network: design, analysis, and applications.\n\nPubMed\n\nDuan, Shukai; Hu, Xiaofang; Dong, Zhekang; Wang, Lidan; Mazumder, Pinaki\n\n2015-06-01\n\nCellular nonlinear/neural network (CNN) has been recognized as a powerful massively parallel architecture capable of solving complex engineering problems by performing trillions of analog operations per second. The memristor was theoretically predicted in the late seventies, but it garnered nascent research interest due to the recent much-acclaimed discovery of nanocrossbar memories by engineers at the Hewlett-Packard Laboratory. The memristor is expected to be co-integrated with nanoscale CMOS technology to revolutionize conventional von Neumann as well as neuromorphic computing. In this paper, a compact CNN model based on memristors is presented along with its performance analysis and applications. In the new CNN design, the memristor bridge circuit acts as the synaptic circuit element and substitutes the complex multiplication circuit used in traditional CNN architectures. In addition, the negative differential resistance and nonlinear current-voltage characteristics of the memristor have been leveraged to replace the linear resistor in conventional CNNs. The proposed CNN design has several merits, for example, high density, nonvolatility, and programmability of synaptic weights. The proposed memristor-based CNN design operations for implementing several image processing functions are illustrated through simulation and contrasted with conventional CNNs. Monte-Carlo simulation has been used to demonstrate the behavior of the proposed CNN due to the variations in memristor synaptic weights.\n\nCorrelating objective and subjective evaluation of texture appearance with applications to camera phone imaging\n\nNASA Astrophysics Data System (ADS)\n\nPhillips, Jonathan B.; Coppola, Stephen M.; Jin, Elaine W.; Chen, Ying; Clark, James H.; Mauer, Timothy A.\n\n2009-01-01\n\nTexture appearance is an important component of photographic image quality as well as object recognition. Noise cleaning algorithms are used to decrease sensor noise of digital images, but can hinder texture elements in the process. The Camera Phone Image Quality (CPIQ) initiative of the International Imaging Industry Association (I3A) is developing metrics to quantify texture appearance. Objective and subjective experimental results of the texture metric development are presented in this paper. Eight levels of noise cleaning were applied to ten photographic scenes that included texture elements such as faces, landscapes, architecture, and foliage. Four companies (Aptina Imaging, LLC, Hewlett-Packard, Eastman Kodak Company, and Vista Point Technologies) have performed psychophysical evaluations of overall image quality using one of two methods of evaluation. Both methods presented paired comparisons of images on thin film transistor liquid crystal displays (TFT-LCD), but the display pixel pitch and viewing distance differed. CPIQ has also been developing objective texture metrics and targets that were used to analyze the same eight levels of noise cleaning. The correlation of the subjective and objective test results indicates that texture perception can be modeled with an objective metric. The two methods of psychophysical evaluation exhibited high correlation despite the differences in methodology.\n\nIsothermal thermogravimetric data acquisition analysis system\n\nNASA Technical Reports Server (NTRS)\n\nCooper, Kenneth, Jr.\n\n1991-01-01\n\nThe description of an Isothermal Thermogravimetric Analysis (TGA) Data Acquisition System is presented. The system consists of software and hardware to perform a wide variety of TGA experiments. The software is written in ANSI C using Borland's Turbo C++. The hardware consists of a 486/25 MHz machine with a Capital Equipment Corp. IEEE488 interface card. The interface is to a Hewlett Packard 3497A data acquisition system using two analog input cards and a digital actuator card. The system provides for 16 TGA rigs with weight and temperature measurements from each rig. Data collection is conducted in three phases. Acquisition is done at a rapid rate during initial startup, at a slower rate during extended data collection periods, and finally at a fast rate during shutdown. Parameters controlling the rate and duration of each phase are user programmable. Furnace control (raising and lowering) is also programmable. Provision is made for automatic restart in the event of power failure or other abnormal terminations. Initial trial runs were conducted to show system stability.\n\nGuidelines for developing vectorizable computer programs\n\nNASA Technical Reports Server (NTRS)\n\nMiner, E. W.\n\n1982-01-01\n\nSome fundamental principles for developing computer programs which are compatible with array-oriented computers are presented. The emphasis is on basic techniques for structuring computer codes which are applicable in FORTRAN and do not require a special programming language or exact a significant penalty on a scalar computer. Researchers who are using numerical techniques to solve problems in engineering can apply these basic principles and thus develop transportable computer programs (in FORTRAN) which contain much vectorizable code. The vector architecture of the ASC is discussed so that the requirements of array processing can be better appreciated. The \"vectorization\" of a finite-difference viscous shock-layer code is used as an example to illustrate the benefits and some of the difficulties involved. Increases in computing speed with vectorization are illustrated with results from the viscous shock-layer code and from a finite-element shock tube code. The applicability of these principles was substantiated through running programs on other computers with array-associated computing characteristics, such as the Hewlett-Packard (H-P) 1000-F.\n\nCardiac elastography: detecting pathological changes in myocardium tissues\n\nNASA Astrophysics Data System (ADS)\n\nKonofagou, Elisa E.; Harrigan, Timothy; Solomon, Scott\n\n2003-05-01\n\nEstimation of the mechanical properties of the cardiac muscle has been shown to play a crucial role in the detection of cardiovascular disease. Elastography was recently shown feasible on RF cardiac data in vivo. In this paper, the role of elastography in the detection of ischemia/infarct is explored with simulations and in vivo experiments. In finite-element simulations of a portion of the cardiac muscle containing an infarcted region, the cardiac cycle was simulated with successive compressive and tensile strains ranging between -30% and 20%. The incremental elastic modulus was also mapped uisng adaptive methods. We then demonstrated this technique utilizing envelope-detected sonographic data (Hewlett-Packard Sonos 5500) in a patient with a known myocardial infarction. In cine-loop and M-Mode elastograms from both normal and infarcted regions in simulations and experiments, the infarcted region was identifed by the up to one order of magnitude lower incremental axial displacements and strains, and higher modulus. Information on motion, deformation and mechanical property should constitute a unique tool for noninvasive cardiac diagnosis.\n\nDevelopment of a tester for evaluation of prototype thermal cells and batteries\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nGuidotti, R.A.\n\n1994-10-01\n\nA tester was developed to evaluate prototype thermal cells and batteries--especially high-voltage units--under a wide range of constant-current and constant-resistance discharge conditions. Programming of the steady-state and pulsing conditions was by software control or by hardware control via an external pulse generator. The tester was assembled from primarily Hewlett-Packard (H-P) instrumentation and was operated under H-P`s Rocky Mountain Basic (RMB). Constant-current electronic loads rated up to 4 kW (400 V at up to 100 A) were successfully used with the setup. For testing under constant-resistance conditions, power metal-oxide field-effect transistors (MOSFETs) controlled by a programmable pulse generator were used tomoreÂ Â» switch between steady-state and pulse loads. The pulses were digitized at up to a 50 kHz rate (20 {mu} s/pt) using high-speed DVMs; steady-state voltages were monitored with standard DVMs. This paper describes several of the test configurations used and discusses the limitations of each. Representative data are presented for a number of the test conditions.Â«Â less\n\nVisual mining business service using pixel bar charts\n\nNASA Astrophysics Data System (ADS)\n\nHao, Ming C.; Dayal, Umeshwar; Casati, Fabio\n\n2004-06-01\n\nBasic bar charts have been commonly available, but they only show highly aggregated data. Finding the valuable information hidden in the data is essential to the success of business. We describe a new visualization technique called pixel bar charts, which are derived from regular bar charts. The basic idea of a pixel bar chart is to present all data values directly instead of aggregating them into a few data values. Pixel bar charts provide data distribution and exceptions besides aggregated data. The approach is to represent each data item (e.g. a business transaction) by a single pixel in the bar chart. The attribute of each data item is encoded into the pixel color and can be accessed and drilled down to the detail information as needed. Different color mappings are used to represent multiple attributes. This technique has been prototyped in three business service applications-Business Operation Analysis, Sales Analysis, and Service Level Agreement Analysis at Hewlett Packard Laboratories. Our applications show the wide applicability and usefulness of this new idea.\n\nThe Mark 3 data base handler\n\nNASA Technical Reports Server (NTRS)\n\nRyan, J. W.; Ma, C.; Schupler, B. R.\n\n1980-01-01\n\nA data base handler which would act to tie Mark 3 system programs together is discussed. The data base handler is written in FORTRAN and is implemented on the Hewlett-Packard 21MX and the IBM 360/91. The system design objectives were to (1) provide for an easily specified method of data interchange among programs, (2) provide for a high level of data integrity, (3) accommodate changing requirments, (4) promote program accountability, (5) provide a single source of program constants, and (6) provide a central point for data archiving. The system consists of two distinct parts: a set of files existing on disk packs and tapes; and a set of utility subroutines which allow users to access the information in these files. Users never directly read or write the files and need not know the details of how the data are formatted in the files. To the users, the storage medium is format free. A user does need to know something about the sequencing of his data in the files but nothing about data in which he has no interest.\n\nA multiprocessor airborne lidar data system\n\nNASA Technical Reports Server (NTRS)\n\nWright, C. W.; Bailey, S. A.; Heath, G. E.; Piazza, C. R.\n\n1988-01-01\n\nA new multiprocessor data acquisition system was developed for the existing Airborne Oceanographic Lidar (AOL). This implementation simultaneously utilizes five single board 68010 microcomputers, the UNIX system V operating system, and the real time executive VRTX. The original data acquisition system was implemented on a Hewlett Packard HP 21-MX 16 bit minicomputer using a multi-tasking real time operating system and a mixture of assembly and FORTRAN languages. The present collection of data sources produce data at widely varied rates and require varied amounts of burdensome real time processing and formatting. It was decided to replace the aging HP 21-MX minicomputer with a multiprocessor system. A new and flexible recording format was devised and implemented to accommodate the constantly changing sensor configuration. A central feature of this data system is the minimization of non-remote sensing bus traffic. Therefore, it is highly desirable that each micro be capable of functioning as much as possible on-card or via private peripherals. The bus is used primarily for the transfer of remote sensing data to or from the buffer queue.\n\nFunction library programming to support B89 evaluation of Sheffield Apollo RS50 DCC (Direct Computer Control) CMM (Coordinate Measuring Machine)\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nFrank, R.N.\n\n1990-02-28\n\nThe Inspection Shop at Lawrence Livermore Lab recently purchased a Sheffield Apollo RS50 Direct Computer Control Coordinate Measuring Machine. The performance of the machine was specified to conform to B89 standard which relies heavily upon using the measuring machine in its intended manner to verify its accuracy (rather than parametric tests). Although it would be possible to use the interactive measurement system to perform these tasks, a more thorough and efficient job can be done by creating Function Library programs for certain tasks which integrate Hewlett-Packard Basic 5.0 language and calls to proprietary analysis and machine control routines. This combinationmoreÂ Â» provides efficient use of the measuring machine with a minimum of keyboard input plus an analysis of the data with respect to the B89 Standard rather than a CMM analysis which would require subsequent interpretation. This paper discusses some characteristics of the Sheffield machine control and analysis software and my use of H-P Basic language to create automated measurement programs to support the B89 performance evaluation of the CMM. 1 ref.Â«Â less\n\nHandheld technology acceptance in radiologic science education and training programs\n\nNASA Astrophysics Data System (ADS)\n\nPowers, Kevin Jay\n\nThe purpose of this study was to explore the behavioral intention of directors of educational programs in the radiologic sciences to adopt handheld devices to aid in managing student clinical data. Handheld devices were described to participants as a technology representing a class of mobile electronic devices including, but not limited to, personal digital assistants such as a Palm TX, Apple iPod Touch, Apple iPad or Hewlett Packard iPaq, and cellular or smartphones with third generation mobile capabilities such as an Apple iPhone, Blackberry or Android device. The study employed a non-experimental, cross-sectional survey design to determine the potential of adopting handheld technologies based on the constructs of Davis's (1989) Technology Acceptance Model. An online self-report questionnaire survey instrument was used to gather study data from 551 entry level radiologic science programs specializing in radiography, radiation therapy, nuclear medicine and medical sonography. The study design resulted in a single point in time assessment of the relationship between the primary constructs of the Technology Acceptance Model: perceived usefulness and perceived ease of use, and the behavioral intention of radiography program directors to adopt the information technology represented by hand held devices. Study results provide justification for investing resources to promote the adoption of mobile handheld devices in radiologic science programs and study findings serve as a foundation for further research involving technology adoption in the radiologic sciences.\n\nSwarm intelligence. A whole new way to think about business.\n\nPubMed\n\nBonabeau, E; Meyer, C\n\n2001-05-01\n\nWhat do ants and bees have to do with business? A great deal, it turns out. Individually, social insects are only minimally intelligent, and their work together is largely self-organized and unsupervised. Yet collectively they're capable of finding highly efficient solutions to difficult problems and can adapt automatically to changing environments. Over the past 20 years, the authors and other researchers have developed rigorous mathematical models to describe this phenomenon, which has been dubbed \"swarm intelligence,\" and they are now applying them to business. Their research has already helped several companies develop more efficient ways to schedule factory equipment, divide tasks among workers, organize people, and even plot strategy. Emulating the way ants find the shortest path to a new food supply, for example, has led researchers at Hewlett-Packard to develop software programs that can find the most efficient way to route phone traffic over a telecommunications network. South-west Airlines has used a similar model to efficiently route cargo. To allocate labor, honeybees appear to follow one simple but powerful rule--they seem to specialize in a particular activity unless they perceive an important need to perform another function. Using that model, researchers at Northwestern University have devised a system for painting trucks that can automatically adapt to changing conditions. In the future, the authors speculate, a company might structure its entire business using the principles of swarm intelligence. The result, they believe, would be the ultimate self-organizing enterprise--one that could adapt quickly and instinctively to fast-changing markets.\n\nTOP500 Sublist for November 2001\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nStrohmaier, Erich; Meuer, Hans W.; Dongarra, Jack J.\n\n2001-11-09\n\n18th Edition of TOP500 List of World's Fastest Supercomputers Released MANNHEIM, GERMANY; KNOXVILLE, TENN.; BERKELEY, CALIF. In what has become a much-anticipated event in the world of high-performance computing, the 18th edition of the TOP500 list of the world's fastest supercomputers was released today (November 9, 2001). The latest edition of the twice-yearly ranking finds IBM as the leader in the field, with 32 percent in terms of installed systems and 37 percent in terms of total performance of all the installed systems. In a surprise move Hewlett-Packard captured the second place with 30 percent of the systems. Most ofmoreÂ Â» these systems are smaller in size and as a consequence HP's share of installed performance is smaller with 15 percent. This is still enough for second place in this category. SGI, Cray and Sun follow in the number of TOP500 systems with 41 (8 percent), 39 (8 percent), and 31 (6 percent) respectively. In the category of installed performance Cray Inc. keeps the third position with 11 percent ahead of SGI (8 percent) and Compaq (8 percent).Â«Â less\n\nThe ALE/GAGE/AGAGE Network (DB1001)\n\nDOE Data Explorer\n\nPrinn, Ronald G. [MIT, Center for Global Change Science; Weiss, Ray F. [University of California, San Diego; Scripps Institution of Oceanography; Krummel, Paul B. [CSIRO Oceans and Atmosphere, Cape Grim; O'Doherty, Simon [University of Bristol, Barbados and Mace Head Stations; Fraser, Paul [CSIRO Oceans and Atmosphere; Muhle, Jens [UCSD Scripps Institution of Oceanography; Cape Matatula Station; Reimann, Stefan [Swiss Federal Laboratories for Materials Science and Research (EMPA); Jungfraujoch Station; Vollmer, Martin [Swiss Federal Laboratories for Materials Science and Research (EMPA); Jungfraujoch Station; Simmonds, Peter G. [University of Bristol, Atmospheric Chemistry Research Group; Mace Head Station; Malone, Michela [University of Urbino; Monte Cimone Station; Arduini, Jgor [University of Urbino; Monte Cimone Station; Lunder, Chris [Norwegian Institute for Air Research; Ny Alesund Station; Hermansen, Ove [Norwegian Inst. for Air Research (NILU), Kjeller (Norway); Ny Alesund Station; Schmidbauer, Norbert [Norwegian Inst. for Air Research (NILU), Kjeller (Norway); Global Network; Young, Dickon [University of Bristol; Ragged Point Station; Wang, Hsiang J. (Ray) [Geogia Institute of Technology, School of Earth and Atmospheric Sciences; Global Network; Huang, Jin; Rigby, Matthew [University of Bristol; Global Network; Harth, Chris [UCSD, Scripps Institutioon of Oceanography; Global Network; Salameh, Peter [UCSD, Scripps Institution of Oceanography; Global Network; Spain, Gerard [National University of Ireland; Global Network; Steele, Paul [CSIRO Oceans and Atmosphere; Global Network; Arnold, Tim; Kim, Jooil [UCSD, Scripps Institution of Oceanography; Global Network; Derek, Nada; mitrevski, Blagoj; Langenfelds, Ray\n\n2008-01-01\n\nIn the ALE/GAGE/AGAGE global network program, continuous high frequency gas chromatographic measurements of four biogenic/anthropogenic gases (methane, CH4; nitrous oxide, N2O; hydrogen, H; and carbon monoxide, CO) and several anthropogenic gases that contribute to stratospheric ozone destruction and/or to the greenhouse effect have been carried out at five globally distributed sites for several years. The program, which began in 1978, is divided into three parts associated with three changes in instrumentation: the Atmospheric Lifetime Experiment (ALE), which used Hewlett Packard HP5840 gas chromatographs; the Global Atmospheric Gases Experiment (GAGE), which used HP5880 gas chromatographs; and the present Advanced GAGE (AGAGE). AGAGE uses two types of instruments: a gas chromatograph with multiple detectors (GC-MD), and a gas chromatograph with mass spectrometric analysis (GC-MS). Beginning in January 2004, an improved cryogenic preconcentration system (Medusa) replaced the absorption-desorption module in the GC-MS systems at Mace Head and Cape Grim; this provided improved capability to measure a broader range of volatile perfluorocarbons with high global warming potentials. More information may be found at the AGAGE home page: http://agage.eas.gatech.edu/instruments-gcms-medusa.htm.\n\nAssessing Pesticide Contamination to Fresh Water in Some Agricultural Sites, Close to Oaxaca City, Mexico\n\nNASA Astrophysics Data System (ADS)\n\nTomas, G.\n\n2002-12-01\n\nThis study presents the results of a survey on pesticides in fresh water in shallow aquifers, rivers and dams in Zaachila, Tlacolula and Etla and agricultural valleys close to Oaxaca City, SW of Mexico. In the study zones, there are generalized uses of pesticides and the impact on the water resources by inadequate use of agricultural activities. Water is used for irrigation and drinking. Surveying criteria was to sample the aquifer (production wells), its water table (dig wells) and a regional water collector (Plan Benito Juarez Yuayapan dam). A total of 14 samples were analyzed for the identification and quantification of organochlorine and organophosphorous pesticides. Method was 508-EPA. Gas chromatographer was a 5890 series II Hewlett Packard, calibrated with several patterns. Results: 10 samples are contaminated with some pesticide of the used patterns; Dieldrin, Chlordano, Malathion, Mirex were not found; Traces of organophosphorus compounds were found in 8 samples, mainly Merphos, Parathion Ethylic and Disulfoton ; There was detected traces of world-forbidden insecticides as Metoxychlor, Parathion Ethylic and Disulfoton; and In one sample (Cuilapam well #1) DDT exceeds, the Mexican maximum limit for potable water (1 mg/l),\n\nMulti-band infrared camera systems\n\nNASA Astrophysics Data System (ADS)\n\nDavis, Tim; Lang, Frank; Sinneger, Joe; Stabile, Paul; Tower, John\n\n1994-12-01\n\nThe program resulted in an IR camera system that utilizes a unique MOS addressable focal plane array (FPA) with full TV resolution, electronic control capability, and windowing capability. Two systems were delivered, each with two different camera heads: a Stirling-cooled 3-5 micron band head and a liquid nitrogen-cooled, filter-wheel-based, 1.5-5 micron band head. Signal processing features include averaging up to 16 frames, flexible compensation modes, gain and offset control, and real-time dither. The primary digital interface is a Hewlett-Packard standard GPID (IEEE-488) port that is used to upload and download data. The FPA employs an X-Y addressed PtSi photodiode array, CMOS horizontal and vertical scan registers, horizontal signal line (HSL) buffers followed by a high-gain preamplifier and a depletion NMOS output amplifier. The 640 x 480 MOS X-Y addressed FPA has a high degree of flexibility in operational modes. By changing the digital data pattern applied to the vertical scan register, the FPA can be operated in either an interlaced or noninterlaced format. The thermal sensitivity performance of the second system's Stirling-cooled head was the best of the systems produced.\n\nUNIX-based operating systems robustness evaluation\n\nNASA Technical Reports Server (NTRS)\n\nChang, Yu-Ming\n\n1996-01-01\n\nRobust operating systems are required for reliable computing. Techniques for robustness evaluation of operating systems not only enhance the understanding of the reliability of computer systems, but also provide valuable feed- back to system designers. This thesis presents results from robustness evaluation experiments on five UNIX-based operating systems, which include Digital Equipment's OSF/l, Hewlett Packard's HP-UX, Sun Microsystems' Solaris and SunOS, and Silicon Graphics' IRIX. Three sets of experiments were performed. The methodology for evaluation tested (1) the exception handling mechanism, (2) system resource management, and (3) system capacity under high workload stress. An exception generator was used to evaluate the exception handling mechanism of the operating systems. Results included exit status of the exception generator and the system state. Reso"
    }
}
{
    "id": "wrong_mix_random_spouse_00126_2",
    "rank": 70,
    "data": {
        "url": "https://medium.com/%40apratimdey/what-statistics-is-doing-wrong-in-the-21st-century-e1cdb77ca763",
        "read_more_link": "",
        "language": "en",
        "title": "Statistics in the 21st century",
        "top_image": "https://miro.medium.com/v2/1*m-R_BkNf1Qjr1YbyOIJY2w.png",
        "meta_img": "https://miro.medium.com/v2/1*m-R_BkNf1Qjr1YbyOIJY2w.png",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/da:true/resize:fill:88:88/0*BdtqwBb1GeBGCyiF",
            "https://miro.medium.com/v2/da:true/resize:fill:144:144/0*BdtqwBb1GeBGCyiF"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Rishi",
            "medium.com"
        ],
        "publish_date": "2023-10-08T22:36:07.070000+00:00",
        "summary": "",
        "meta_description": "David Donoho has recently written a very powerful and evocative article on how data science has evolved and will evolve in the 21st century. He has identified the main pillars on which it stands —…",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/1*m-R_BkNf1Qjr1YbyOIJY2w.png",
        "meta_site_name": "Medium",
        "canonical_link": "https://medium.com/@apratimdey/what-statistics-is-doing-wrong-in-the-21st-century-e1cdb77ca763",
        "text": "David Donoho has recently written a very powerful and evocative article on how data science has evolved and will evolve in the 21st century. He has identified the main pillars on which it stands — the common task framework and frictionless reproducibility — which have led to a dramatic explosion in engagement, research and competition in empirical machine learning. Researchers are actively involved in trying out new models — more and more complex by the day, involving billions of parameters and intensive compute— with the hope that they will be the champions (temporarily) on certain common competition tasks.\n\nThe thrill and adrenaline rush one gets in outperforming others is a very basic human characteristic, and we would be wrong to dismiss it. Even if the boom in data competitions did not happen, we have known this for a long time — this is one of the reasons behind the success of various olympiads and competitive college entrance examinations!\n\nSomehow, amidst all the excitement with data today, one discipline — statistics — seems to have been, quite surprisingly, siloed. We do not see sufficient participation of statisticians in today’s data science research, which, perhaps it is fair to say, seems to be dominated by machine learning groups in computer science departments. It may be counter-intuitive to many people, since they learn about data, models and randomness from statistics courses. So, it’s obvious that statisticians should be the leaders of any research that happens with data, right? Wrong.\n\nAs a statistician, this has puzzled me for quite some time too! I can safely say that we as statisticians understand the models and methods much better than several communities, many of whom participate in the competitions and conferences, yet we do not seem to enjoy nearly the same level of success! We keep getting sidelined. I have thought about this for some time, and I would like to share my own opinion based on my personal experiences. This might explain why statisticians find it difficult to adhere to Donoho’s pillars of data science.\n\nFor quite a long time, the conventional wisdom in statistics was that research should be about depth. Deeply understanding a method is what would qualify as a successful dissertation. People who have studied a particular method in a lot of depth and proved results about it, became famous. Tools were rare, data were scanty, one had to devote a significant portion of one’s career to a single problem, and suspicion about correctness was pervasive. Careful analysis of a particular method through theorems based on the well-accepted axioms of mathematics was, in some sense, the way to glory, and the world would respect the researcher as the expert in that area.\n\nWhile that certainly inspires respect, it came with two fundamental flaws, which, for a young field like statistics, have only begun to surface now.\n\nFirstly, I will argue that most of the fundamental questions in statistics have answers today, thanks to the depth pursued by generations of mathematical statisticians who have contributed richly to our knowledge and understanding of the various principles. For example, thanks to the theory developed by Bickel and Freedman, we know the very widely used technology known as the bootstrap, devised by Efron, can be trusted in a wide variety of statistical problems. However, as decades pass, the simpler questions get answered, and harder questions remain. Sometimes, they are so complex that any reasonable mathematical formulation and solution get defeated. We encounter the lamp-post issue: existing proof techniques can only answer certain questions, and others are simply left in the dark.\n\nBut science progresses, in the meanwhile, without caring for theorems! People manage to find reasonably good and working solutions to their problems, and do not have the pressing need to know the theoretical answer to a highly complicated question. Statisticians seemingly pursue such questions today, which are difficult to answer, and hence they need a lot of heavy lifting, but whether somebody really needs to know the answer, is an unanswered question!\n\nHence, answering such complicated questions, getting too much into depth of any one method, does not seem to be working to generate sufficient scientific interest and attention. Despite all the hard work, statisticians are and will be under-appreciated.\n\nSecondly, traditionally statistical papers have focussed on the following theme: take a method/model, and explain what it does. However, thanks to the ubiquity of data and resources today, a model is nothing more than a tool! A statistician who knows too much about a single model will find it difficult to communicate its relevance to a scientist who has their own question to pursue. The scientist would ask lots of questions the statistician won’t be simply equipped to answer — how do I know the assumptions hold? what if they don’t hold? can I do something else?\n\nThe statistician will soon realize that deep knowledge about that one single model is highly insufficient when faced with such questions. They would not have the confidence to recommend other methods, particularly heuristic ones that probably work just fine, but do not have well-grounded theory.\n\nAs I discuss above, the main “culprit” is pursuit of depth. What I see today, is the victory of breadth. In some sense, Donoho’s pillars can never work if people practised depth. The whole idea of the common task framework and frictionless reproducibility is built around the notion of success in terms of a few metrics. One single model, no matter how well you understand it, may not be powerful enough to make you successful, but if you combine a lot of models, even those you don’t understand at all, you may get to be at the top of the leaderboard!\n\nStatisticians, typically, I have seen, do not collaborate widely with scientists, which would have made them much more perceptive to these issues. It seems that statistics as a discipline has built a cocoon around itself, well-shielded from science. The eminent statistician P. C. Mahalanobis had pointed out about a century ago that statistics is an applied technology, it does not exist for the purpose of developing theory. Over the years, intellectuals like Tukey, Breiman and Donoho have warned that statisticians are getting trapped in their narrow depths. Statistics students are typically unaware of the tools actual scientists use.\n\nUnlike depth, breadth teaches one to be open-minded and accepting of completely different sets of ideas. The reason why statistical research has been about depth stems from its mathematical origins — in many departments (possibly even today), statistics used to be a part of mathematics. This has created an inherent (and unhealthy!) resistance to new ideas and methods, unless they are backed by strong theory. This is, in my view, the sole reason why statisticians, despite being the torch-bearers of data in any college campus, have not been able to dominate machine learning research. A change in this mindset will go a long way."
    }
}
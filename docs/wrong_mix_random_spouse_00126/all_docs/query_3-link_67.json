{
    "id": "wrong_mix_random_spouse_00126_3",
    "rank": 67,
    "data": {
        "url": "https://scikit-image.org/docs/stable/api/skimage.restoration.html",
        "read_more_link": "",
        "language": "en",
        "title": "skimage.restoration — skimage 0.24.0 documentation",
        "top_image": "https://scikit-image.org/docs/stable/_static/favicon.ico",
        "meta_img": "https://scikit-image.org/docs/stable/_static/favicon.ico",
        "images": [
            "https://scikit-image.org/docs/stable/_static/logo.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_j_invariant_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_j_invariant_tutorial_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_cycle_spinning_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_denoise_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_rank_filters_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_j_invariant_tutorial_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_nonlocal_means_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_j_invariant_tutorial_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_denoise_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_j_invariant_tutorial_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_solidification_tracking_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_j_invariant_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_denoise_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_cycle_spinning_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_denoise_wavelet_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_j_invariant_tutorial_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_rolling_ball_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_denoise_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_nonlocal_means_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_denoise_wavelet_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_j_invariant_tutorial_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_inpaint_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_cornea_spot_inpainting_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_deconvolution_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_rolling_ball_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_restoration_thumb.png",
            "https://scikit-image.org/docs/stable/_images/sphx_glr_plot_phase_unwrap_thumb.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "../_static/favicon.ico",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Perform non-local means denoising on 2D-4D grayscale or RGB images.\n\nParameters:\n\nimage2D or 3D ndarray\n\nInput image to be denoised, which can be 2D or 3D, and grayscale or RGB (for 2D images only, see channel_axis parameter). There can be any number of channels (does not strictly have to be RGB).\n\npatch_sizeint, optional\n\nSize of patches used for denoising.\n\npatch_distanceint, optional\n\nMaximal distance in pixels where to search patches used for denoising.\n\nhfloat, optional\n\nCut-off distance (in gray levels). The higher h, the more permissive one is in accepting patches. A higher h results in a smoother image, at the expense of blurring features. For a Gaussian noise of standard deviation sigma, a rule of thumb is to choose the value of h to be sigma of slightly less.\n\nfast_modebool, optional\n\nIf True (default value), a fast version of the non-local means algorithm is used. If False, the original version of non-local means is used. See the Notes section for more details about the algorithms.\n\nsigmafloat, optional\n\nThe standard deviation of the (Gaussian) noise. If provided, a more robust computation of patch weights is computed that takes the expected noise variance into account (see Notes below).\n\npreserve_rangebool, optional\n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html\n\nchannel_axisint or None, optional\n\nIf None, the image is assumed to be a grayscale (single channel) image. Otherwise, this parameter indicates which axis of the array corresponds to channels.\n\nAdded in version 0.19: channel_axis was added in 0.19.\n\nReturns:\n\nresultndarray\n\nDenoised image, of same shape as image.\n\nNotes\n\nThe non-local means algorithm is well suited for denoising images with specific textures. The principle of the algorithm is to average the value of a given pixel with values of other pixels in a limited neighborhood, provided that the patches centered on the other pixels are similar enough to the patch centered on the pixel of interest.\n\nIn the original version of the algorithm [1], corresponding to fast=False, the computational complexity is:\n\nimage.size * patch_size ** image.ndim * patch_distance ** image.ndim\n\nHence, changing the size of patches or their maximal distance has a strong effect on computing times, especially for 3-D images.\n\nHowever, the default behavior corresponds to fast_mode=True, for which another version of non-local means [2] is used, corresponding to a complexity of:\n\nimage.size * patch_distance ** image.ndim\n\nThe computing time depends only weakly on the patch size, thanks to the computation of the integral of patches distances for a given shift, that reduces the number of operations [1]. Therefore, this algorithm executes faster than the classic algorithm (fast_mode=False), at the expense of using twice as much memory. This implementation has been proven to be more efficient compared to other alternatives, see e.g. [3].\n\nCompared to the classic algorithm, all pixels of a patch contribute to the distance to another patch with the same weight, no matter their distance to the center of the patch. This coarser computation of the distance can result in a slightly poorer denoising performance. Moreover, for small images (images with a linear size that is only a few times the patch size), the classic algorithm can be faster due to boundary effects.\n\nThe image is padded using the reflect mode of skimage.util.pad before denoising.\n\nIf the noise standard deviation, sigma, is provided a more robust computation of patch weights is used. Subtracting the known noise variance from the computed patch distances improves the estimates of patch similarity, giving a moderate improvement to denoising performance [4]. It was also mentioned as an option for the fast variant of the algorithm in [3].\n\nWhen sigma is provided, a smaller h should typically be used to avoid oversmoothing. The optimal value for h depends on the image content and noise level, but a reasonable starting point is h = 0.8 * sigma when fast_mode is True, or h = 0.6 * sigma when fast_mode is False.\n\nReferences\n\n[1] (1,2)\n\nA. Buades, B. Coll, & J-M. Morel. A non-local algorithm for image denoising. In CVPR 2005, Vol. 2, pp. 60-65, IEEE. DOI:10.1109/CVPR.2005.38\n\n[2]\n\nJ. Darbon, A. Cunha, T.F. Chan, S. Osher, and G.J. Jensen, Fast nonlocal filtering applied to electron cryomicroscopy, in 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro, 2008, pp. 1331-1334. DOI:10.1109/ISBI.2008.4541250\n\n[3] (1,2)\n\nJacques Froment. Parameter-Free Fast Pixelwise Non-Local Means Denoising. Image Processing On Line, 2014, vol. 4, pp. 300-326. DOI:10.5201/ipol.2014.120\n\n[4]\n\nA. Buades, B. Coll, & J-M. Morel. Non-Local Means Denoising. Image Processing On Line, 2011, vol. 1, pp. 208-212. DOI:10.5201/ipol.2011.bcm_nlm\n\nExamples\n\n>>> a = np.zeros((40, 40)) >>> a[10:-10, 10:-10] = 1. >>> rng = np.random.default_rng() >>> a += 0.3 * rng.standard_normal(a.shape) >>> denoised_a = denoise_nl_means(a, 7, 5, 0.1)\n\nNon-local means denoising for preserving textures\n\nNon-local means denoising for preserving textures\n\nFull tutorial on calibrating Denoisers Using J-Invariance\n\nFull tutorial on calibrating Denoisers Using J-Invariance\n\nPerform total variation denoising using split-Bregman optimization.\n\nGiven \\(f\\), a noisy image (input data), total variation denoising (also known as total variation regularization) aims to find an image \\(u\\) with less total variation than \\(f\\), under the constraint that \\(u\\) remain similar to \\(f\\). This can be expressed by the Rudin–Osher–Fatemi (ROF) minimization problem:\n\n\\[\\min_{u} \\sum_{i=0}^{N-1} \\left( \\left| \\nabla{u_i} \\right| + \\frac{\\lambda}{2}(f_i - u_i)^2 \\right)\\]\n\nwhere \\(\\lambda\\) is a positive parameter. The first term of this cost function is the total variation; the second term represents data fidelity. As \\(\\lambda \\to 0\\), the total variation term dominates, forcing the solution to have smaller total variation, at the expense of looking less like the input data.\n\nThis code is an implementation of the split Bregman algorithm of Goldstein and Osher to solve the ROF problem ([1], [2], [3]).\n\nParameters:\n\nimagendarray\n\nInput image to be denoised (converted using img_as_float()).\n\nweightfloat, optional\n\nDenoising weight. It is equal to \\(\\frac{\\lambda}{2}\\). Therefore, the smaller the weight, the more denoising (at the expense of less similarity to image).\n\nepsfloat, optional\n\nTolerance \\(\\varepsilon > 0\\) for the stop criterion: The algorithm stops when \\(\\|u_n - u_{n-1}\\|_2 < \\varepsilon\\).\n\nmax_num_iterint, optional\n\nMaximal number of iterations used for the optimization.\n\nisotropicboolean, optional\n\nSwitch between isotropic and anisotropic TV denoising.\n\nchannel_axisint or None, optional\n\nIf None, the image is assumed to be grayscale (single-channel). Otherwise, this parameter indicates which axis of the array corresponds to channels.\n\nAdded in version 0.19: channel_axis was added in 0.19.\n\nReturns:\n\nundarray\n\nDenoised image.\n\nSee also\n\ndenoise_tv_chambolle\n\nPerform total variation denoising in nD.\n\nNotes\n\nEnsure that channel_axis parameter is set appropriately for color images.\n\nThe principle of total variation denoising is explained in [4]. It is about minimizing the total variation of an image, which can be roughly described as the integral of the norm of the image gradient. Total variation denoising tends to produce cartoon-like images, that is, piecewise-constant images.\n\nReferences\n\n[1]\n\nTom Goldstein and Stanley Osher, “The Split Bregman Method For L1 Regularized Problems”, https://ww3.math.ucla.edu/camreport/cam08-29.pdf\n\n[2]\n\nPascal Getreuer, “Rudin–Osher–Fatemi Total Variation Denoising using Split Bregman” in Image Processing On Line on 2012–05–19, https://www.ipol.im/pub/art/2012/g-tvd/article_lr.pdf\n\nPerform total variation denoising in nD.\n\nGiven \\(f\\), a noisy image (input data), total variation denoising (also known as total variation regularization) aims to find an image \\(u\\) with less total variation than \\(f\\), under the constraint that \\(u\\) remain similar to \\(f\\). This can be expressed by the Rudin–Osher–Fatemi (ROF) minimization problem:\n\n\\[\\min_{u} \\sum_{i=0}^{N-1} \\left( \\left| \\nabla{u_i} \\right| + \\frac{\\lambda}{2}(f_i - u_i)^2 \\right)\\]\n\nwhere \\(\\lambda\\) is a positive parameter. The first term of this cost function is the total variation; the second term represents data fidelity. As \\(\\lambda \\to 0\\), the total variation term dominates, forcing the solution to have smaller total variation, at the expense of looking less like the input data.\n\nThis code is an implementation of the algorithm proposed by Chambolle in [1] to solve the ROF problem.\n\nParameters:\n\nimagendarray\n\nInput image to be denoised. If its dtype is not float, it gets converted with img_as_float().\n\nweightfloat, optional\n\nDenoising weight. It is equal to \\(\\frac{1}{\\lambda}\\). Therefore, the greater the weight, the more denoising (at the expense of fidelity to image).\n\nepsfloat, optional\n\nTolerance \\(\\varepsilon > 0\\) for the stop criterion (compares to absolute value of relative difference of the cost function \\(E\\)): The algorithm stops when \\(|E_{n-1} - E_n| < \\varepsilon * E_0\\).\n\nmax_num_iterint, optional\n\nMaximal number of iterations used for the optimization.\n\nchannel_axisint or None, optional\n\nIf None, the image is assumed to be grayscale (single-channel). Otherwise, this parameter indicates which axis of the array corresponds to channels.\n\nAdded in version 0.19: channel_axis was added in 0.19.\n\nReturns:\n\nundarray\n\nDenoised image.\n\nSee also\n\ndenoise_tv_bregman\n\nPerform total variation denoising using split-Bregman optimization.\n\nNotes\n\nMake sure to set the channel_axis parameter appropriately for color images.\n\nThe principle of total variation denoising is explained in [2]. It is about minimizing the total variation of an image, which can be roughly described as the integral of the norm of the image gradient. Total variation denoising tends to produce cartoon-like images, that is, piecewise-constant images.\n\nReferences\n\n[1]\n\nA. Chambolle, An algorithm for total variation minimization and applications, Journal of Mathematical Imaging and Vision, Springer, 2004, 20, 89-97.\n\nExamples\n\n2D example on astronaut image:\n\n>>> from skimage import color, data >>> img = color.rgb2gray(data.astronaut())[:50, :50] >>> rng = np.random.default_rng() >>> img += 0.5 * img.std() * rng.standard_normal(img.shape) >>> denoised_img = denoise_tv_chambolle(img, weight=60)\n\n3D example on synthetic data:\n\n>>> x, y, z = np.ogrid[0:20, 0:20, 0:20] >>> mask = (x - 22)**2 + (y - 20)**2 + (z - 17)**2 < 8**2 >>> mask = mask.astype(float) >>> rng = np.random.default_rng() >>> mask += 0.2 * rng.standard_normal(mask.shape) >>> res = denoise_tv_chambolle(mask, weight=100)\n\nDenoising a picture\n\nDenoising a picture\n\nFull tutorial on calibrating Denoisers Using J-Invariance\n\nFull tutorial on calibrating Denoisers Using J-Invariance\n\nTrack solidification of a metallic alloy\n\nTrack solidification of a metallic alloy\n\nPerform wavelet denoising on an image.\n\nParameters:\n\nimagendarray (M[, N[, …P]][, C]) of ints, uints or floats\n\nInput data to be denoised. image can be of any numeric type, but it is cast into an ndarray of floats for the computation of the denoised image.\n\nsigmafloat or list, optional\n\nThe noise standard deviation used when computing the wavelet detail coefficient threshold(s). When None (default), the noise standard deviation is estimated via the method in [2].\n\nwaveletstring, optional\n\nThe type of wavelet to perform and can be any of the options pywt.wavelist outputs. The default is ‘db1’. For example, wavelet can be any of {'db2', 'haar', 'sym9'} and many more.\n\nmode{‘soft’, ‘hard’}, optional\n\nAn optional argument to choose the type of denoising performed. It noted that choosing soft thresholding given additive noise finds the best approximation of the original image.\n\nwavelet_levelsint or None, optional\n\nThe number of wavelet decomposition levels to use. The default is three less than the maximum number of possible decomposition levels.\n\nconvert2ycbcrbool, optional\n\nIf True and channel_axis is set, do the wavelet denoising in the YCbCr colorspace instead of the RGB color space. This typically results in better performance for RGB images.\n\nmethod{‘BayesShrink’, ‘VisuShrink’}, optional\n\nThresholding method to be used. The currently supported methods are “BayesShrink” [1] and “VisuShrink” [2]. Defaults to “BayesShrink”.\n\nrescale_sigmabool, optional\n\nIf False, no rescaling of the user-provided sigma will be performed. The default of True rescales sigma appropriately if the image is rescaled internally.\n\nAdded in version 0.16: rescale_sigma was introduced in 0.16\n\nchannel_axisint or None, optional\n\nIf None, the image is assumed to be grayscale (single-channel). Otherwise, this parameter indicates which axis of the array corresponds to channels.\n\nAdded in version 0.19: channel_axis was added in 0.19.\n\nReturns:\n\noutndarray\n\nDenoised image.\n\nNotes\n\nThe wavelet domain is a sparse representation of the image, and can be thought of similarly to the frequency domain of the Fourier transform. Sparse representations have most values zero or near-zero and truly random noise is (usually) represented by many small values in the wavelet domain. Setting all values below some threshold to 0 reduces the noise in the image, but larger thresholds also decrease the detail present in the image.\n\nIf the input is 3D, this function performs wavelet denoising on each color plane separately.\n\nChanged in version 0.16: For floating point inputs, the original input range is maintained and there is no clipping applied to the output. Other input types will be converted to a floating point value in the range [-1, 1] or [0, 1] depending on the input image range. Unless rescale_sigma = False, any internal rescaling applied to the image will also be applied to sigma to maintain the same relative amplitude.\n\nMany wavelet coefficient thresholding approaches have been proposed. By default, denoise_wavelet applies BayesShrink, which is an adaptive thresholding method that computes separate thresholds for each wavelet sub-band as described in [1].\n\nIf method == \"VisuShrink\", a single “universal threshold” is applied to all wavelet detail coefficients as described in [2]. This threshold is designed to remove all Gaussian noise at a given sigma with high probability, but tends to produce images that appear overly smooth.\n\nAlthough any of the wavelets from PyWavelets can be selected, the thresholding methods assume an orthogonal wavelet transform and may not choose the threshold appropriately for biorthogonal wavelets. Orthogonal wavelets are desirable because white noise in the input remains white noise in the subbands. Biorthogonal wavelets lead to colored noise in the subbands. Additionally, the orthogonal wavelets in PyWavelets are orthonormal so that noise variance in the subbands remains identical to the noise variance of the input. Example orthogonal wavelets are the Daubechies (e.g. ‘db2’) or symmlet (e.g. ‘sym2’) families.\n\nReferences\n\n[1] (1,2)\n\nChang, S. Grace, Bin Yu, and Martin Vetterli. “Adaptive wavelet thresholding for image denoising and compression.” Image Processing, IEEE Transactions on 9.9 (2000): 1532-1546. DOI:10.1109/83.862633\n\n[2] (1,2,3)\n\nD. L. Donoho and I. M. Johnstone. “Ideal spatial adaptation by wavelet shrinkage.” Biometrika 81.3 (1994): 425-455. DOI:10.1093/biomet/81.3.425\n\nExamples\n\n>>> from skimage import color, data >>> img = img_as_float(data.astronaut()) >>> img = color.rgb2gray(img) >>> rng = np.random.default_rng() >>> img += 0.1 * rng.standard_normal(img.shape) >>> img = np.clip(img, 0, 1) >>> denoised_img = denoise_wavelet(img, sigma=0.1, rescale_sigma=True)\n\nCalibrating Denoisers Using J-Invariance\n\nCalibrating Denoisers Using J-Invariance\n\nDenoising a picture\n\nDenoising a picture\n\nShift-invariant wavelet denoising\n\nShift-invariant wavelet denoising\n\nWavelet denoising\n\nWavelet denoising\n\nFull tutorial on calibrating Denoisers Using J-Invariance\n\nFull tutorial on calibrating Denoisers Using J-Invariance\n\nEstimate background intensity by rolling/translating a kernel.\n\nThis rolling ball algorithm estimates background intensity for a ndimage in case of uneven exposure. It is a generalization of the frequently used rolling ball algorithm [1].\n\nParameters:\n\nimagendarray\n\nThe image to be filtered.\n\nradiusint, optional\n\nRadius of a ball-shaped kernel to be rolled/translated in the image. Used if kernel = None.\n\nkernelndarray, optional\n\nThe kernel to be rolled/translated in the image. It must have the same number of dimensions as image. Kernel is filled with the intensity of the kernel at that position.\n\nnansafe: bool, optional\n\nIf False (default) assumes that none of the values in image are np.nan, and uses a faster implementation.\n\nnum_threads: int, optional\n\nThe maximum number of threads to use. If None use the OpenMP default value; typically equal to the maximum number of virtual cores. Note: This is an upper limit to the number of threads. The exact number is determined by the system’s OpenMP library.\n\nReturns:\n\nbackgroundndarray\n\nThe estimated background of the image.\n\nNotes\n\nFor the pixel that has its background intensity estimated (without loss of generality at center) the rolling ball method centers kernel under it and raises the kernel until the surface touches the image umbra at some pos=(y,x). The background intensity is then estimated using the image intensity at that position (image[pos]) plus the difference of kernel[center] - kernel[pos].\n\nThis algorithm assumes that dark pixels correspond to the background. If you have a bright background, invert the image before passing it to the function, e.g., using utils.invert. See the gallery example for details.\n\nThis algorithm is sensitive to noise (in particular salt-and-pepper noise). If this is a problem in your image, you can apply mild gaussian smoothing before passing the image to this function.\n\nThis algorithm’s complexity is polynomial in the radius, with degree equal to the image dimensionality (a 2D image is N^2, a 3D image is N^3, etc.), so it can take a long time as the radius grows beyond 30 or so ([2], [3]). It is an exact N-dimensional calculation; if all you need is an approximation, faster options to consider are top-hat filtering [4] or downscaling-then-upscaling to reduce the size of the input processed.\n\nReferences\n\nExamples\n\n>>> import numpy as np >>> from skimage import data >>> from skimage.restoration import rolling_ball >>> image = data.coins() >>> background = rolling_ball(data.coins()) >>> filtered_image = image - background\n\n>>> import numpy as np >>> from skimage import data >>> from skimage.restoration import rolling_ball, ellipsoid_kernel >>> image = data.coins() >>> kernel = ellipsoid_kernel((101, 101), 75) >>> background = rolling_ball(data.coins(), kernel=kernel) >>> filtered_image = image - background\n\nUse rolling-ball algorithm for estimating background intensity\n\nUse rolling-ball algorithm for estimating background intensity\n\nUnsupervised Wiener-Hunt deconvolution.\n\nReturn the deconvolution with a Wiener-Hunt approach, where the hyperparameters are automatically estimated. The algorithm is a stochastic iterative process (Gibbs sampler) described in the reference below. See also wiener function.\n\nParameters:\n\nimage(M, N) ndarray\n\nThe input degraded image.\n\npsfndarray\n\nThe impulse response (input image’s space) or the transfer function (Fourier space). Both are accepted. The transfer function is automatically recognized as being complex (np.iscomplexobj(psf)).\n\nregndarray, optional\n\nThe regularisation operator. The Laplacian by default. It can be an impulse response or a transfer function, as for the psf.\n\nuser_paramsdict, optional\n\nDictionary of parameters for the Gibbs sampler. See below.\n\nclipboolean, optional\n\nTrue by default. If true, pixel values of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.\n\nrng{numpy.random.Generator, int}, optional\n\nPseudo-random number generator. By default, a PCG64 generator is used (see numpy.random.default_rng()). If rng is an int, it is used to seed the generator.\n\nAdded in version 0.19.\n\nReturns:\n\nx_postmean(M, N) ndarray\n\nThe deconvolved image (the posterior mean).\n\nchainsdict\n\nThe keys noise and prior contain the chain list of noise and prior precision respectively.\n\nOther Parameters:\n\nThe keys of ``user_params`` are:\n\nthresholdfloat\n\nThe stopping criterion: the norm of the difference between to successive approximated solution (empirical mean of object samples, see Notes section). 1e-4 by default.\n\nburninint\n\nThe number of sample to ignore to start computation of the mean. 15 by default.\n\nmin_num_iterint\n\nThe minimum number of iterations. 30 by default.\n\nmax_num_iterint\n\nThe maximum number of iterations if threshold is not satisfied. 200 by default.\n\ncallbackcallable (None by default)\n\nA user provided callable to which is passed, if the function exists, the current image sample for whatever purpose. The user can store the sample, or compute other moments than the mean. It has no influence on the algorithm execution and is only for inspection.\n\nNotes\n\nThe estimated image is design as the posterior mean of a probability law (from a Bayesian analysis). The mean is defined as a sum over all the possible images weighted by their respective probability. Given the size of the problem, the exact sum is not tractable. This algorithm use of MCMC to draw image under the posterior law. The practical idea is to only draw highly probable images since they have the biggest contribution to the mean. At the opposite, the less probable images are drawn less often since their contribution is low. Finally, the empirical mean of these samples give us an estimation of the mean, and an exact computation with an infinite sample set.\n\nReferences\n\n[1]\n\nFrançois Orieux, Jean-François Giovannelli, and Thomas Rodet, “Bayesian estimation of regularization and point spread function parameters for Wiener-Hunt deconvolution”, J. Opt. Soc. Am. A 27, 1593-1607 (2010)\n\nhttps://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-27-7-1593\n\nhttps://hal.archives-ouvertes.fr/hal-00674508\n\nExamples\n\n>>> from skimage import color, data, restoration >>> img = color.rgb2gray(data.astronaut()) >>> from scipy.signal import convolve2d >>> psf = np.ones((5, 5)) / 25 >>> img = convolve2d(img, psf, 'same') >>> rng = np.random.default_rng() >>> img += 0.1 * img.std() * rng.standard_normal(img.shape) >>> deconvolved_img = restoration.unsupervised_wiener(img, psf)\n\nImage Deconvolution\n\nImage Deconvolution\n\nWiener-Hunt deconvolution\n\nReturn the deconvolution with a Wiener-Hunt approach (i.e. with Fourier diagonalisation).\n\nParameters:\n\nimagendarray\n\nInput degraded image (can be n-dimensional).\n\npsfndarray\n\nPoint Spread Function. This is assumed to be the impulse response (input image space) if the data-type is real, or the transfer function (Fourier space) if the data-type is complex. There is no constraints on the shape of the impulse response. The transfer function must be of shape (N1, N2, …, ND) if is_real is True, (N1, N2, …, ND // 2 + 1) otherwise (see np.fft.rfftn).\n\nbalancefloat\n\nThe regularisation parameter value that tunes the balance between the data adequacy that improve frequency restoration and the prior adequacy that reduce frequency restoration (to avoid noise artifacts).\n\nregndarray, optional\n\nThe regularisation operator. The Laplacian by default. It can be an impulse response or a transfer function, as for the psf. Shape constraint is the same as for the psf parameter.\n\nis_realboolean, optional\n\nTrue by default. Specify if psf and reg are provided with hermitian hypothesis, that is only half of the frequency plane is provided (due to the redundancy of Fourier transform of real signal). It’s apply only if psf and/or reg are provided as transfer function. For the hermitian property see uft module or np.fft.rfftn.\n\nclipboolean, optional\n\nTrue by default. If True, pixel values of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.\n\nReturns:\n\nim_deconv(M, N) ndarray\n\nThe deconvolved image.\n\nNotes\n\nThis function applies the Wiener filter to a noisy and degraded image by an impulse response (or PSF). If the data model is\n\n\\[y = Hx + n\\]\n\nwhere \\(n\\) is noise, \\(H\\) the PSF and \\(x\\) the unknown original image, the Wiener filter is\n\n\\[\\hat x = F^\\dagger \\left( |\\Lambda_H|^2 + \\lambda |\\Lambda_D|^2 \\right)^{-1} \\Lambda_H^\\dagger F y\\]\n\nwhere \\(F\\) and \\(F^\\dagger\\) are the Fourier and inverse Fourier transforms respectively, \\(\\Lambda_H\\) the transfer function (or the Fourier transform of the PSF, see [Hunt] below) and \\(\\Lambda_D\\) the filter to penalize the restored image frequencies (Laplacian by default, that is penalization of high frequency). The parameter \\(\\lambda\\) tunes the balance between the data (that tends to increase high frequency, even those coming from noise), and the regularization.\n\nThese methods are then specific to a prior model. Consequently, the application or the true image nature must correspond to the prior model. By default, the prior model (Laplacian) introduce image smoothness or pixel correlation. It can also be interpreted as high-frequency penalization to compensate the instability of the solution with respect to the data (sometimes called noise amplification or “explosive” solution).\n\nFinally, the use of Fourier space implies a circulant property of \\(H\\), see [2].\n\nReferences\n\n[1]\n\nFrançois Orieux, Jean-François Giovannelli, and Thomas Rodet, “Bayesian estimation of regularization and point spread function parameters for Wiener-Hunt deconvolution”, J. Opt. Soc. Am. A 27, 1593-1607 (2010)\n\nhttps://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-27-7-1593\n\nhttps://hal.archives-ouvertes.fr/hal-00674508\n\n[2]\n\nB. R. Hunt “A matrix theory proof of the discrete convolution theorem”, IEEE Trans. on Audio and Electroacoustics, vol. au-19, no. 4, pp. 285-288, dec. 1971\n\nExamples\n\n>>> from skimage import color, data, restoration >>> img = color.rgb2gray(data.astronaut()) >>> from scipy.signal import convolve2d >>> psf = np.ones((5, 5)) / 25 >>> img = convolve2d(img, psf, 'same') >>> rng = np.random.default_rng() >>> img += 0.1 * img.std() * rng.standard_normal(img.shape) >>> deconvolved_img = restoration.wiener(img, psf, 0.1)"
    }
}
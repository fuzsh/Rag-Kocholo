{
    "id": "correct_foundationPlace_00087_1",
    "rank": 79,
    "data": {
        "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper",
        "read_more_link": "",
        "language": "en",
        "title": "A pro-innovation approach to AI regulation",
        "top_image": "https://www.gov.uk/assets/static/govuk-opengraph-image-03837e1cec82f217cf32514635a13c879b8c400ae3b1c207c5744411658c7635.png",
        "meta_img": "https://www.gov.uk/assets/static/govuk-opengraph-image-03837e1cec82f217cf32514635a13c879b8c400ae3b1c207c5744411658c7635.png",
        "images": [
            "https://www.gov.uk/assets/government-frontend/open-government-licence-min-93b6a51b518ff99714a1aa2a7d2162735c155ec3cb073c75fb88b2a332fa83d3.png",
            "https://assets.publishing.service.gov.uk/media/6420d13632a8e00012fa94d9/Minister_Michelle_Donelan_UK_Parliament_960px_640px.jpg",
            "https://assets.publishing.service.gov.uk/media/6423094d3d885d000fdadd30/Diagram01__1_.svg",
            "https://assets.publishing.service.gov.uk/media/6422fec960a35e000c0cafb0/Diagram03-blackText__1_.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Department for Science, Innovation and Technology"
        ],
        "publish_date": "2023-03-29T11:12:11+01:00",
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/assets/static/favicon-f54816fc15997bd42cd90e4c50b896a1fc098c0c32957d4e5effbfa9f9b35e53.ico",
        "meta_site_name": "GOV.UK",
        "canonical_link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper",
        "text": "Presented to Parliament by the Secretary of State for Science, Innovation and Technology by Command of His Majesty on 29 March 2023.\n\nCommand Paper Number: 815\n\n© Crown copyright 2023\n\nISBN: 978-1-5286-4009-1\n\nCorrection slip\n\nCorrection:\n\nText currently reads in Annex C:\n\n1. Do you agree that requiring organisations to make it clear when they are using AI would adequately ensure transparency?\n\n2. What other transparency measures would be appropriate, if any?\n\n3. Do you agree that current routes to contestability or redress for AI-related harms are adequate?\n\n4. How could routes to contestability or redress for AI-related harms be improved, if at all?\n\n[…]\n\nL3. If you are a business that develops, uses, or sells AI, how do you currently manage AI risk including through the wider supply chain? How could government support effective AI-related risk management?\n\n[…]\n\nS1. Which of the sandbox models described in section 3.3.4 would be most likely to support innovation?\n\nText should read:\n\n1: Do you agree that requiring organisations to make it clear when they are using AI would improve transparency?\n\n2: Are there other measures we could require of organisations to improve transparency for AI?\n\n3: Do you agree that current routes to contest or get redress for AI-related harms are adequate?\n\n4: How could current routes to contest or seek redress for AI-related harms be improved, if at all?\n\n[…]\n\nL3: If you work for a business that develops, uses, or sells AI, how do you currently manage AI risk including through the wider supply chain? How could government support effective AI-related risk management?\n\n[…]\n\nS1: To what extent would the sandbox models described in section 3.3.4 support innovation?\n\nDate of correction: 4 July 2023\n\nMinisterial foreword\n\nI believe that a common-sense, outcomes-oriented approach is the best way to get right to the heart of delivering on the priorities of people across the UK. Better public services, high quality jobs and opportunities to learn the skills that will power our future – these are the priorities that will drive our goal to become a science and technology superpower by 2030.\n\nArtificial Intelligence (AI) will play a central part in delivering and enabling these goals, and this white paper will ensure we are putting the UK on course to be the best place in the world to build, test and use AI technology. But we are not starting from zero. Having invested over £2.5 billion in AI since 2014, this paper builds on our recent announcements of £110 million for our AI Tech Missions Fund, £900 million to establish a new AI Research Resource and to develop an exascale supercomputer capable of running large AI models – backed up by our new £8 million AI Global Talent Network and £117 million of existing funding to create hundreds of new PhDs for AI researchers.\n\nMost of us are only now beginning to understand the transformative potential of AI as the technology rapidly improves. But in many ways, AI is already delivering fantastic social and economic benefits for real people – from improving NHS medical care to making transport safer. Recent advances in things like generative AI give us a glimpse into the enormous opportunities that await us in the near future if we are prepared to lead the world in the AI sector with our values of transparency, accountability and innovation.\n\nMy vision for an AI-enabled country is one where our NHS heroes are able to save lives using AI technologies that were unimaginable just a few decades ago. I want our police, transport networks and climate scientists and many more to be empowered by AI technologies that will make the UK the smartest, healthiest, safest and happiest place to live and work. That is why AI is one of this government’s 5 technologies of tomorrow – bringing stronger growth, better jobs, and bold new discoveries. It is a vision that has been shaped by stakeholders and experts in AI, whose expertise and ideas I am determined to see reflected in our department.\n\nThe UK has been at the forefront of this progress, placing third in the world for AI research and development. We are home to a third of Europe’s total AI companies and twice as many as any other European country. Our world-leading status is down to our thriving research base and the pipeline of expertise graduating through our universities, the ingenuity of our innovators and the government’s long-term commitment to invest in AI.\n\nTo ensure we become an AI superpower, though, it is crucial that we do all we can to create the right environment to harness the benefits of AI and remain at the forefront of technological developments. That includes getting regulation right so that innovators can thrive and the risks posed by AI can be addressed.\n\nThese risks could include anything from physical harm, an undermining of national security, as well as risks to mental health. The development and deployment of AI can also present ethical challenges which do not always have clear answers. Unless we act, household consumers, public services and businesses will not trust the technology and will be nervous about adopting it. Unless we build public trust, we will miss out on many of the benefits on offer.\n\nIndeed, the pace of change itself can be unsettling. Some fear a future in which AI replaces or displaces jobs, for example. Our white paper and our vision for a future AI-enabled country is one in which our ways of working are complemented by AI rather than disrupted by it. In the modern world, too much of our professional lives are taken up by monotonous tasks – inputting data, filling out paperwork, scanning through documents for one piece of information and so on. AI in the workplace has the potential to free us up from these tasks, allowing us to spend more time doing the things we trained for – teachers with more time to teach, clinicians with more time to spend with patients, police officers with more time on the beat rather than behind a desk – the list goes on.\n\nIndeed, since AI is already in our day-to-day lives, there are numerous examples that can help to illustrate the real, tangible benefits that AI can bring once any risks are mitigated. Streaming services already use advanced AI to recommend TV shows and films to us. Our satnav uses AI to plot the fastest routes for our journeys, or helps us avoid traffic by intelligently predicting where congestion will be on our journey. And of course, almost all of us carry a smartphone in our pockets that uses advanced AI in all sorts of ways. These common devices all carried risks at one time or another, but today they benefit us enormously.\n\nThat is why our white paper details how we intend to support innovation while providing a framework to ensure risks are identified and addressed. However, a heavy-handed and rigid approach can stifle innovation and slow AI adoption. That is why we set out a proportionate and pro-innovation regulatory framework. Rather than target specific technologies, it focuses on the context in which AI is deployed. This enables us to take a balanced approach to weighing up the benefits versus the potential risks.\n\nWe recognise that particular AI technologies, foundation models for example, can be applied in many different ways and this means the risks can vary hugely. For example, using a chatbot to produce a summary of a long article presents very different risks to using the same technology to provide medical advice. We understand the need to monitor these developments in partnership with innovators while also avoiding placing unnecessary regulatory burdens on those deploying AI.\n\nTo ensure our regulatory framework is effective, we will leverage the expertise of our world class regulators. They understand the risks in their sectors and are best placed to take a proportionate approach to regulating AI. This will mean supporting innovation and working closely with business, but also stepping in to address risks when necessary. By underpinning the framework with a set of principles, we will drive consistency across regulators while also providing them with the flexibility needed.\n\nFor innovators working at the cutting edge and developing novel technologies, navigating regulatory regimes can be challenging. That’s why we are confirming our commitment to taking forward a key recommendation made by Sir Patrick Vallance to establish a regulatory sandbox for AI. This will bring together regulators to support innovators directly and help them get their products to market. The sandbox will also enable us to understand how regulation interacts with new technologies and refine this interaction where necessary.\n\nHaving exited the European Union we are free to establish a regulatory approach that enables us to establish the UK as an AI superpower. It is an approach that will actively support innovation while addressing risks and public concerns. The UK is home to thriving start-ups, which our framework will support to scale-up and compete internationally. Our pro-innovation approach will also act as a strong incentive when it comes to AI businesses based overseas establishing a presence in the UK. The white paper sets out our commitment to engaging internationally to support interoperability across different regulatory regimes. Not only will this ease the burden on business but it will also allow us to embed our values as global approaches to governing AI develop.\n\nOur approach relies on collaboration between government, regulators and business. Initially, we do not intend to introduce new legislation. By rushing to legislate too early, we would risk placing undue burdens on businesses. But alongside empowering regulators to take a lead, we are also setting expectations. Our new monitoring functions will provide a real time assessment of how the regulatory framework is performing so that we can be confident that it is proportionate. The pace of technological development also means that we need to understand new and emerging risks, engaging with experts to ensure we take action where necessary. A critical component of this activity will be engaging with the public to understand their expectations, raising awareness of the potential of AI and demonstrating that we are responding to concerns.\n\nThe framework set out in this white paper is deliberately designed to be flexible. As the technology evolves, our regulatory approach may also need to adjust. Our principles-based approach, with central functions to monitor and drive collaboration, will enable us to adapt as needed while providing industry with the clarity needed to innovate. We will continue to develop our approach, building on our commitment to making the UK the best place in the world to be a business developing and using AI. Responses to the consultation will inform how we develop the regulatory framework – I encourage all of those with an interest to respond.\n\nExecutive summary\n\nArtificial intelligence – the opportunity and the challenge\n\n1. Artificial intelligence (AI) is already delivering wide societal benefits, from medical advances[footnote 1] to mitigating climate change.[footnote 2] For example, an AI technology developed by DeepMind, a UK-based business, can now predict the structure of almost every protein known to science.[footnote 3] This breakthrough will accelerate scientific research and the development of life-saving medicines – it has already helped scientists to make huge progress in combating malaria, antibiotic resistance, and plastic waste.\n\n2. The UK Science and Technology Framework[footnote 4] sets out government’s strategic vision and identifies AI as one of 5 critical technologies. The framework notes the role of regulation in creating the environment for AI to flourish. We know that we have yet to see AI technologies reach their full potential. Under the right conditions, AI will transform all areas of life[footnote 5] and stimulate the UK economy by unleashing innovation and driving productivity,[footnote 6] creating new jobs and improving the workplace.\n\n3. Across the world, countries and regions are beginning to draft the rules for AI. The UK needs to act quickly to continue to lead the international conversation on AI governance and demonstrate the value of our pragmatic, proportionate regulatory approach. The need to act was highlighted by Sir Patrick Vallance in his recent Regulation for Innovation review. The report identifies the short time frame for government intervention to provide a clear, pro-innovation regulatory environment in order to make the UK one of the top places in the world to build foundational AI companies.[footnote 7]\n\n4. While we should capitalise on the benefits of these technologies, we should also not overlook the new risks that may arise from their use, nor the unease that the complexity of AI technologies can produce in the wider public. We already know that some uses of AI could damage our physical[footnote 8] and mental health, [footnote 9] infringe on the privacy of individuals[footnote 10] and undermine human rights.[footnote 11]\n\n5. Public trust in AI will be undermined unless these risks, and wider concerns about the potential for bias and discrimination, are addressed. By building trust, we can accelerate the adoption of AI across the UK to maximise the economic and social benefits that the technology can deliver, while attracting investment and stimulating the creation of high-skilled AI jobs.[footnote 12] In order to maintain the UK’s position as a global AI leader, we need to ensure that the public continues to see how the benefits of AI can outweigh the risks.[footnote 13]\n\n6. Responding to risk and building public trust are important drivers for regulation. But clear and consistent regulation can also support business investment and build confidence in innovation. Throughout our extensive engagement, industry repeatedly emphasised that consumer trust is key to the success of innovation economies. We therefore need a clear, proportionate approach to regulation that enables the responsible application of AI to flourish. Instead of creating cumbersome rules applying to all AI technologies, our framework ensures that regulatory measures are proportionate to context and outcomes, by focusing on the use of AI rather than the technology itself.\n\n7. People and organisations develop and use AI in the UK within the rules set by our existing laws, informed by standards, guidance and other tools. But AI is a general purpose technology and its uses can cut across regulatory remits. As a result, AI technologies are currently regulated through a complex patchwork of legal requirements. We are concerned by feedback from across industry that the absence of cross-cutting AI regulation creates uncertainty and inconsistency which can undermine business and consumer confidence in AI, and stifle innovation. By providing a clear and unified approach to regulation, our framework will build public confidence, making it clear that AI technologies are subject to cross-cutting, principles-based regulation.\n\nOur pro-innovation framework\n\n8. The government will put in place a new framework to bring clarity and coherence to the AI regulatory landscape. This regime is designed to make responsible innovation easier. It will strengthen the UK’s position as a global leader in AI, harness AI’s ability to drive growth and prosperity,[footnote 14] and increase public trust in its use and application.\n\n9. We are taking a deliberately agile and iterative approach, recognising the speed at which these technologies are evolving. Our framework is designed to build the evidence base so that we can learn from experience and continuously adapt to develop the best possible regulatory regime. Industry has praised our pragmatic and proportionate approach.\n\n10. Our framework is underpinned by 5 principles to guide and inform the responsible development and use of AI in all sectors of the economy:\n\nSafety, security and robustness\n\nAppropriate transparency and explainability\n\nFairness\n\nAccountability and governance\n\nContestability and redress\n\n11. We will not put these principles on a statutory footing initially. New rigid and onerous legislative requirements on businesses could hold back AI innovation and reduce our ability to respond quickly and in a proportionate way to future technological advances. Instead, the principles will be issued on a non-statutory basis and implemented by existing regulators. This approach makes use of regulators’ domain-specific expertise to tailor the implementation of the principles to the specific context in which AI is used. During the initial period of implementation, we will continue to collaborate with regulators to identify any barriers to the proportionate application of the principles, and evaluate whether the non-statutory framework is having the desired effect.\n\n12. Following this initial period of implementation, and when parliamentary time allows, we anticipate introducing a statutory duty on regulators requiring them to have due regard to the principles. Some feedback from regulators, industry and academia suggested we should implement further measures to support the enforcement of the framework. A duty requiring regulators to have regard to the principles should allow regulators the flexibility to exercise judgement when applying the principles in particular contexts, while also strengthening their mandate to implement them. In line with our proposal to work collaboratively with regulators and take an adaptable approach, we will not move to introduce such a statutory duty if our monitoring of the framework shows that implementation is effective without the need to legislate.\n\n13. In the 2022 AI regulation policy paper,[footnote 15] we proposed a small coordination layer within the regulatory architecture. Industry and civil society were supportive of our intention to ensure coherence across the AI regulatory framework. However, feedback often argued strongly for greater central coordination to support regulators on issues requiring cross-cutting collaboration and ensure that the overall regulatory framework functions as intended.\n\n14. We have identified a number of central support functions required to make sure that the overall framework offers a proportionate but effective response to risk while promoting innovation across the regulatory landscape:\n\nMonitoring and evaluation of the overall regulatory framework’s effectiveness and the implementation of the principles, including the extent to which implementation supports innovation. This will allow us to remain responsive and adapt the framework if necessary, including where it needs to be adapted to remain effective in the context of developments in AI’s capabilities and the state of the art.\n\nAssessing and monitoring risks across the economy arising from AI.\n\nConducting horizon scanning and gap analysis, including by convening industry, to inform a coherent response to emerging AI technology trends.\n\nSupporting testbeds and sandbox initiatives to help AI innovators get new technologies to market.\n\nProviding education and awareness to give clarity to businesses and empower citizens to make their voices heard as part of the ongoing iteration of the framework.\n\nPromoting interoperability with international regulatory frameworks.\n\n15. The central support functions will initially be provided from within government but will leverage existing activities and expertise from across the broader economy. The activities described above will neither replace nor duplicate the work undertaken by regulators and will not involve the creation of a new AI regulator.\n\n16. Our proportionate approach recognises that regulation is not always the most effective way to support responsible innovation. The proposed framework is aligned with, and supplemented by, a variety of tools for trustworthy AI, such as assurance techniques, voluntary guidance and technical standards. Government will promote the use of such tools. We are collaborating with partners like the UK AI Standards Hub to ensure that our overall governance framework encourages responsible AI innovation (see part 4 for details).\n\n17. In keeping with the global nature of these technologies, we will also continue to work with international partners to deliver interoperable measures that incentivise the responsible design, development and application of AI. During our call for views, industry, academia and civil society stressed that international alignment should support UK businesses to capitalise on global markets and protect UK citizens from cross-border harms.\n\n18. The UK is frequently ranked third in the world across a range of measures, including level of investment, innovation and implementation of AI.[footnote 16] To make the UK the most attractive place in the world for AI innovation and support UK companies wishing to export and attract international investment, we must ensure international compatibility between approaches. Countries around the world, as well as multilateral forums, are exploring approaches to regulating AI. Thanks to our reputation for pragmatic regulation, the UK is rightly seen by international partners as a leader in this global conversation.\n\nPart 1: Introduction\n\n1.1 The power and potential of artificial intelligence\n\n19. AI is already delivering major advances and efficiencies in many areas. AI quietly automates aspects of our everyday activities, from systems that monitor traffic to make our commutes smoother,[footnote 17] to those that detect fraud in our bank accounts.[footnote 18] AI has revolutionised large-scale safety-critical practices in industry, like controlling the process of nuclear fusion.[footnote 19] And it has also been used to accelerate scientific advancements, such as the discovery of new medicine[footnote 20] or the technologies we need to tackle climate change.[footnote 21]\n\n20. But this is just the beginning. AI can be used in a huge variety of settings and has the extraordinary potential to transform our society and economy.[footnote 22] It could have as much impact as electricity or the internet, and has been identified as one of the 5 critical technologies in the UK Science and Technology Framework.[footnote 23] As AI becomes more powerful, and as innovators explore new ways to use it, we will see more applications of AI emerge. As a result, AI has a huge potential to drive growth[footnote 24] and create jobs.[footnote 25] It will support people to carry out their existing jobs, by helping to improve workforce efficiency and workplace safety.[footnote 26] To remain world leaders in AI, attract global talent and create high-skilled jobs in the UK, we must create a regulatory environment where such innovation can thrive.\n\n21. Technological advances like large language models (LLMs) are an indication of the transformative developments yet to come.[footnote 27] LLMs provide substantial opportunities to transform the economy and society. For example, LLMs can automate the process of writing code and fixing programming bugs. The technology can support genetic medicine by identifying links between genetic sequences and medical conditions. It can support people to review and summarise key points from lengthy documents. In the last 4 years, LLMs have been developed beyond expectations and they are becoming applicable to an increasingly wide range of tasks.[footnote 28] We expand on the development of LLM and other foundation models in section 3.3.3 below.\n\n1.2 Managing AI risks\n\n22. The concept of AI is not new, but recent advances in data generation and processing have changed the field and the technology it produces. For example, while recent developments in the capabilities of generative AI models have created exciting opportunities, they have also sparked new debates about potential AI risks.[footnote 39] As AI research and development continues at pace and scale, we expect to see even greater impact and public awareness of AI risks.[footnote 40]\n\n23. We know that not all AI risks arise from the deliberate action of bad actors. Some AI risks can emerge as an unintended consequence or from a lack of appropriate controls to ensure responsible AI use.[footnote 41]\n\n24. We have made an initial assessment of AI-specific risks and their potential to cause harm, with reference in our analysis to the values that they threaten if left unaddressed. These values include safety, security, fairness, privacy and agency, human rights, societal well-being and prosperity.\n\n25. Our assessment of cross-cutting AI risk identified a range of high-level risks that our framework will seek to prioritise and mitigate with proportionate interventions. For example, safety risks include physical damage to humans and property, as well as damage to mental health.[footnote 42] AI creates a range of new security risks to individuals, organisations, and critical infrastructure.[footnote 43] Without government action, AI could cause and amplify discrimination that results in, for example, unfairness in the justice system.[footnote 44] Similarly, without regulatory oversight, AI technologies could pose risks to our privacy and human dignity, potentially harming our fundamental liberties.[footnote 45] Our regulatory intervention will ensure that AI does not cause harm at a societal level, threatening democracy[footnote 46] or UK values.\n\n1.3 A note on terminology\n\nTerminology used in this paper:[footnote 50]\n\nAI or AI system or AI technologies: products and services that are ‘adaptable’ and ‘autonomous’ in the sense outlined in our definition in section 3.2.1.\n\nAI supplier: any organisation or individual who plays a role in the research, development, training, implementation, deployment, maintenance, provision or sale of AI systems.\n\nAI user: any individual or organisation that uses an AI product.\n\nAI life cycle: all events and processes that relate to an AI system’s lifespan, from inception to decommissioning, including its design, research, training, development, deployment, integration, operation, maintenance, sale, use and governance.\n\nAI ecosystem: the complex network of actors and processes that enable the use and supply of AI throughout the AI life cycle (including supply chains, markets, and governance mechanisms).\n\nFoundation model: a type of AI model that is trained on a vast quantity of data and is adaptable for use on a wide range of tasks. Foundation models can be used as a base for building more specific AI models. Foundation models are discussed in more detail in section 3.3.3 below.[footnote 51]\n\nImpacted third party: an individual or company that is impacted by the outcomes of the AI systems that they do not use or supply themselves.\n\nPart 2: The current regulatory environment\n\n2.1 Navigating the current landscape\n\n26. The UK’s AI success is, in part, due to our reputation for high-quality regulators and our strong approach to the rule of law, supported by our technology-neutral legislation and regulations. UK laws, regulators and courts already address some of the emerging risks posed by AI technologies (see box 2.1 for examples). This strong legal foundation encourages investment in new technologies, enabling AI innovation to thrive,[footnote 52] and high-quality jobs to flourish.[footnote 53]\n\n27. While AI is currently regulated through existing legal frameworks like financial services regulation,[footnote 60] some AI risks arise across, or in the gaps between, existing regulatory remits. Industry told us that conflicting or uncoordinated requirements from regulators create unnecessary burdens and that regulatory gaps may leave risks unmitigated, harming public trust and slowing AI adoption.\n\n28. Industry has warned us that regulatory incoherence could stifle innovation and competition by causing a disproportionate amount of smaller businesses to leave the market. If regulators are not proportionate and aligned in their regulation of AI, businesses may have to spend excessive time and money complying with complex rules instead of creating new technologies. Small businesses and start-ups often do not have the resources to do both.[footnote 61] With the vast majority of digital technology businesses employing under 50 people,[footnote 62] it is important to ensure that regulatory burdens do not fall disproportionately on smaller companies, which play an essential role in the AI innovation ecosystem and act as engines for economic growth and job creation.[footnote 63]\n\n29. Regulatory coordination will support businesses to invest confidently in AI innovation and build public trust by ensuring real risks are effectively addressed. While some regulators already work together to ensure regulatory coherence for AI through formal networks like the AI and digital regulations service in the health sector[footnote 64] and the Digital Regulation Cooperation Forum (DRCF), other regulators have limited capacity and access to AI expertise. This creates the risk of inconsistent enforcement across regulators. There is also a risk that some regulators could begin to dominate and interpret the scope of their remit or role more broadly than may have been intended in order to fill perceived gaps in a way that increases incoherence and uncertainty. Industry asked us to support further system-wide coordination to clarify who is responsible for addressing cross-cutting AI risks and avoid duplicate requirements across multiple regulators.\n\n30. Government intervention is needed to improve the regulatory landscape. We intend to leverage and build on existing regimes, maximising the benefits of what we already have, while intervening in a proportionate way to address regulatory uncertainty and gaps. This will deliver a pro-innovation regulatory framework that is designed to be adaptable and future-proof, supported by tools for trustworthy AI including assurance techniques and technical standards. This approach will provide more clarity and encourage collaboration between government, regulators and industry to unlock innovation.\n\nPart 3: An innovative and iterative approach\n\n3.1 Aims of the regulatory framework\n\n31. Regulation can increase innovation by giving businesses the incentive to solve important problems while addressing the risk of harm to citizens. For example, product safety legislation has increased innovation towards safer products and services.[footnote 68] In the case of AI, a context-based, proportionate approach to regulation will help strengthen public trust and increase AI adoption.[footnote 69]\n\n32. The National AI Strategy set out our aim to regulate AI effectively and support innovation.[footnote 70] In line with the principles set out in the Plan for Digital Regulation,[footnote 71] our approach to AI regulation will be proportionate; balancing real risks against the opportunities and benefits that AI can generate. We will maintain an effective balance as we implement the framework by focusing on the context and outcomes of AI.\n\n33. Our policy paper proposed a pro-innovation framework designed to give consumers the confidence to use AI products and services, and provide businesses the clarity they need to invest in AI and innovate responsibly.[footnote 72] This approach was broadly welcomed – particularly by industry. Based on feedback, we have distilled our aims into 3 objectives that our framework is designed to achieve:\n\nDrive growth and prosperity by making responsible innovation easier and reducing regulatory uncertainty. This will encourage investment in AI and support its adoption throughout the economy, creating jobs and helping us to do them more efficiently.\n\nTo achieve this objective we must act quickly to remove existing barriers to innovation and prevent the emergence of new ones. This will allow AI companies to capitalise on early development successes and achieve long term market advantage.[footnote 73] By acting now, we can give UK innovators a headstart in the global race to convert the potential of AI into long term advantages for the UK, maximising the economic and social value of these technologies and strengthening our current position as a world leader in AI.[footnote 74]\n\nIncrease public trust in AI by addressing risks and protecting our fundamental values.\n\nTrust is a critical driver for AI adoption.[footnote 75] If people do not trust AI, they will be reluctant to use it. Such reluctance can reduce demand for AI products and hinder innovation. Therefore we must demonstrate that our regulatory framework (described in section 3.2) effectively addresses AI risks.\n\nStrengthen the UK’s position as a global leader in AI. The development of AI technologies can address some of the most pressing global challenges, from climate change to future pandemics. There is also growing international recognition that AI requires new regulatory responses to guide responsible innovation.\n\nThe UK can play a central role in the global conversation by shaping international governance and regulation to maximise opportunities and build trust in the technology, while mitigating potential cross-border risks and protecting our democratic values. There is also an important leadership role for the UK in the development of the global AI assurance industry,[footnote 76] including in auditing and safety.\n\nWe will ensure that the UK remains attractive to innovators and investors by promoting interoperability with other regulatory approaches and minimising cross-border frictions. We will work closely with global partners through multilateral and bilateral engagements to learn from, influence and adapt as international and domestic approaches to AI regulation continue to emerge (see part 6).\n\n34. The proposed regulatory framework does not seek to address all of the wider societal and global challenges that may relate to the development or use of AI. This includes issues relating to access to data, compute capability, and sustainability, as well as the balancing of the rights of content producers and AI developers. These are important issues to consider – especially in the context of the UK’s ability to maintain its place as a global leader in AI – but they are outside of the scope of our proposals for a new overarching framework for AI regulation.\n\n35. Government is taking wider action to ensure the UK retains its status as a global leader in AI, for example by taking forward Sir Patrick Vallance’s recommendation relating to intellectual property law and generative AI.[footnote 77] This will ensure we keep the right balance between protecting rights holders and our thriving creative industries, while supporting AI developers to access the data they need.\n\n3.2 The proposed regulatory framework\n\n36. Our innovative approach to AI regulation uses a principles-based framework for regulators to interpret and apply to AI within their remits. This collaborative and iterative approach can keep pace with a fast moving technology that requires proportionate action to balance risk and opportunity and to strengthen the UK’s position as a global leader in AI. Our agile approach aligns with Sir Patrick Vallance’s Regulation for Innovation report,[footnote 78] which highlights that flexible regulatory approaches can better strike the balance between providing clarity, building trust and enabling experimentation. Our framework will provide more clarity to innovators by encouraging collaboration between government, regulators, industry and civil society.\n\n37. We have identified the essential characteristics of our regulatory regime. Our framework will be pro-innovation, proportionate, trustworthy, adaptable, clear and collaborative.[footnote 79]\n\nPro-innovation: enabling rather than stifling responsible innovation.\n\nProportionate: avoiding unnecessary or disproportionate burdens for businesses and regulators.\n\nTrustworthy: addressing real risks and fostering public trust in AI in order to promote and encourage its uptake.\n\nAdaptable: enabling us to adapt quickly and effectively to keep pace with emergent opportunities and risks as AI technologies evolve.\n\nClear: making it easy for actors in the AI life cycle, including businesses using AI, to know what the rules are, who they apply to, who enforces them, and how to comply with them.\n\nCollaborative: encouraging government, regulators, and industry to work together to facilitate AI innovation, build trust and ensure that the voice of the public is heard and considered.\n\n38. The framework, built around the 4 key elements below, is designed to empower our existing regulators and promote coherence across the regulatory landscape. The 4 key elements are:\n\nDefining AI based on its unique characteristics to support regulator coordination (section 3.2.1).\n\nAdopting a context-specific approach (section 3.2.2).\n\nProviding a set of cross-sectoral principles to guide regulator responses to AI risks and opportunities(section 3.2.3).\n\nThe principles clarify government’s expectations for responsible AI and describe good governance at all stages of the AI life cycle.\n\nThe application of the principles will initially be at the discretion of the regulators, allowing prioritisation according to the needs of their sectors.\n\nFollowing this initial non-statutory period of implementation, and when parliamentary time allows, we anticipate introducing a statutory duty requiring regulators to have due regard to the principles.\n\nDelivering new central functions to support regulators to deliver the AI regulatory framework, maximising the benefits of an iterative approach and ensuring that the framework is coherent (section 3.2.4).\n\n3.2.1 Defining Artificial Intelligence\n\n39. To regulate AI effectively, and to support the clarity of our proposed framework, we need a common understanding of what is meant by ‘artificial intelligence’. There is no general definition of AI that enjoys widespread consensus.[footnote 80] That is why we have defined AI by reference to the 2 characteristics that generate the need for a bespoke regulatory response.\n\nThe ‘adaptivity’ of AI can make it difficult to explain the intent or logic of the system’s outcomes:\n\nAI systems are ‘trained’ – once or continually – and operate by inferring patterns and connections in data which are often not easily discernible to humans.\n\nThrough such training, AI systems often develop the ability to perform new forms of inference not directly envisioned by their human programmers.\n\nThe ‘autonomy’ of AI can make it difficult to assign responsibility for outcomes:\n\nSome AI systems can make decisions without the express intent or ongoing control of a human.\n\n40. The combination of adaptivity and autonomy can make it difficult to explain, predict, or control the outputs of an AI system, or the underlying logic by which they are generated. It can also be challenging to allocate responsibility for the system’s operation and outputs. For regulatory purposes, this has potentially serious implications, particularly when decisions are made relating to significant matters, like an individual’s health, or where there is an expectation that a decision should be justifiable in easily understood terms, like a legal ruling.\n\n41. By defining AI with reference to these functional capabilities and designing our approach to address the challenges created by these characteristics, we future-proof our framework against unanticipated new technologies that are autonomous and adaptive. Because we are not creating blanket new rules for specific technologies or applications of AI, like facial recognition or LLMs, we do not need to use rigid legal definitions. Our use of these defining characteristics was widely supported in responses to our policy paper,[footnote 81] as rigid definitions can quickly become outdated and restrictive with the rapid evolution of AI.[footnote 82] We will, however, retain the ability to adapt our approach to defining AI if necessary, alongside the ongoing monitoring and iteration of the wider regulatory framework.\n\n42. Below, we provide some illustrative examples of AI systems to demonstrate their autonomous and adaptive characteristics. While many aspects of the technologies described in these case studies will be covered by existing law, they illustrate how AI-specific characteristics introduce novel risks and regulatory implications.\n\nFigure 1: Illustration of our strategy for regulating AI\n\n43. Industry, regulators, and civil society responded positively to our proposed definition, recognising that it supports our context-based and flexible approach to AI regulation. We will monitor how regulators interpret and apply adaptivity and autonomy when formulating domain-specific definitions of AI. Government will support coordination between regulators when we see potential for better alignment between their interpretations and use of our defining characteristics.\n\n44. Active and collaborative horizon scanning will ensure that we can identify developments and emerging trends, and adapt our framework accordingly. We will convene industry, academia and other key stakeholders to inform economy-wide horizon scanning activity. This work will build on the activity of individual regulators.\n\n3.2.2 Regulating the use – not the technology\n\n45. Our framework is context-specific.[footnote 83] We will not assign rules or risk levels to entire sectors or technologies. Instead, we will regulate based on the outcomes AI is likely to generate in particular applications. For example, it would not be proportionate or effective to classify all applications of AI in critical infrastructure as high risk. Some uses of AI in critical infrastructure, like the identification of superficial scratches on machinery, can be relatively low risk. Similarly, an AI-powered chatbot used to triage customer service requests for an online clothing retailer should not be regulated in the same way as a similar application used as part of a medical diagnostic process.\n\n46. A context-specific approach allows regulators to weigh the risks of using AI against the costs of missing opportunities to do so.[footnote 84] Regulators told us that AI risk assessments should include the failure to exploit AI capabilities. For example, there can be a significant opportunity cost related to not having access to AI in safety-critical operations, from heavy industry,[footnote 85] to personal healthcare (see box 1.1). Sensitivity to context will allow the framework to respond to the level of risk in a proportionate manner and avoid stifling innovation or missing opportunities to capitalise on the social benefits made available by AI.\n\n47. To best achieve this context-specificity we will empower existing UK regulators to apply the cross-cutting principles. Regulators are best placed to conduct detailed risk analysis and enforcement activities within their areas of expertise. Creating a new AI-specific, cross-sector regulator would introduce complexity and confusion, undermining and likely conflicting with the work of our existing expert regulators.\n\n3.2.3 A principles-based approach\n\n48. Existing regulators will be expected to implement the framework underpinned by 5 values-focused cross-sectoral principles:\n\nSafety, security and robustness\n\nAppropriate transparency and explainability\n\nFairness\n\nAccountability and governance\n\nContestability and redress\n\nThese build on, and reflect our commitment to, the Organisation for Economic Co-operation and Development (OECD) values-based AI principles, which promote the ethical use of AI.\n\n49. The principles set out the key elements of responsible AI design, development and use, and will help guide businesses. Regulators will lead the implementation of the framework, for example by issuing guidance on best practice for adherence to these principles.\n\n50. Regulators will be expected to apply the principles proportionately to address the risks posed by AI within their remits, in accordance with existing laws and regulations. In this way, the principles will complement existing regulation, increase clarity, and reduce friction for businesses operating across regulatory remits.\n\n51. A principles-based approach allows the framework to be agile and proportionate. It is in line with the Plan for Digital Regulation,[footnote 86] the findings from the independent Taskforce on Innovation, Growth and Regulatory Reform,[footnote 87] the Regulatory Horizons Council’s Closing the Gap report on implementing innovation-friendly regulation,[footnote 88] and Sir Patrick Vallance’s Regulation for Innovation report.[footnote 89]\n\n52. Since publishing the AI regulation policy paper,[footnote 90] we have updated and strengthened the principles. We have:\n\nReflected stakeholder feedback by expanding on concepts such as robustness and governance. We have also considered the results of public engagement research that highlighted an expectation for principles such as transparency, fairness and accountability to be included within an AI governance framework.[footnote 91]\n\nMerged the safety principle with security and robustness, given the significant overlap between these concepts.\n\nBetter reflected concepts of accountability and responsibility.\n\nRefined each principle’s definition and rationale.\n\n53. We anticipate that regulators will need to issue guidance on the principles or update existing guidance to provide clarity to business. Regulators may also publish joint guidance on one or more of the principles, focused on AI use cases that cross multiple regulatory remits. We are keen to work with regulators and industry to understand the best approach to providing guidance. We expect that practical guidance will support actors in the AI life cycle to adhere to the principles and embed them into their technical and operational business processes. Regulators may also use alternative measures and introduce other tools or resources, in addition to issuing guidance, within their existing remits and powers to implement the principles.\n\n54. Government will monitor the overall effectiveness of the principles and the wider impact of the framework.[footnote 104] This will include working with regulators to understand how the principles are being applied and whether the framework is adequately supporting innovation.\n\nConsultation questions:\n\n1. Do you agree that requiring organisations to make it clear when they are using AI would improve transparency?\n\n2. Are there other measures we could require of organisations to improve AI transparency?\n\n3. Do you agree that current routes to contest or get redress for AI-related harms are adequate?\n\n4. How could current routes to contest or seek redress for AI-related harms be improved, if at all?\n\n5. Do you agree that, when implemented effectively, the revised cross-sectoral principles will cover the risks posed by AI technologies?\n\n6. What, if anything, is missing from the revised principles?\n\n3.2.4 Our preferred model for applying the principles\n\n55. Initially, the principles will be issued by government on a non-statutory basis and applied by regulators within their remits. We will support regulators to apply the principles using the powers and resources available to them. This initial period of implementation will provide a valuable opportunity to ensure that the principles are effective and that the wider framework is supporting innovation while addressing risks appropriately.\n\n56. While industry has strongly supported non-statutory measures in the first instance, favouring flexibility and fewer burdens, some businesses and regulators have suggested that government should go beyond a non-statutory approach to ensure the principles have the desired impact.[footnote 108] Some regulators have also expressed concerns that they lack the statutory basis to consider the application of the principles. We are committed to an approach that leverages collaboration with our expert regulators but we agree that we may need to intervene further to ensure that our framework is effective.\n\n57. Following a period of non-statutory implementation, and when parliamentary time allows, we anticipate that we will want to strengthen and clarify regulators’ mandates by introducing a new duty requiring them to have due regard to the principles. Such a duty would give a clear signal that we expect regulators to act and support coherence across the regulatory landscape, ensuring that the framework displays the characteristics that we have identified.[footnote 109] One of the strengths of this approach is that regulators would still be able to exercise discretion and expert judgement regarding the relevance of each principle to their individual domains.\n\n58. A duty would ensure that regulators retain the ability to exercise judgement when applying the principles in particular contexts – benefiting from some of the flexibility expected through non-statutory implementation. For example, while the duty to have due regard would require regulators to demonstrate that they had taken account of the principles, it may be the case that not every regulator will need to introduce measures to implement every principle. In having due regard to a particular principle, a regulator may exercise their expert judgement and determine that their sector or domain does not require action to be taken. The introduction of the duty will, however, give regulators a clear mandate and incentive to apply the principles where relevant to their sectors or domains.\n\n59. If our monitoring of the effectiveness of the initial, non-statutory framework suggests that a statutory duty is unnecessary, we would not introduce it. Similarly, we will monitor whether particular principles cannot be, or are not being, applied in certain circumstances or by specific regulators because of the interpretation of existing legal requirements or because of technical constraints. Such circumstances may require broader legislative changes. Should we decide there is a need for statutory measures, we will work with regulators to review the interaction of our principles with their existing duties and powers.\n\nConsultation questions:\n\n7. Do you agree that introducing a statutory duty on regulators to have due regard to the principles would clarify and strengthen regulators’ mandates to implement our principles while retaining a flexible approach to implementation?\n\n8. Is there an alternative statutory intervention that would be more effective?\n\n3.2.5 The role of individual regulators in applying the principles\n\n60. In some sectors, principles for AI governance will already exist and may even go further than the cross-cutting principles we propose. Our framework gives sectors the ability to develop and apply more specific principles to suit their own domains, where government or regulators identify these are needed.\n\n61. The Ministry of Defence published its own AI ethical principles and policy in June 2022, which determines HM Government’s approach regarding AI-enabled military capabilities. We will ensure appropriate coherence and alignment in the application of this policy through a context specific approach and thereby promote UK leadership in the employment of AI for defence purposes. Ahead of introducing any statutory duty to have due regard to our principles, and in advance of introducing other material iterations of the framework, we will consider whether exemptions are needed to allow existing regulators (such as those working in areas like national security) to continue their domain-level approach\n\n62. Not all principles will be equally relevant in all contexts and sometimes two or more principles may come into conflict. For example, it may be difficult to assess the fairness of an algorithm’s outputs without access to sensitive personal data about the subjects of the processing. Regulators will need to use their expertise and judgement to prioritise and apply the principles in such cases, sharing information where possible with government and other regulators about how they are assessing the relevance of each principle. This collaboration between regulators and government will allow the framework to be adapted to ensure it is practical, coherent and supporting innovation.\n\n63. In implementing the new regulatory framework we expect that regulators will:\n\nAssess the cross-cutting principles and apply them to AI use cases that fall within their remit.\n\nIssue relevant guidance on how the principles interact with existing legislation to support industry to apply the principles. Such guidance should also explain and illustrate what compliance looks like.\n\nSupport businesses operating within the remits of multiple regulators by collaborating and producing clear and consistent guidance, including joint guidance where appropriate.\n\n64. Regulators will need to monitor and evaluate their own implementation of the framework and their own effectiveness at regulating AI within their remits. We understand that there may be AI-related risks that do not clearly fall within the remits of the UK’s existing regulators.[footnote 110] Not every new AI-related risk will require a regulatory response and there is a growing ecosystem of tools for trustworthy AI that can support the application of the cross-cutting principles. These are described further in part 4.\n\n65. Where prioritised risks fall within a gap in the legal landscape, regulators will need to collaborate with government to identify potential actions. This may include identifying iterations to the framework such as changes to regulators’ remits, updates to the Regulators’ Code,[footnote 111] or additional legislative interventions. Our approach benefits from our strong sovereign parliamentary system, which reliably allows for the introduction of targeted and proportionate measures in response to emerging issues, including by adapting existing legislation if necessary.[footnote 112]\n\n66. The Sir Patrick Vallance review has highlighted that rushed attempts to regulate AI too early would risk stifling innovation.[footnote 113] Our approach aligns with this perspective. We recognise the need to build a stronger evidence base before making decisions on statutory interventions. In doing so, we will ensure that we strike the right balance between retaining flexibility in our iterative approach and providing clarity to businesses. As detailed in section 3.3.1, we will deliver a range of central functions, including horizon scanning and risk monitoring, to identify and respond to situations where prioritised risks are not adequately covered by the framework, or where gaps between regulators’ remits are negatively impacting innovation.\n\n3.2.6 Guidance to regulators on applying the principles\n\n67. The proposed regulatory framework is dependent upon the implementation of the principles by our expert regulators. This regulator-led approach has received broad support from across industry, with stakeholders acknowledging the importance of the sector-specific expertise held by individual regulators. We expect regulators to collaborate proactively to achieve the best outcomes for the economy and society. We will work with regulators to monitor the wider framework and ensure that this collaborative approach to implementation is effective. If improvements are needed, including interventions to drive stronger collaboration across regulators, we will take further action.\n\n68. Our engagement with regulators and industry highlighted the need for central government to support regulators. We will work with regulators to develop guidance that helps them implement the principles in a way that aligns with our expectations for how the framework should operate. Existing legal frameworks already mandate and guide regulators’ actions. For example, nearly all regulators are bound by the Regulators’ Code[footnote 116] and all regulators – as public bodies – are required to comply with the Human Rights Act.[footnote 117] Our proposed guidance to regulators will seek to ensure that when applying the principles, regulators are supported and encouraged to:\n\nAdopt a proportionate approach that promotes growth and innovation by focusing on the risks that AI poses in a particular context.\n\nConsider proportionate measures to address prioritised risks, taking into account cross-cutting risk assessments undertaken by, or on behalf of, government.\n\nDesign, implement and enforce appropriate regulatory requirements and, where possible, integrate delivery of the principles into existing monitoring, investigation and enforcement processes.\n\nDevelop joint guidance, where appropriate, to support industry compliance with the principles and relevant regulatory requirements.\n\nConsider how tools for trustworthy AI like assurance techniques and technical standards can support regulatory compliance.\n\nEngage proactively and collaboratively with government’s monitoring and evaluation of the framework.\n\nCase study 3.7: What this means for businesses\n\nA fictional company, ‘AI Fairness Insurance Limited’, has delayed the deployment of a new AI application as – under the current patchwork of relevant regulatory requirements – it has been challenging to identify appropriate compliance actions for AI-driven insurance products.\n\nFollowing implementation of the UK’s new pro-innovation framework to regulate AI, we could expect to see joint guidance produced collaboratively by the Information Commissioner’s Office (ICO), Equality and Human Rights Commission (EHRC), Financial Conduct Authority (FCA) and other relevant regulatory authorities. This would provide greater clarity on the regulatory requirements relevant to AI as well as guidance on how to satisfy those requirements in the context of insurance and consumer-facing financial services.\n\nUnder the proposed regulatory framework, AI Fairness Insurance Limited could be supported by new or updated guidance issued by regulators to address the AI regulatory principles. The company may also be able to follow joint regulatory guidance, issued as a result of collaboration between regulators, and use a set of tools provided by regulators, such as template risk assessments and transparency measures, and relevant technical standards (for example, international standards for transparency and bias mitigation). The collaboration between regulators and focus on practical implementation measures will guide the responsible deployment of AI Fairness Insurance Limited’s AI product by making it easier for the company to navigate the regulatory landscape and address specific risks such as discrimination.\n\n69. Further details about the implementation of the regulatory framework will be provided through an AI regulation roadmap which will be published alongside the government response to the consultation on this white paper.\n\n3.3.1 New central functions to support the framework\n\n70. Government has a responsibility to make sure the regulatory framework operates proportionately and supports innovation. Feedback to our proposals from businesses has been clear that the current patchwork of regulation, with relatively little in the way of central coordination or oversight, will create a growing barrier to innovation if left unaddressed. Responses from over 130 organisations and individuals to our 2022 policy paper highlighted the need for a greater level of monitoring and coordination to achieve the coherence and improved clarity we need to support innovation. Businesses, particularly small to medium sized enterprises, noted that regulatory coordination could improve business certainty and investment, resulting in more and better jobs in the sector.\n\n71. Government therefore intends to put mechanisms in place to coordinate, monitor and adapt the framework as a whole. Further detail on these functions is set out below. Enhanced monitoring activity will allow us to take a structured approach to gathering feedback from industry on the impact of our regime as it is introduced. These mechanisms will supplement and support the work of regulators, without undermining their independence. Equally, such mechanisms are not intended to duplicate existing activities.\n\n72. Delivering some functions centrally provides government with an overarching view of how the framework is working, where it is effective and where it may need improving. A central suite of functions will also facilitate collaboration by bringing together a wide range of interested parties, including regulators, international partners, industry, civil society organisations such as trade unions and advocacy groups, academia and the general public. Our engagement with these groups has highlighted the need for our proposed central functions. We will continue to convene a wide range of stakeholders to ensure that we hear the full spectrum of viewpoints. This breadth of engagement and collaboration will be integral to government’s ability to monitor and improve the framework. The functions will identify and support opportunities for further coordination between regulators, resulting in greater clarity for businesses and stronger consumer trust.\n\n73. We have identified a set of functions that will drive regulatory coherence and support regulators to implement the pro-innovation approach that we have outlined. These functions have been informed by our discussions with industry, research organisations, and regulators following the publication of the AI policy paper.\n\nConsultation questions:\n\n9. Do you agree that the functions outlined in Box 3.1 would benefit our AI regulation framework if delivered centrally?\n\n10. What, if anything, is missing from the central functions?\n\n11. Do you know of any existing organisations who should deliver one or more of our proposed central functions?\n\n12. Are there additional activities that would help businesses confidently innovate and use AI technologies?\n\n12.1. If so, should these activities be delivered by government, regulators or a different organisation?\n\n13. Are there additional activities that would help individuals and consumers confidently use AI technologies?\n\n13.1. If so, should these activities be delivered by government, regulators or a different organisation?\n\n14. How can we avoid overlapping, duplicative or contradictory guidance on AI issued by different regulators?\n\nBox 3.2: Supporting coherence in risk assessment\n\nWhy?\n\nMany AI risks do not fall neatly into the remit of one individual regulator and they could go unaddressed if not monitored at a cross-sector level. A central, cross-economy risk function will also enable government to monitor future risks in a rigorous, coherent and balanced way. This will include ‘high impact but low probability’ risks such as existential risks posed by artificial general intelligence or AI biosecurity risks.\n\nA pro-innovation approach to regulation involves tolerating a certain degree of risk rather than intervening in all cases. Government needs the ability to assess and prioritise AI risks, ensuring that any intervention is proportionate and consistent with levels of risk mitigation activity elsewhere across the economy or AI life cycle.\n\nEstablishing a central risk function will bring coherence to the way regulators and industry think about AI risk. It will also foster collaboration between government, regulators, industry and civil society to provide clarity for businesses managing AI risk across sectors.\n\nWhat?\n\nThe central risk function will identify, assess, prioritise and monitor cross-cutting AI risks that may require government intervention.\n\nHow?\n\nThe central risk function will bring together cutting-edge knowledge from industry, regulators, academia and civil society – including skilled computer scientists with a deep technical understanding of AI.\n\nGiven the importance of risk management expertise, we will seek inspiration and learning from sectors where operational risk management is highly developed. This will include looking for examples of how failures and near misses can be recorded and used to inform good practice.\n\nRegulators will have a key role in designing the central risk framework and ensuring alignment with their existing practices. Where a risk that has been prioritised for intervention falls outside of any existing regulator’s remit, the central risk function will identify measures that could be taken to address the gap (for example, updates to regulatory remits). The central risk function will also support smaller regulators that lack technical AI expertise to better understand AI risks.\n\nFigure 2: Central risks function activities\n\nConsultation questions:\n\n15. Do you agree with our overall approach to monitoring and evaluation?\n\n16. What is the best way to measure the impact of our framework?\n\n17. Do you agree that our approach strikes the right balance between supporting AI innovation; addressing known, prioritised risks; and future-proofing the AI regulation framework?\n\n74. It is important to have the right architecture in place to oversee the delivery of the central functions described above. The AI ecosystem already benefits from a range of organisations with extensive expertise in regulatory issues. Ground-breaking coordination initiatives like the Digital Regulation Cooperation Forum (DRCF) play a valuable role in enhancing regulatory alignment and fostering dialogue on digital issues across regulators. However, the DRCF was not created to support the delivery of all the functions we have identified or the implementation of our proposed regulatory framework for AI.\n\n75. Government will initially be responsible for delivering the central functions described above, working in partnership with regulators and other key actors in the AI ecosystem to leverage existing activities where possible. This is aligned with our overall iterative approach and enables system-wide review of the framework. We recognise that there may be value in a more independent delivery of the central functions in the longer term.\n\n76. Where relevant activities are already undertaken by organisations either within or outside of government, the primary role of the central functions will be to leverage these activities and assess their effectiveness. Where this is not the case – for example, where new bespoke capabilities are needed to monitor and evaluate the operation of the framework as a whole – these functions will initially be established in government.\n\n77. We are deliberately taking an iterative approach to the delivery of the regulatory framework and we anticipate that the model for providing the central functions will develop over time. We will identify where existing structures may need to be supplemented or adapted. In particular, we are focused on understanding:\n\nWhether existing regulatory forums could be expanded to include the full range of regulators involved in the regulation of AI or whether additional mechanisms are needed.\n\nWhat additional expertise government may need to support the implementation and monitoring of the principles, including the potential role that could be played by established advisory bodies.\n\nThe most effective way to convene input from across industry and consumers to ensure a broad range of opinions.\n\n78. Government, in fulfilling the regulatory central functions and overseeing the framework, will benefit from engaging external expertise to gather insights and advice from experts in industry, academia and civil society. The AI Council has been an important source of expertise over the last 3 years, advising government on the development of the National AI Strategy as well as our approach to AI governance. As we enter a new phase we will review the role of the AI Council and consider how best to engage expertise to support the implementation of the regulatory framework.\n\n79. As the regulatory framework evolves and we develop a clearer understanding of the system-level functions that are needed, we will review the operational model outlined above. In particular, we will consider if a government unit is the most appropriate mechanism for delivering the central functions in the longer term or if an independent body would be more effective.\n\nConsultation questions:\n\n18. Do you agree that regulators are best placed to apply the principles and government is best placed to provide oversight and deliver central functions?\n\n3.3.2 Government’s role in addressing accountability across the life cycle\n\n80. The clear allocation of accountability and legal responsibility is important for effective AI governance. Legal responsibility for compliance with the principles should be allocated to the actors in the AI life cycle best able to identify, assess and mitigate AI risks effectively. Incoherent or misplaced allocation of legal responsibility could hinder innovation or adoption of AI.\n\n81. However, AI supply chains can be complex and opaque, making effective governance of AI and supply chain risk management difficult. Inappropriate allocation of AI risk, liability, and responsibility for AI governance throughout the AI life cycle and within AI supply chains could impact negatively on innovation. For example, inappropriate allocation of liability to a business using, but not developing, AI could stifle AI adoption. Similarly, allocating too much responsibility to businesses developing foundation models, on the grounds that these models could be used by third parties in a range of contexts, would hamper innovation.\n\n82. We recognise the need to consider which actors should be responsible and liable for complying with the principles, which may not be the same actors who bear the burden under current legal frameworks. For example, data protection law differentiates between data controllers and data processors. Similarly, product safety laws include the concepts of producers and distributors. In the context of those specific legal frameworks, liability for compliance with various existing legal obligations is allocated by law to those identified supply chain actors. It is not yet clear how responsibility and liability for demonstrating compliance with the AI regulatory principles will be or should ideally be, allocated to existing supply chain actors within the AI life cycle.\n\n83. We are not proposing to intervene and make changes to life cycle accountability at this stage. It is too soon to make decisions about liability as it is a complex, rapidly evolving issue which must be handled properly to ensure the success of our wider AI ecosystem. However, to further our understanding of this topic we will engage a range of experts, including technicians and lawyers. It may become apparent that current legal frameworks, when combined with implementation of our AI principles by regulators, will allocate legal responsibility and liability across the supply chain in a way that is not fair or effective. We would consider proportionate interventions to address such issues which could otherwise undermine our pro-innovation approach to AI regulation. Our agile approach benefits our sovereign parliamentary system’s reliable ability to introduce targeted measures – for example by amending existing legislation if necessary – in response to new evidence.[footnote 125]\n\n84. Tools for trustworthy AI like assurance techniques and technical standards can support supply chain risk management. These tools can also drive the uptake and adoption of AI by building justified trust in these systems, giving users confidence that key AI-related risks have been identified, addressed and mitigated across the supply chain. For example, by describing measures that manufacturers should take to ensure the safety of AI systems, technical standards can provide reassurance to purchasers and users of AI systems that appropriate safety-focused measures have been adopted, ultimately encouraging adoption of AI.\n\n85. Our evaluation of the framework will assess whether the legal responsibility for AI is effectively and fairly distributed. As we implement the framework, we will continue our extensive engagement to gather evidence from regulators, industry, academia, and civil society on its impact on different actors across the AI life cycle. This will allow us to monitor the effects of our framework on actors across the AI supply chain on an ongoing basis. We will need a particular focus on foundation models given the potential challenges they pose to life cycle accountability, especially when available as open-source. By centrally evaluating whether there are adequate measures for AI accountability, we can assess the need for further interventions into AI liability across the whole economy and AI life cycle.\n\nConsultation questions:\n\nL1. What challenges might arise when regulators apply the principles across different AI applications and systems? How could we address these challenges through our proposed AI regulatory framework?\n\nL2.1 Do you agree that the implementation of our principles through existing legal frameworks will fairly and effectively allocate legal responsibility for AI across the life cycle?\n\nL.2.2. How could it be improved, if at all?\n\nL3. If you work for a business that develops, uses, or sells AI, how do you currently manage AI risk including through the wider supply chain? How could government support effective AI-related risk management?\n\n3.3.3 Foundation models and the regulatory framework\n\n86. Foundation models are an emerging type of general purpose AI that are trained on vast quantities of data and can be adapted to a wide range of tasks. The fast-paced development of foundation models brings novel challenges for governments seeking to regulate AI. Despite high levels of interest in the topic, the research community has not found a consensus on how foundation models work, the risks they pose or even the extent of their capabilities.[footnote 126]\n\n87. Foundation models have been described as paradigm shifting and could have significant impacts on society and the economy.[footnote 127] They can be used for a wide variety of purposes and deployed in many already complex ecosystems. Given the widely acknowledged transformative potential of foundation models, we must give careful attention to how they might interact with our proposed regulatory framework. Our commitment to an adaptable, proportionate approach presents a clear opportunity for the UK to lead the global conversation and set global norms for the future-proof regulation of foundation models.\n\n88. There is a relatively small number of organisations developing foundation models. Some organisations exercise close control over the development and distribution of their foundation models. Other organisations take an open-source approach to the development and distribution of the technology. Open-source models can improve access to the transformational power of foundation models, but can cause harm without adequate guardrails.[footnote 128] The variation in organisational approaches to developing and supplying foundation models introduces a wide range of complexities for the regulation of AI. The potential opacity of foundation models means that it can also be challenging to identify and allocate accountability for outcomes generated by AI systems that rely on or integrate them.\n\n89. Our proposed framework considers the issues raised by foundation models in light of our life cycle accountability analysis, outlined in section 3.3.2 above. Given the small number of organisations supplying foundation models and a proportionately larger number of businesses integrating or otherwise deploying foundation models elsewhere in the AI ecosystem, we recognise the important role of tools for trustworthy AI, including assurance techniques and technical standards.\n\n90. The proposed central functions described in section 3.3.1 will play an important role in informing our approach to regulating foundation models. The central risk function’s proactive, rigorous monitoring of risks associated with foundation models and the horizon scanning function’s identification of related opportunities will be critical to ensuring that we strike the balance needed as part of our proportionate, pro-innovation regulatory approach. It will be crucial to ensure that the proposed monitoring and evaluation function has access to the technical skills and capabilities needed to assess the impact that our framework has on the opportunities and risks presented by foundation models.\n\n91. We recognise that industry, academia, research organisations and global partners are looking for ways to address the challenges related to the regulation of foundation models.[footnote 129] For example, we know that developers of foundation models are exploring ways to embed alignment theory into their models. This is an important area of research, and government will need to work closely with the AI research community to leverage insights and inform our iteration of the regulatory framework. Our collaborative, adaptable framework will draw on the expertise of those researchers and other stakeholders as we continue to develop policy in this evolving area.\n\n92. The UK is committed to building its capabilities in foundation models. Our Foundation Model Taskforce announced in the Integrated Review Refresh 2023[footnote 130] will support government to build UK capability and ensure the UK harnesses the benefits presented by this emerging technology. Our proposed framework will ensure we create the right regulatory environment as we move to maximise the transformative potential of foundation models.\n\nCase-study 3.9: Life cycle accountability for large language models\n\nConsultation questions:\n\nF1. What specific challenges will foundation models such as large language models (LLMs) or open-source models pose for regulators trying to determine legal responsibility for AI outcomes?\n\nF2. Do you agree that measuring compute provides a potential tool that could be considered as part of the governance of foundation models?\n\nF3. Are there other approaches to governing foundation models that would be more effective?\n\n3.3.4 Artificial intelligence sandboxes and testbeds\n\n93. Government is committed to supporting innovators by addressing regulatory challenges that prevent new, cutting-edge products from getting to market. Barriers can be particularly high when a path to market requires interaction with multiple regulators or regulatory guidance is nascent. Sir Patrick Vallance’s Digital Report recommends that government works with regulators to develop an AI sandbox to support innovators. At the Budget, government confirmed our commitment to taking forward this recommendation.[footnote 139]\n\n94. The Information Commissioner’s Office (ICO) and the Financial Conduct Authority (FCA) have already successfully piloted digital sandboxes in their sectors.[footnote 140] The FCA sandbox has worked with over 800 businesses and accelerated their speed to market by an estimated 40% on average.[footnote 141] Sandbox participation has also been found to have significant financial benefits, particularly for smaller organisations.[footnote 142] We have heard from regulators, including those with less experience of taking part in previous initiatives, that they are keen to participate in new AI sandboxes to support their regulated sectors.\n\n95. Regulatory sandboxes and testbeds will play an important role in our proposed regulatory regime. Such initiatives enable government and regulators to:\n\nSupport innovators to get novel products and services to market faster, so they can start generating economic and social benefits.\n\nTest how the regulatory framework is operating in practice and illuminate unnecessary barriers to innovation that need to be addressed.\n\nIdentify emerging technology and market trends to which our regulatory framework may need to adapt.\n\n96. To deliver an effective sandbox, we would like to understand more deeply what service focus would be most useful to industry. We are considering 4 options:\n\nSingle sector, single regulator: support innovators to bring AI products to the market in collaboration with a single regulator, focusing on only one chosen industry sector.[footnote 143]\n\nMultiple sectors, single regulator: support AI innovators in collaboration with a single regulator that is capable of working across multiple industry sectors.[footnote 144]\n\nSingle sector, multiple regulator: establish a sandbox that only operates in one industry sector but is capable of supporting AI innovators whose path to market requires interaction with one or more regulators operating in that sector.[footnote 145]\n\nMultiple sectors, multiple regulators: a sandbox capable of operating with one or more regulators in one or more industry sectors to help AI innovators reach their target market. The DRCF is piloting a version of this model.[footnote 146]\n\n97. We intend to focus an initial pilot on a single sector, multiple regulator sandbox. Recognising the importance of AI innovations that have implications in multiple sectors (like generative AI models), we would look to expand this capability to cover multiple industry sectors over time.\n\n98. Initially, we envisage focusing the sandbox on a sector where there is a high degree of AI investment, industry demand for a sandbox, and appetite for improved collaboration between regulators to help AI innovators take their products to market. We invite consultation feedback on this proposal as well as suggestions for industry sectors that meet these criteria.\n\n99. We would also like to build a deeper understanding of what service offering would be most helpful to industry. Some sandboxes offer supervised real-life or simulated test environments where innovators can trial new products, often under relaxed regulatory requirements.[footnote 147] In other scenarios, a team of technologists and regulation experts give customised advice and support to participating innovators over a number of months to help them understand and overcome regulatory barriers so they can reach their target market.[footnote 148] Our current preference is for the customised advice and support model, as we think this is where we can deliver benefits most effectively in the short term. We will explore options for developing a safe test environment capability at a later date, informed by our initial pilot work.\n\n100. The implementation of an AI regulatory sandbox will also be closely informed by Sir Patrick Vallance’s review into digital regulation and his recommendation to establish a multi-regulator sandbox.[footnote 149] The review sets out a number of design principles, which we will build into our pilot approach. This includes targeting such initiatives at start-ups and small to medium-sized businesses. As a matter of priority, we will engage with businesses to understand how such an approach should be designed and delivered to best support their needs.\n\nConsultation questions:\n\nS1. To what extent would the sandbox models described in section 3.3.4 support innovation?\n\nS2. What could government do to maximise the benefit of sandboxes to AI innovators?\n\nS3. What could government do to facilitate participation in an AI regulatory sandbox?\n\nS4. Which industry sectors or classes of product would most benefit from an AI sandbox?\n\n3.3.5 Regulator capabilities\n\n101. Government has prioritised the ongoing assessment of the different capability needs across the regulatory landscape. We will keep this under close review as part of our ongoing monitoring and evaluation activity.\n\n102. While our approach does not currently involve or anticipate extending any regulator’s remit,[footnote 150] regulating AI uses effectively will require many of our regulators to acquire new skills and expertise. Our research[footnote 151] has highlighted different levels of capability among regulators when it comes to understanding AI and addressing its unique characteristics. Our engagement has also elicited a wide range of views on the capabilities regulators require to address AI risks and on the best way for regulators to acquire these.\n\n103. We identified potential capability gaps among many, but not all, regulators, primarily in relation to:\n\nAI expertise. Particularly:\n\nTechnical expertise in AI technology.[footnote 152] For example, on how AI is being used to deliver products and services and on the development, use and applicability of technical standards.[footnote 153]\n\nExpertise on how AI use cases interact across multiple regulatory regimes.\n\nMarket intelligence on how AI technologies are being used to disrupt existing business models, both in terms of the potential opportunities and risks that can impact regulatory objectives.\n\nOrganisational capacity. A regulator’s ability to:\n\nEffectively adapt to the emergence of AI use cases and applications, and assimilate and share this knowledge throughout the organisation.\n\nWork with organisations that provide assurance techniques (such as assurance service providers) and develop technical standards (such as standards development organisations), to identify relevant tools and embed them into the regulatory framework and best practice.\n\nWork across regulators to share knowledge and cooperate in the regulation of AI use cases that interact across multiple regulatory regimes.\n\nEstablish relationships and communicate effectively with organisations and groups not normally within their remit.\n\n104. In the initial phases of implementation, government will work collaboratively with key partners to leverage existing work on this topic. For example, the Digital Regulation Cooperation Forum (DRCF) is already exploring ways of addressing capability gaps within its members.\n\n105. There are options for addressing capability gaps within individual regulators and across the wider regulatory landscape, which we will continue to explore. It may, for example, be appropriate to establish a common pool of expertise that could establish best practice for supporting innovation through regulatory approaches and make it easier for regulators to work with each other on common issues. An alternative approach would be to explore and facilitate collaborative initiatives between regulators – including, where appropriate, further supporting existing initiatives such as the DRCF – to share skills and expertise.\n\nConsultation questions:\n\n19. As a regulator, what support would you need in order to apply the principles in a proportionate and pro-innovation way?\n\n20. Do you agree that a pooled team of AI experts would be the most effective way to address capability gaps and help regulators apply the principles?\n\n4.1 AI assurance techniques\n\n106. Tools for trustworthy AI including assurance techniques and technical standards will play a critical role in enabling the responsible adoption of AI and supporting the proposed regulatory framework. Industry and civil society were keen to see a range of practical tools to aid compliance. Government is already supporting the development of these tools by publishing a Roadmap to an effective AI assurance ecosystem in the UK[footnote 154] and establishing the UK AI Standards Hub[footnote 155] to champion the use of technical standards.[footnote 156]\n\n107. To assure AI systems effectively, we need a toolbox of assurance techniques to measure, evaluate and communicate the trustworthiness of AI systems across the development and deployment life cycle. These techniques include impact assessment, audit, and performance testing along with formal verification methods.\n\n108. It is unlikely that demand for AI assurance can be entirely met through organisations building in-house capability. The emerging market for AI assurance services and expertise will have an important role to play in providing a range of assurance techniques to actors within the AI supply chain. There is an opportunity for the UK to become a global leader in this market as the AI assurance industry develops. This will enable organisations to determine whether AI technologies are aligned with relevant regulatory requirements.\n\n109. To help innovators understand how AI assurance techniques can support wider AI governance, the government will launch a Portfolio of AI assurance techniques in Spring 2023. The Portfolio is a collaboration with industry to showcase how these tools are already being applied by businesses to real-world use cases and how they align with the AI regulatory principles.\n\n4.2 AI technical standards\n\n110. Assurance techniques need to be underpinned by available technical standards, which provide common understanding across assurance providers. Technical standards and assurance techniques will also enable organisations to demonstrate that their systems are in line with the UK’s AI regulatory principles.\n\n111. Multiple international and regional standards development organisations are developing, or have already released, AI-specific technical standards, addressing topics such as risk management, transparency, bias, safety and robustness. Accordingly, technical standards can be used by regulators to complement sector-specific approaches to AI regulation by providing common benchmarks and practical guidance to organisations.[footnote 157] Overall, technical standards can embed flexibility[footnote 158] into regulatory regimes and drive responsible innovation by helping organisations to address AI-related risks.[footnote 159]\n\n112. The UK plays a leading role in the development of international technical standards, working with industry, international and UK partners.[footnote 160] The government will continue to support the role of technical standards in complementing our approach to AI regulation, including through the UK AI Standards Hub.\n\nConsultation questions:\n\n21. Which non-regulatory tools for trustworthy AI would most help organisations to embed the AI regulation principles into existing business processes?\n\nPart 5: Territorial application\n\n5.1 Territorial application of the regulatory framework\n\n113. Our AI regulation framework applies to the whole of the UK. AI is used in various sectors and impacts on a wide range of policy areas, some of which are reserved and some of which are devolved. We will continue to consider any devolution impacts of AI regulation as the policy develops and in advance of any legislative action. Some regulators share remits with their counterparts in the devolved administrations. Our framework, to be initially set out on a non-statutory basis, will not alter the current territorial arrangement of AI policy. We will rely on the interactions with existing legislation on reserved matters, such as the Data Protection Act 2018 and the Equality Act 2010, to implement our framework.\n\n114. We will continue to engage devolved administrations, businesses, and members of the public from across the UK to ensure that every part of the country benefits from our pro-innovation approach. We will, for example, convene the devolved administrations for views on the functions we expect the government to perform and on the potential implications of introducing a statutory duty on regulators to have due regard to the principles.\n\n5.2 Extraterritorial application of the regulatory framework\n\n115. While we expect our principles-based approach to influence the global conversation on AI governance, we are not currently proposing the introduction of new legal requirements. Our framework will not therefore change the territorial applicability of existing legislation relevant to AI (including, for example, data protection legislation).\n\nPart 6: Global interoperability and international engagement\n\n6.1 Our regulatory framework on the world stage\n\n116. Countries and jurisdictions around the world are moving quickly to set the rules that govern AI. The UK is a global leader in AI with a strategic advantage that places us at the forefront of these developments. The UK is ranked third in the world for AI publications and also has the third highest number of AI companies.[footnote 164] We want to build on this position, making the UK the best place to research AI and to create and build innovative AI companies. At the same time, we recognise the importance of working closely with international partners. As such, the UK’s approach to both our domestic regulation and international discussions will continue to be guided by our ambition to develop AI frameworks that champion our democratic values and economic priorities.\n\n117. In line with our domestic approach, we will focus on supporting the positive global opportunities AI can bring while protecting citizens against the potential harms and risks that can emanate across borders. We will work closely with international partners to both learn from, and influence, regulatory and non-regulatory developments (see examples in box 6.1). Given the complex and cross-border nature of AI supply chains, with many AI businesses operating across multiple jurisdictions, close international cooperation will strengthen the impact of our proposed framework.\n\n118. We will promote interoperability and coherence between different approaches, challenging barriers which may stand in the way of businesses operating internationally. We will ensure that the UK’s regulatory framework encourages the development of a responsive and compatible system of global AI governance. We will build our international influence, allowing the UK to engage meaningfully with like-minded partners on issues such as cross-border AI risks and opportunities.\n\n119. The UK will continue to pursue an inclusive, multi-stakeholder approach, from negotiating new global norms to helping partner countries build their awareness and capacity in relation to the benefits and risks of AI technology. We will, for example, support other nations to implement regulation and technical standards that support inclusive, responsible and sustainable artificial intelligence. More widely, the International Tech Strategy will reiterate how we will shape global AI activities in line with UK values and priorities, protecting against efforts to adopt and apply AI technologies in the service of authoritarianism and repression. We will work with UK industry leaders to ensure that we stay at the forefront of AI and share our best practice with like-minded nations. Similarly, we will learn from our international partners, encouraging them to share lessons we can integrate into our framework.\n\n120. Our international approach will include ensuring that proven, effective, and agreed upon assurance techniques and international technical standards play a role in the wider regulatory ecosystem. Such measures will also support cross-border trade by setting out risk management and AI governance practices that are globally recognised by trading partners, reducing technical barriers to trade and increasing market access. We will also use our world-leading innovation provisions in Free Trade Agreements to address the challenges innovators in AI may face and ensure that businesses are able to take advantage of the opportunities it presents.\n\n121. In multilateral engagements, we will work to leverage each forum’s strengths, expertise and membership to ensure they are adding maximum value to global AI governance discussions and are relevant to our democratic values and economic priorities.\n\nPart 7: Conclusion and next steps\n\n7.1 Conclusion and next steps\n\n122. Our proportionate approach to regulating AI is designed to strengthen the UK’s position as a global leader in artificial intelligence, harness AI’s ability to drive growth and prosperity,[footnote 175] and increase public trust in these technologies. The approach we set out is proportionate, adaptable, and context-sensitive to strike the right balance between responding to risks and maximising opportunities.\n\n123. The proposals set out in this document have been informed by the feedback we received from over 130 respondents as part of our call for views on our 2022 policy paper. We will continue to work closely with businesses and regulators as we start to establish the central functions we have identified. Ongoing engagement with industry will be key to our monitoring and evaluation. Feedback will ensure the framework can adapt to new evidence, future-proofing the UK’s role as a leader in AI innovation and ensuring that we can take a leading role in shaping the global narrative on AI regulation.\n\n124. Given the pace at which AI technologies and risks emerge, and the scale of the opportunities at stake, we know that there is no time to waste if we are to strengthen the UK’s position as one of the best places in the world to start an AI company. In collaboration with regulators, we are already exploring approaches to implementing the framework and will scale up this activity over the coming months. We are committed to an adaptable, iterative approach that allows us to learn and improve the framework. Our sovereign parliamentary system enables us to deliver targeted and proportionate measures – including by adapting existing legislation if necessary – based on emerging evidence.[footnote 176] There are therefore aspects of our implementation work that will be delivered in parallel with the wider consultation set out in this white paper.\n\n125. In the first 6 months following publication we will:\n\nEngage with industry, the public sector, regulators, academia and civil society through the consultation period.\n\nPublish the government’s response to this consultation.\n\nIssue the cross-sectoral principles to regulators, together with initial guidance to regulators for their implementation. We will work with regulators to understand how the description of AI’s characteristics can be applied within different regulatory remits and the impact this will have on the application of the cross-sectoral principles.\n\nDesign and publish an AI Regulation Roadmap with plans for establishing the central functions (detailed in section 3.3.1), including monitoring and coordinating implementation of the principles. This roadmap will set out key partner organisations and identify existing initiatives that will be scaled-up or leveraged to deliver the central functions. It will also include plans to pilot a new AI sandbox or testbed.\n\nAnalyse findings from commissioned research projects and improve our understanding of:\n\nPotential barriers faced by businesses seeking to comply with our framework and ways to overcome these.\n\nHow accountability for regulatory compliance is currently assigned throughout the AI life cycle in real-world scenarios.\n\nThe ability of key regulators to implement our regulatory framework, and how we can best support them.\n\nBest practice in measuring and reporting on AI-related risks across regulatory frameworks.\n\n126. In the 6 to 12 months after publication we will:\n\nAgree partnership arrangements with leading organisations and existing initiatives to deliver the first central functions.\n\nEncourage key regulators to publish guidance on how the cross-sectoral principles apply within their remit.\n\nPublish proposals for the design of a central M&E framework including identified metrics, data sources, and any identified thresholds or triggers for further intervention or iteration of the framework. This will be published for consultation.\n\nContinue to develop a regulatory sandbox or testbed with innovators and regulators.\n\n127. In the longer-term, 12 months or more after publication, we will:\n\nDeliver a first iteration of all the central functions required to ensure the framework is effective.\n\nWork with key regulators that have not published guidance on how the cross-sectoral principles apply within their remit to encourage and support them to do so.\n\nPublish a draft central, cross-economy AI risk register for consultation.\n\nDevelop the regulatory sandbox or testbed drawing on insights from the pilot.\n\nPublish the first monitoring and evaluation rep"
    }
}
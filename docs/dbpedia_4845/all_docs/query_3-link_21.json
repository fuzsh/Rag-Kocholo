{
    "id": "dbpedia_4845_3",
    "rank": 21,
    "data": {
        "url": "https://primerascientific.com/psen",
        "read_more_link": "",
        "language": "en",
        "title": "PriMera Scientific Engineering",
        "top_image": "https://primerascientific.com/assets/img/favicon.png",
        "meta_img": "https://primerascientific.com/assets/img/favicon.png",
        "images": [
            "https://primerascientific.com/assets/img/site-logo.png",
            "https://primerascientific.com/assets/img/site-logo.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "PriMera Scientific Engineering journal",
            "PSEN",
            "PS PriMera Scientific Engineering journal",
            "The Journal of Engineering",
            "International Journal of Engineering",
            "engineering journal publishers",
            "SCOPUS indexed Engineering Journal",
            "civil engineering journal",
            "The Open Mechanical Engineering Journal",
            "Biomedical Engineering",
            "Biomechanics",
            "Chemical and Energy industries",
            "Earth Science",
            "Engineering Mechanics",
            "Geodesy",
            "Hydraulic Engineering",
            "Traffic Engineering",
            "Environmental Chemistry",
            "Engineering Management",
            "Structures & Geosystems",
            "Nanostructure",
            "Urban Engineering",
            "Bioengineering",
            "Environmental Engineering",
            "Automation engineering",
            "Ocean Engineering",
            "Structural Mechanics",
            "Biofluid Mechanics",
            "Biochemical Engineering",
            "Advancements in Automation",
            "Industrial Robotics",
            "Robotic Systems",
            "Spark Ignition",
            "Field Robotics",
            "Fiber Engineering",
            "Cell and Tissue Engineering",
            "Designing",
            "Rock Engineering",
            "Composite Materials",
            "Micro propagation",
            "Construction Material Handling",
            "Bone engineering",
            "Organ engineering",
            "Introduction to Plastics",
            "Biological Engineering",
            "Automation Devices",
            "Construction Engineering",
            "Automobile Engineer",
            "Bioreactor Design",
            "Dynamic Information",
            "computer engineering & information technology hybrid journals",
            "computer engineering & information technology open access journals",
            "journal of computer engineering & information technology impact factor",
            "journal of electrical engineering and electronic technology hybrid journals",
            "journal of electrical engineering and electronic technology open access journals",
            "journal of journal of electrical engineering and electronic technology impact factor",
            "electrical",
            "electronics",
            "electronic circuits and devices",
            "communications & networking",
            "power electronics & systems",
            "signal & image processing",
            "control systems",
            "open access journals",
            "eee proceedings",
            "bio-medical engineering",
            "artificial intelligence",
            "embedded system",
            "journal of computer engineering & information technology online journals",
            "journal of fashion technology & textile engineering hybrid journals",
            "journal of fashion technology & textile engineering open access journals",
            "journal of journal of fashion technology & textile engineering impact factor",
            "journal of fashion technology & textile engineering online journal",
            "textile materials",
            "fibre science & technology",
            "clothing & apparel technology",
            "textile finishing",
            "fashion theory",
            "fashion history",
            "fashion design",
            "High quality original research work with newer research concepts in the fields of Computer Engineering"
        ],
        "tags": null,
        "authors": [
            "Constantinos S Mammas*",
            "Adamantia S Mamma",
            "Yu Okano*",
            "Takeshi Kaneshita*",
            "Katsuki Okuno",
            "Shimpei Takemoto",
            "Yoshishige Okuno",
            "Elias Raul Acosta*",
            "Carlos J Ortega",
            "Gina Vega Riveros"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "PriMera Scientific Engineering (PSEN) is Established for the publication of novel ideas, which states the research results and applications in the field of Engineering. PSEN covers wide areas like Computer Sciences, Mechanical Engineering, Civil Engineering, Data mining & Bioinformatics Engineering etc. And it is immense pleasure & privilege to invite you to submit your articles in all the above disciplines. All the papers submitted to PSEN undergo double-blined peer review process. All the research which are published by the PSEN is for the prompt impact to the scientific community.",
        "meta_lang": "",
        "meta_favicon": "https://primerascientific.com/assets/img/favicon.png",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "What is the solution?\n\n1. If the driver's seat is on the left side, if you look in the mirror on the immediate left side, you can see people getting on, off, and falling, as well as oncoming vehicles and pedestrians at the same time. If any passenger falls from the vehicle, the vehicle can be stopped immediately.\n\n2. There is no need to turn the head to the left side so that time can be obtained. The danger due to it can also be avoided.\n\n3. When passing another vehicle on the right side, the driver's seat is on the left side so that the sides of the vehicle on the left side can be clearly seen.\n\n4. When a driver driving a vehicle sitting on the left side on the roads allowing travel in both directions tries to overtake another vehicle on the right side, when a vehicle suddenly comes from the opposite direction, the driver's action is according to the reflex action, so the vehicle on the left side is likely to be in danger, so the vehicle is kept to the left side only with great care.\n\nWhen overtaking another vehicle on the right side, if the driver's seat is on the left side, the sides of the vehicle on the immediate left can be clearly seen.\n\n5. The life of the driver sitting on the left side of the vehicle is not safe if the vehicle hits the vehicle in front from the rear or side while overtaking another vehicle on the right side of the one way roads. Be very careful and force the other vehicle to pass as your life may be in danger.\n\n6. A driver driving a vehicle sitting on the left side will have to maintain a greater distance from the vehicle in front to see the position of the vehicle coming from the opposite side to overtake the vehicle in front. This reduces the risk of hitting the rear of the vehicle in front.\n\nA driver sitting on the left side of the vehicle is more likely to have an accident while the vehicle moves to the left side to avoid an accident. The driver will realize that his life is in danger if he hits the one immediately to his left. This will prompt the driver to drive cautiously.\n\nThe accident which caused the death of 9 people in Vadakancheri took place on a one-way road. It can be seen that the vehicle that caused the accident hit from the rear and right side of the vehicle in front. . Had the driver's seat been on the left side, such an accident would never have happened. The driver himself tried to save the life of the driver. The driver was not seriously injured.\n\n7. If the driver gets out of the left side driver's seat by opening the door, such accidents will be avoided as the two-wheeler coming behind will not be affected.\n\n8. Driver's position in vehicles should be on the left side of the vehicle if pedestrians are also walking on the right side.\n\nIn right-hand driving, the advantage is that the driver sits on the right side\n\n1. Pedestrians can be made to walk on the left side as pedestrians have a natural tendency to walk on the left side in right-hand driving. It is also possible to back off to the left side as per the reflex action when any vehicle is approaching.\n\nIt is said that pedestrians should walk on the right side of the road in order to see the vehicles coming from the opposite side.\n\n2. If you want to overtake the vehicle while driving on the right side, you should keep more distance from the vehicle in front.\n\nAs the driver has an innate tendency to veer to the left side while driving on the right side\n\nOn one-way roads, when the driver tries to pass the vehicle in front of him on the left side, the driver's action is based on the reflex action, so the risk of accident is high for the driver.\n\nThis defect cannot be avoided as passengers should be prioritized over the driver. A driver who has experienced the risk of an accident many times will be more cautious.\n\n3. If the driver gets down from the right side driver's seat by opening the door, such accidents will be avoided as it will not affect the two-wheeler coming behind.\n\n4. When stopping a two-wheeler while driving on the right side, the vehicle does not lean towards the footpath as the left foot sticks. This will make it more convenient for pedestrians to walk.\n\nIf the driver's seat is on the right side of the vehicle then it should be right side driving and the door of the vehicle should be on the right side.\n\nWhy walk on the left side?\n\nIndians generally have a tendency to walk on the left side. A soldier starts his march on the left foot. When one stands to attack another, stand with the left leg crossed. The left side is always defensive and retreating.\n\nWhile walking along the footpath, you can see that most of the people walk on the left side to pass the person coming from the opposite side.\n\nA person or a group of people walking in the middle of the road on a road with few vehicles has been observed to move to the left side of the road when they hear the bell of a bicycle behind them.\n\nThose standing beyond the halfway mark have also been seen shifting to the left side of the road.\n\nA cyclist should hit the ground first with the left foot.\n\nIt is wrong to suggest walking on the right side when there is a natural tendency to walk on the left side. Therefore, it is essential that pedestrians walk on the left side, vehicles drive on the right side, and the driver's seat is on the right side of the vehicle.\n\nThe need for uniform traffic rules all over the world\n\nToday, many countries have different traffic laws and practices. In some countries you drive on the right side while in some countries you drive on the left side. Similarly there are vehicles with driver's seat on left side and right side.\n\nBecause of this, many people do not like to drive for fear of getting into trouble when going to countries that are against the ways they are used to. Many people are not used to driving against a traffic pattern in another country.\n\nMany people travel and work in different countries with different traffic laws on the same day. Different road traffic rules cause various doubts in the mind.\n\nMany existing traffic laws are against the reflex action that comes from our unconscious mind.\n\nMost drivers cannot concentrate on driving full time. Many people think about many other things while driving. Most of the time driving is by reflex action.\n\nWhen danger occurs, the mind often simultaneously reacts in favor of reflex action but against traffic law. This causes various types of accidents.\n\nIf the traffic rules are the same around the world based on reflex action, accidents will be reduced for those who cannot concentrate on driving full time.\n\nIt is very important to enforce the same traffic rules all over the world.\n\nKeywords: pedestrians; right hand driving; safety; drivers seat; accident\n\nAn anomaly of the current right-hand driver's position\n\n1. It is difficult for the driver sitting on the right side of the vehicle to accurately know the position of the vehicle passing on his left side and the pedestrian passenger.\n\nIt is common for pedestrians to be struck by vehicles because the left front of the vehicle, including the driver's seat, cannot be seen clearly enough.\n\n2. Presently the driver's seat is on the right side and the bus door is on the left side. The driver sitting on the right side has to turn his head to the left side to look at the mirror sitting on the left side to see the people getting on and off on the left side.\n\nWhen the head is turned to the left, it is impossible to see the vehicles coming from the right behind and the opposite direction in front. Similarly, when the head is turned from the left side to the right side, it is not possible to see what is happening on the left side of the vehicle.\n\nBuses do not have doors in most places in India. Do not close the doors. Due to this, a large number of passengers are seriously injured and die every year due to falling from the vehicle, people getting out of the vehicle after the vehicle starts moving, and getting into the vehicle.\n\n3. Take time to turn the head to the left and back to the right to look at the mirror on the left.\n\nThis makes it impossible to look in the left side mirror and pay attention to the oncoming vehicle in front at the same time. This causes danger.\n\n4. When overtaking another vehicle on the right side, the overtaking driver's seat is on the right side and cannot see the sides of the vehicle on the left.\n\n5. When a driver driving a vehicle sitting on the right side on the roads that allows travel in both directions tries to pass another vehicle on the right side, when a vehicle suddenly comes from the opposite direction, the driver's action is based on reflex action, without the ability to pay attention to the vehicle on the left side, the driver keeps the vehicle to the left side to save his life\n\n6. When overtaking another vehicle on the right side of one-way roads, even if the vehicle hits the vehicle in front from the rear or side, the life of the driver sitting on the right side is safe. This makes many drivers arrogant.\n\nThis is what happened in the accident that resulted in the immediate death of 9 people in Vadakancherry.\n\nThe accident took place in Vadakancherry on one way. It can be seen that the vehicle that caused the accident hit from the rear and right side of the vehicle in front.\n\n7. It is comparatively difficult to get out from the right side driver's seat to the left side when the vehicle is stopped, so the driver opens the right side door and gets out to the right side. When the door is opened in this way, there are many incidents of death due to being hit by the following two-wheeler.\n\nThe main reason for the sudden death of 9 people in Vadakancherry. Driving on the left side is the driver's seat on the right side.\n\n8. Pedestrians should be given more priority on the road.\n\nIn India, vehicles stop and park on the left. Two-wheeler drivers put their left foot on the ground and stop the vehicle by tilting it to the left side. This also causes more inconvenience and danger to the pedestrians walking on the right side.\n\nIf the two-wheelers passing by on the left side lean to the right side to avoid causing inconvenience to the pedestrians, the danger to the two-wheeler will increase. If one tilts to the right side and stops the vehicle, the balance will go. The other will go through the leg of the person driving the two-wheeler stopped in front and the wheel of the vehicle of the motorist coming from behind will go up.\n\nAfter identifying the central idea of Nano [1], as \"the power of mind over matter\", was to characterize with these elements of Lagrangian and Hamiltonian dynamics concepts such as conscience of a particle, due action to a force of the field, the field itself, seen as an intention [2, 3] and to unite the synergistic action of many particles acting together to create an organized transformation on matter.F.1\n\nThese are the first elements considered to create a mathematical theory of nanotechnology [1] at the advanced level of field theory, which could be achieved in the coming years. Although the latter is mentioned in a first tribute to me [4], this theory is now reality (figures 1).\n\nWhile we carry out some interesting research in nanomedicine [5], and nanomaterials [6, 7], being part of the technological developments realized around of the mathematical theory of nanotechnology that are being and have been realized and published in many references.\n\nThis study consider the causal structure of the scattering phenomena through past and future light cones, create the possibility of energy for one thousand years, perpetual motion machines and star-gates (worm holes in the space) with advanced analogues as the synchrotronic propulsion (advanced spaceships) and disintegrative mass weapons, using the same principles in all case.\n\nDuring many years developing many ideas around the electromagnetic propulsion as a source of take-off and movement, including landing, of advanced magnetic levitation vehicles. In the year 2010, [8] starts a research sub-program of his research program, dedicated to the development of advanced vehicles by electromagnetic propulsion.\n\nSeveral papers are published in more than 20 different journals and book chapters on quantum mechanics and superconductivity [9, 10] for the purpose of proving a consistent theory for the creation of these advanced vehicles and reinforces in that sense, taking advantage of his position as editor-in-chief of the journal on photonics and spintronics, with works by other researchers related to the study of supercinductivity, Majorana fermions, and other related topics (figure 2 and figure 3). F.2\n\nJust as electromagnetic fields are caused by a charge and gravitational fields are caused by weight (mass and force), any rotating objects create torsion fields.\n\nTorsion fields can interact with laser beams (change frequency); creating effects of diverse nature, for example, in the biological processes, where torsion affect directly the ADN. Also can melting or solidifying some materials, affect quartz crystals increasing their properties as resonators. Also affect some electronic components creating radiation coverings. The torsion can favourably change some beverages, and have been noted to affects gravity. F.3\n\nAccording to this theory, every substance has its own \"chronal charge\" defined by the quantity of \"chronal\" particles which were named \"chronons\". In [13] was supposed that while the object is spinning, \"chronons\" are interacting with other \"chronons\" that surround this object and therefore the weight of the object changes. According to A.I. Veinik's theory, \"chronons\" generate the so called \"chronal\" field. A.I. Veinik found experimentally that strong \"chronal\" fields can be generated by spinning masses. In [14] explained through schemes of coverings and measured some properties of \"chronal\" fields and found that two types of \"chronons\" exist (\"plus\" and \"minus\" chronons). It is important to emphasize that can be concluded that the sign of the \"chronon\" depended on orientation of their spin. Some of these facts could explain the hyperbolicity of the space and their law of the minimal action and geometrical trajectories “braquistrocrona curve” satisfying this law of minimal action, and consigned in the inertial law inside the Einstein equations with expansion reflected into the Christoffel symbols considered in the gravitation equations.\n\nThe spinning space can be consigned in a smooth space (as the apparent uniformity of the space-time in the ground) when the energy fluxes of the spins derive in neutrinos and these full all space of energy. The problem with the manage of the chronogeometry with this particles is the definition of the synchronicity that is required to some process to quantum level to create some process and their integration inside synergic action or “organized transformations” to obtain the reality of the space-time [2].\n\nActually, and with research groups that I direct, we are considering the torsion [15, 16] as principal effect to generate propulsion in an advanced model of ship, considering the way through electromagnetic plasma [17] (figure 4 A).\n\nLikewise through of a caption and detection camera, is measured the electromagnetic properties of the ionic flow of the space ~IIe (ρ, u), derived from the electromagnetic plasma ~IIH, and is proposed an ionic propeller considering the pressure gradients due to electrons and ions concentrated in a little region of the shock waves produced with an electric field. Also is used mean curvature energy [18, 19] to measure and control the ionic flow. Then researches are in this step (figure 4 B). F.4\n\nWe must to create or invent new mathematics to describe a complete quantum mechanics based in the synchronicity; a synchronic quantum mechanics, which is there in the causal structure of the Universe but in a more deep level. All these are theories developed around field theory to define the intentionality of a field and apply it in nanotechnology, on matter.\n\nStudies in condensed matter and MHD, also have been incorporated. The only one energy to all ship (even to nanomedicine process) must come from the reactor of electromagnetic plasma.\n\nReferences\n\nF Bulnes. Mathematical Nanotechnology, Generis Publishing, Slovenia (2021).\n\nBulnes F. “Mathematical Nanotechnology: Quantum Field Intentionality”. Journal of Applied Mathematics and Physics 1 (2013): 25-44.\n\nFrancisco Bulnes. “Quantum Intentionality and Determination of Realities in the Space-Time Through Path Integrals and Their Integral Transforms”. Advances in Quantum Mechanics, Prof. Paul Bracken (Ed.), InTech (2013).\n\nHelen Thrift, Yuri Stropovsvky and Francisco Bulnes (Honored). “A Mathematician’s Search for Technologies of Understanding the Universe”. Cambridge Scholars Publishing, New Castle, United Kingdom (2020).\n\nF. Bulnes., et al. “Quantum Developments in Nanomedicine: Nanocurative Actions by Soft Photons Sources and their Path Integrals”. Open Central Press, United Kingdom (2015).\n\nF Bulnes, Y Stropovsvky and V Yermishkin. \"Quasi-Relaxation Transforms, Meromorphic Curves and Hereditary Integrals of the Stress-Deformation Tensor to Metallic Specimens\". Modern Mechanical Engineering 2.3 (2012): 92-105.\n\nF Bulnes, V Yermishkin and E Toledano. “Constitutive Equations of the Stress-Strain Tensor for a Metal Speci- men Rehearsal in Quasi-Relaxation Regime and Their Generalized Functional of Energy”. Proceedings of the 2nd CIMM, Vol. III, Department of Mechanical Engineering, Universidad Nacional de Colombia, Bogota (2009).\n\nF Bulnes, E Hernández and J Maya. Volume 7: Fluid Flow, Heat Transfer and Thermal Systems, Parts A and B, ASME 2010 International Mechanical Engineering Congress and Exposition, Vancouver, British Columbia, Canada (2010).\n\nF Bulnes, J Maya and I Martínez. \"Design and Development of Impeller Synergic Systems of Electromagnetic Type to Levitation/Suspension Flight of Symmetrical Bodies\". Journal of Electromagnetic Analysis and Applications 4.1 (2012): 42-52.\n\nF Bulnes and A Álvarez. \"Homological Electromagnetism and Electromagnetic Demonstrations on the Existence of Superconducting Effects Necessaries to Magnetic Levitation/Suspension\". Journal of Electromagnetic Analysis and Applications 5.6 (2013): 255-263.\n\nFrancisco Bulnes. “A Lie-QED-Algebra and their Fermionic Fock Space in the Superconducting Phenomena, Selected Topics in Applications of Quantum Mechanics”. Prof. Mohammad Reza Pahlavani (Ed.), InTech (2015).\n\nF Bulnes. An electromagnetic ship designed by Lie-QED and their fermionic fock space principles in superconducting to the future flights, 4th International Conference and Exhibition on Mechanical & Aerospace Engineering, Orlando, USA (2016).\n\nVeinik AI. Thermodynamics of Real Process, Minsk (1991) (In Russian).\n\nRamirez M., et al. “Field Ramifications: The Energy-Vacuum Interaction that Produces Movement”. Journal on Photonics and Spintronics 2.4 (2013): 4-11.\n\nFrancisco Bulnes., et al. “Detector of Torsion as Field Observable and Applications”. American Journal of Electrical and Electronic Engineering 8.4 (2020): 108-115.\n\nFrancisco Bulnes. “Deep Study of the Universe through Torsion, Cambridge Scholars Press”. United Kingdom (2022).\n\nFrancisco Bulnes., et al. “Electromagnetic plasma reactor: Implicit application of field torsion I”. Int J Electron Microcircuits 1.1 (2021): 30-35.\n\nF Bulnes, I Martínez and O Zamudio. “Fine Curvature Measurements through Curvature Energy and their Gauging and Sensoring in the Space”. Book of Sensors and Applications in Measuring and Automation Control Systems (Advances in Sensors: Reviews, (Ed.) Sergey Y. Yurish, IFSA Publishimg, Barcelona, Spain 4.20 (2017).\n\nF Bulnes., et al. “Electronic Sensor Prototype to Detect and Measure Curvature Through Their Curvature Energy”. Science Journal of Circuits, Systems and Signal Processing 4.5 (2015): 41-54.\n\nFrancisco Bulnes., et al. “Electromagnetic plasma reactor: Implicit application of field torsion I”. Int J Electron Microcircuits Accepted (2022).\n\nAfter the Sumerians, people learned to write on clay board. This when people came with the idea of abacus using the wood and clay board. This board is divided into columns with order of base 60 number system. They used different shaped and sized objects in those columns for calculations. The order use to be 1’s, 10’s, 60’s, 600’s, and 3600’s and placing tokens and removing does the addition and subtraction.\n\nAfter this came modern abacus, as people are used to count with fingers, and we can do this to count till 10. So, to count more than 10 we need more fingers or any object. This made to implement the abacus with base ten i.e., 1’s, 10’s, 100’s, 1000’s…… etc.\n\nNow, the era of computers came and all we have is just electricity to communicate with these electronic devices. So, like morse code dots and dashes to decode alphabets for communicating with people. We have on and off switch to communicate with computer. As calculations are an integral part of computing which is a core functionality of computers, we need to build everything around this as part of communication. This made us to choose the reliable calculation option of 10 finger option of base 10 and reducing it to base 2 to accommodate the 2 level on and off switch functionality. The base 2 binary code has 0’s and 1’s filling the 20, 21, 22, …. etc.\n\nComputers\n\nA computer can store and process data. Most computers use binary code, which uses two variables, 0 and 1, to complete storing data and calculations. Throughout history many prototypes have been developed leading to the modern-day computer. During World War II, physicist John Mauchly, engineer J. Presper Eckert, Jr., and their colleagues at the University of Pennsylvania designed the first programmable general-purpose electronic digital computer, the Electronic Numerical Integrator and Computer (ENIAC). Programming languages, such as C, C++, JavaScript and Python, work using many forms of programming patterns. Programming, which uses mathematical functions to give outputs based on data input, is one of the regular ways to provide instructions for a computer.\n\nBinary Code and Transistors\n\nComputers are made using transistors and they operate based on electricity flow. The binary code is just representation of whether transistor is conducting or not. A simple addition operation using transistors:\n\nLet’s see we want to do 1+1=2, take two transistors allow voltage to flow(transfer) from we get the 2 times the voltage.\n\nif we want 0+1=1, same two transistors but make one resist the voltage allowing only voltage we get just 1 time the voltage.\n\nThe bigger the number and operation, the more transistors required in computers for computing.\n\nOn the other side, when sensitive information is stored on a cloud server, which is not in a direct control of end user, then the risk for the information is increased dramatically. Many unauthorised users may try to intercept secure data to compromise data centre server. Therefore in cloud communication, cloud provider will help to provide complete security measures for user to user communication but at the end cloud protection is not a network protection. Network level threads can compromise the security of information, so to provide the protection of information from intruders, security shield at the different level of cloud network is required.\n\nWe need three layers of security shield in cloud based network (1) Connectivity Level (2) Storage Level (3) Application Level. Cloud based protocol provide a broad set of policies and technologies to control these attack.\n\nIn connectivity level, when the real world communicate with multiple data centres and multiple end users in a cloud network, we have to consider all aspects of infrastructure including computer centre, data centre and cloud network to overcome the challenges related to architecture, performance , reliability, security, maintainability and virtualization as well.\n\nAs the information move between the communication channels, end to end encryption and authentication with no data leakage is mostly needed. During this transmission to protect our cloud network, Host Identity Protocol is used for authenticate IPv4, IPv6 client server cloud network from the intruders. MQTT and HTTP Protocol provide a core support for device connection and communication. In general MQTT is supported by embedded devices and for machine to machine interaction. In HTTP protocol based devices do not maintain a connection to IoT cloud, however it maintain a half duplex TCP connection where transmission is connectionless.\n\nAt this level some standardized protocol like connectionless network protocol, Secure Shell protocol, Spanning Tree Protocol, Equal Cost Multi-Pathing Protocol and many more are used. In these protocols some are communication protocols which are used for message failure detection, message monitoring and data unit identification.\n\nIn storage level, privacy plays an important role for the cloud service customer. Cloud service provider must ensure that the information and identity of client service customer will not disclose in any case. On the other hand, most pressing issue is that the data must be encrypted so no one, even administrator can’t see the information without permission of customer. It is a duty of customer service provider to make sure the customer that no duplicate copy of keys (Private /Public) is generated and data form stored in is in encrypted multiple safe locations.\n\nSince the cloud is a very vast storage and it deals with a large amount of data information, the service provider should provide a separate address space to each customer with their individual memory space. This virtual isolation is provided with the help of dedicated virtual machines. Some time cloud deploys firewalls to protect the data and to overcome it Session Initiation Protocol (SIP) is used on VoIP based communication. This protocol helps the cloud for protecting their network by attacks like Denial of service, IP traffic management, Toll fraud protection and encryption of data.\n\nIn application level, cloud provides a facility to the end user to design their own application as per their requirement and the platform provided by service provider. Each cloud has its own different platform to execute user idea. The application must be tested and verified by the cloud service provider. The complete testing and module checking are done at the cloud service provider level before application being made available for the end user. There are some chances that attacker can also create a malicious application on cloud and launch it with attractive features to attract new user in a network. Application security protocol can handle all these malicious application service provider and user also. In addition, Cloud service providers have their own application firewall for monitoring incoming and outgoing traffic.\n\nIn these days, Equal Cost Multi-Pathing protocol is widely adopted by the cloud computing because it has the ability to create multiple load balanced paths which play a very important role for providing variable bandwidths depending upon the requirement of the application. Moreover, Extensible messaging and presence protocol (XMPP) can be used for public subscribe system and file transfer.\n\nTo conclude, there is no doubt that Cloud computing is the latest field in communication and for technology friendly users which promises immense benefits. Most of the Information technology giants like IBM, Cisco, Google and Microsoft have adopted it and continuously working in this area to handle security and privacy issues. It is expected that the use of cloud computing would exponentially increases in the upcoming days and simultaneously we all will face new challenges in cloud security. Hacking and various attacks to cloud infrastructure and cloud network would affect multiple clients even if only one site or one machine is attacked. These risks can be mitigated by using most secure protocols, security applications, most dynamic encrypted file systems, minimum data leakage and recovery software, and buying security hardware to track unusual behaviour across servers. However, there a lot of research work by the experts is still required in cloud area because many of the concerns related to security and privacy issues are not been answered yet.\n\nDr. Varun Prakash Saxena (BE, ME, PhD) is presently Assistant Professor, Department of Computer Engineering, Government Women Engg. College Ajmer since 2012. He has 10 year of teaching and research experience. He has published more than 18 research paper and 5 research article in national/International journal of repute. He is a member of 5 national/ international professional societies. His interest includes cryptography, Networks and programming. He is also guiding more than 10 PG students.\n\nIt is known that phases are formed in the Fe-N system: α-phases (nitrogenous austenite with a face-centered cubic lattice) [1, 2]. At a temperature of 590°C, the α-phase represents the eutectoid decomposition γ→α+ γ'. When supercooling the γ-phase by the shear mechanism, it represents a martensitic transformation. Nitrogenous martensite (α ' - phase) has a tetragonal body-centered lattice [2, 3]. A separate γ'-phase is a solid solution of Fe and N nitride, the ε-phase is also formed during nitriding and is a solid solution of Fe3N. In the Fe-C-N system in steel, the main strengthening phase is ε-carbonitride Fe2-3(N-C) formed during nitriding, the carbonitride phase obtained by simultaneous diffusion of carbon and nitrogen into steel has high hardness and high wear resistance. In the γ-phase, carbon practically does not dissolve, and the γ-phase is a solution of nitrogen and carbon intercalation. According to [4]. The introduction of nitrogen into the cementite lattice facilitates the formation of the carbonitride phase. Carbonitride with a cementite lattice is formed in the process of nitrocarburizing at a temperature of 680°C. When steel is nitrided, cementite, after saturation with nitrogen atoms, turns into ε-carbonitride. It has been established in the studies that during the nitriding of alloyed steels, phase points are formed, which are also formed during the nitriding of iron, only during alloying does the composition of the phases and the temperature intervals of their formation change. Studies have shown that in alloyed steel, due to the nitrogen content in the ε-phase, the hardness is increased to HRC 63.\n\nFollowing the nitride zone during nitriding in steels, there is a layer of the α-phase, which is the main part of the diffusion layer. Refractory alloying elements increase the solubility of nitrogen in the α-phase. During nitriding, the mosaic blocks are also refined and the α-phase lattice is distorted. The overall change in the defectiveness of the crystal structure of the α-phase depends on the nitriding temperature. When nitriding in the region of 500-600°C, the α-phase zone provides ferrite grains, and at a higher temperature of more than 600°C, a darkly etched zone is formed, also consisting of ferrite grains. Moreover, the darkening of ferrite grains increases with an increase in the content of alloying elements. During slow cooling after nitriding in steel, a γ'-phase of an acicular character is released from the α-phase. All alloying elements to some extent reduce the diffusion coefficient of nitrogen in the α-phase and, accordingly, reduce its depth.\n\nThe structure of the nitrided layer is formed not only at saturation temperature but also during subsequent cooling. During cooling of the nitrided layer, the α-solid solution decomposes. This decay is strongly influenced by the rate of cooling. If there is a slow cooling, then simultaneously granular needle-like nitrides are formed that are released from the α-phase. The properties of the nitrided layer are determined by the structure that was formed during the saturation of the steel with nitrogen and subsequent transformations occurring during cooling. Two phases have high hardness: γ'-phase and nitrogenous martensite α'-phase. All alloying elements reduce the thickness of the nitrided layer but significantly increase the hardness of the steel surface. It was found that the high hardness of the nitrided layer is obtained by separating dispersed nitride alloying elements from solid solutions, which distort the α-phase lattice and serve as an obstacle to the movement of dislocations.\n\nMoreover, the greatest increase in hardness corresponds to the nitriding temperatures at which nitrides are actively formed. The nitride layer forms strong elastic distortions of the crystal lattice of the α-phase. These distortions prevent the movement of dislocations and contribute to the hardening mechanism of the steel. By changing the temperature and time of nitriding, it is possible to fix various stages of nitride precipitation in the diffusion zone and thus control the degree of steel hardening. When alloying steel with several elements, the degree of distortion of the diffusion layer is much higher than that of steel alloyed with one element. Therefore, complex alloy steels tend to have higher hardness than low alloy steels.\n\nReferences\n\nLakhtin YuM and Kozlovsky IS. “Fundamentals of chemical heat treatment technology”. M. Mashinostroeniya (1985): 256.\n\nGuriev AM., et al. “Improving the technology of chemical-thermal processing of tool steels and metal processing”. 6.1 (2009): 19-21.\n\nDolzhenkov VN. Cyanidation of improved steels in pastes: Cand. tech. Sciences. - Kursk (2001): 127.\n\nLakhtin YuM and Kogan YaD. “Nitriding of steel”. M.: Engineering (2005): 254.\n\nNorkhudjayev FR., et al. “Influence of nitrocementation modes on the change in the hardness of the surface layer of structural steels”. Journal NX- A Multidisciplinary Peer Reviewed Journal 7.11 (2021): 75-77.\n\nAlimbabaeva ZL and Bektemirov BSh. “Composite materials production technology for machining materials”. Lityo i metallurgiya 2020: sbornik nauchnih rabot III Mejdunarodnoy nauchno-prakticheskoy internet konferenciy studentsov i magistrantov, 18-19 noyabrya 2020 g./sost. AP Bejok. Minsk: BNTU (2021): 92-93.\n\nTilavov Yunus Suvonovich., et al. “Research of Technological Modes of Production of Small Diameter Rods from Niobium”. In: Cioboată, D.D. (eds) International Conference on Reliable Systems Engineering (ICoRSE) - 2022. ICoRSE 2022. Lecture Notes in Networks and Systems, Springer 534 (2022).\n\nAlimbabaeva ZL., et al. “Physical and technological basis for formation of coatings by electric contact sintering”. Casting and metallurgy 2022: collection of scientific papers of the International Scientific and Practical Internet Conference of Students and Undergraduates. Minsk: BNTU (2022): 175-176.\n\nApplications in Various Industries\n\nThe versatility of Functionally Graded Materials has sparked interest and found applications in a wide range of industries. In aerospace, FGMs are used in turbine blades, where the gradual variation in material properties helps withstand high temperatures and reduce thermal stresses. In automotive applications, FGMs contribute to lightweight designs while improving structural integrity. The energy sector benefits from FGMs in heat exchangers, which efficiently manage thermal gradients. Biomedical implants take advantage of FGMs' ability to mimic the natural transition between bone and implant materials, promoting better integration and reducing complications. Moreover, FGMs find applications in electronics, where they enable precise control of electrical conductivity and thermal management. Some key application areas where FGMs are used.\n\nThermal Management Systems\n\nFGMs find use in thermal management systems, such as heat sinks and cooling devices. By tailoring the thermal conductivity gradient within the material, FGMs can efficiently dissipate heat, ensuring optimal thermal performance and minimizing temperature gradients.\n\nWear-Resistant Coatings\n\nFGMs can be employed as wear-resistant coatings in various industries. By gradually transitioning from a hard and wear-resistant material to a tough and ductile material, FGM coatings can provide superior resistance to wear, abrasion, and erosion.\n\nOptics and Photonics\n\nFGMs play a vital role in optical and photonics applications. They can be designed to possess varying refractive indices, allowing for the controlled manipulation of light and the creation of gradient-index lenses, waveguides, and optical filters.\n\nEnergy Storage and Conversion\n\nFGMs have the potential to enhance energy storage and conversion devices. For example, in lithium-ion batteries, FGM electrodes can optimize the transport of ions and electrons, improving the battery's performance and durability.\n\nStructural Components\n\nFGMs offer advantages in structural applications where there are significant thermal or mechanical loads. By gradually adjusting the material properties, FGMs can minimize stress concentrations, reduce thermal expansion mismatches, and enhance the overall structural integrity of components.\n\nAerospace Propulsion Systems\n\nFGMs have garnered attention in aerospace propulsion systems, particularly in combustion chambers and turbine components. The tailored composition and property gradients can withstand extreme temperatures, resist thermal fatigue, and enhance the efficiency and durability of propulsion systems.\n\nAcoustic Applications\n\nFGMs can be utilized in acoustic devices and systems. By controlling the density and stiffness gradients, FGMs can effectively manipulate sound waves, enabling the design of improved acoustic lenses, sound barriers, and noise-reduction materials.\n\nMicroelectronics and MEMS\n\nFGMs find applications in microelectronics and microelectromechanical systems (MEMS). By tailoring the electrical conductivity and thermal properties, FGMs can facilitate better heat dissipation, improved electrical interconnects, and enhanced performance of microdevices.\n\nCorrosion Protection\n\nFGMs can serve as protective coatings in corrosive environments. By gradually transitioning from a corrosion-resistant material to a sacrificial layer, FGM coatings can effectively inhibit corrosion and extend the lifespan of structures and equipment.\n\nThese applications highlight the diverse range of industries that benefit from the unique properties and capabilities of Functionally Graded Materials. By tailoring material compositions and properties, FGMs offer opportunities for innovation and optimization, leading to improved performance, efficiency, and reliability in various technological domains.\n\nDesign Challenges and Fabrication Techniques\n\nThe design and fabrication of Functionally Graded Materials present unique challenges. Achieving the desired composition gradients requires careful consideration of material selection, processing techniques, and modeling approaches. Techniques such as powder metallurgy, thermal spray, sol-gel processes, and additive manufacturing methods like 3D printing offer the means to fabricate FGMs with controlled composition variations. Additive manufacturing, in particular, allows for the creation of complex geometries and precise property gradients, opening up new possibilities for material design.\n\nFuture Prospects\n\nFunctionally Graded Materials continue to captivate researchers and industry professionals, with ongoing efforts to refine fabrication techniques, characterize properties, and optimize designs. Advances in computational modeling and simulation techniques aid in predicting material behavior and guiding the development of novel FGMs. As additive manufacturing technologies advance, FGMs are poised to revolutionize material design, offering unprecedented opportunities for customization and performance optimization.\n\nConclusion\n\nFunctionally Graded Materials represent a remarkable advancement in material science and engineering. By carefully tailoring composition gradients, FGMs offer unique combinations of properties that can overcome the limitations of homogeneous materials. From aerospace and automotive applications to biomedical implants and electronics, FGMs unlock new possibilities for innovation and performance optimization. As research and development in this field continue to progress, we can anticipate exciting breakthroughs that will shape the future of advanced materials and their applications in various industries.\n\nA well-known classification algorithm we often hear about and use in solving several problems is the k-nearest Neighbor (KNN) Classifier algorithm, which can also be used in regression processes such as the KNN regression algorithm. Also, the Naïve Bayes Classifier algorithm, including the Artificial Neural Network (ANN) algorithm, where the Neural Network (NN) algorithm can also carry out the regression process. Furthermore, the Support Vector Machine (SVM) algorithm can also conduct the regression process with the SVM regression algorithm and the Decision Tree Classifier (DT) algorithm, including the regression process with the DT regression algorithm. Apart from that, the random forest classifier algorithm can also be used for classification, and the regression process is carried out with the RF regression algorithm. Furthermore, the classification process can also be carried out using the Generalized Regression NN (GRNN) algorithm as a variation of the Radial Basis NN (RBNN) algorithm, where GRNN can be used for classification or regression. Finally, the classification process can also use the algorithm. The gradient-boosted tree classifier can also be chosen to carry out the classification. The regression process can also be done using the Gradient gradient-boosting machine-regression algorithm. Furthermore, the classification process can be carried out with multilayer perceptron Classifier algorithms, One-vs-Rest (aka One-vs-All) Classifier algorithms, and factorisation machine Classifier algorithms.\n\nIn addition, the classification process can be used to classify with two output results, namely the binary classification algorithm. The following classification algorithms can be used for binary classification and regression processes: the Logistic Regression algorithm, LightGBM algorithm, XGBoost algorithm and Neural Network (NN) algorithm (Deep Learning). Apart from that, the following binary classification algorithms are only specific for classifying with two output results. They are the Naive Bayes (Gaussian) algorithm, the Naive Bayes (Bernoulli) algorithm, the K Nearest Neighbors (KNN) algorithm, the Support Vector Machine (SVM) algorithm, the Decision Tree (DT) algorithm, Random Forest (RF) algorithm and Gradient Boosting Machine algorithm.\n\nA classification model can be measured using a confusion matrix, a Matrix for Summarizing the performance of the Classification algorithm and is known as an error matrix. A confusion matrix is a matrix between the predicted and actual conditions in the population. The predicted condition has PP as the number of positively predicted cases in the population, whilst PN is the number of negatively predicted cases in the population. Meanwhile, the actual condition has P as the number of positive actual cases in the population and N as the number of negative actual cases. The confusion matrix has 4 scores: TP as True Positive, TN as True Negative, FN as False Negative and FP as False Positive. TP is a test result that correctly indicates the presence of a condition or characteristic, whilst TN is a test result that accurately displays the absence of a condition or characteristic. Meanwhile, FN is a test result that wrongly indicates that a particular condition or attribute is absent, and FP is a result that wrongly suggests that a specific condition or attribute is present.\n\nThe following are metrics that can be used in measuring classification models, and they are:\n\nAccuracy.\n\nClassification Error Rate.\n\nTrue Positive Rate (TPR).\n\nFalse Positive Rate (FPR).\n\nFalse Negative Rate (FNR).\n\nTrue Negative Rate (TNR).\n\nPositive Predictive Value (PPV).\n\nNegative Predictive Value (NPV).\n\nFalse Discovery Rate (FDR).\n\nFalse Omission Rate (FOR).\n\nMisclassification Rate.\n\nF1 Score.\n\nKappa Cohen.\n\nMatthew Correlation Coefficient (MCC).\n\n+LR (Positive Likelihood Ratio).\n\n-LR (Negative Likelihood Ratio).\n\nJ Youden.\n\nG Measure.\n\nD2H (Distance to Heaven).\n\nAUC=Area under the Receiver Operating Characteristic (ROC) curve.\n\nThis metric can only be applied to the classification process. It cannot possibly be used for other supervised methods, such as regression, and is even less applied to clustering processes as unsupervised models.\n\nThis means that data assets are formally included in the scope of financial accounting, and the attributes and values of assets are accounted for and reflected by the way of enterprise financial accounting, which is of decisive significance for establishing data elements as an important component of enterprise assets, especially intangible assets.\n\nHowever, due to the special properties of data assets, there are still some difficulties in the process of financial entry.\n\nFirst of all, we interpret from the perspective of financial accounting: the three major characteristics of assets are: 1. Assets should be owned or controlled by enterprises; 2. The assets are expected to bring economic benefits to the enterprise; 3. An asset is a resource formed by a past transaction or event.\n\nSimilarly, data assets must also conform to the above three characteristics, so data ownership is to prove that the asset belongs to the resources owned or controlled by the enterprise. Secondly, the economic benefits that data assets can bring to the enterprise must be measured and accurately calculated.\n\nProblems in accounting recognition of data assets\n\nThe rights and responsibilities of data assets are uncertain\n\nHow to prove that the data asset belongs to the resources owned or controlled by the enterprise, it is necessary to carry out data ownership. Perhaps the data we use internally can identify the so-called data owner. However, many data are related to data ethics and are not well defined as resources controlled by enterprises, such as owner membership information, business operation information, and so on. Does the data belonging to this part really belong to the enterprise? Is it personal privacy, or is it public data agreed by the government, which needs to be provided free of charge and so on.\n\nThe revalidation of data assets is unclear\n\nIf we regard data assets as a special category of intangible assets, the capitalization and cost problems existing in the subsequent recognition of intangible assets will also be faced with data assets. There is a difference between capitalized expenditure and expensed expenditure. In the production and operation activities of an enterprise, the consumption of assets is tracked and further refined into capitalized and expensed expenditure. The standard of division is to consider the place of consumption, if at the cost of this part of consumption in exchange for new assets, is capitalized expenditure, if this part of consumption is used for business operation, then this economic benefit outflow, called expensed expenditure. However, the data in a data asset has special properties, and it is difficult to define which data generates value and how much value. So it's harder to define capitalized or expensed expenditures.\n\nThe conditions for the confirmation of data assets are not uniform\n\nAs we all know, in the existing balance sheet, the conditions for the recognition of fixed assets and intangible assets are clearly defined (for example, when fixed assets are not put into use, they are generally regarded as projects in progress). With these recognition principles, it is possible to better distinguish which intangible assets are, so as to carry out the next work. However, the current research has not made clear the relevant recognition principles of data assets, and the relevant theories have not been perfected. Data quality determines whether to include assets, and similarly, changes, derivatives, destruction, and other actions during the use of data also affect the confirmation of data assets. Then, data collection, storage, processing, cleaning and other stages need to calculate the corresponding accounting value, and clarify the recognition principle of each stage.\n\nProblems in accounting measurement of data assets\n\nInitial measurement of data assets\n\nThe complete selection of traditional units of measurement has been unable to meet the demand from the statistical data, in the various standards that can be used for accounting measurement, there are often multiple units of measurement, and most of the standards have their own units of measurement. Therefore, different measurement objects have different requirements, and the specific measurement objects may have different measurement units because of the different purposes of measurement. In fact, the selection of accounting units of measurement is also based on the actual operation and management of enterprises. When the social and economic environment changes significantly, more valuable accounting units of measurement should be selected in time for specific economic businesses, so as to provide relevant accounting information that is more conducive to making management decisions. Therefore, for data assets, taking into account its particularity, it is not conducive to accurate accounting treatment by using conventional currency measurement.\n\nThe selection of measurement attributes of data assets is not clear. The measurement attributes of data assets are generally registered in the accounting accounts and reported in the financial statements of the enterprise to determine its actual amount. The measurement of data assets is not suitable to rely on replacement cost and present value attributes. In addition, historical cost refers to the actual cost and cash to be paid when completing a certain production activity or creating a certain wealth, which is the actual cost at the time of acquisition. Net realizable value is the net value of the expected selling price after deducting processing costs, etc. In the face of these measurement attributes, it is necessary to choose which data assets are suitable for.\n\nThe criteria for determining the initial confirmation amount of data assets are not clear. Since the initial recognition amount of an asset is the basis for its entry into the account, the recognition is an important basis for subsequent accounting treatment. Taking into account the particularity of data assets and referring to the initial recognition method of intangible assets, it is necessary to determine the initial recognition amount of data assets more accurately from the two aspects of outsourcing and self-made, so as to lay a good foundation for re-recognition and measurement. However, given the special nature of data assets, its particularity should also be considered in determining the initial confirmation amount. How to more accurately measure the initial confirmation amount of data assets from different sources is worth further research.\n\nSubsequent measurement of data assets\n\nIt is difficult to determine the service life. As a special intangible asset, the service life of data assets is like intangible assets, and the specific amount cannot be determined. The service life of the intangible assets legally acquired by both parties shall not exceed the actual period prescribed by the enterprise; If the law does not specify the service life of intangible assets in detail, the enterprise needs to consider the basis of judgment based on appropriate factors, and cannot adopt the above methods, it can be considered that the service life is uncertain. Therefore, to determine the service life of data assets, it is necessary to refer to the relevant treatment methods of intangible assets in accordance with the characteristics of data assets.\n\nThe amortization method is not clear. When choosing the amortization method, the enterprise should combine its own economic needs, determine the specific consumption mode from the maximum expected benefit, and apply it uniformly in different accounting periods. The amortization methods of intangible assets mainly include straight-line method and accelerated depreciation method. The straight-line method is to evenly distribute the amortization of intangible assets in each accounting period, while the accelerated depreciation method is a method of amortization in the initial use of intangible assets and less in the later use of intangible assets. Data assets need to select the most appropriate of these amortization methods to refine subsequent measurements.\n\nThe economic value of data assets is subject to fluctuations. The economic value of data assets is easily affected by many factors. Compared with other intangible assets, data assets are easy to fluctuate under the influence of application scenarios and environment. In order to better measure the changes in the value of intangible assets, China's accounting standards and related systems require enterprises to formulate corresponding supervision and management regulations to periodically review the book value of their financial statements. When the book value is higher than the recoverable amount, intangible assets impairment provisions should be made according to the difference; In fact, data assets should also refer to its principles, such as the purchase of land data from data suppliers, then after one or two years, this part of the data should actually be impaired, but the specific amount of impairment needs to be formulated.\n\nProposals for the development of accounting recognition of data assets\n\nClearly reconfirm data assets 1. Data assets obtained from external sources. In particular, it should be emphasized that for data assets obtained from the outside, if there is a transfer of ownership or partial ownership during the transaction process, the asset can be confirmed and included in the \"data assets\" column under the \"intangible assets\" account. If it does not involve the transfer of ownership, but at the same time as obtaining the right to use the data, it has certain rights such as agency, distribution, resale, etc., which can obtain income through the transaction of the data asset, the enterprise can include it in the asset column. If only the right to use data is obtained, such as having a use license, and the enterprise cannot obtain future income through external transactions, it does not involve the transfer of \"data assets\" and cannot cause relevant changes in the subject, and the enterprise can only include it in the cost or expense column. 2. Internally generated data assets According to the formation mode of internally generated data assets, the enterprise's own data assets can be divided into active research and development data assets and associated data assets generated with production and operation. In general, according to the relevant provisions on intangible assets, expenditures in the production and research stage should be included in the current profit or loss; Expenditures related to the development stage, which meet the capitalization conditions, shall be capitalized, and research and development expenditures that cannot be distinguished by object shall be fully included in the profit and loss of the current period. However, in the actual operation process, due to the influence of many unforeseen factors in R&D activities, it is not easy to specifically and clearly divide the two stages, which requires special consideration.\n\nClarify the conditions for the recognition of data assets. The definition of data assets should meet the clear principles. This paper holds that only when the four conditions of realizability, controllability, quantification and identification are met can they enter the statistical work of accounting financial statements and data resources can be regarded as data assets. The relevant validation conditions for data assets are:\n\nThe first is the realizability, data assets must be able to bring economic benefits to the enterprise;\n\nThe second is controllability. Data assets must be data resources that enterprises can own or control under the premise of compliance with laws and accounting standards.\n\nThe third is quantification, that is, data assets need to be able to be separated or divided from the actual production and operation of the enterprise, and can be reliably measured in currency;\n\nFourth, it is identifiable, that is, data assets can be separated or derived from contractual rights.\n\nProposals for the development of accounting measurement of data assets\n\nInitial measurement of data assets\n\nThe unit of measurement is a combination of monetary and non-monetary. Data assets can not be fully measured in money, for example, by using their own data assets to make valuable analysis for the development direction of the enterprise, project decisions, etc., can not be fully reflected by monetary measurement. Therefore, I personally believe that the measurement of data assets can take the form of a combination of monetary and non-monetary, and adopt a suitable form, so as to realize the comprehensive value of data assets statistics.\n\nThe measurement attribute is mainly fair value. The value of data assets is constantly fluctuating, and the actual value of data assets is constantly changing under the influence of the market, so the historical cost method can not measure its actual value in time. Similarly, considering that the processing and acquisition of data assets is a continuous long-term process, the net realisable value method is not applicable. This paper argues that data assets are more suitable for fair value measurement, because compared with other measurement attributes, fair value can better reflect the change of market value.\n\nDetermine the initial confirmation amount according to different data acquisition methods. Data assets acquired from external sources usually have a certain degree of activity in the open market, at which time their fair value can be used as the initial recognition amount; If there is no active market, the consideration paid by the data asset will be received as the initial recognition amount. Internally generated data assets will inevitably cause certain loss of human, financial, material and other important resources. Therefore, in the initial stage of cost recognition of data assets, various costs should be taken into account and weighted average calculation of each cost.\n\nSubsequent measurement of data assets\n\nAccording to the service life of intangible assets, the approximate age is determined. Considering the particularity of data assets, the determination of their service life is related to whether they can be fully utilized. Since the timeliness of the data has a great impact on the value of the data itself, the definition of its service life should not be too long. Considering the characteristics of data assets, we should make a comprehensive analysis of their specific service life, so as to make a more scientific forecast for the future and realize the maximum economic value of data assets. According to the accounting standards of intangible assets, this paper puts forward the following principles: First, the service life of the data asset should be until the data asset loses its use value; Second, the data assets under contract shall not exceed the period specified in the contract; Third, for data assets without a contract, the period of time that the data assets can bring economic benefits should be determined comprehensively with reference to various influencing factors; Fourth, data assets that do not meet the above principles can be identified as data assets with uncertain service life.\n\nAmortization by accelerated depreciation method. For the subsequent amortization of data assets, it is suggested that accelerated depreciation method is more appropriate. On the one hand, the value of data assets is greatly affected by timeliness, and with the passing of time, the timeliness of data assets will also decrease year by year. Data assets have great utilization value, and the initial data resources are advanced and timely, and the data assets often excavated have high value. In the later stage, with the passage of time and the generation of new data resources, coupled with the constant changes of the market economy, the potential economic value of the original data resources will also be decreasing. On the other hand, in the process of the continuous development of the information society, the replacement of data assets will be more rapid, and its life cycle will be shorter. When the accelerated amortization method is adopted, the amortization of data assets is larger in the early stage and smaller in the later stage, which meets the realistic demand of data assets' own characteristics.\n\nProvision for impairment of data assets. Although data assets have the characteristics of reusable, its value will decrease with the change of information timeliness, the progress of technical means and the change of internal and external market environment, that is to say, there is a phenomenon of impairment of data assets. Although the value of data assets is impaired with external changes, it does not mean that the data assets have no economic value, but the relative value declines, so it is necessary to do a good job in the measurement of data assets in a timely manner. For internal data assets that cannot be quoted publicly, when the recoverable amount is compared with the carrying value, a provision for data asset impairment is made according to the difference between the two accounting treatment methods for intangible assets.\n\nSummary\n\nAlthough the digital economy has been in full swing and the economic value of data has become increasingly apparent, the accounting treatment of data assets is still a relatively new research topic. We need to truly master the systematic theory of data asset accounting in order to successfully enter data assets into the table.\n\nAccording to the findings of many researchers [1-3], for the design of RC structures taking into account the requirements of ensuring their strength and durability, the principle of safety can be realized to the maximum level only if the following fundamental issues are further developed:\n\nRepresentation of loads and materials strength in the form of random variables and processes;\n\nProbabilistic nature of reliability coefficients;\n\nFailure probability as a multidimensional integral over the failure area;\n\nSafety characteristic (reliability index) β;\n\nCharacteristics and functions of random variables;\n\nMethods for assessing the reliability of structures;\n\nOptimal and normative level of reliability;\n\nProbabilistic design optimization;\n\nRisk identification, etc.\n\nOne of the most important tasks in calculating the reliability of RC structures is the selection and justification of probabilistic models of random variables. This task in practice is significantly complicated by the data uncertainty, obtained as a result of a lack of statistical information. Uncertainty, in this case, represents material properties (for example, strength of reinforcement or concrete), external loads, geometric dimensions, operating conditions, etc. [4, 5]. These uncertainties, if ignored, can lead to low reliability of engineering structures and even catastrophic consequences (especially relevant for buildings of the CC3 consequence class). To solve this problem, the reliability theory of building structures was developed.\n\nAll reliability indicators that can be used in formulating normative requirements for structures are functions of the failure probability over a certain time. Therefore, the main task of probabilistic calculations is to calculate the failure probability.\n\nWhen random changes in input parameters (variables) are insignificant (up to 20 %), we can use the statistical linearization method. In this case, to calculate the function’s statistical characteristics, it is linearized by expanding into the Taylor series at the point of its mathematical expectation. However, no random variables distribution function is strictly linear and since the error permissible value in the nonlinear systems failures calculation depends not only on the number of input random variables but on the volumes of their statistical samples - the search for an analytical solution using this method will be almost impossible.\n\nOn the contrary, the method of statistical simulation (so-called Monte Carlo methods) - the universal method for calculating a wide class of probabilistic problems - is especially effective for nonlinear systems. The main idea of these methods consists of the sample (based on the statistical distribution) construction for each random variable involved in the task - and as these methods deal with the simulation of the limit state function - the larger the sample is taken, the more accurate the structure's failure probability will be [6].\n\nAs a result, the choice of random variable model probabilities for further calculation of the reliability of structure members will depend on the amount and type of statistical information obtained about the random variable. It can be added that the promising and relevant directions in the development of probabilistic models of random variables and methods for analyzing the reliability of RC structures (especially with incomplete statistical information) are the following:\n\nUse of numerical modeling methods (Monte Carlo methods);\n\nProgress of approaches to the modeling of members work based on FEM (including BIM technology);\n\nDurability monitoring (including based on IoT concepts).\n\nReferences\n\nGeiker MR, MAN Hendriks and Elsener B. “Durability-based design: the European perspective”. Sustainable and Resilient Infrastructure 8.2 (2023): 169-184.\n\nKhmil RY., et al. “Improvement of the method of probability evaluation of the failure-free operation of reinforced concrete beams strengthened under load”. IOP Conference Series: Materials Science and Engineering 1021.1 (2021): 012014.\n\nZignago D and M Barbato. “Reliability-Based Calibration of New Design Procedure for Reinforced Concrete Columns under Simultaneous Confinement by Fiber-Reinforced Polymers and Steel”. Journal of Composites for Construction 26.3 (2022): 04022017.\n\nKhmil R., et al. “Development of the procedure for the estimation of reliability of reinforced concrete beams, strengthened by building up the stretched reinforcing bars under load”. Eastern-European Journal of Enterprise Technologies 5.7.95 (2018): 32-42.\n\nTytarenko R., et al. “Probabilistic durability assessment of RC structures in operation: an analytical review of existing methods”. Lecture Notes in Civil Engineering 290 (2023): 408-415.\n\nNogueira CG, ED Leonel and Coda HB. “Reliability algorithms applied to reinforced concrete structures durability assessment”. IBRACON Structures and Materials Journal 5.4 (2012): 440-450.\n\nOn a continent that is frequently portrayed in a condition of permanent crisis, development appears to be an impossibility. In fact, observers of African affairs, especially those in the West, cannot feel reassured by recent military takeovers and armed conflicts like those in Sudan and Gabon that Africa is rising—a claim once made by influential figures in world opinion like The New York Times, The Economist, and others. It appears that development is in critical need of an immediate revival. To put things in perspective, the Organization for Economic Co-operation and Development (OECD) reports that official development assistance (ODA) reached a total of USD 185.9 billion in 2021. However, the depressing results of development demonstrate the futility of international development. For instance, since 2019, the majority of the nations receiving aid from abroad have seen increases in their rates of poverty, with 50% to 70% of their people living below the poverty line (1,2).\n\nSituations around the world are not promising. The World Bank estimates that in 2022, there will be over 700 million individuals worldwide who are living in extreme poverty. The UN's most recent SDG 2023 progress report (1.3) presents a dismal picture. On almost 50% of the targets, there has been insufficient and weak development. Even worse, almost 30% of the SDG targets have seen either a standstill in development or a reversal in them. This contains important goals about hunger, poverty, and the environment. Moreover, the research finishes on a very concerning note: over half of the world is falling behind, and most of those falling behind reside, you guessed it, in the Global South.\n\nArtificial intelligence (AI) is being positioned as a useful tool for accelerating development objectives and targets and repairing the flawed international development paradigm as the global development agenda suffers. International development organizations and regional partners have implemented innovative AI for development (AI4D) initiatives in a number of African nations, including those in Sub-Saharan Africa and West Africa. With all of the hype around artificial intelligence, this seems like a reasonable and necessary endeavor. However, the deficit model of development serves as the foundation for AI initiatives in Africa. This deficit argument highlights how the lack of human and technological capability is the direct cause of the Majority World's inability to progress.\n\nIn an effort to maximize the amount of electricity available, the Responsible AI Lab (RAIL) in Ghana (1.4) is attempting to integrate efficient energy distribution models into the system. Natural language processing (NLP) is arguably one of the most promising uses of AI in the region. Emerging start-ups using development funding programs like the Lacuna Fund are attempting to create language models for indigenous African languages like Igbo, Hausa, Yoruba, Twi, Akan, and others. These models can be integrated into further applications in fields like education and healthcare. Given the regional circumstances in the majority of African nations, the advantages of these programs and apps may be obvious.\n\nActually, though, large multinational corporations' CSR programs (4) and the policies of international development organizations have a significant influence on most AI development in Africa. In an effort to become future bright spots in the field of technology, these initiatives which are carried out in partnership with Big Tech and regional players like scientists and practitioners are unduly focused on developing technological solutions and local African datasets. Much time and money are being spent collecting local datasets so that machine learning models for predictive analysis can be updated based on the local context.\n\nBut how much is known about the goals and applications of these AI programs, and which social groups and communities stand to benefit from them? How will the local context respond to these technology solutions? To put it bluntly, there isn't enough deliberate interaction with the political imaginations of the various local communities in terms of their aspirations for an AI-powered technological future1."
    }
}
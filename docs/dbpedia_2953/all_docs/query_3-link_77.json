{
    "id": "dbpedia_2953_3",
    "rank": 77,
    "data": {
        "url": "https://arxiv.org/html/2401.12926v1",
        "read_more_link": "",
        "language": "en",
        "title": "Aware Dataset Selection with Datamodels",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.\n\nfailed: kotex\n\nfailed: xstring\n\nfailed: contour\n\nAuthors: achieve the best HTML results from your LaTeX submissions by following these best practices.\n\nLicense: CC BY 4.0\n\narXiv:2401.12926v1 [cs.LG] 23 Jan 2024\n\n\\contourlength\n\n0.8pt\n\nDsDm: Model-Aware Dataset Selection with Datamodels\n\nLogan Engstrom\n\nengstrom@mit.edu\n\nMIT Axel Feldmann\n\naxelf@mit.edu\n\nMIT Aleksander Mądry\n\nmadry@mit.edu\n\nMIT\n\nAbstract\n\nWhen selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with “high quality” data sources may not increase (and can even hurt) performance compared to randomly selecting data.\n\nTo develop better methods for selecting data, we start by framing dataset selection as an optimization problem that we can directly solve for: given target tasks, a learning algorithm, and candidate data, select the subset that maximizes model performance. This framework thus avoids handpicked notions of data quality, and instead models explicitly how the learning process uses train datapoints to predict on the target tasks. Our resulting method greatly improves language model (LM) performance on both pre-specified tasks and previously unseen tasks. Specifically, choosing target tasks representative of standard LM problems and evaluating on diverse held-out benchmarks, our selected datasets provide a 2× compute multiplier over baseline methods.\n\n1 Introduction\n\nSuppose we want to train a large-scale machine learning model. What data should we train on? The simple answer is: as much data as possible. For example, we train language and vision models on vast quantities of text [Radford et al., 2019] and image-caption [Ramesh et al., 2021] data from sources like internet crawls. This seemingly straightforward recipe yields models that generalize remarkably well to a broad range of tasks.\n\nA closer look, however, reveals that choosing training data is not actually so straightforward. Indeed, not all data is equally useful; for example, internet data sources frequently contain “low quality” data like spam, poor writing, or nonsense text. Therefore, in practice, we tend to filter training data according to intuitive notions of quality, e.g., choosing documents similar to a “high quality” data source like Wikipedia or discarding documents with fewer than five sentences. These steps choose (qualitatively) “clean” samples that should intuitively improve performance. However, do such samples improve performance in practice too?\n\nContributions. We find that the opposite can happen: selecting data according to similarity with “high quality” data sources may not improve (and, in fact, can even hurt) model performance. Specifically, we train language models with standard, similarity-based selection methods previously used to select data for models like PaLM and GPT-3 [Brown et al., 2020, Xie et al., 2023b], and find these methods do not outperform (and can even underperform) selecting data at random (cf. Section 4).\n\nTo develop better methods for selecting training data, we start from first principles. That is, we avoid intuitive notions of data quality, and instead frame dataset selection as an optimization problem where the goal is to—given target tasks, a learning algorithm, and a candidate data pool—select the data that maximizes model performance. However, actually finding the optimal solution to this problem is difficult. While we can calculate the performance of a specific training set by training a model on that set (and then evaluating), it is (generally) unclear how to calculate the best possible training subset without examining every possible subset one by one, a computationally infeasible procedure.\n\nWe instead approximate the optimal subset by (approximately) modeling how the learning algorithm actually uses training data to predict. Specifically, in Section 2, we model target task performance as a function of training subset using datamodels (which efficiently approximate the mapping between training subset and model performance [Ilyas et al., 2022]), and select the subset that maximizes our estimate. Then, in Section 3, we demonstrate that our resulting method, dataset selection with datamodels (DsDm), consistently improves language model performance on diverse target tasks (e.g., SQuAD [Rajpurkar et al., 2016] and LAMBADA [Paperno et al., 2016]), even when existing selection methods do not.\n\nDsDm-selected data can improve performance on pre-specified tasks. However, in practice we train large-scale models to generalize to yet unseen tasks. Our framework suggests a principled approach to selecting data in this scenario too: choose target tasks similar to those we expect at deployment time, then select the optimal dataset subset for these target tasks. Following this strategy, in Section 4, we choose target tasks that cover a range of natural language problem categories (SQuAD, Jeopardy [MosaicML, 2023], and LAMBADA), and select data from C4, a canonical web crawl [Raffel et al., 2020]. Our selections deliver a 2× compute multiplier on a diverse set of test benchmarks: DsDm-selected datasets yield LMs that perform as well as those trained with 2× the compute budget on randomly selected data (we train up to 1.8B parameter models). In contrast, no baseline method outperforms randomly selecting data—even at the same compute budget.\n\n2 Estimating the optimal dataset selection with DsDm\n\nTo select better data for training large-scale models, we start by defining the optimal dataset selection as an optimization problem. We then select data by finding a train subset that is approximately the best solution to that problem. Specifically, we use datamodels [Ilyas et al., 2022] to approximate how the learning algorithm uses data to predict on the tasks of interest. We describe the resulting framework in more detail below.\n\n2.1 Task-optimal dataset selection\n\nWe frame dataset selection as an optimization problem where the goal is to minimize trained model loss on a set of target tasks with respect to training data choice. Given a learning algorithm 𝒜𝒜\\mathcal{A}caligraphic_A (e.g., SGD on a neural network) that maps train set to trained model, and a target distribution 𝒟targsubscript𝒟targ\\mathcal{D}_{\\mathrm{targ}}{}caligraphic_D start_POSTSUBSCRIPT roman_targ end_POSTSUBSCRIPT (e.g., a language modeling task), the size-k𝑘kitalic_k task-optimal dataset selection over the set 𝒮𝒮\\mathcal{S}caligraphic_S of available data (e.g., documents from an internet scrape) is the subset\n\nS*superscript𝑆\\displaystyle S^{*}italic_S start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT :=arg⁢minS⊂𝒮,|S|=k⁡ℒ𝒟targ⁢(S),assignabsentsubscriptargminformulae-sequence𝑆𝒮𝑆𝑘subscriptℒsubscript𝒟targ𝑆\\displaystyle\\vcentcolon={}\\operatorname*{arg\\,min}\\limits_{S\\subset\\mathcal{S% },|S|=k}\\mathcal{L}_{\\mathcal{D}_{\\mathrm{targ}}{}}(S),:= start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_S ⊂ caligraphic_S , | italic_S | = italic_k end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT roman_targ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_S ) , (1) where ⁢ℒ𝒟⁢(S)where subscriptℒ𝒟𝑆\\displaystyle\\mbox{where }\\mathcal{L}_{\\mathcal{D}}(S)where caligraphic_L start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ( italic_S ) :=𝔼x∼𝒟⁢[ℓ⁢(x;𝒜⁢(S))],assignabsentsubscript𝔼similar-to𝑥𝒟delimited-[]ℓ𝑥𝒜𝑆\\displaystyle\\vcentcolon={}\\mathbb{E}_{x\\sim\\mathcal{D}}\\left[\\ell(x;\\mathcal{% A}(S))\\right],:= blackboard_E start_POSTSUBSCRIPT italic_x ∼ caligraphic_D end_POSTSUBSCRIPT [ roman_ℓ ( italic_x ; caligraphic_A ( italic_S ) ) ] ,\n\nthat minimizes the trained model population loss ℒ𝒟targ⁢(S)subscriptℒsubscript𝒟targ𝑆\\mathcal{L}_{\\mathcal{D}_{\\mathrm{targ}}{}}(S)caligraphic_L start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT roman_targ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_S ), where ℓ⁢(x;g)ℓ𝑥𝑔\\ell(x;g)roman_ℓ ( italic_x ; italic_g ) denotes the loss (e.g., cross-entropy loss) for model g𝑔gitalic_g on example x𝑥xitalic_x. Note the expectation in the population loss is over both target dataset and learning algorithm randomness (as, e.g., SGD is a non-deterministic algorithm).\n\nIn our setting, minimizing (1) is difficult. Indeed, we do not have an easy-to-optimize, closed-form expression for trained model loss in terms of training set choice S𝑆Sitalic_S for large-scale model learning algorithms. While we can directly calculate the trained model loss for a given S𝑆Sitalic_S by actually training on S𝑆Sitalic_S with 𝒜𝒜\\mathcal{A}caligraphic_A (and then evaluating loss), using this method to find the best subset is generally computationally infeasible: we would need to train (and evaluate) a model for each of the (|𝒮|k)binomial𝒮𝑘{|\\mathcal{S}|\\choose k}( binomial start_ARG | caligraphic_S | end_ARG start_ARG italic_k end_ARG ) possible size-k𝑘kitalic_k train subsets.\n\n2.2 Estimating model loss efficiently with datamodels\n\nTo circumvent this computational challenge, we trade optimality for feasibility, and instead estimate the best train subset. Specifically, we approximate the trained model loss in place of calculating it directly, then select the subset that minimizes our approximation.\n\nThe core primitive we use to approximate the trained model loss is datamodeling [Ilyas et al., 2022], a framework originally designed to predict how choice of training set changes model predictions. More precisely, a datamodel for a fixed sample x𝑥xitalic_x approximates the mapping from train subset choice S𝑆Sitalic_S (out of the available dataset 𝒮𝒮\\mathcal{S}caligraphic_S) to resulting trained model loss on a sample x𝑥xitalic_x, i.e., the function:\n\nℒx⁢(S):=𝔼⁢[ℓ⁢(x;𝒜⁢(S))].assignsubscriptℒ𝑥𝑆𝔼delimited-[]ℓ𝑥𝒜𝑆\\mathcal{L}_{x}(S)\\vcentcolon=\\mathbb{E}\\left[\\ell(x;\\mathcal{A}(S))\\right].caligraphic_L start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ( italic_S ) := blackboard_E [ roman_ℓ ( italic_x ; caligraphic_A ( italic_S ) ) ] .\n\nPrevious work used datamodels primarily for reliability purposes, e.g., to detect data poisoning [Khaddaj et al., 2022] or train-test leakage [Ilyas et al., 2022]. In contrast, we leverage datamodels to cheaply approximate the trained model loss ℒxsubscriptℒ𝑥\\mathcal{L}_{x}caligraphic_L start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT. Formally, given a candidate data subset S⊂𝒮𝑆𝒮S\\subset\\mathcal{S}italic_S ⊂ caligraphic_S, datamodels take as input the corresponding characteristic vector\n\n𝟙S∈{0,1}|𝒮| such that(𝟙S)i={1if ⁢𝒮i∈S0otherwise,formulae-sequencesubscript𝟙𝑆superscript01𝒮 such thatsubscriptsubscript𝟙𝑆𝑖cases1if subscript𝒮𝑖𝑆0otherwise\\text{1}_{S}\\in\\{0,1\\}^{|\\mathcal{S}|}\\hskip 10.0pt\\hbox{ such that}\\hskip 10.% 0pt\\left(\\text{1}_{S}\\right)_{i}=\\begin{cases}1&\\text{if }\\mathcal{S}_{i}\\in S% \\\\ 0&\\text{otherwise}\\end{cases},𝟙 start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ∈ { 0 , 1 } start_POSTSUPERSCRIPT | caligraphic_S | end_POSTSUPERSCRIPT such that ( 𝟙 start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { start_ROW start_CELL 1 end_CELL start_CELL if caligraphic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_S end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL otherwise end_CELL end_ROW , (2)\n\ninstead of the subset S𝑆Sitalic_S directly. Then, the datamodel τθxsubscript𝜏subscript𝜃𝑥\\tau_{\\theta_{x}}italic_τ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUBSCRIPT for x𝑥xitalic_x is the parameterized function that optimally predicts ℒxsubscriptℒ𝑥\\mathcal{L}_{x}caligraphic_L start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT over a (chosen) distribution of train subsets 𝒟𝒮subscript𝒟𝒮\\mathcal{D}_{\\mathcal{S}}caligraphic_D start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT, i.e.,\n\nτθx:{0,1}|𝒮|→ℝ, where θx=arg⁡minθ⁡𝔼^Si∼𝒟𝒮(m)⁢[Lreg⁢(τθ⁢(𝟙Si),ℒx⁢(Si))],:subscript𝜏subscript𝜃𝑥formulae-sequence→superscript01𝒮ℝ where subscript𝜃𝑥subscript𝜃subscriptsuperscript^𝔼𝑚similar-tosubscript𝑆𝑖subscript𝒟𝒮delimited-[]subscript𝐿regsubscript𝜏𝜃subscript𝟙subscript𝑆𝑖subscriptℒ𝑥subscript𝑆𝑖\\displaystyle\\tau_{\\theta_{x}}:\\{0,1\\}^{|\\mathcal{S}|}\\to\\mathbb{R},\\qquad% \\text{ where }\\qquad\\theta_{x}=\\arg\\min_{\\theta}\\ \\widehat{\\mathbb{E}}^{(m)}_{% S_{i}\\sim\\mathcal{D}_{\\mathcal{S}}}\\left[L_{{\\text{reg}}}\\left(\\tau_{\\theta}(% \\text{1}_{S_{i}}),\\ \\mathcal{L}_{x}(S_{i})\\right)\\right],italic_τ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUBSCRIPT : { 0 , 1 } start_POSTSUPERSCRIPT | caligraphic_S | end_POSTSUPERSCRIPT → blackboard_R , where italic_θ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = roman_arg roman_min start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT over^ start_ARG blackboard_E end_ARG start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∼ caligraphic_D start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT ( italic_τ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( 𝟙 start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) , caligraphic_L start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) ] , (3)\n\nwhere Lreg⁢(⋅,⋅)subscript𝐿reg⋅⋅L_{{\\text{reg}}}(\\cdot,\\cdot)italic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT ( ⋅ , ⋅ ) is a regression loss function (e.g., mean squared error), and 𝔼^(m)superscript^𝔼𝑚\\widehat{\\mathbb{E}}^{(m)}over^ start_ARG blackboard_E end_ARG start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT is an m𝑚mitalic_m-sample empirical expectation. Note that in practice, we estimate the datamodel parameters that minimize (3) (i.e., we estimate the parameters of the function we use to approximate model loss).\n\nLinear datamodels.\n\nSo far we have only defined the datamodeling framework; we have not actually defined the parameterized function τθsubscript𝜏𝜃\\tau_{\\theta}italic_τ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT or described how to estimate the parameters θ𝜃\\thetaitalic_θ. In this work, we instantiate datamodels as a linear function of the characteristic vector 𝟙Ssubscript𝟙𝑆\\text{1}_{S}𝟙 start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT (a standard choice [Ilyas et al., 2022, Saunshi et al., 2023]), such that\n\nτθx⁢(𝟙S):=θx⊤⁢𝟙S.assignsubscript𝜏subscript𝜃𝑥subscript𝟙𝑆superscriptsubscript𝜃𝑥topsubscript𝟙𝑆\\tau_{\\theta_{x}}(\\text{1}_{S})\\vcentcolon=\\theta_{x}^{\\top}\\text{1}_{S}.italic_τ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( 𝟙 start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ) := italic_θ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT 𝟙 start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT .\n\nNote that, being a linear model, τθxsubscript𝜏subscript𝜃𝑥\\tau_{\\theta_{x}}italic_τ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUBSCRIPT treats the inclusion of an example 𝒮isubscript𝒮𝑖\\mathcal{S}_{i}caligraphic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in the train set as having a fixed effect on ℒx⁢(S)subscriptℒ𝑥𝑆\\mathcal{L}_{x}(S)caligraphic_L start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ( italic_S ) irrespective of the other examples in S𝑆Sitalic_S (this fixed effect is exactly the value of index i𝑖iitalic_i of θxsubscript𝜃𝑥\\theta_{x}italic_θ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT).\n\nIn this work, to estimate linear datamodel parameters θxsubscript𝜃𝑥\\theta_{x}italic_θ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT we largely follow the procedures of previous work [Park et al., 2023, Ilyas et al., 2022]—in particular, we use the TRAK estimator—but make changes needed for the language modeling domain (see Appendix B for full details).\n\n2.3 DsDm: Dataset Selection with Datamodels\n\nRecall that our goal is to estimate the candidate data subset that minimizes trained model loss on the target task (cf. (1)). To do so, we approximate the mapping between training subset S𝑆Sitalic_S and target distribution loss (i.e., ⁢ℒ𝒟targ⁢(S))i.e., subscriptℒsubscript𝒟targ𝑆\\left(\\textrm{i.e., }\\mathcal{L}_{\\mathcal{D}_{\\mathrm{targ}}}(S)\\right)( i.e., caligraphic_L start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT roman_targ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_S ) ) with datamodels as a primitive, then select the candidate data subset that minimizes our approximation of the target loss.\n\nSpecifically, given a train subset S𝑆Sitalic_S, we estimate the corresponding target distribution loss with an n𝑛nitalic_n-sample empirical expectation of datamodel loss estimates over 𝒟targsubscript𝒟targ\\mathcal{D}_{\\mathrm{targ}}{}caligraphic_D start_POSTSUBSCRIPT roman_targ end_POSTSUBSCRIPT samples:\n\nℒ^𝒟targ⁢(S)=𝔼^xi∼𝒟targ(n)⁢[τθxi⁢(𝟙S)]=1n⁢∑i=1nθxi⊤⁢𝟙S=𝟙S⊤⁢(1n⁢∑i=1nθxi).subscript^ℒsubscript𝒟targ𝑆subscriptsuperscript^𝔼𝑛similar-tosubscript𝑥𝑖subscript𝒟targdelimited-[]subscript𝜏subscript𝜃subscript𝑥𝑖subscript𝟙𝑆1𝑛superscriptsubscript𝑖1𝑛superscriptsubscript𝜃subscript𝑥𝑖topsubscript𝟙𝑆superscriptsubscript𝟙𝑆top1𝑛superscriptsubscript𝑖1𝑛subscript𝜃subscript𝑥𝑖\\widehat{\\mathcal{L}}_{\\mathcal{D}_{\\mathrm{targ}}{}}{}(S)=\\widehat{\\mathbb{E}% {}}^{(n)}_{x_{i}\\sim\\mathcal{D}_{\\mathrm{targ}}{}}\\left[\\tau_{\\theta_{x_{i}}}(% \\text{1}_{S})\\right]=\\frac{1}{n}\\sum_{i=1}^{n}\\theta_{x_{i}}^{\\top}\\text{1}_{S% }=\\text{1}_{S}^{\\top}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\theta_{x_{i}}\\right).over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT roman_targ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_S ) = over^ start_ARG blackboard_E end_ARG start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∼ caligraphic_D start_POSTSUBSCRIPT roman_targ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_τ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( 𝟙 start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ) ] = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_θ start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT 𝟙 start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT = 𝟙 start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_θ start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) .\n\nThen, our size-k𝑘kitalic_k dataset selection with datamodels (DsDm) estimate of the optimal dataset selection is the subset that minimizes the approximated target loss ℒ^𝒟targ⁢(S)subscript^ℒsubscript𝒟targ𝑆\\widehat{\\mathcal{L}}_{\\mathcal{D}_{\\mathrm{targ}}{}}{}(S)over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT roman_targ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_S ) with respect to training set choice:\n\nS^DM:=arg⁢minS⊂𝒮,|S|=k⁡ℒ^𝒟targ⁢(S)=arg⁢minS⊂𝒮,|S|=k⁡𝟙S⊤⁢(1n⁢∑i=1nθxi)=arg⁢bot⁢ -⁢k(1n⁢∑i=1nθxi).assignsubscript^𝑆DMsubscriptargminformulae-sequence𝑆𝒮𝑆𝑘subscript^ℒsubscript𝒟targ𝑆subscriptargminformulae-sequence𝑆𝒮𝑆𝑘superscriptsubscript𝟙𝑆top1𝑛superscriptsubscript𝑖1𝑛subscript𝜃subscript𝑥𝑖argbot -𝑘1𝑛superscriptsubscript𝑖1𝑛subscript𝜃subscript𝑥𝑖\\widehat{S}_{\\mathrm{DM}}\\vcentcolon=\\operatorname*{arg\\,min}_{S\\subset% \\mathcal{S},|S|=k}\\widehat{\\mathcal{L}}_{\\mathcal{D}_{\\mathrm{targ}}{}}{}(S)=% \\operatorname*{arg\\,min}_{S\\subset\\mathcal{S},|S|=k}\\text{1}_{S}^{\\top}\\left(% \\frac{1}{n}\\sum_{i=1}^{n}\\theta_{x_{i}}\\right)=\\mathop{\\mathrm{arg\\,bot}\\mbox{% \\hskip 0.6pt-}k}{}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\theta_{x_{i}}\\right).over^ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_DM end_POSTSUBSCRIPT := start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_S ⊂ caligraphic_S , | italic_S | = italic_k end_POSTSUBSCRIPT over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT roman_targ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_S ) = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_S ⊂ caligraphic_S , | italic_S | = italic_k end_POSTSUBSCRIPT 𝟙 start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_θ start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) = start_BIGOP roman_arg roman_bot - italic_k end_BIGOP ( divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_θ start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) .\n\nIn our instantiation, the considered datamodels are linear, so DsDm selects the examples corresponding to the smallest k𝑘kitalic_k indices of 1n⁢∑i=1nθxi1𝑛superscriptsubscript𝑖1𝑛subscript𝜃subscript𝑥𝑖\\frac{1}{n}\\sum_{i=1}^{n}\\theta_{x_{i}}divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_θ start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT. (Note that linear datamodels are a design choice: DsDm can use any datamodel parameterization that can be optimized over.)\n\n3 Evaluating DsDm\n\nTo what extent does DsDm actually minimize trained model target task loss? In this section, we demonstrate that DsDm consistently reduces LM target task loss in practice. In contrast, baseline targeted dataset selection methods—all of which ignore the model training process and instead select data according to textual similarity with target task samples—often do not outperform randomly selecting training data. Below, we describe our experimental setup, then discuss results.\n\n3.1 Setup\n\nTo capture the effectiveness of a given data selection method, we measure the extent to which it reduces the optimal dataset selection objective of (1),\n\nℒ𝒟targ⁢(S):=𝔼x∼𝒟⁢[ℓ⁢(x;𝒜⁢(S))],assignsubscriptℒsubscript𝒟targ𝑆subscript𝔼similar-to𝑥𝒟delimited-[]ℓ𝑥𝒜𝑆\\mathcal{L}_{\\mathcal{D}_{\\mathrm{targ}}{}}(S)\\vcentcolon=\\mathbb{E}_{x\\sim% \\mathcal{D}}\\left[\\ell(x;\\mathcal{A}(S))\\right],caligraphic_L start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT roman_targ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_S ) := blackboard_E start_POSTSUBSCRIPT italic_x ∼ caligraphic_D end_POSTSUBSCRIPT [ roman_ℓ ( italic_x ; caligraphic_A ( italic_S ) ) ] ,\n\nacross varying target tasks. For each considered target task, we split samples into a target set and a separate test set, and only use the target set to select training subsets. We then train an LM on the resulting dataset, and inspect target task performance (using the test set). Below, we describe the experimental setup as well as the baselines we use (see Appendix C for more setup details).\n\nTarget tasks, candidate dataset, and model training.\n\nWe consider four separate LM target tasks: LAMBADA [Paperno et al., 2016], CS-Algorithms [Srivastava et al., 2022], SQuAD [Rajpurkar et al., 2016], and Jeopardy [Tunguz, 2019]; see Appendix C.1 for more details on each task. Our candidate dataset 𝒮𝒮\\mathcal{S}caligraphic_S is the English subset of the Colossal Cleaned Common Crawl (C4), a standard web scrape [Raffel et al., 2020]. On each selected train dataset, we train a 125M parameter GPT-2 style model on 6 billion tokens.\n\nBaselines.\n\nWe compare DsDm with two standard targeted dataset selection methods, both of which select according to textual similarity between candidate training samples and 𝒟targsubscript𝒟targ\\mathcal{D}_{\\mathrm{targ}}{}caligraphic_D start_POSTSUBSCRIPT roman_targ end_POSTSUBSCRIPT samples: Classifier (selects the top examples in 𝒮𝒮\\mathcal{S}caligraphic_S given by a logistic model trained to classify, on FastText features, between 𝒮𝒮\\mathcal{S}caligraphic_S and 𝒟targsubscript𝒟targ\\mathcal{D}_{\\mathrm{targ}}{}caligraphic_D start_POSTSUBSCRIPT roman_targ end_POSTSUBSCRIPT samples; used by GPT-3/PaLM/The Pile [Chowdhery et al., 2022, Gao et al., 2020]) and DSIR (Data Selection with Importance Resampling chooses train samples with n-grams that distributionally match those of 𝒟targsubscript𝒟targ\\mathcal{D}_{\\mathrm{targ}}{}caligraphic_D start_POSTSUBSCRIPT roman_targ end_POSTSUBSCRIPT [Xie et al., 2023b]). We also compare with randomly selecting data (Random).\n\n3.2 Results\n\nIn Figure 1 we display the mean log-probability (of the label given the context, across task samples; larger is better) achieved on each target task by training a model with each selection method (varying dataset selection size). Each model was trained on the same number of total tokens, with models trained on smaller fractions of C4 traversing more epochs. We find that DsDm most improves target task performance on all tasks. Models trained with DsDm even outperform a larger model trained with 10× the compute on randomly selected data. Additionally, DsDm performance decreases with larger selection fraction, indicating that the samples predicted by DsDm to most improve performance actually do so in practice. After all, smaller selections will contain more useful data (as predicted by DsDm) on average compared to larger selections (e.g., all methods select the same subset for selection fraction 1111).\n\nIn contrast, baselines that select according to textual similarity with the target task, Classifier and DSIR, do not consistently outperform randomly selecting data (e.g., on SQuAD and CS-Algorithms). These results suggest that similarity with the target task does not suffice to find useful train samples. Note that the only task that baselines match DsDm on, LAMBADA (a passage completion task), is also the only one without contextual instructions. We hypothesize that n-gram similarity may not capture how instructions define tasks.\n\nTo better understand how dataset choice relates to performance, we inspect the datapoints each method is most and least likely to select (for SQuAD: in Figure 2, for all other targets: in Appendix C.3). We find that:\n\nUseful data is not necessarily similar to the target task (or intuitively helpful at all).\n\nLooking at selected data for SQuAD in Figure 2, DSIR and Classifier select data that is more qualitatively similar to SQuAD samples (which are Wikipedia excerpts with questions, cf. Appendix Figure 5) than DsDm. Instead, DsDm samples often contain question answering-related text that does not match the SQuAD format; DsDm performance shows that qualitatively similar data is not necessarily the best data. However, helpful data is not always intuitively useful. Indeed, the DsDm examples for CS-Algorithms and Jeopardy (cf. Appendix Figures 21 and 15) often contain seemingly nonsense text. Yet, DsDm yields the best models for these tasks.\n\nDsDm discards “mislabeled” data.\n\nSamples that DSIR and Classifier are least likely to select are qualitatively different from those of DsDm. Inspecting Appendix Figure 11 for data selected for SQuAD: least likely samples for all methods are incoherent/malformed, but those of DsDm also often contain QA text. Despite this, such DsDm samples examples hurt model performance: training on them is worse than selecting randomly (cf. Appendix Figure 10). We liken these samples to “mislabeled” examples from supervised learning, and conjecture excluding such data could (in part) explain DsDm performance.\n\n4 Selecting data for broad model capabilities\n\nSo far, we have shown that DsDm consistently reduces loss on pre-specified target tasks. However, when we train large-scale models in practice our hope is that they will perform well on yet unseen tasks too. Our framework suggests a straightforward approach to improving this kind of performance: choose target tasks that match those we expect to see at model deployment time, then estimate the optimal dataset selection for these “proxy” target tasks.\n\nIn this section, we demonstrate that this approach to selecting data can greatly improve held-out task performance compared to baselines. Specifically, we consider three target tasks that cover a broad range of language modeling problem categories—LAMBADA (language understanding problems), SQuAD (reading comprehension problems), and Jeopardy (world knowledge problems)—and estimate the optimal training dataset selection for these tasks (all together) via DsDm. We then compare models trained on this data with models trained via existing dataset selection baselines. Overall, evaluating on a diverse set of held-out benchmarks (meant to model “yet unseen tasks”), we find that: (a) randomly selecting data is a surprisingly strong baseline—no baseline selection method outperforms selecting data at random—and (b) our approach yields models that match those trained with 2× the training compute on randomly selected data. In particular, models trained with our approach reliably improve performance on benchmarks that are qualitatively related to the target tasks. We describe our setup below, and defer additional details to Appendix D.\n\nModel training, scaling DsDm, selection baselines, and evaluation.\n\nWe train GPT-2 style LMs with varying compute budgets. To train the best possible model for a given compute budget, we use Chinchilla-optimal parameter-to-train-tokens ratios [Hoffmann et al., 2022] and train up to 1.8B parameter models. To select with DsDm, we use 125M proxy models: we calculate DsDm subsets for 125M models, then train on these selections at each compute budget (instead of computing DsDm separately for each model class). DsDm cost scales linearly with model size, so this procedure greatly reduces overhead (cf. Appendix B.5). For baselines, we compare with two methods that select via textual similarity with a specified “high quality” data source (DSIR and Classifier, the baselines of Section 3), a data deduplication method (SemDeDup [Abbas et al., 2023]), and selecting data randomly. We evaluate on 15 standard benchmarks (cf. Table 1).\n\nTarget tasks.\n\nWe execute each targeted dataset selection method using its originally proposed target task. For DsDm, we apply the framework described above: we select three target tasks that cover a broad range of LM problem categories—LAMBADA, SQuAD, and Jeopardy—then estimate the optimal dataset selection for these tasks together (i.e., 𝒟targsubscript𝒟targ\\mathcal{D}_{\\mathrm{targ}}{}caligraphic_D start_POSTSUBSCRIPT roman_targ end_POSTSUBSCRIPT as an equal mix of these tasks). For Classifier and DSIR, we target a replication of the “high quality” target distribution proposed by these methods (a mix of Wikipedia [Foundation, 2022], Books1 [Presser, 2021], and OpenWebText [Gokaslan et al., 2019], cf. Appendix D.1).\n\n4.1 Results\n\nIn Figure 3, we display the mean benchmark performance of models trained with each selection method, varying training compute budget. Randomly selecting data is a strong baseline: all baseline methods generally match or perform worse than random selection across training compute budgets (Figure 3 left). In the case of Classifier and DSIR, we hypothesize that data selected via similarity with a fixed source hurts model performance by trading off data diversity for (qualitatively) “cleaner” data.\n\nIn contrast, DsDm is a 2× compute multiplier: DsDm yields 1.3B models that match models trained with 2× the compute budget on randomly selected data (Figure 3, right). Furthermore, across compute budgets, DsDm consistently outperforms all selection baselines (Figure 3, left).\n\nGoing beyond aggregate performance, we find that DsDm greatly improves on benchmarks related to the target tasks, while simultaneously not reducing performance on unrelated categories (on average). More precisely, inspecting individual benchmark performance in Table 1, DsDm most improves reading comprehension and world knowledge benchmarks compared to selecting randomly. We hypothesize that our choice of target tasks leads to improved performance on these benchmarks (which are qualitatively similar to SQuAD, a reading comprehension task, and Jeopardy, a world knowledge task). Furthermore, in these categories DsDm consistently matches or outperforms training with 2× the compute budget on randomly selected data (i.e., the 1.8B model in Table 1). Crucially, DsDm improves on these categories while also not reducing performance in other categories. As a comparison, DSIR—which targets mostly formal text—performs well on language understanding tasks but poorly on other categories (e.g., world knowledge and symbolic problem solving).\n\nTarget tasks improve performance on qualitatively similar benchmarks.\n\nSo far, we have only targeted DsDm with a mix of LAMBADA, Jeopardy and SQuAD. How does target task choice change model behavior? We find that targeting a specific task generally improves performance on qualitatively related tasks. To demonstrate, in Figure 4 we display accuracy by benchmark category while varying target task across LAMBADA, Jeopardy, SQuAD, and all at once. Here, targeting a task generally improves accuracy on related tasks, e.g., SQuAD most improves reading comprehension, and Jeopardy most improves world knowledge. Furthermore, targeting all tasks at once improves overall accuracy the most. However, targeting can also decrease accuracy on unrelated tasks. For example, targeting LAMBADA, a language understanding task, reduces world knowledge accuracy compared to randomly selecting data. Our results suggest that we can tailor target tasks to improve deployment-time performance, but also that we need to be careful to choose targets that are diverse enough to capture a range of downstream problems.\n\nDsDm is necessary to improve performance (with the targeted tasks).\n\nDsDm selections yield much better models than Classifier and DSIR selections. However, we have not yet compared these selection methods head-to-head with the same target task. Classifier and DSIR target a mix of “high quality” sources, while DsDm targets three LM tasks (Jeopardy, SQuAD, and LAMBADA). To what extent does selecting with DsDm drive performance compared to the difference in target tasks? We demonstrate that selecting with DsDm is necessary to improve performance on the considered target tasks. Specifically, we train models on data selected with DSIR and Classifier targeting LAMBADA, Jeopardy and SQuAD, and find that (just as when targeting “high quality text”) neither outperforms randomly selecting data (cf. Appendix Figure 25).\n\n5 Discussion\n\nOstensibly, the sole goal of our dataset selection framework is improve model performance by better selecting training data. However, one can view our framework more broadly. That is, one can also use our framework to select data that boosts any chosen downstream property of our trained models—not just performance on a given benchmark. In this sense, our framework (and accompanying method) unlocks data curation as another stage of the model training pipeline that we can intervene on to control the downstream model behavior in a fine-grained manner. Below, we discuss in more detail the broader opportunities this view opens up as well as the other aspects of the framework, such as proxy modeling and computational efficiency.\n\nApplications and broader opportunities.\n\nDsDm can optimize for any specified downstream model behavior. Indeed, by an appropriate choice of the target tasks, we can use our framework to improve a wide range of model behaviors, including: “aligning” models at pretraining time (in addition to or in place of existing methods, which typically operate post model training [Bai et al., 2022, Ziegler et al., 2019, Taori et al., 2023]); optimizing for notions of fairness; and improving performance on specific domains of interest (such as low-resource languages or programming).\n\nTraining stronger models with weaker proxy models.\n\nWe select data for large models by using smaller models to proxy large model behavior (recall that we use DsDm to select data for smaller proxy models, then train large models on these selections). Despite that these proxy models are much worse than larger models on benchmarks (cf. Appendix Table 2), the corresponding selections nonetheless greatly improve performance. Furthermore, training on proxy models’ selections is the simplest possible approach to scaling. Therefore, we suspect that scaling to larger models less naïvely could yield even better results. More broadly, our findings are in line with previous work showing that smaller models can still be leveraged to determine better training hyperparameters for larger models [Kaplan et al., 2020, Coleman et al., 2020, Hoffmann et al., 2022, Yang et al., 2022, Xie et al., 2023a].\n\nComputational cost.\n\nDsDm is relatively inexpensive to compute in practical model training scenarios. At a high level, the most expensive part of estimating DsDm is computing the gradient for each training example on a handful of small proxy models (in our case, four 125M parameter LMs—see Appendix B.5 for a full cost breakdown). To contextualize DsDm cost with model training: computing gradients also dominates the cost of training LMs. Since the cost of computing a 125M model gradient is orders of magnitude lower than the cost of computing gradients for standard model sizes, even a small compute multiplier (let alone the 2× improvement DsDm seems to offer) quickly makes the overhead of computing DsDm worthwhile. Additionally, after computing DsDm on a set of datapoints once, the cost of computing DsDm on those datapoints again is essentially negligible (as the required computations are easy to cache). Therefore, we can amortize DsDm’s computational cost over the entire “lifetime” of training on the given dataset.\n\n6 Related Work\n\nCurrent methods for selecting LM pretraining datasets tend to follow a two-step framework: (a) choose an intuitively “high quality” reference corpus, like Wikipedia [Foundation, 2022], then (b) select data that matches it. There are two standard methods that adhere to this framework: DSIR (Dataset Selection with Importance Reweighting [Xie et al., 2023b]) and Classifier (originally introduced in Brown et al. [2020] and used by other work [Gao et al., 2020, Chowdhery et al., 2022, Du et al., 2022]). Other work on selecting data for LM pretraining has included deduplicating examples in LM activation space [Abbas et al., 2023], and selecting examples with the largest difference in loss between LMs trained on the candidate and reference sets [Moore and Lewis, 2010, Axelrod, 2017, Feng et al., 2022]. Simpler methods for selecting data are also commonplace. These include removing documents that are too short or contain too many special characters [Raffel et al., 2020, Computer, 2023, Xie et al., 2023b]. In the LM domain, a related (but different) task to dataset selection is choosing weights for sampling from mixtures of data sources [Chen et al., 2023b, Xie et al., 2023a, Albalak et al., 2023].\n\nBeyond LM pre-training, previous work also selects data in other domains. These works aim to: improve the performance on a given task [Wei et al., 2015, Kaushal et al., 2019, Wang et al., 2020, Killamsetty et al., 2021a, Chitta et al., 2021, Mindermann et al., 2022], identify core-sets of large training datasets [Sener and Savarese, 2017, Phillips, 2017, Coleman et al., 2020, Mirzasoleiman et al., 2020, Paul et al., 2021, Killamsetty et al., 2021b, Okanovic et al., 2023], and instruction fine-tune LMs [Chen et al., 2023a, Cao et al., 2023]. Broadly, such methods select by prompting pretrained models, discriminating on proxies for model uncertainty like loss or gradient norm, matching on gradients, or deduplicating in model output space.\n\n7 Conclusion\n\nIn this work, we cast dataset selection as an optimization problem: given target tasks, a learning algorithm, and a candidate training dataset, choose the training maximizes performance. We then propose a method for approximating the solution to this optimization problem, DsDm, that selects by modeling how the learning algorithm uses training data to predict on the target tasks. We show that our method reliably improves target task performance in the LM setting, and furthermore use our framework to improve broader model generalization. By choosing target tasks similar to those we expect to see at deployment time, we can greatly improve model performance on yet unseen tasks.\n\nOur findings prompt us to take on a much broader view of the role of dataset selection stage in model training. In particular, our framework demonstrates that dataset selection can be an effective tool for fine-grain control of model behavior. Indeed, we hypothesize that carefully choosing data can not only improve downstream task performance, but also other downstream properties of trained models, such as notions of predictor fairness, alignment with human preferences, or capabilities in specific domains like low-resource languages or programming. We also suspect that current methods for datamodeling only scratch the surface of understanding how models learn from data—and that we can greatly improve our ability to manipulate model behavior through training data by developing better datamodeling techniques.\n\nAcknowledgments\n\nLE funded by the Google PhD Fellowship. Work supported in part by the NSF grants CNS-1815221 and DMS-2134108, and Open Philanthropy. This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001120C0015.\n\nReferences\n\nAbbas et al. [2023] Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023.\n\nAlbalak et al. [2023] Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online data mixing for language model pre-training. Training, 20000(40000):60000, 2023.\n\nAxelrod [2017] Amittai Axelrod. Cynical selection of language model training data. arXiv preprint arXiv:1709.02279, 2017.\n\nBai et al. [2022] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.\n\nBiderman et al. [2023] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397–2430. PMLR, 2023.\n\nBierens [1988] Hermanus Josephus Bierens. The nadaraya-watson kernel regression function estimator. 1988.\n\nBisk et al. [2019] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019.\n\nBlack et al. [2022] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. Gpt-neox-20b: An open-source autoregressive language model, 2022. URL https://arxiv.org/abs/2204.06745.\n\nBrown et al. [2020] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n\nCao et al. [2023] Yihan Cao, Yanbin Kang, and Lichao Sun. Instruction mining: High-quality instruction data selection for large language models. arXiv preprint arXiv:2307.06290, 2023.\n\nChen et al. [2023a] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training a better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023a.\n\nChen et al. [2023b] Mayee F Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, and Christopher Ré. Skill-it! a data-driven skills framework for understanding and training language models. arXiv preprint arXiv:2307.14430, 2023b.\n\nChitta et al. [2021] Kashyap Chitta, José M Álvarez, Elmar Haussmann, and Clément Farabet. Training data subset search with ensemble active learning. IEEE Transactions on Intelligent Transportation Systems, 23(9):14741–14752, 2021.\n\nChowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nClark et al. [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019.\n\nClark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018.\n\nColeman et al. [2020] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. Selection via proxy: Efficient data selection for deep learning. In International Conference on Learning Representations (ICLR), 2020.\n\nComputer [2023] Together Computer. Redpajama: an open dataset for training large language models. https://github.com/togethercomputer/RedPajama-Data, October 2023.\n\nCook [1977] R Dennis Cook. Detection of influential observation in linear regression. Technometrics, 19(1):15–18, 1977.\n\nDu et al. [2022] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 5547–5569. PMLR, 2022.\n\nFeng et al. [2022] Yukun Feng, Patrick Xia, Benjamin Van Durme, and João Sedoc. Automatic document selection for efficient encoder pretraining. arXiv preprint arXiv:2210.10951, 2022.\n\nFoundation [2022] Wikimedia Foundation. English wikipedia. https://huggingface.co/datasets/wikipedia, 2022.\n\nGao et al. [2020] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n\nGiordano et al. [2019] Ryan Giordano, William Stephenson, Runjing Liu, Michael Jordan, and Tamara Broderick. A swiss army infinitesimal jackknife. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1139–1147. PMLR, 2019.\n\nGokaslan et al. [2019] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019.\n\nHill et al. [2015] Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. The goldilocks principle: Reading children’s books with explicit memory representations. arXiv preprint arXiv:1511.02301, 2015.\n\nHoffmann et al. [2022] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. In arXiv preprint arXiv:2203.15556, 2022.\n\nIlyas et al. [2022] Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Datamodels: Predicting predictions from training data. In International Conference on Machine Learning (ICML), 2022.\n\nJohnson and Lindenstrauss [1984] William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. In Contemporary mathematics, 1984.\n\nJoshi et al. [2017] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.\n\nJoulin et al. [2016] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759, 2016.\n\nKaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\n\nKaushal et al. [2019] Vishal Kaushal, Rishabh Iyer, Suraj Kothawade, Rohan Mahadev, Khoshrav Doctor, and Ganesh Ramakrishnan. Learning from less data: A unified data subset selection and active learning framework for computer vision. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1289–1299. IEEE, 2019.\n\nKhaddaj et al. [2022] Alaa Khaddaj, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Andrew Ilyas, Hadi Salman, and Aleksander Madry. Backdoor or feature? a new perspective on data poisoning. 2022.\n\nKillamsetty et al. [2021a] Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer. Glister: Generalization based data subset selection for efficient and robust learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8110–8118, 2021a.\n\nKillamsetty et al. [2021b] Krishnateja Killamsetty, Xujiang Zhao, Feng Chen, and Rishabh Iyer. Retrieve: Coreset selection for efficient and robust semi-supervised learning. Advances in Neural Information Processing Systems, 34:14488–14501, 2021b.\n\nKoh and Liang [2017] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International Conference on Machine Learning, 2017.\n\nLiu et al. [2018] Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.\n\nMihaylov et al. [2018] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.\n\nMindermann et al. [2022] Sören Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt Höltgen, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, et al. Prioritized training on points that are learnable, worth learning, and not yet learnt. In International Conference on Machine Learning, pages 15630–15649. PMLR, 2022.\n\nMirzasoleiman et al. [2020] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training of machine learning models. In International Conference on Machine Learning, pages 6950–6960. PMLR, 2020.\n\nMoore and Lewis [2010] Robert C Moore and William Lewis. Intelligent selection of language model training data. In Proceedings of the ACL 2010 conference short papers, pages 220–224, 2010.\n\nMosaicML [2021] MosaicML. Composer, 2021. URL https://www.github.com/mosaicml/composer.\n\nMosaicML [2023] MosaicML. LLM Foundry, 2023. URL https://www.github.com/mosaicml/llm-foundry.\n\nOkanovic et al. [2023] Patrik Okanovic, Roger Waleffe, Vasilis Mageirakos, Konstantinos E Nikolakakis, Amin Karbasi, Dionysis Kalogerias, Nezihe Merve Gürel, and Theodoros Rekatsinas. Repeated random sampling for minimizing the time-to-accuracy of learning. arXiv preprint arXiv:2305.18424, 2023.\n\nPaperno et al. [2016] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.\n\nPark et al. [2023] Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak: Attributing model behavior at scale. In Arxiv preprint arXiv:2303.14186, 2023.\n\nPaul et al. [2021] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Finding important examples early in training. Advances in Neural Information Processing Systems, 34:20596–20607, 2021.\n\nPhillips [2017] Jeff M Phillips. Coresets and sketches. In Handbook of discrete and computational geometry, pages 1269–1288. Chapman and Hall/CRC, 2017.\n\nPregibon [1981] Daryl Pregibon. Logistic regression diagnostics. In The Annals of Statistics, 1981.\n\nPresser [2021] Shawn Presser. Bookcorpusopen. https://huggingface.co/datasets/bookcorpusopen, 2021.\n\nRadford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nRaffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR), 2020.\n\nRajpurkar et al. [2016] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n\nRamesh et al. [2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821–8831. PMLR, 2021.\n\nReddy et al. [2019] Siva Reddy, Danqi Chen, and Christopher D. Manning. Coqa: A conversational question answering challenge, 2019.\n\nRoemmele et al. [2011] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.\n\nSakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.\n\nSaunshi et al. [2023] Nikunj Saunshi, Arushi Gupta, Mark Braverman, and Sanjeev Arora. Understanding influence functions and datamodels via harmonic analysis. In ICLR, 2023.\n\nSener and Savarese [2017] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. arXiv preprint arXiv:1708.00489, 2017.\n\nSrivastava et al. [2022] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\n\nTaori et al. [2023] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\n\nTrischler et al. [2016] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. Newsqa: A machine comprehension dataset. arXiv preprint arXiv:1611.09830, 2016.\n\nTunguz [2019] Bojan Tunguz. Jeopardy! questions, 2019. URL https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions.\n\nWang et al. [2020] Xinyi Wang, Hieu Pham, Paul Michel, Antonios Anastasopoulos, Jaime Carbonell, and Graham Neubig. Optimizing data usage via differentiable rewards. In International Conference on Machine Learning, pages 9983–9995. PMLR, 2020.\n\nWei et al. [2015] Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active learning. In International conference on machine learning, pages 1954–1963. PMLR, 2015.\n\nXie et al. [2023a] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. arXiv preprint arXiv:2305.10429, 2023a.\n\nXie et al. [2023b] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. arXiv preprint arXiv:2302.03169, 2023b.\n\nYang et al. [2022] Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022.\n\nZellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\n\nZhang et al. [2010] Yin Zhang, Rong Jin, and Zhi-Hua Zhou. Understanding bag-of-words model: a statistical framework. International journal of machine learning and cybernetics, 2010.\n\nZhu and Hastie [2005] Ji Zhu and Trevor Hastie. Kernel logistic regression and the import vector machine. Journal of Computational and Graphical Statistics, pages 185–205, 2005.\n\nZhu et al. [2015] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV), December 2015.\n\nZiegler et al. [2019] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.\n\n\\appendixpage\n\nAppendix A Experimental Setup\n\nIn this section we discuss general experimental setup, including candidate data pool, considered target tasks, baselines, evaluation metrics, and model training choices.\n\nA.1 Candidate dataset\n\nOur candidate dataset is the full English subset of C4 [Raffel et al., 2020]. We use the train split of the en.noblocklist subset of the C4 version prepared by AllenAI at https://huggingface.co/datasets/c4. The subset name noblocklist signifies that curse words were not filtered in the subset.\n\nTo split the text from the documents into examples, we tokenize all the documents, concatenate them together (separated by end-of-text tokens), and then slice the result into 1024 token chunks. These 1024 token examples generally contain between 3,000 and 6,000 characters (roughly a thousand words). The final candidate dataset has 216,948,746 examples. We tokenize with the Pythia tokenizer [Black et al., 2022, Biderman et al., 2023].\n\nAs a public internet crawl, C4 contains diverse text. To contextualize the dataset, we show (excerpts) of random C4 samples in Figure 24.\n\nA.2 Target tasks\n\nWe describe each of the considered target tasks below. We both describe the tasks, and how we split samples into distinct sets of “target samples” (to select datasets for a target task) and “holdout samples” (to evaluate models on the target task):\n\n•\n\nSQuAD. The Stanford Question-Answering Dataset (SQuAD [Rajpurkar et al., 2016]) is an open book, reading comprehension dataset of questions about Wikipedia articles. The goal is to answer questions using the corresponding article as context. Our target set is 25% of the SQuAD train set (23107 examples), our holdout set is the SQuAD validation set (10557 examples).\n\n•\n\nJeopardy. Jeopardy [Tunguz, 2019] is a set of trivia questions taken directly from the show “Jeopardy!” We use the version of Jeopardy published by MosaicML [MosaicML, 2023]. We include all the samples save for the “Word Origins” subset. We randomly partition the remaining samples into 876 target samples and 876 holdout samples.\n\n•\n\nLAMBADA. LAnguage Modeling Broadened to Account for Discourse Aspects (LAMBADA [Paperno et al., 2016]) is an open-ended cloze task measuring broad context text understanding. The goal is to predict the last word of curated passages from BooksCorpus [Zhu et al., 2015] given the rest of the passage as context. The task is meant to be challenging: Paperno et al. [2016] only select passages such that crowdworkers could not guess the final word given the final sentence alone (up until the final word), but could guess the final word given the entire passage. We use the LAMBADA version curated by EleutherAI. Finally, we split the LAMBADA test set into separate target and holdout sets, then remove 6 samples from the LAMBADA holdout set due to overlap with samples in our candidate train dataset (cf. Subsection A.2.1 for details on this procedure). We conclude with 2570 holdout samples and 2577 target samples.\n\n•\n\nCS-Algorithms. BIG-bench CS Algorithms [Srivastava et al., 2022] measures the ability of models to solve basic algorithmic problems. In particular, this benchmark contains two kinds of problems: testing for balanced parentheses, and finding the longest common subsequence of multiple strings. For each considered example, the goal is to directly output the answer to the posed algorithmic question. We randomly split the test set into 660 target samples and 660 holdout samples.\n\nWe include samples of each benchmark in Figure 5 (SQuAD), Figure 6 (Jeopardy), Figure 7 (LAMBADA), and Figure 8 (CS-Algorithms). We evaluate in the 0-shot (for LAMBADA and CS-Algorithms) and 3-shot (for SQuAD and Jeopardy) regimes. In the 3-shot setting, we separate each example with a single newline. We use standard prompts for each task (see the samples for details).\n\nA.2.1 Mitigating train-test leakage\n\nWe mitigate train-test leakage by filtering out test examples that overlap with our candidate data samples. Specifically, we define a test example as “leaked” if both its context and continuation are present in a single C4 example. To upper-bound train-test leakage, we test for the context and continuation separately (i.e., for a given test sample, the context and continuation do not have to be contiguous in a train sample to count as leaked) We investigate train-test leakage for all the test examples in each of the test sets (i.e., LAMBADA, SQuAD, Jeopardy, and CS-Algorithms) across the entire candidate train set (i.e., the C4 English subset). Note that we match strings after lowercasing and removing whitespace.\n\nWe find 6 LAMBADA test examples with overlap in C4, and remove them from our LAMBADA test split. We do not find any train-test leakage for SQuAD, Jeopardy, or CS-Algorithms.\n\nA.3 Data selection baselines\n\nWe consider four baselines for selecting language modeling data. These fall into two categories: targeted data selection methods (which select data according to a target distribution), and untargeted data selection methods (which do not take in a target distribution).\n\nA.3.1 Targeted baselines\n\nThe two targeted dataset selection methods we consider, Classifier (originally used to select the GPT-3 dataset [Brown et al., 2020]) and DSIR, both select according to textual similarity with a target distribution. We describe the details of these methods below:\n\nClassifier.\n\nThe dataset selection method originally developed to select data for GPT-3, and additionally used to select data for PaLM [Chowdhery et al., 2022] and The Pile [Gao et al., 2020]. The method trains a logistic regression model on FastText [Joulin et al., 2016] features to classify between (held-out) samples of the candidate dataset (in our case, C4) and the target distribution, then chooses training data according to how likely the model predicts the data as being sampled from the target distribution. To more specifically describe Classifier: the method keeps a given document if the scored document satisfies:\n\nϵ>1−𝚍𝚘𝚌𝚞𝚖𝚎𝚗𝚝⁢_⁢𝚜𝚌𝚘𝚛𝚎,ϵ∼Lomax⁢(α),formulae-sequenceitalic-ϵ1𝚍𝚘𝚌𝚞𝚖𝚎𝚗𝚝_𝚜𝚌𝚘𝚛𝚎similar-toitalic-ϵLomax𝛼\\epsilon>1-\\mathtt{document\\_score},\\epsilon\\sim\\mathrm{Lomax}(\\alpha),italic_ϵ > 1 - typewriter_document _ typewriter_score , italic_ϵ ∼ roman_Lomax ( italic_α ) ,\n\nwhere a Lomax sample is drawn for each considered document, and where 𝚍𝚘𝚌𝚞𝚖𝚎𝚗𝚝⁢_⁢𝚜𝚌𝚘𝚛𝚎𝚍𝚘𝚌𝚞𝚖𝚎𝚗𝚝_𝚜𝚌𝚘𝚛𝚎\\mathtt{document\\_score}typewriter_document _ typewriter_score is the classifier-given probability that the given sample is in the target distribution. Sampling a threshold according to the Lomax distribution is meant to improve diversity of the selected data. In this work, we learn the classifier on the C4 en.noblocklist validation set, and choose α=12𝛼12\\alpha=12italic_α = 12 via the parameter selection procedure described in Brown et al. [2020] (score each document in C4 with the classifier, then fit the parameters of a Lomax distribution via maximum likelihood estimation according to these scores).\n\nDSIR.\n\nDataset Selection with Importance Resampling [Xie et al., 2023b] aims to select a data subset with a similar distribution as the target task in terms of n-gram counts. DSIR comprises two steps: (a) find the (hashed) n-gram counts for each train set example (each example is represented as a vector of counts, with n-grams hashed into buckets to reduce dimensionality), then (b) importance sample to select candidate train set examples that are distributed similarly to target distribution samples in terms of n-gram counts. DSIR calculates importance weights by modeling the distribution of examples (in feature space) under the target distribution and under the candidate data distribution separately, using bag-of-words style models. In greater detail, DSIR consists of the following steps:\n\n1.\n\nFit p^featsubscript^𝑝feat\\hat{p}_{\\mathrm{feat}}over^ start_ARG italic_p end_ARG start_POSTSUBSCRIPT roman_feat end_POSTSUBSCRIPT and q^featsubscript^𝑞feat\\hat{q}_{\\mathrm{feat}}over^ start_ARG italic_q end_ARG start_POSTSUBSCRIPT roman_feat end_POSTSUBSCRIPT, estimates of the distributions of target examples and candidate training examples in hashed n-gram space (respectively). DSIR parameterizes p^featsubscript^𝑝feat\\hat{p}_{\\mathrm{feat}}over^ start_ARG italic_p end_ARG start_POSTSUBSCRIPT roman_feat end_POSTSUBSCRIPT and qfeatsubscript𝑞featq_{\\mathrm{feat}}italic_q start_POSTSUBSCRIPT roman_feat end_POSTSUBSCRIPT through the following general procedure for estimating the distribution of hashed n-grams for a given set of documents. First, calculate the hashed n-gram counts (with d𝑑ditalic_d hash buckets) across the documents as the vector γ∈ℝd𝛾superscriptℝ𝑑\\gamma\\in\\mathbb{R}^{d}italic_γ ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, where γksubscript𝛾𝑘\\gamma_{k}italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT corresponds to the number of n-grams that hash to k𝑘kitalic_k in the documents. Then, normalize γ𝛾\\gammaitalic_γ so that its values sum to 1111, forming a probability distribution over buckets. Finally, parameterize the distribution of hashed n-grams for this set of documents as a bag-of-words style model [Zhang et al., 2010] such that the probability of a document with hashed n-gram counts c𝑐citalic_c is ∏i=1dγdcisuperscriptsubscriptproduct𝑖1𝑑superscriptsubscript𝛾𝑑subscript𝑐𝑖\\prod_{i=1}^{d}\\gamma_{d}^{c_{i}}∏ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_γ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT (here, the bag-of-words model is over hashed n-grams instead of words).\n\n2.\n\nCalculate importance weights for each example in the candidate training set, such that example i𝑖iitalic_i with counts c𝑐citalic_c has weight wi=p^feat⁢(c)q^feat⁢(c)subscript𝑤𝑖subscript^𝑝feat𝑐subscript^𝑞feat𝑐w_{i}=\\frac{\\hat{p}_{\\mathrm{feat}}(c)}{\\hat{q}_{\\mathrm{feat}}(c)}italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG over^ start_ARG italic_p end_ARG start_POSTSUBSCRIPT roman_feat end_POSTSUBSCRIPT ( italic_c ) end_ARG start_ARG over^ start_ARG italic_q end_ARG start_POSTSUBSCRIPT roman_feat end_POSTSUBSCRIPT ( italic_c ) end_ARG.\n\n3.\n\nSample examples without replacement according to the categorical distribution with (unscaled) weights wisubscript𝑤𝑖w_{i}italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.\n\nFor more details on DSIR, see Section 4 of Xie et al. [2023b]. We adapt implementations of both DSIR and Classifier from https://github.com/p-lambda/dsir.\n\nConsidered target distributions.\n\nWe apply targeted dataset selection methods with different target distributions depending on the context. In Section 3, we measure the extent to which different selection methods can reduce loss on individual target tasks, so we select data for individual tasks (i.e., Jeopardy, SQuAD, CS-Algorithms, and LAMBADA). In Section 4 we use these targeted baselines to select data for general purpose language modeling, so we use the recommended target task from each work (intuitively high-quality data sources; see Appendix D.1 for more details).\n\nA.3.2 Untargeted baselines\n\nThe two untargeted dataset selection methods we consider are: Random (select data randomly) and SemDeDup (Semantic Deduplication [Abbas et al., 2023]). SemDeDup selects by clustering data according to the last layer activations for the last token in the given document, then choosing only the examples in each cluster that have the lowest cosine similarity with the cluster centroid. We follow the hyperparameters from the original work (11,000 clusters, deduplicating down to 20% of the dataset for optimal model performance). We use the implementation from https://github.com/facebookresearch/SemDeDup/.\n\nA.4 LM training details\n\nWe train GPT-2 family decoder-only transformer models [Radford et al., 2019, Liu et al., 2018] using LLM-Foundry [MosaicML, 2023]. To train models, we use ADAM (β1=0.9,β2=0.95,ϵ=10−8formulae-sequencesubscript𝛽10.9formulae-sequencesubscript𝛽20.95italic-ϵsuperscript108\\beta_{1}=0.9,\\beta_{2}=0.95,\\epsilon=10^{-8}italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9 , italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.95 , italic_ϵ = 10 start_POSTSUPERSCRIPT - 8 end_POSTSUPERSCRIPT), sequence length 1024, batch size 1024, a cosine learning rate schedule (with 200 warm up batches and α=0.1𝛼0.1\\alpha=0.1italic_α = 0.1), and ℓ2subscriptℓ2\\ell_{2}roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT gradient clipping with threshold 1111. We train on A100s (with BF16 precision) and H100s (with FP8 precision), and tokenize text with the BPE tokenizer used by Pythia [Biderman et al., 2023].\n\nWe summarize the remaining hyperparameter choices used to train the models in this work in Table 2 (including weight decay, learning rate, model architecture, and training token count). We select all hyperparameters to minimize 125M held-out perplexity on C4. The only exception: we increase the weight decay for the Section 4 models to ensure that larger parameter model training runs converge (with smaller weight decay, larger models diverge in loss). Model parameterization choices (i.e., number of heads or layers), optimizer hyperparameters, and learning rate schedule generally chosen according to the default LM training configurations in LLM-Foundry.\n\nChinchilla-optimal compute ratios.\n\nTo train the best possible LM for a given compute budget, one must trade off two hyperparameters that control used compute: model size and number of training tokens. We use Chinchilla-optimal parameter-to-training-token ratios to trade these parameters off [Hoffmann et al., 2022]. In our compute regime, this (roughly) amounts to training on a number of tokens equal to 20× the number of parameters.\n\nA.5 Evaluation metrics\n\nIn this work, we measure model performance using two different metrics: log-probability (in Section 3, to compare model performance on target tasks) and accuracy (in Section 4, to compare model performance on a broad set of yet unseen tasks). Below, we describe how we measure both metrics.\n\nA.5.1 Log-probability\n\nTo calculate mean log-probability, we compute the log-probability of the model generating the correct label, then aggregate the mean across benchmark samples. More specifically, all the tasks we evaluate with log-probability are open-ended LM tasks (e.g., LAMBADA), where the goal is to generate a desired continuation from the context (e.g., for LAMBADA, generate the last word of a paragraph, given the rest of the paragraph as context). Therefore, the log-probability of the model answering correctly is the log-probability that the model generates the label, given the context. This is, for a sample x𝑥xitalic_x with k𝑘kitalic_k continuation tokens starting at index C𝐶Citalic_C,\n\nLog⁢_⁢Probability⁢(x;gw)=∑i=CC+klog⁡(pi), where pi is the correct-label probability given by model gw at index i.Log_Probability𝑥subscript𝑔𝑤superscriptsubscript𝑖𝐶𝐶𝑘subscript𝑝𝑖 where pi is the correct-label probability given by model gw at index i\\mathrm{Log\\_Probability}(x;g_{w}{})=\\sum_{i=C}^{C+k}\\log(p_{i}),\\textrm{ % where $p_{i}$ is the correct-label probability given by model $g_{w}$ at index% $i$}.roman_Log _ roman_Probability ( italic_x ; italic_g start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) = ∑ start_POSTSUBSCRIPT italic_i = italic_C end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C + italic_k end_POSTSUPERSCRIPT roman_log ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , where italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the correct-label probability given by model italic_g start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT at index italic_i . (4)\n\nA.5.2 Accuracy\n\nTo evaluate accuracy, we use one of three separate accuracy procedures depending on the considered benchmark: (a) multiple choice accuracy, (b) exact text match, or (c) fuzzy text match. These are:\n\n•\n\nMultiple choice accuracy: For multiple choice question benchmarks, we choose the answer with the maximal predicted probability out of the possible choices, then measure the accuracy as the fraction of correct answers.\n\n•\n\nExact match: We mark an example as correct if the generated tokens for the context exactly match the label tokens, then measure the accuracy as the fraction of correct answers.\n\n•\n\nFuzzy match: For open-ended benchmarks like TriviaQA whose questions have multiple textually different but correct answers, we measure whether our model is correct on a given example through the following procedure. We generate text for the example context, then normalize this text with the standard TriviaQA text normalizer (which removes articles/extraneous white space/punctuation and normalizes underscores/casing), and finally count the example as correct if the resulting normalized text exactly matches any of the (normalized) labels. We then measure accuracy as the fraction of correct answers.\n\nTable 3 lists the exact accuracy procedure used for each considered benchmark.\n\nAppendix B Datamodel estimation\n\nIn this section, we describe how we estimate datamodels for GPT-2 style LMs. We start by briefly giving an overview of datamodels (cf. Appendix B.1), then describe the datamodel estimator we use, TRAK (cf. Appendix B.2). Finally, we conclude by instantiating datamodels for language modeling (cf. Appendix B.3), and analyzing the computational cost of our procedure (cf. Appendix B.5). For the impatient reader, we include a standalone section on how to mechanically compute datamodel estimates with TRAK (without background) in Appendix B.4.\n\nB.1 Datamodels refresher\n\nThe goal of datamodeling is to approximate the mapping from choice of training subset to trained model loss on a given, fixed sample. Datamodels frame this problem as a supervised learning problem: datamodels learn an approximation from the former to the latter. Recall from Section 2.2 that the datamodel τθsubscript𝜏𝜃\\tau_{\\theta}italic_τ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT for an example x𝑥xitalic_x is a parameterized function that, given a candidate training dataset 𝒮𝒮\\mathcal{S}caligraphic_S, learning algorithm 𝒜𝒜\\mathcal{A}caligraphic_A (mapping train set to trained model), and model output function f𝑓fitalic_f (in the main text, we simplify by refering to this quantity as the loss ℓℓ\\ellroman_ℓ; but in reality f𝑓fitalic_f can capture any function of the trained model) that maps test example and model to resulting loss, optimally predicts the model output on x𝑥xitalic_x over a (chosen) distribution of train subsets 𝒟𝒮subscript𝒟𝒮\\mathcal{D}_{\\mathcal{S}}caligraphic_D start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT, i.e.,\n\nτθx:{0,1}|𝒮|→ℝ, where θx=arg⁡minθ⁡𝔼^Si∼𝒟𝒮(m)⁢[Lreg⁢(τθ⁢(𝟙Si),f⁢(x;𝒜⁢(S)))],:subscript𝜏subscript𝜃𝑥formulae-sequence→superscript01𝒮ℝ where subscript𝜃𝑥subscript𝜃subscriptsuperscript^𝔼𝑚similar-tosubscript𝑆𝑖subscript𝒟𝒮delimited-[]subscript𝐿regsubscript𝜏𝜃subscript𝟙subscript𝑆𝑖𝑓𝑥𝒜𝑆\\displaystyle\\tau_{\\theta_{x}}:\\{0,1\\}^{|\\mathcal{S}|}\\to\\mathbb{R},\\qquad% \\text{ where }\\qquad\\theta_{x}=\\arg\\min_{\\theta}\\ \\widehat{\\mathbb{E}}^{(m)}_{% S_{i}\\sim\\mathcal{D}_{\\mathcal{S}}}\\left[L_{{\\text{reg}}}\\left(\\tau_{\\theta}(% \\text{1}_{S_{i}}),\\ f(x;\\mathcal{A}(S))\\right)\\right],italic_τ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUBSCRIPT : { 0 , 1 } start_POSTSUPERSCRIPT | caligraphic_S | end_POSTSUPERSCRIPT → blackboard_R , where italic_θ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = roman_arg roman_min start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT over^ start_ARG blackboard_E end_ARG start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∼ caligraphic_D start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT ( italic_τ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( 𝟙 start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) , italic_f ( italic_x ; caligraphic_A ( italic_S ) ) ) ] , (5)\n\nwhere Lreg⁢(⋅,⋅)subscript𝐿reg⋅⋅L_{{\\text{reg}}}(\\cdot,\\cdot)italic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT ( ⋅ , ⋅ ) is a regression loss function (e.g., mean squared error), and 𝔼^(m)superscript^𝔼𝑚\\widehat{\\mathbb{E}}^{(m)}over^ start_ARG blackboard_E end_ARG start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT is an m𝑚mitalic_m-sample empirical expectation. Note that datamodels operate on the characteristic vector 𝟙Ssubscript𝟙𝑆\\text{1}_{S}𝟙 start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT of each subset (cf. Equation 2), not the subset directly.\n\nIn this work, we parameterize τθxsubscript𝜏subscript𝜃𝑥\\tau_{\\theta_{x}}italic_τ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUBSCRIPT as linear in the choice of training data, i.e., such that\n\nτθx⁢(𝟙S)=𝟙S⊤⁢θx.subscript𝜏subscript𝜃𝑥subscript𝟙𝑆superscriptsubscript𝟙𝑆topsubscript𝜃𝑥\\tau_{\\theta_{x}}(\\text{1}_{S})=\\text{1}_{S}^{\\top}{\\theta_{x}}.italic_τ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( 𝟙 start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ) = 𝟙 start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_θ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT .\n\nIntuitively, such linear datamodels model each datapoint 𝒮isubscript𝒮𝑖\\mathcal{S}_{i}caligraphic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as having a constant effect on the loss when included in the training set (this effect is exactly the value of θxsubscript𝜃𝑥\\theta_{x}italic_θ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT in index i𝑖iitalic_i).\n\nB.1.1 Estimating datamodels with data regression\n\nSo far we have only defined linear datamodels. How do we actually estimate the linear parameters θxsubscript𝜃𝑥\\theta_{x}italic_θ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT? When introducing datamodels, Ilyas et al. [2022] originally did so with a linear regression predicting loss from training subset—i.e., directly minimizing Equation 5 by collecting a large amount of “training data”—pairs of (randomly chosen training data subset, corresponding trained model output on x𝑥xitalic_x)—then learning the mapping from train subset to output on the collected training data.\n\nThis estimator, which we refer to as data regression, proceeds in two steps. The first step is to collect regression data. Here, we repeatedly: sample a random train subset Sisubscript𝑆𝑖S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (from a chosen distribution Si∼𝒟Ssimilar-tosubscript𝑆𝑖subscript𝒟𝑆S_{i}\\sim\\mathcal{D}_{S}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∼ caligraphic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ), train a model 𝒜⁢(Si)𝒜subscript𝑆𝑖\\mathcal{A}(S_{i})caligraphic_A ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) on the subset, then evaluate the model output on x𝑥xitalic_x (and record the train subset, model output pairs). This step yields “training data” for the regression in the form of m𝑚mitalic_m train subset, loss pairs: {(𝟙Si,ℓ⁢(x;𝒜⁢(Si)))}i=1msuperscriptsubscriptsubscript𝟙subscript𝑆𝑖ℓ𝑥𝒜subscript𝑆𝑖𝑖1𝑚\\{\\left(\\text{1}_{S_{i}},\\ell(x;\\mathcal{A}\\left(S_{i}\\right))\\right)\\}_{i=1}^% {m}{ ( 𝟙 start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT , roman_ℓ ( italic_x ; caligraphic_A ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT (recall that our datamodel takes as input the characteristic vector of subsets rather than subsets directly). Then, the second step is to actually estimate the linear datamodel parameters with linear regression. Here, the regression minimizes the (empirical) squared error over datamodel parameters:\n\nθxsubscript𝜃𝑥\\displaystyle\\theta_{x}italic_θ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT =arg⁢minθ⁡𝔼^Si∼𝒟𝒮(m)⁢[Lreg⁢(τθ⁢(𝟙Si),ℓ⁢(x;𝒜⁢(S)))]absentsubscriptargmin𝜃subscriptsuperscript^𝔼𝑚similar-tosubscript𝑆𝑖subscript𝒟𝒮delimited-[]subscript𝐿regsubscript𝜏𝜃subscript𝟙subscript𝑆𝑖ℓ𝑥𝒜𝑆\\displaystyle=\\operatorname*{arg\\,min}_{\\theta}\\widehat{\\mathbb{E}}^{(m)}_{S_{% i}\\sim\\mathcal{D}_{\\mathcal{S}}}\\left[L_{{\\text{reg}}}\\left(\\tau_{\\theta}(% \\text{1}_{S_{i}}),\\ \\ell(x;\\mathcal{A}(S))\\right)\\right]= start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT over^ start_ARG blackboard_E end_ARG start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∼ caligraphic_D start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT ( italic_τ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( 𝟙 start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) , roman_ℓ ( italic_x ; caligraphic_A ( italic_S ) ) ) ] =arg⁢minθ⁡𝔼^Si∼𝒟𝒮(m)⁢[(𝟙Si⊤⁢θ−ℓ⁢(x;𝒜⁢(S)))2].absentsubscriptargmin𝜃subscriptsuperscript^𝔼𝑚similar-tosubscript𝑆𝑖subscript𝒟𝒮delimited-[]superscriptsuperscriptsubscript𝟙subscript𝑆𝑖top𝜃ℓ𝑥𝒜𝑆2\\displaystyle=\\operatorname*{arg\\,min}_{\\theta}\\widehat{\\mathbb{E}}^{(m)}_{S_{% i}\\sim\\mathcal{D}_{\\mathcal{S}}}\\left[\\left(\\text{1}_{S_{i}}^{\\top}\\theta-\\ell% (x;\\mathcal{A}(S))\\right)^{2}\\right].= start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT over^ start_ARG blackboard_E end_ARG start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∼ caligraphic_D start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ ( 𝟙 start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_θ - roman_ℓ ( italic_x ; caligraphic_A ( italic_S ) ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .\n\nLinear regression estimates the datamodel parameters directly, and asymptotically yields the true datamodel parameters (with enough “training data,” or pairs of training subset, corresponding trained model output).\n\nWhile data regression optimally estimates linear datamodel parameters, it is expensive to estimate due to the “training data” collection process. Obtaining a single training datapoint for the regression—i.e., a single train set, corresponding loss on x𝑥xitalic_x pair—is expensive because training even a single model can be expensive (particularly for the large-scale model setting), and in practice, previous work has found that we need to train at (at least) thousands of models to collect enough regression datapoints [Ilyas et al., 2022].\n\nB.2 Estimating datamodels with TRAK\n\nRather than estimating with data regression, we estimate linear datamodel parameters with a more computationally efficient linear datamodel estimator: TRAK [Park et al., 2023]. TRAK estimates datamodels more efficiently by exploiting the fact that datamodels are efficient to calculate for convex learning problems: TRAK (approximately) transforms the original learning algorithm into a convex learning problem, computes datamodels in this new regime, then returns these datamodels as an estimate of the datamodels for the originally considered learning algorithm. TRAK trades off approximation error (i.e., the transformation is inexact) for computational efficiency.\n\nTo actually estimate datamodels, the method operates in two high level stages. Given a held out sample x𝑥xitalic_x, learning algorithm 𝒜𝒜\\mathcal{A}caligraphic_A and training dataset 𝒮𝒮\\mathcal{S}caligraphic_S, TRAK first constructs a new algorithm 𝒜′superscript𝒜′\\mathcal{A}^{\\prime}caligraphic_A start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT that approximates the corresponding trained model output on x𝑥xitalic_x as if the model output were obtained by solving a convex problem over the train set datapoints, such that f⁢(x;𝒜⁢(S))≈f⁢(x;𝒜′⁢(S))𝑓𝑥𝒜𝑆𝑓𝑥superscript𝒜′𝑆f(x;\\mathcal{A}(S))\\approx f(x;\\mathcal{A}^{\\prime}(S))italic_f ( italic_x ; caligraphic_A ( italic_S ) ) ≈ italic_f ( italic_x ; caligraphic_A start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_S ) ). Then, TRAK estimates the datamodel parameters for the original learning problem by estimating the datamodel parameters for executing 𝒜′superscript𝒜′\\mathcal{A}^{\\prime}caligraphic_A start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT on S𝑆Sitalic_S (datamodels are inexpensive to compute for convex problems like 𝒜′superscript𝒜′\\mathcal{A}^{\\prime}caligraphic_A start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT). We break these stages into two steps below, and start with a primer on calculating datamodels for the logistic regression setting.\n\nB.2.1 Datamodels for logistic regression\n\nWe first describe how to efficiently estimate datamodels for models with a convex objective. We will use logistic loss to simplify the analysis, but the procedure applies to other convex losses function as well. Consider a (generalized, including biases) binary classification task learning from n=|𝒮|𝑛𝒮n=|\\mathcal{S}|italic_n = | caligraphic_S | candidate training samples:\n\n𝒮={z1,…,zn:zi=(xi,bi,yi)},𝒮conditional-setsubscript𝑧1…subscript𝑧𝑛subscript𝑧𝑖subscript𝑥𝑖subscript𝑏𝑖subscript𝑦𝑖\\mathcal{S}=\\{z_{1},...,z_{n}:z_{i}=(x_{i},b_{i},y_{i})\\},caligraphic_S = { italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT : italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } ,\n\nwhere each sample zisubscript𝑧𝑖z_{i}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is a triplet containing an input xisubscript𝑥𝑖x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, a bias bisubscript𝑏𝑖b_{i}italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and a binary label yi∈{−1,1}subscript𝑦𝑖11y_{i}\\in\\{-1,1\\}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ { - 1 , 1 }. In this setup, training a logistic regression model on a training subset S⊂𝒮𝑆𝒮S\\subset\\mathcal{S}italic_S ⊂ caligraphic_S yields the corresponding parameters 𝒜Log⁢(S)subscript𝒜Log𝑆\\mathcal{A}_{\\mathrm{Log}}{}(S)caligraphic_A start_POSTSUBSCRIPT roman_Log end_POSTSUBSCRIPT ( italic_S ):\n\n𝒜Log⁢(S):=arg⁡minθ⁢∑zi∈Slog⁡(1+exp⁡(−yi⋅(xi⊤⁢θ+bi))).assignsubscript𝒜Log𝑆subscript𝜃subscriptsubscript𝑧𝑖𝑆1⋅subscript𝑦𝑖superscriptsubscript𝑥𝑖top𝜃subscript𝑏𝑖\\mathcal{A}_{\\mathrm{Log}}{}(S)\\vcentcolon=\\arg\\min_{\\theta}\\sum_{z_{i}\\in S}% \\log\\left(1+\\exp\\left(-y_{i}\\cdot\\left(x_{i}^{\\top}\\theta+b_{i}\\right)\\right)% \\right).caligraphic_A start_POSTSUBSCRIPT roman_Log end_POSTSUBSCRIPT ( italic_S ) := roman_arg roman_min start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_S end_POSTSUBSCRIPT roman_log ( 1 + roman_exp ( - italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⋅ ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_θ + italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) ) . (6)\n\nNote that including biases bisubscript𝑏𝑖b_{i}italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT makes this version of logistic regression more general; setting bi=0subscript𝑏𝑖0b_{i}=0italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0 yields standard logistic regression.\n\nHow do we estimate datamodels for logistic regression? We start by defining the output function that we want to approximate using datamodels in the first place: we approximate the logistic linear model output\n\nf⁢(z;θ):=x⊤⁢θ+b, where ⁢z=(x,b,y).formulae-sequenceassign𝑓𝑧𝜃superscript𝑥top𝜃𝑏 where 𝑧𝑥𝑏𝑦f(z;\\theta)\\vcentcolon=x^{\\top}\\theta+b,\\mbox{ where }z=(x,b,y).italic_f ( italic_z ; italic_θ ) := italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_θ + italic_b , where italic_z = ( italic_x , italic_b , italic_y ) .\n\nThat is, we aim to construct datamodels that approximate the map from train subset S𝑆Sitalic_S to linear model output f⁢(z;𝒜Log⁢(S))𝑓𝑧subscript𝒜Log𝑆f(z;\\mathcal{A}_{\\mathrm{Log}}{}(S))italic_f ( italic_z ; caligraphic_A start_POSTSUBSCRIPT roman_Log end_POSTSUBSCRIPT ( italic_S ) ).\n\nTo efficiently estimate these logistic regression datamodels, TRAK uses influence functions. Influence functions are a standard method for efficiently approximating the effect of excluding a single training point (hence, “leave-one-out”) on linear regression outputs compared to training on the entire set [Pregibon, 1981] (and apply to other classes of models as well [Giordano et al., 2019]). Specifically, the leave-one-out influence for training example i𝑖iitalic_i on example z𝑧zitalic_z, IF⁢(z)iIFsubscript𝑧𝑖\\mathrm{IF}(z)_{i}roman_IF ( italic_z ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, approximates this effect as:\n\nIF⁢(z)i:=x⊤⁢(X⊤⁢R⁢X)−1⁢xi1−xi⊤⁢(X⊤⁢R⁢X)−1⋅pi*⁢(1−pi*)⁢(1−pi*)≈f⁢(z;𝒜Log⁢(𝒮))−f⁢(z;𝒜Log⁢(𝒮∖zi)),assignIFsubscript𝑧𝑖superscript𝑥topsuperscriptsuperscript𝑋top𝑅𝑋1subscript𝑥𝑖1⋅superscriptsubscript𝑥𝑖topsuperscriptsuperscript𝑋top𝑅𝑋1superscriptsubscript𝑝𝑖1superscriptsubscript𝑝𝑖1superscriptsubscript𝑝𝑖𝑓𝑧subscript𝒜Log𝒮𝑓𝑧subscript𝒜Log𝒮subscript𝑧𝑖\\mathrm{IF}(z)_{i}\\vcentcolon=\\frac{x^{\\top}(X^{\\top}RX)^{-1}x_{i}}{1-x_{i}^{% \\top}(X^{\\top}RX)^{-1}\\cdot p_{i}^{*}(1-p_{i}^{*})}(1-p_{i}^{*})\\approx f(z;% \\mathcal{A}_{\\mathrm{Log}}{}(\\mathcal{S}))-f(z;\\mathcal{A}_{\\mathrm{Log}}{}(% \\mathcal{S}\\setminus z_{i})),roman_IF ( italic_z ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT := divide start_ARG italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_R italic_X ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG 1 - italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_R italic_X ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ⋅ italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( 1 - italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ) end_ARG ( 1 - italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ) ≈ italic_f ( italic_z ; caligraphic_A start_POSTSUBSCRIPT roman_Log end_POSTSUBSCRIPT ( caligraphic_S ) ) - italic_f ( italic_z ; caligraphic_A start_POSTSUBSCRIPT roman_Log end_POSTSUBSCRIPT ( caligraphic_S ∖ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) , (7)\n\nwhere X∈ℝn×k𝑋superscriptℝ𝑛𝑘X\\in\\mathbb{R}^{n\\times k}italic_X ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_k end_POSTSUPERSCRIPT is the matrix of stacked train example inputs (k𝑘kitalic_k the input dimension of each xisubscript𝑥𝑖x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT), pi*=(1+exp⁡(−yi⋅f⁢(zi;θ*)))−1subscriptsuperscript𝑝𝑖superscript1⋅subscript𝑦𝑖𝑓subscript𝑧𝑖superscript𝜃1p^{*}_{i}=\\left(1+\\exp\\left(-y_{i}\\cdot f(z_{i};\\theta^{*})\\right)\\right)^{-1}italic_p start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( 1 + roman_exp ( - italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⋅ italic_f ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; italic_θ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ) ) ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, and R𝑅Ritalic_R is an n×n𝑛𝑛n\\times nitalic_n × italic_n matrix with Ri⁢i=pi*⁢(1−pi*)subscript𝑅𝑖𝑖superscriptsubscript𝑝𝑖1superscriptsubscript𝑝𝑖R_{ii}=p_{i}^{*}(1-p_{i}^{*})italic_R start_POSTSUBSCRIPT italic_i italic_i end_POSTSUBSCRIPT = italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( 1 - italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ); this estimate arises from performing a Newton step from logistic model parameters for 𝒮𝒮\\mathcal{S}caligraphic_S to minimize loss on 𝒮∖zi𝒮subscript𝑧𝑖\\mathcal{S}\\setminus z_{i}caligraphic_S ∖ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. In practice, influence functions closely approximate the effect of removing a single train example on logistic model predictions [Koh and Liang, 2017]. Furthermore, influences are efficient to estimate: computing the influence of i𝑖iitalic_i on example z𝑧zitalic_z requires only a few inner products and scalar multiplications (the most expensive term to compute, the inverse (X⊤⁢R⁢X)−1superscriptsuperscript𝑋top𝑅𝑋1\\left(X^{\\top}RX\\right)^{-1}( italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_R italic_X ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, does not depend on z𝑧zitalic_z or i𝑖iitalic_i and therefore can be computed just once).\n\nIt is straightforward to estimate parameters for logistic regression datamodels using influence functions. We consider leave-one-out datamodels, i.e., referring back to the datamodel definition of (5), datamodels for a distribution of training sets 𝒟Ssubscript𝒟𝑆\\mathcal{D}_{S}caligraphic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT that is supported on train subsets missing a single train example. In this setting, we can estimate a leave-one-out linear datamodel τθsubscript𝜏𝜃\\tau_{\\theta}italic_τ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT with θ=IF⁢(z)𝜃IF𝑧\\theta=\\mathrm{IF}(z)italic_θ = roman_IF ( italic_z ) and including a bias f⁢(z;𝒜Log⁢(𝒮))−∑k=1nIF⁢(z)k𝑓𝑧subscript𝒜Log𝒮superscriptsubscript𝑘1𝑛IFsubscript𝑧𝑘f(z;\\mathcal{A}_{\\mathrm{Log}}{}(\\mathcal{S}))-\\sum_{k=1}^{n}\\mathrm{IF}(z)_{k}italic_f ( italic_z ; caligraphic_A start_POSTSUBSCRIPT roman_Log end_POSTSUBSCRIPT ( caligraphic_S ) ) - ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_IF ( italic_z ) start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, i.e., in full:\n\nτθ⁢(S)=IF⁢(z)⊤⁢𝟙S+f⁢(z;𝒜Log⁢(𝒮))−∑k=1nIF⁢(z)ksubscript𝜏𝜃𝑆IFsuperscript𝑧topsubscript𝟙𝑆𝑓𝑧subscript𝒜Log𝒮superscriptsubscript𝑘1𝑛IFsubscript𝑧𝑘\\tau_{\\theta}(S)=\\mathrm{IF}(z)^{\\top}\\text{1}_{S}+f(z;\\mathcal{A}_{\\mathrm{% Log}}{}(\\mathcal{S}))-\\sum_{k=1}^{n}\\mathrm{IF}(z)_{k}italic_τ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_S ) = roman_IF ( italic_z ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT 𝟙 start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT + italic_f ( italic_z ; caligraphic_A start_POSTSUBSCRIPT roman_Log end_POSTSUBSCRIPT ( caligraphic_S ) ) - ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_IF ( italic_z ) start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT (8)\n\nThen, on a data subset with a single removed example S∖xi𝑆subscript𝑥𝑖S\\setminus x_{i}italic_S ∖ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, the datamodel approximation of f⁢(z;𝒜Log⁢(S∖xi))𝑓𝑧subscript𝒜Log𝑆subscript𝑥𝑖f(z;\\mathcal{A}_{\\mathrm{Log}}{}(S\\setminus x_{i}))italic_f ( italic_z ; caligraphic_A start_POSTSUBSCRIPT roman_Log end_POSTSUBSCRIPT ( italic_S ∖ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) is:\n\nτθ⁢(S∖zi)subscript𝜏𝜃𝑆subscript𝑧𝑖\\displaystyle\\tau_{\\theta}(S\\setminus z_{i})italic_τ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_S ∖ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) =IF⁢(z)⊤⁢𝟙S∖xi+f⁢(z;𝒜Log⁢(𝒮))−∑k=1nIF⁢(z)kabsentIFsuperscript𝑧topsubscript𝟙𝑆subscript𝑥𝑖𝑓𝑧subscript𝒜Log𝒮superscriptsubscript𝑘1𝑛IFsubscript𝑧𝑘\\displaystyle=\\mathrm{IF}(z)^{\\top}\\text{1}_{S\\setminus x_{i}}+f(z;\\mathcal{A}% _{\\mathrm{Log}}{}(\\mathcal{S}))-\\sum_{k=1}^{n}\\mathrm{IF}(z)_{k}= roman_IF ( italic_z ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT 𝟙 start_POSTSUBSCRIPT italic_S ∖ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_f ( italic_z ; caligraphic_A start_POSTSUBSCRIPT roman_Log end_POSTSUBSCRIPT ( caligraphic_S ) ) - ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_IF ( italic_z ) start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT =f⁢(z;𝒜Log⁢(𝒮))−IF⁢(z)iabsent𝑓𝑧subscript𝒜Log𝒮IFsubscript𝑧𝑖\\displaystyle=f(z;\\mathcal{A}_{\\mathrm{Log}}{}(\\mathcal{S}))-\\mathrm{IF}(z)_{i}= italic_f ( italic_z ; caligraphic_A start_POSTSUBSCRIPT roman_Log end_POSTSUBSCRIPT ( caligraphic_S ) ) - roman_IF ( italic_z ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≈f⁢(z;𝒜Log⁢(𝒮))−(f⁢(z;𝒜Log⁢(𝒮))−f⁢(z;𝒜Log⁢(𝒮∖zi)))absent𝑓𝑧subscript𝒜Log𝒮𝑓𝑧subscript𝒜Log𝒮𝑓𝑧subscript𝒜Log𝒮subscript𝑧𝑖\\displaystyle\\approx f(z;\\mathcal{A}_{\\mathrm{Log}}{}(\\mathcal{S}))-\\left(f(z;% \\mathcal{A}_{\\mathrm{Log}}{}(\\mathcal{S}))-f(z;\\mathcal{A}_{\\mathrm{Log}}{}(% \\mathcal{S}\\setminus z_{i}))\\right)≈ italic_f ( italic_z ; caligraphic_A start_POSTSUBSCRIPT roman_Log end_POSTSUBSCRIPT ( caligraphic_S ) ) - ( italic_f ( italic_z ; caligraphic_A start_POSTSUBSCRIPT roman_Log end_POSTSUBSCRIPT ( caligraphic_S ) ) - italic_f ( italic_z ; caligraphic_A start_POSTSUBSCRIPT roman_Log end_POSTSUBSCRIPT ( caligraphic_S ∖ italic_z start_POSTSUBSCRIPT italic_i e"
    }
}
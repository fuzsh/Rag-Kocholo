{
    "id": "dbpedia_8306_3",
    "rank": 60,
    "data": {
        "url": "https://mdsr-book.github.io/mdsr2e/ch-learningII.html",
        "read_more_link": "",
        "language": "en",
        "title": "Chapter 12 Unsupervised learning",
        "top_image": "https://mdsr-book.github.io/mdsr2e//gfx/mdsr2e_cover.jpg",
        "meta_img": "https://mdsr-book.github.io/mdsr2e//gfx/mdsr2e_cover.jpg",
        "images": [
            "https://mdsr-book.github.io/mdsr2e/gfx/evolutionary-tree.jpeg",
            "https://mdsr-book.github.io/mdsr2e/gfx/City_Distances.png",
            "https://mdsr-book.github.io/mdsr2e/12-learningII_files/figure-html/cars-tree-1.png",
            "https://mdsr-book.github.io/mdsr2e/12-learningII_files/figure-html/cluster-cities-1.png",
            "https://mdsr-book.github.io/mdsr2e/12-learningII_files/figure-html/ballot-grid-1.png",
            "https://mdsr-book.github.io/mdsr2e/12-learningII_files/figure-html/two-ballots-1.png",
            "https://mdsr-book.github.io/mdsr2e/12-learningII_files/figure-html/many-ballots-1.png",
            "https://mdsr-book.github.io/mdsr2e/12-learningII_files/figure-html/ballot-PCA-1.png",
            "https://mdsr-book.github.io/mdsr2e/12-learningII_files/figure-html/issue-clusters-1.png",
            "https://mdsr-book.github.io/mdsr2e/12-learningII_files/figure-html/SVD-ballots-1.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Benjamin S. Baumer",
            "Daniel T. Kaplan",
            "Nicholas J. Horton"
        ],
        "publish_date": "2021-07-28T00:00:00",
        "summary": "",
        "meta_description": "A comprehensive data science textbook for undergraduates that incorporates statistical and computational thinking to solve real-world problems with data.",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://mdsr-book.github.io/mdsr2e/",
        "text": "12.1.1 Hierarchical clustering\n\nWhen the description of an object consists of a set of numerical variables (none of which is a response variable), there are two main steps in constructing a tree to describe the relationship among the cases in the data:\n\nRepresent each case as a point in a Cartesian space.\n\nMake branching decisions based on how close together points or clouds of points are.\n\nTo illustrate, consider the unsupervised learning process of identifying different types of cars. The United States Department of Energy maintains automobile characteristics for thousands of cars: miles per gallon, engine size, number of cylinders, number of gears, etc. Please see their guide for more information. Here, we download a ZIP file from their website that contains fuel economy rating for the 2016 model year.\n\nsrc <- \"https://www.fueleconomy.gov/feg/epadata/16data.zip\" lcl <- usethis::use_zip(src)\n\nNext, we use the readxl package to read this file into R, clean up some of the resulting variable names, select a small subset of the variables, and filter for distinct models of Toyota vehicles. The resulting data set contains information about 75 different models that Toyota produces.\n\nlibrary(tidyverse) library(mdsr) library(readxl) filename <- fs::dir_ls(\"data\", regexp = \"public\\\\.xlsx\") %>% head(1) cars <- read_excel(filename) %>% janitor::clean_names() %>% select( make = mfr_name, model = carline, displacement = eng_displ, number_cyl, number_gears, city_mpg = city_fe_guide_conventional_fuel, hwy_mpg = hwy_fe_guide_conventional_fuel ) %>% distinct(model, .keep_all = TRUE) %>% filter(make == \"Toyota\") glimpse(cars)\n\nRows: 75 Columns: 7 $ make <chr> \"Toyota\", \"Toyota\", \"Toyota\", \"Toyota\", \"Toyota\", \"To… $ model <chr> \"FR-S\", \"RC 200t\", \"RC 300 AWD\", \"RC 350\", \"RC 350 AW… $ displacement <dbl> 2.0, 2.0, 3.5, 3.5, 3.5, 5.0, 1.5, 1.8, 5.0, 2.0, 3.5… $ number_cyl <dbl> 4, 4, 6, 6, 6, 8, 4, 4, 8, 4, 6, 6, 6, 4, 4, 4, 4, 6,… $ number_gears <dbl> 6, 8, 6, 8, 6, 8, 6, 1, 8, 8, 6, 8, 6, 6, 1, 4, 6, 6,… $ city_mpg <dbl> 25, 22, 19, 19, 19, 16, 33, 43, 16, 22, 19, 19, 19, 2… $ hwy_mpg <dbl> 34, 32, 26, 28, 26, 25, 42, 40, 24, 33, 26, 28, 26, 3…\n\nAs a large automaker, Toyota has a diverse lineup of cars, trucks, SUVs, and hybrid vehicles. Can we use unsupervised learning to categorize these vehicles in a sensible way with only the data we have been given?\n\nFor an individual quantitative variable, it is easy to measure how far apart any two cars are: Take the difference between the numerical values. The different variables are, however, on different scales and in different units. For example, gears ranges only from 1 to 8, while city_mpg goes from 13 to 58. This means that some decision needs to be made about rescaling the variables so that the differences along each variable reasonably reflect how different the respective cars are. There is more than one way to do this, and in fact, there is no universally “best” solution—the best solution will always depend on the data and your domain expertise. The dist() function takes a simple and pragmatic point of view: Each variable is equally important.\n\nThe output of dist() gives the distance from each individual car to every other car.\n\ncar_diffs <- cars %>% column_to_rownames(var = \"model\") %>% dist() str(car_diffs)\n\n'dist' num [1:2775] 4.52 11.29 9.93 11.29 15.14 ... - attr(*, \"Size\")= int 75 - attr(*, \"Labels\")= chr [1:75] \"FR-S\" \"RC 200t\" \"RC 300 AWD\" \"RC 350\" ... - attr(*, \"Diag\")= logi FALSE - attr(*, \"Upper\")= logi FALSE - attr(*, \"method\")= chr \"euclidean\" - attr(*, \"call\")= language dist(x = .)\n\ncar_mat <- car_diffs %>% as.matrix() car_mat[1:6, 1:6] %>% round(digits = 2)\n\nFR-S RC 200t RC 300 AWD RC 350 RC 350 AWD RC F FR-S 0.00 4.52 11.29 9.93 11.29 15.14 RC 200t 4.52 0.00 8.14 6.12 8.14 11.49 RC 300 AWD 11.29 8.14 0.00 3.10 0.00 4.93 RC 350 9.93 6.12 3.10 0.00 3.10 5.39 RC 350 AWD 11.29 8.14 0.00 3.10 0.00 4.93 RC F 15.14 11.49 4.93 5.39 4.93 0.00\n\nThis point-to-point distance matrix is analogous to the tables that used to be printed on road maps giving the distance from one city to another, like Figure 12.2, which states that it is 1,095 miles from Atlanta to Boston, or 715 miles from Atlanta to Chicago. Notice that the distances are symmetric: It is the same distance from Boston to Los Angeles as from Los Angeles to Boston (3,036 miles, according to the table).\n\nKnowing the distances between the cities is not the same thing as knowing their locations. But the set of mutual distances is enough information to reconstruct the relative positions of the cities.\n\nCities, of course, lie on the surface of the earth. That need not be true for the “distance” between automobile types. Even so, the set of mutual distances provides information equivalent to knowing the relative positions of these cars in a \\(p\\)-dimensional space. This can be used to construct branches between nearby items, then to connect those branches, and so on until an entire tree has been constructed. The process is called hierarchical clustering. Figure 12.3 shows a tree constructed by hierarchical clustering that relates Toyota car models to one another.\n\nlibrary(ape) car_diffs %>% hclust() %>% as.phylo() %>% plot(cex = 0.8, label.offset = 1)\n\nThere are many ways to graph such trees, but here we have borrowed from biology by graphing these cars as a phylogenetic tree, similar to Figure 12.1. Careful inspection of Figure 12.3 reveals some interesting insights. The first branch in the tree is evidently between hybrid vehicles and all others. This makes sense, since hybrid vehicles use a fundamentally different type of power to achieve considerably better fuel economy. Moreover, the first branch among conventional cars divides large trucks and SUVs (e.g., Sienna, Tacoma, Sequoia, Tundra, Land Cruiser) from smaller cars and cross-over SUVs (e.g., Camry, Corolla, Yaris, RAV4). We are confident that the gearheads in the readership will identify even more subtle logic to this clustering. One could imagine that this type of analysis might help a car-buyer or marketing executive quickly decipher what might otherwise be a bewildering product line.\n\n12.1.2 \\(k\\)-means\n\nAnother way to group similar cases is to assign each case to one of several distinct groups, but without constructing a hierarchy. The output is not a tree but a choice of group to which each case belongs. (There can be more detail than this; for instance, a probability for each specific case that it belongs to each group.) This is like classification except that here there is no response variable. Thus, the definition of the groups must be inferred implicitly from the data.\n\nAs an example, consider the cities of the world (in world_cities). Cities can be different and similar in many ways: population, age structure, public transportation and roads, building space per person, etc. The choice of features (or variables) depends on the purpose you have for making the grouping.\n\nOur purpose is to show you that clustering via machine learning can actually identify genuine patterns in the data. We will choose features that are utterly familiar: the latitude and longitude of each city.\n\nYou already know about the location of cities. They are on land. And you know about the organization of land on earth: most land falls in one of the large clusters called continents. But the world_cities data doesn’t have any notion of continents. Perhaps it is possible that this feature, which you long ago internalized, can be learned by a computer that has never even taken grade-school geography.\n\nFor simplicity, consider the 4,000 biggest cities in the world and their longitudes and latitudes.\n\nbig_cities <- world_cities %>% arrange(desc(population)) %>% head(4000) %>% select(longitude, latitude) glimpse(big_cities)\n\nRows: 4,000 Columns: 2 $ longitude <dbl> 121.46, 28.95, -58.38, 72.88, -99.13, 116.40, 67.01, 117… $ latitude <dbl> 31.22, 41.01, -34.61, 19.07, 19.43, 39.91, 24.86, 39.14,…\n\nNote that in these data, there is no ancillary information—not even the name of the city. However, the \\(k\\)-means clustering algorithm will separate these 4,000 points—each of which is located in a two-dimensional plane—into \\(k\\) clusters based on their locations alone.\n\nset.seed(15) library(mclust) city_clusts <- big_cities %>% kmeans(centers = 6) %>% fitted(\"classes\") %>% as.character() big_cities <- big_cities %>% mutate(cluster = city_clusts) big_cities %>% ggplot(aes(x = longitude, y = latitude)) + geom_point(aes(color = cluster), alpha = 0.5) + scale_color_brewer(palette = \"Set2\")\n\nAs shown in Figure 12.4, the clustering algorithm seems to have identified the continents. North and South America are clearly distinguished, as is most of Africa. The cities in North Africa are matched to Europe, but this is consistent with history, given the European influence in places like Morocco, Tunisia, and Egypt. Similarly, while the cluster for Europe extends into what is called Asia, the distinction between Europe and Asia is essentially historic, not geographic. Note that to the algorithm, there is little difference between oceans and deserts—both represent large areas where no big cities exist.\n\n12.2.2 Singular value decomposition\n\nYou may ask why the choice was made to add up the first half of the ballots as \\(x\\) and the remaining ballots as \\(y\\). Perhaps there is a better choice to display the underlying patterns. Perhaps we can think of a way to add up the ballots in a more meaningful way.\n\nIn fact, there is a mathematical approach to finding the best approximation to the ballot–voter matrix using simple matrices, called singular value decomposition (SVD). (The statistical dimension reduction technique of principal component analysis (PCA) can be accomplished using SVD.) The mathematics of SVD draw on a knowledge of matrix algebra, but the operation itself is accessible to anyone. Geometrically, SVD (or PCA) amounts to a rotation of the coordinate axes so that more of the variability can be explained using just a few variables. Figure 12.8 shows the position of each member on the two principal components that explain the most variability.\n\nVotes_wide <- Votes %>% pivot_wider(names_from = bill, values_from = vote) vote_svd <- Votes_wide %>% select(-name) %>% svd() num_clusters <- 5 # desired number of clusters library(broom) vote_svd_tidy <- vote_svd %>% tidy(matrix = \"u\") %>% filter(PC < num_clusters) %>% mutate(PC = paste0(\"pc_\", PC)) %>% pivot_wider(names_from = PC, values_from = value) %>% select(-row) clusts <- vote_svd_tidy %>% kmeans(centers = num_clusters) tidy(clusts)\n\n# A tibble: 5 × 7 pc_1 pc_2 pc_3 pc_4 size withinss cluster <dbl> <dbl> <dbl> <dbl> <int> <dbl> <fct> 1 -0.0529 0.142 0.0840 0.0260 26 0.0118 1 2 0.0851 0.0367 0.0257 -0.182 20 0.160 2 3 -0.0435 0.109 0.0630 -0.0218 10 0.0160 3 4 -0.0306 -0.116 0.183 -0.00962 20 0.0459 4 5 0.106 0.0206 0.0323 0.0456 58 0.112 5\n\nvoters <- clusts %>% augment(vote_svd_tidy) ggplot(data = voters, aes(x = pc_1, y = pc_2)) + geom_point(aes(x = 0, y = 0), color = \"red\", shape = 1, size = 7) + geom_point(size = 5, alpha = 0.6, aes(color = .cluster)) + xlab(\"Best Vector from SVD\") + ylab(\"Second Best Vector from SVD\") + ggtitle(\"Political Positions of Members of Parliament\") + scale_color_brewer(palette = \"Set2\")\n\nFigure 12.8 shows, at a glance, that there are three main clusters. The red circle marks the average member. The three clusters move away from average in different directions. There are several members whose position is in-between the average and the cluster to which they are closest. These clusters may reveal the alignment of Scottish members of parliament according to party affiliation and voting history.\n\nFor a graphic, one is limited to using two variables for position. Clustering, however, can be based on many more variables. Using more SVD sums may allow the three clusters to be split up further. The color in Figure 12.8 above shows the result of asking for five clusters using the five best SVD sums. The confusion matrix below compares the actual party of each member to the cluster memberships.\n\nvoters <- voters %>% mutate(name = Votes_wide$name) %>% left_join(Parties, by = c(\"name\" = \"name\")) mosaic::tally(party ~ .cluster, data = voters)\n\n.cluster party 1 2 3 4 5 Member for Falkirk West 0 1 0 0 0 Scottish Conservative and Unionist Party 0 0 0 20 0 Scottish Green Party 0 1 0 0 0 Scottish Labour 0 1 0 0 57 Scottish Liberal Democrats 0 16 0 0 1 Scottish National Party 26 0 10 0 0 Scottish Socialist Party 0 1 0 0 0\n\nHow well did the clustering algorithm do? The party affiliation of each member of parliament is known, even though it wasn’t used in finding the clusters. Cluster 1 consists of most of the members of the Scottish National Party (SNP). Cluster 2 includes a number of individuals plus all but one of the Scottish Liberal Democrats. Cluster 3 picks up the remaining 10 members of the SNP. Cluster 4 includes all of the members of the Scottish Conservative and Unionist Party, while Cluster 5 accounts for all but one member of the Scottish Labour party. For most parties, the large majority of members were placed into a unique cluster for that party with a small smattering of other like-voting colleagues. In other words, the technique has identified correctly nearly all of the members of the four different parties with significant representation (i.e., Conservative and Unionist, Labour, Liberal Democrats, and National).\n\nballots <- vote_svd %>% tidy(matrix = \"v\") %>% filter(PC < num_clusters) %>% mutate(PC = paste0(\"pc_\", PC)) %>% pivot_wider(names_from = PC, values_from = value) %>% select(-column) clust_ballots <- kmeans(ballots, centers = num_clusters) ballots <- clust_ballots %>% augment(ballots) %>% mutate(bill = names(select(Votes_wide, -name)))\n\nggplot(data = ballots, aes(x = pc_1, y = pc_2)) + geom_point(aes(x = 0, y = 0), color = \"red\", shape = 1, size = 7) + geom_point(size = 5, alpha = 0.6, aes(color = .cluster)) + xlab(\"Best Vector from SVD\") + ylab(\"Second Best Vector from SVD\") + ggtitle(\"Influential Ballots\") + scale_color_brewer(palette = \"Set2\")\n\nThere is more information to be extracted from the ballot data. Just as there are clusters of political positions, there are clusters of ballots that might correspond to such factors as social effect, economic effect, etc. Figure 12.9 shows the position of ballots, using the first two principal components.\n\nThere are obvious clusters in this figure. Still, interpretation can be tricky. Remember that, on each issue, there are both “aye” and “nay” votes. This accounts for the symmetry of the dots around the center (indicated in red). The opposing dots along each angle from the center might be interpreted in terms of socially liberal versus socially conservative and economically liberal versus economically conservative. Deciding which is which likely involves reading the bill itself, as well as a nuanced understanding of Scottish politics.\n\nFinally, the principal components can be used to rearrange members of parliament and separately rearrange ballots while maintaining each person’s vote. This amounts simply to re-ordering the members in a way other than alphabetical and similarly with the ballots. Such a transformation can bring dramatic clarity to the appearance of the data—as shown in Figure 12.10—where the large, nearly equally sized, and opposing voting blocs of the two major political parties (the National and Labour parties) become obvious. Alliances among the smaller political parties muddy the waters on the lower half of Figure 12.10.\n\nVotes_svd <- Votes %>% mutate(Vote = factor(vote, labels = c(\"Nay\", \"Abstain\", \"Aye\"))) %>% inner_join(ballots, by = \"bill\") %>% inner_join(voters, by = \"name\") ggplot(data = Votes_svd, aes(x = reorder(bill, pc_1.x), y = reorder(name, pc_1.y), fill = Vote)) + geom_tile() + xlab(\"Ballot\") + ylab(\"Member of Parliament\") + scale_fill_manual(values = c(\"darkgray\", \"white\", \"goldenrod\")) + scale_x_discrete(breaks = NULL, labels = NULL) + scale_y_discrete(breaks = NULL, labels = NULL)\n\nThe person represented by the top row in Figure 12.10 is Nicola Sturgeon, the leader of the Scottish National Party. Along the primary vector identified by our SVD, she is the most extreme voter. According to Wikipedia, the National Party belongs to a “mainstream European social democratic tradition.”\n\nVotes_svd %>% arrange(pc_1.y) %>% head(1)\n\nbill name vote Vote pc_1.x pc_2.x pc_3.x pc_4.x 1 S1M-1 Sturgeon, Nicola 1 Aye -0.00391 -0.00167 0.0498 -0.0734 .cluster.x pc_1.y pc_2.y pc_3.y pc_4.y .cluster.y party 1 4 -0.059 0.153 0.0832 0.0396 1 Scottish National Party\n\nConversely, the person at the bottom of Figure 12.10 is Paul Martin, a member of the Scottish Labour Party. It is easy to see in Figure 12.10 that Martin opposed Sturgeon on most ballot votes.\n\nVotes_svd %>% arrange(pc_1.y) %>% tail(1)\n\nbill name vote Vote pc_1.x pc_2.x pc_3.x pc_4.x 103582 S1M-4064 Martin, Paul 1 Aye 0.0322 -0.00484 0.0653 -0.0317 .cluster.x pc_1.y pc_2.y pc_3.y pc_4.y .cluster.y party 103582 4 0.126 0.0267 0.0425 0.056 5 Scottish Labour\n\nThe beauty of Figure 12.10 is that it brings profound order to the chaos apparent in Figure 12.5. This was accomplished by simply ordering the rows (members of Parliament) and the columns (ballots) in a sensible way. In this case, the ordering was determined by the primary vector identified by the SVD of the voting matrix. This is yet another example of how machine learning techniques can identify meaningful patterns in data, but human beings are required to bring domain knowledge to bear on the problem in order to extract meaningful contextual understanding."
    }
}
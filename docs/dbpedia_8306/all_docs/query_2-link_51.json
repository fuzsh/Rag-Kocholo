{
    "id": "dbpedia_8306_2",
    "rank": 51,
    "data": {
        "url": "https://benschmidt.org/HDA/clustering.html",
        "read_more_link": "",
        "language": "en",
        "title": "Humanities Data Analysis",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://benschmidt.org/HDA/Clustering_and_topic_modeling_files/figure-html/kmeans_cities-1.png",
            "https://benschmidt.org/HDA/Clustering_and_topic_modeling_files/figure-html/unnamed-chunk-3-1.png",
            "https://benschmidt.org/HDA/Clustering_and_topic_modeling_files/figure-html/unnamed-chunk-4-1.png",
            "https://benschmidt.org/HDA/Clustering_and_topic_modeling_files/figure-html/SOTU-dendrogram-1.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Ben Schmidt"
        ],
        "publish_date": "2020-04-21T00:00:00",
        "summary": "",
        "meta_description": "Data Analysis using tidy principles for humanists",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "13.1 K-means clustering\n\nThe simplest method is to choose a fixed number of clusters. An especially widely used form of find the best group of 5 (or 10, or whatever) is k-means clustering. K means is an algorithm that starts by defining a random number of centers, and then assigns each point in your dataset to the closest centerâs cluster. It then readjusts each center by making it be the average of all the points in its cluster; and then it checks to see which centers are closest. A nice example video is online here.\n\nThe important thing to know about this algorithm is that it can give a different results based on the cluster centers you start with! So a good use of k-means will use a lot of different starting configurations, as defined by the nstart parameter in the code below.\n\nLetâs go back to our data to see how k-means handles the cities. If we ask it for six clusters, we get centers in California, New Mexico, Oklahoma, Indiana, and North Carolina. These are relatively coherent, but the South is sort of divided up; it might be worth trying it with a different number of clusters instead.\n\nlibrary(broom) cities %>%select(LAT, LON) %>% kmeans(centers = 6, iter.max = 1000, nstart = 50) -> city_clusters city_clusters %>% broom::augment(cities) %>% inner_join(tidy(city_clusters), by = c(\".cluster\"=\"cluster\")) -> cities_with_clusters ggplot(cities_with_clusters) + aes(x = LON, y = LAT, color = .cluster) + geom_point(size = 1) + borders(database = \"state\") + coord_quickmap() + geom_point(aes(y=x1, x = x2), size = 10, pch=18, alpha = 0.2) + geom_segment(aes(xend = x2, yend = x1)) +xlim(-130, -60) + labs(title=\"Kmeans cluster centers\")\n\n## Warning: Removed 1 rows containing missing values (geom_point). ## Warning: Removed 1 rows containing missing values (geom_point).\n\n## Warning: Removed 1 rows containing missing values (geom_segment).\n\nChanging the value of centers gives very different clusterings. There is never a single correct one: you can just choose between a wide variety of them.\n\n13.2 Hierarchical Clustering\n\nAnother method is to build a clustering that works up from the smallest clusters to the largest clusters at once. This tactic is called âhierarchical clusteringâ, and is best visualized using something called a âdendrogramâ (or tree-chart; âdendroâ is Greek for âtreeâ.)\n\nHierarchical clustering is a family methods. They find the two closest points, join them together, pretending theyâre a single point It then finds the next two closest points, and joins them together; and keeps on going until thereâs a single group, encompassing everything.\n\nTo train a hierarchical cluster is a two step process. First we create a hierarchical clustering model using the hclust function. Then, to visualize the model, we use the ggdendro package, and two functions inside of it: dendro_data and **segment**.\n\n# Maybe necessary to run: # install.packages(\"ggdendro\") library(ggdendro) dendrogram = cities %>% column_to_rownames(\"CityST\") %>% select(LON, LAT) %>% dist %>% hclust(method = \"centroid\") %>% dendro_data dendrogram %>% segment %>% ggplot() + geom_segment(aes(xend = xend, yend = yend)) + aes(x = x, y = y) + coord_flip() + scale_y_reverse(expand = c(0.3, 0)) + geom_text(data = dendrogram$labels, aes(label = label), adj=0)\n\nBut you could also do a three-dimensional clustering, that relates both location and population change.\n\nThe scales will be very different on these, so you can use the R function scale to make them all occupy roughly the same range. (Try fiddling around with some numbers to see what it does, or just put in ?scale.)\n\nThat starts to cluster cities together based on things like both location and demographic characteristics. Now the growing cities of San Francisco and San Jose, for instance, are closer to each other than to Oakland: and New Orleans ends up connected to the Rust Belt cities of Detroit and Cleveland because it, too, experienced large population decreases.\n\ncities %>% mutate(LON = scale(LON), LAT = scale(LAT), change = (`2010` / `2000`) %>% log %>% scale %>% as.numeric) %>% column_to_rownames(\"CityST\") %>% select(LON, LAT, change) %>% dist %>% hclust %>% dendro_data -> dendrogram dendrogram %>% segment %>% ggplot() + geom_segment(aes(xend = xend, yend = yend)) + aes(x = x, y = y) + coord_flip() + scale_y_reverse(expand = c(0.2, 0)) + geom_text(data = dendrogram$labels, aes(label = label), adj=0, size=2)\n\nA spatialized version of texts is, just as always, a document vectorization. Letâs look again at State of the Unions. A sensible vector space way to cluster them is based on term vectors; here weâll use a tf_idf version to filter down the impact of extremely common words. (Remember, if you leave in âtheâ and âaâ, they will completely dominate any spatial representation of text.)\n\nremotes::install_github(\"HumanitiesDataAnalysis/HumanitiesDataAnalysis\")\n\n## Skipping install of 'HumanitiesDataAnalysis' from a github remote, the SHA1 (5ce2903f) has not changed since last install. ## Use `force = TRUE` to force installation\n\nsotu = HumanitiesDataAnalysis::get_recent_SOTUs(1960)\n\n## Joining, by = \"year\"\n\ntf = sotu %>% group_by(year, filename, president) %>% summarize_tf_idf(word)\n\n## Joining, by = \"word\"\n\ntf %>% group_by(word) %>% summarize(tot = sum(.tf_idf)) %>% arrange(-tot) %>% # Take the top 100 words by tf_idf slice(1:100) %>% # Join back into the original data frame--this like filtering down. inner_join(tf) %>% select(year, president, word, .tf_idf) %>% mutate(id = paste(president, year)) %>% spread(word, .tf_idf, fill = 0) -> words\n\n## Joining, by = \"word\"\n\nwords %>% column_to_rownames(\"id\") %>% dist %>% hclust %>% dendro_data -> dendrogram\n\n## Warning in dist(.): NAs introduced by coercion\n\ndendrogram %>% segment %>% ggplot() + geom_segment(aes(xend = xend, yend = yend)) + aes(x = x, y = y) + coord_flip() + scale_y_reverse(expand = c(0.2, 0)) + geom_text(data = dendrogram$labels, aes(label = label), adj=0, size=3)\n\nTo make a dendrogram, we once again take the distances and plot a hierarchical clustering.\n\nBy making the labels be the authors, we can see whether the clusters are grouping similar people together. In this case, it turns out to do quite a good job.\n\nBut note that itâs harder in this case to determine whether the same sorts of people are clustered together.\n\nCeglowski, Maciej. 2015. âHaunted by Data.â October 1, 2015. https://idlewords.com/talks/haunted_by_data.htm.\n\nLincoln, Matthew D. 2019. âLittle Package, Big Dependency. Matthew Lincoln, PhD.â January 13, 2019. https://matthewlincoln.net/2019/01/13/little-package-big-dependency.html.\n\nRosenberg, Daniel. 2013. âData Before the Fact.â In Raw Data Is an Oxymoron, edited by Lisa Gitelman. Cambridge: MIT Press.\n\nWilliams, Donald C. 1936. âTokens, Types, Words, and Terms.â The Journal of Philosophy 33 (26): 701â7. https://doi.org/10.2307/2015665."
    }
}
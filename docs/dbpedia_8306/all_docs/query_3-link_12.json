{
    "id": "dbpedia_8306_3",
    "rank": 12,
    "data": {
        "url": "https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/",
        "read_more_link": "",
        "language": "en",
        "title": "Clustering | Different Methods, and Applications (Updated 2024)",
        "top_image": "https://cdn.analyticsvidhya.com/wp-content/uploads/2016/11/Clustering-_-Introduction-Different-Methods-and-Applications-scaled.jpg",
        "meta_img": "https://cdn.analyticsvidhya.com/wp-content/uploads/2016/11/Clustering-_-Introduction-Different-Methods-and-Applications-scaled.jpg",
        "images": [
            "https://av-public-assets.s3.ap-south-1.amazonaws.com/logos/av-logo-svg.svg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/default_avatar.svg",
            "https://secure.gravatar.com/avatar/402d3766d8614483e877af7d0eef2925?s=500",
            "https://av-eks-blogoptimized.s3.amazonaws.com/clustering-2-293x300.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/clustering-2-1.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/clustering-3.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/clustering-4.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/clustering-5.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/clustering-6.png",
            "https://av-eks-blogoptimized.s3.amazonaws.com/clustering-7.png",
            "https://secure.gravatar.com/avatar/402d3766d8614483e877af7d0eef2925?s=500",
            "https://secure.gravatar.com/avatar/2cb7163def359a52a1c99dcd76222b01?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/402d3766d8614483e877af7d0eef2925?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/745b6b797c9a13a1fe6d2ed6425a0885?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/402d3766d8614483e877af7d0eef2925?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/1501a4d4b53fe947a65b93797619efb4?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/402d3766d8614483e877af7d0eef2925?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/1cbb7c857318ac3af69fe2da6f49bfc8?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/402d3766d8614483e877af7d0eef2925?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/5d3ce8eeab86e5f744263f1e74d61354?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/402d3766d8614483e877af7d0eef2925?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/5d3ce8eeab86e5f744263f1e74d61354?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/402d3766d8614483e877af7d0eef2925?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/e7b2b929c604fe5b9e2bd1bc6c9119f6?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/402d3766d8614483e877af7d0eef2925?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/178511aba457d28eaf878c25ddd7a9c0?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/ba815411c4c8f48cfcb70689be8b9241?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/eb00407b6895461c67ffd0d30be548b4?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/ed86e8afbff6370f0abb3a33b7cf6cc9?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/402d3766d8614483e877af7d0eef2925?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/ac84fe94b6a5822d0537c18532110207?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/cfbf728d1357fa1e7f2b5c5ffe7feb2e?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/42d9fe171dd0beb6a84850da86284698?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/5a0d13418c52af7326ec1362a95106dc?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/669266420f62eaef612c6cc9e150c451?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/2e33d4ebec0f763b44ffd5f4f557532e?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/376f68ae1eba1045e6d041f2f47946f0?s=74&d=mm&r=g",
            "https://secure.gravatar.com/avatar/d5e5533f7a7607ab193a6ec27b73af5c?s=74&d=mm&r=g",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/path-digital.png",
            "https://av-identity.s3.amazonaws.com/users/user/bGnsep7nT0GMWuLpkDl15Q.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/R7HrsWl1QrGRiw_e9m4fDA.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/ZcU4ALTFT96MVCzfiGuhsQ.jpeg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/aM3WrxdNSTGLg7LoqX-q0w.png",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/zy4FL_yyQlG4PkWcyGYvhw.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/a4ByfUyoQRmdGzLpBzHVLw.jpeg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/ZTsmKl-1Qvqn07FUzgaBNw.png",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://d2cd20fxv8fgim.cloudfront.net/homepage/images/Play_Store.svg",
            "https://d2cd20fxv8fgim.cloudfront.net/homepage/images/App_Store.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Sauravkaushik8 Kaushik",
            "www.facebook.com"
        ],
        "publish_date": "2016-11-03T04:07:20+00:00",
        "summary": "",
        "meta_description": "Clustering in machine learning is used to group similar objects. Learn more about the different clustering methods and their applications.",
        "meta_lang": "en",
        "meta_favicon": "https://imgcdn.analyticsvidhya.com/favicon/av-fav.ico",
        "meta_site_name": "Analytics Vidhya",
        "canonical_link": "https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/",
        "text": "Introduction\n\nWhen encountering an unsupervised learning problem initially, confusion may arise as you aren’t seeking specific insights but rather identifying data structures. This process, known as clustering or cluster analysis, identifies similar groups within a dataset.\n\nIt is one of the most popular clustering techniques in data science used by data scientists. Entities in each group are comparatively more similar to entities of that group than those of the other groups. In this article, I will be taking you through the types of clustering, different clustering algorithms, and a comparison between two of the most commonly used methods of clustering in machine learning.\n\nAlso, in this article you will get to know about the clustering methods, about the data clustering and at the end of this article you will get to know clustering in machine learning.\n\nNote: To learn more about clustering and other machine learning algorithms (both supervised and unsupervised) check out the following courses-\n\nApplied Machine Learning Course\n\nCertified AI & ML Blackbelt+ Program\n\nLearning Objectives\n\nLearn about Clustering in machine learning, one of the most popular unsupervised classification techniques.\n\nGet to know K means and hierarchical clustering and the difference between the two.\n\nWhat Is Clustering in Machine Learning?\n\nClustering in machine learning is the task of dividing the unlabeled data or data points into different clusters such that similar data points fall in the same cluster than those which differ from the others. In simple words, the aim of the clustering process is to segregate groups with similar traits and assign them into clusters.\n\nLet’s understand this with an example. Suppose you are the head of a rental store and wish to understand the preferences of your customers to scale up your business. Is it possible for you to look at the details of each customer and devise a unique business strategy for each one of them? Definitely not. But, what you can do is cluster all of your customers into, say 10 groups based on their purchasing habits and use a separate strategy for customers in each of these 10 groups. And this is what we call clustering methods.\n\nNow that we understand what clustering is. Let’s take a look at its different types.\n\nTypes of Clustering in Machine Learning\n\nClustering broadly divides into two subgroups:\n\nHard Clustering: Each input data point either fully belongs to a cluster or not. For instance, in the example above, every customer is assigned to one group out of the ten.\n\nSoft Clustering: Rather than assigning each input data point to a distinct cluster, it assigns a probability or likelihood of the data point being in those clusters. For example, in the given scenario, each customer receives a probability of being in any of the ten retail store clusters.\n\nDifferent Types of Clustering Algorithms\n\nSince the task of clustering methods is subjective, the means that can be used for achieving this goal are plenty. Every methodology follows a different set of rules for defining the ‘similarity’ among data points. In fact, there are more than 100 clustering algorithms known. But few of the algorithms are used popularly. Let’s look at them in detail:\n\nConnectivity Models\n\nAs the name suggests, these models are based on the notion that the data points closer in data space exhibit more similarity to each other than the data points lying farther away. These models can follow two approaches. In the first approach, they start by classifying all data points into separate clusters & then aggregating them as the distance decreases. In the second approach, all data points are classified as a single cluster and then partitioned as the distance increases. Also, the choice of distance function is subjective. These models are very easy to interpret but lack scalability for handling big datasets. Examples of these models are the hierarchical clustering algorithms and their variants.\n\nCentroid Models\n\nThese clustering algorithms iterate, deriving similarity from the proximity of a data point to the centroid or cluster center. The k-Means clustering algorithm, a popular example, falls into this category. These models necessitate specifying the number of clusters beforehand, requiring prior knowledge of the dataset. They iteratively run to discover local optima.\n\nDistribution Models\n\nThese clustering models are based on the notion of how probable it is that all data points in the cluster belong to the same distribution (For example: Normal, Gaussian). These models often suffer from overfitting. A popular example of these models is the Expectation-maximization algorithm which uses multivariate normal distributions.\n\nDensity Models\n\nThese models search the data space for areas of the varied density of data points in the data space. They isolate different dense regions and assign the data points within these regions to the same cluster. Popular examples of density models are DBSCAN and OPTICS. These models are particularly useful for identifying clusters of arbitrary shape and detecting outliers, as they can detect and separate points that are located in sparse regions of the data space, as well as points that belong to dense regions.\n\nNow I will be taking you through two of the most popular clustering algorithms in detail – K Means and Hierarchical. Let’s begin.\n\nK Means Clustering\n\nK means is an iterative clustering algorithm that aims to find local maxima in each iteration. This algorithm works in these 5 steps:\n\nStep1:\n\nSpecify the desired number of clusters K: Let us choose k=2 for these 5 data points in 2-D space.\n\nStep 2:\n\nRandomly assign each data point to a cluster: Let’s assign three points in cluster 1, shown using red color, and two points in cluster 2, shown using grey color.\n\nStep 3:\n\nCompute cluster centroids: The centroid of data points in the red cluster is shown using the red cross, and those in the grey cluster using a grey cross.\n\nStep 4:\n\nRe-assign each point to the closest cluster centroid: Note that only the data point at the bottom is assigned to the red cluster, even though it’s closer to the centroid of the grey cluster. Thus, we assign that data point to the grey cluster.\n\nStep 5:\n\nRe-compute cluster centroids: Now, re-computing the centroids for both clusters.\n\nRepeat steps 4 and 5 until no improvements are possible: Similarly, we’ll repeat the 4th and 5th steps until we’ll reach global optima, i.e., when there is no further switching of data points between two clusters for two successive repeats. It will mark the termination of the algorithm if not explicitly mentioned.\n\nHere is a live coding window where you can try out K Means Algorithm using the scikit-learn library.\n\nHierarchical Clustering\n\nHierarchical clustering methods, as the name suggests, is an algorithm that builds a hierarchy of clusters. This algorithm starts with all the data points assigned to a cluster of their own. Then two nearest clusters are merged into the same cluster. In the end, this algorithm terminates when there is only a single cluster left.\n\nThe results of hierarchical clustering can be shown using a dendrogram. The dendrogram can be interpreted as:\n\nAt the bottom, we start with 25 data points, each assigned to separate clusters. The two closest clusters are then merged till we have just one cluster at the top. The height in the dendrogram at which two clusters are merged represents the distance between two clusters in the data space.\n\nThe decision of the no. of clusters that can best depict different groups can be chosen by observing the dendrogram. The best choice of the no. of clusters is the no. of vertical lines in the dendrogram cut by a horizontal line that can transverse the maximum distance vertically without intersecting a cluster.\n\nIn the above example, the best choice of no. of clusters will be 4 as the red horizontal line in the dendrogram below covers the maximum vertical distance AB.\n\nImportant Points for Hierarchical Clustering\n\nThis algorithm has been implemented above using a bottom-up approach. It is also possible to follow a top-down approach starting with all data points assigned in the same cluster and recursively performing splits till each data point is assigned a separate cluster.\n\nThe decision to merge two clusters is taken on the basis of the closeness of these clusters. There are multiple metrics for deciding the closeness of two clusters:\n\nEuclidean distance: ||a-b||2 = √(Σ(ai-bi))\n\nSquared Euclidean distance: ||a-b||22 = Σ((ai-bi)2)\n\nManhattan distance: ||a-b||1 = Σ|ai-bi|\n\nMaximum distance:||a-b||INFINITY = maxi|ai-bi|\n\nMahalanobis distance: √((a-b)T S-1 (-b)) {where, s : covariance matrix}\n\nDifference Between K Means and Hierarchical Clustering\n\nHierarchical clustering methods can’t handle big data well, but K Means can. This is because the time complexity of K Means is linear, i.e., O(n), while that of hierarchical is quadratic, i.e., O(n2).\n\nSince we start with a random choice of clusters, the results produced by running the algorithm multiple times might differ in K Means clustering. While in Hierarchical clustering, the results are reproducible.\n\nK Means is found to work well when the shape of the clusters is hyperspherical (like a circle in 2D or a sphere in 3D).\n\nK Means clustering requires prior knowledge of K, i.e., no. of clusters you want to divide your data into. But, you can stop at whatever number of clusters you find appropriate in hierarchical clustering by interpreting the dendrogram.\n\nApplications of Clustering\n\nClustering has a large no. of application of clustering spread across various domains. Some of the most popular applications of clustering are recommendation engines, market segmentation, social network analysis, search result grouping, medical imaging, image segmentation, and anomaly detection.\n\nImproving Supervised Learning Algorithms With Clustering\n\nClustering is an unsupervised machine learning approach, but can it be used to improve the accuracy of supervised machine learning algorithms as well by clustering the data points into similar groups and using these cluster labels as independent variables in the supervised machine learning algorithm? Let’s find out.\n\nLet’s check out the impact of clustering on the accuracy of our model for the classification problem using 3000 observations with 100 predictors of stock data to predict whether the stock will go up or down using R. This dataset contains 100 independent variables from X1 to X100 representing the profile of a stock and one outcome variable Y with two levels: 1 for the rise in stock price and -1 for drop in stock price.\n\nLet’s first try applying random forest without clustering in python.\n\n#loading required libraries\n\nlibrary('randomForest') library('Metrics') #set random seedset.seed(101) #loading dataset data<-read.csv(\"train.csv\",stringsAsFactors= T) #checking dimensions of datadim(data) ## [1] 3000 101 #specifying outcome variable as factor data$Y<-as.factor(data$Y) #dividing the dataset into train and testtrain<-data[1:2000,] test<-data[2001:3000,] #applying randomForest model_rf<-randomForest(Y~.,data=train) preds<-predict(object=model_rf,test[,-101]) table(preds) ## preds ## -1 1 ## 453 547 #checking accuracy auc(preds,test$Y) ## [1] 0.4522703\n\nSo, the accuracy we get is 0.45. Now let’s create five clusters based on values of independent variables using k-means and reapply random forest.\n\n#combing test and train all<-rbind(train,test) #creating 5 clusters using K- means clustering Cluster <- kmeans(all[,-101], 5) #adding clusters as independent variable to the dataset.all$cluster<-as.factor(Cluster$cluster) #dividing the dataset into train and testtrain<-all[1:2000,] test<-all[2001:3000,] #applying randomforestmodel_rf<-randomForest(Y~.,data=train) preds2<-predict(object=model_rf,test[,-101]) table(preds2) ## preds2 ## -1 1 ##548 452 auc(preds2,test$Y) ## [1] 0.5345908\n\nWhoo! In the above example, even though the final accuracy is poor but clustering has given our model a significant boost from an accuracy of 0.45 to slightly above 0.53.\n\nThis shows that clustering can indeed be helpful for supervised machine-learning tasks.\n\nConclusion\n\nIn this article, we have discussed the various ways of performing clustering. We came across application of clustering for unsupervised learning in a large no. of domains and also saw how to improve the accuracy of a supervised machine learning algorithm using clustering.\n\nAlthough clustering is easy to implement, you need to take care of some important aspects, like treating outliers in your data and making sure each cluster has a sufficient population. These aspects of clustering are dealt with in great detail in this article.\n\nHope you like the article and know you have clear about the clustering methods, data clustering and you get a clear understanding about the clustering in machine learning.\n\nKey Takeaways"
    }
}
{
    "id": "dbpedia_8306_0",
    "rank": 30,
    "data": {
        "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters",
        "read_more_link": "",
        "language": "en",
        "title": "Creating a private cluster",
        "top_image": "https://cloud.google.com/_static/cloud/images/social-icon-google-cloud-1200-630.png",
        "meta_img": "https://cloud.google.com/_static/cloud/images/social-icon-google-cloud-1200-630.png",
        "images": [
            "https://www.gstatic.com/devrel-devsite/prod/v20ab951cf37b43fc7a428ae75ce91d8269f391204ca16525bc8a5ececea0ab56/cloud/images/cloud-logo.svg",
            "https://www.gstatic.com/devrel-devsite/prod/v20ab951cf37b43fc7a428ae75ce91d8269f391204ca16525bc8a5ececea0ab56/cloud/images/cloud-logo.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "https://www.gstatic.com/devrel-devsite/prod/v20ab951cf37b43fc7a428ae75ce91d8269f391204ca16525bc8a5ececea0ab56/cloud/images/favicons/onecloud/favicon.ico",
        "meta_site_name": "Google Cloud",
        "canonical_link": "https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters",
        "text": "Stay organized with collections Save and categorize content based on your preferences.\n\nThis page explains how to create a private Google Kubernetes Engine (GKE) cluster, which is a type of VPC-native cluster. In a private cluster, nodes only have internal IP addresses, which means that nodes and Pods are isolated from the internet by default. You may choose to have no client access, limited access, or unrestricted access to the control plane.\n\nYou cannot convert an existing, non-private cluster to a private cluster. To learn more about how private clusters work, see Overview of private clusters.\n\nRestrictions and limitations\n\nPrivate clusters must be VPC-native clusters. VPC-native clusters don't support legacy networks.\n\nExpand the following sections to view the rules around IP address ranges and traffic when creating a private cluster.\n\nControl plane\n\nWhen you use 172.17.0.0/16 for your control plane IP range, you cannot use this range for nodes, Pods, or Services IP addresses.\n\nThe size of the RFC 1918 block for the cluster control plane must be /28.\n\nYou can add up to 50 authorized networks (allowed CIDR blocks) in a project. For more information, refer to Add an authorized network to an existing cluster.\n\nWhile GKE can detect overlap with the control plane address block, it cannot detect overlap within a Shared VPC network.\n\nCluster networking\n\nInternal IP addresses for nodes come from the primary IP address range of the subnet you choose for the cluster. Pod IP addresses and Service IP addresses come from two subnet secondary IP address ranges of that same subnet. For more information, see IP ranges for VPC-native clusters.\n\nGKE versions 1.14.2 and later support any internal IP address ranges, including private ranges (RFC 1918 and other private ranges) and privately used external IP address ranges. See the VPC documentation for a list of valid internal IP address ranges.\n\nIf you expand the primary IP range of a subnet to accommodate additional nodes, then you must add the expanded subnet's primary IP address range to the list of authorized networks for your cluster. If you don't, ingress-allow firewall rules relevant to the control plane aren't updated, and new nodes created in the expanded IP address space won't be able to register with the control plane. This can lead to an outage where new nodes are continuously deleted and replaced. Such an outage can happen when performing node pool upgrades or when nodes are automatically replaced due to liveness probe failures.\n\nAll nodes in a private cluster are created without an external IP; they have limited access to Google Cloud APIs and services. To provide outbound internet access for your private nodes, you can use Cloud NAT.\n\nPrivate Google Access is enabled automatically when you create a private cluster unless you are using Shared VPC. You must not disable Private Google Access unless you are using NAT to access the internet.\n\nBefore you begin\n\nBefore you start, make sure you have performed the following tasks:\n\nEnable the Google Kubernetes Engine API.\n\nEnable Google Kubernetes Engine API\n\nIf you want to use the Google Cloud CLI for this task, install and then initialize the gcloud CLI. If you previously installed the gcloud CLI, get the latest version by running gcloud components update.\n\nEnsure you have the correct permission to create clusters. At minimum, you should be a Kubernetes Engine Cluster Admin.\n\nEnsure you have a route to the Default Internet Gateway.\n\nCreating a private cluster with no client access to the public endpoint\n\nIn this section, you create the following resources:\n\nA private cluster named private-cluster-0 that has private nodes, and that has no client access to the public endpoint.\n\nA network named my-net-0.\n\nA subnet named my-subnet-0.\n\nConsole\n\nCreate a network and subnet\n\nGo to the VPC networks page in the Google Cloud console.\n\nGo to VPC networks\n\nClick add_box Create VPC network.\n\nFor Name, enter my-net-0.\n\nFor Subnet creation mode, select Custom.\n\nIn the New subnet section, for Name, enter my-subnet-0.\n\nIn the Region list, select the region that you want.\n\nFor IP address range, enter 10.2.204.0/22.\n\nSet Private Google Access to On.\n\nClick Done.\n\nClick Create.\n\nCreate a private cluster\n\nGo to the Google Kubernetes Engine page in the Google Cloud console.\n\nGo to Google Kubernetes Engine\n\nClick add_box Create then in the Standard or Autopilot section, click Configure.\n\nFor the Name, specify private-cluster-0.\n\nIn the navigation pane, click Networking.\n\nIn the Network list, select my-net-0.\n\nIn the Node subnet list, select my-subnet-0.\n\nSelect the Private cluster radio button.\n\nClear the Access control plane using its external IP address checkbox.\n\n(Optional for Autopilot): Set Control plane IP range to 172.16.0.32/28.\n\nClick Create.\n\ngcloud\n\nFor Autopilot clusters, run the following command:\n\ngcloud container clusters create-auto private-cluster-0 \\ --create-subnetwork name=my-subnet-0 \\ --enable-master-authorized-networks \\ --enable-private-nodes \\ --enable-private-endpoint\n\nFor Standard clusters, run the following command:\n\ngcloud container clusters create private-cluster-0 \\ --create-subnetwork name=my-subnet-0 \\ --enable-master-authorized-networks \\ --enable-ip-alias \\ --enable-private-nodes \\ --enable-private-endpoint \\ --master-ipv4-cidr 172.16.0.32/28\n\nwhere:\n\n--create-subnetwork name=my-subnet-0 causes GKE to automatically create a subnet named my-subnet-0.\n\n--enable-master-authorized-networks specifies that access to the public endpoint is restricted to IP address ranges that you authorize.\n\n--enable-ip-alias makes the cluster VPC-native (not required for Autopilot).\n\n--enable-private-nodes indicates that the cluster's nodes don't have external IP addresses.\n\n--enable-private-endpoint indicates that the cluster is managed using the internal IP address of the control plane API endpoint.\n\n--master-ipv4-cidr 172.16.0.32/28 specifies an internal IP address range for the control plane (optional for Autopilot). This setting is permanent for this cluster and must be unique within the VPC. The use of non RFC 1918 internal IP addresses is supported.\n\nAPI\n\nTo create a cluster without a publicly-reachable control plane, specify the enablePrivateEndpoint: true field in the privateClusterConfig resource.\n\nAt this point, these are the only IP addresses that have access to the control plane:\n\nThe primary range of my-subnet-0.\n\nThe secondary range used for Pods.\n\nFor example, suppose you created a VM in the primary range of my-subnet-0. Then on that VM, you could configure kubectl to use the internal IP address of the control plane.\n\nIf you want to access the control plane from outside my-subnet-0, you must authorize at least one address range to have access to the private endpoint.\n\nSuppose you have a VM that is in the default network, in the same region as your cluster, but not in my-subnet-0.\n\nFor example:\n\nmy-subnet-0: 10.0.0.0/22\n\nPod secondary range: 10.52.0.0/14\n\nVM address: 10.128.0.3\n\nYou could authorize the VM to access the control plane by using this command:\n\ngcloud container clusters update private-cluster-0 \\ --enable-master-authorized-networks \\ --master-authorized-networks 10.128.0.3/32\n\nCreating a private cluster with limited access to the public endpoint\n\nWhen creating a private cluster using this configuration, you can choose to use an automatically generated subnet, or a custom subnet.\n\nUsing an automatically generated subnet\n\nIn this section, you create a private cluster named private-cluster-1 where GKE automatically generates a subnet for your cluster nodes. The subnet has Private Google Access enabled. In the subnet, GKE automatically creates two secondary ranges: one for Pods and one for Services.\n\nYou can use the Google Cloud CLI or the GKE API.\n\ngcloud\n\nFor Autopilot clusters, run the following command:\n\ngcloud container clusters create-auto private-cluster-1 \\ --create-subnetwork name=my-subnet-1 \\ --enable-master-authorized-networks \\ --enable-private-nodes\n\nFor Standard clusters, run the following command:\n\ngcloud container clusters create private-cluster-1 \\ --create-subnetwork name=my-subnet-1 \\ --enable-master-authorized-networks \\ --enable-ip-alias \\ --enable-private-nodes \\ --master-ipv4-cidr 172.16.0.0/28\n\nwhere:\n\n--create-subnetwork name=my-subnet-1 causes GKE to automatically create a subnet named my-subnet-1.\n\n--enable-master-authorized-networks specifies that access to the public endpoint is restricted to IP address ranges that you authorize.\n\n--enable-ip-alias makes the cluster VPC-native (not required for Autopilot).\n\n--enable-private-nodes indicates that the cluster's nodes don't have external IP addresses.\n\n--master-ipv4-cidr 172.16.0.0/28 specifies an internal IP address range for the control plane (optional for Autopilot). This setting is permanent for this cluster and must be unique within the VPC. The use of non RFC 1918 internal IP addresses is supported.\n\nAPI\n\nSpecify the privateClusterConfig field in the Cluster API resource:\n\n{ \"name\": \"private-cluster-1\", ... \"ipAllocationPolicy\": { \"createSubnetwork\": true, }, ... \"privateClusterConfig\" { \"enablePrivateNodes\": boolean # Creates nodes with internal IP addresses only \"enablePrivateEndpoint\": boolean # false creates a cluster control plane with a publicly-reachable endpoint \"masterIpv4CidrBlock\": string # CIDR block for the cluster control plane \"privateEndpoint\": string # Output only \"publicEndpoint\": string # Output only } }\n\nAt this point, these are the only IP addresses that have access to the cluster control plane:\n\nThe primary range of my-subnet-1.\n\nThe secondary range used for Pods.\n\nSuppose you have a group of machines, outside of your VPC network, that have addresses in the range 203.0.113.0/29. You could authorize those machines to access the public endpoint by entering this command:\n\ngcloud container clusters update private-cluster-1 \\ --enable-master-authorized-networks \\ --master-authorized-networks 203.0.113.0/29\n\nNow these are the only IP addresses that have access to the control plane:\n\nThe primary range of my-subnet-1.\n\nThe secondary range used for Pods.\n\nAddress ranges that you have authorized, for example, 203.0.113.0/29.\n\nUsing a custom subnet\n\nIn this section, you create the following resources:\n\nA private cluster named private-cluster-2.\n\nA network named my-net-2.\n\nA subnet named my-subnet-2, with primary range 192.168.0.0/20, for your cluster nodes. Your subnet has the following secondary address ranges:\n\nmy-pods for the Pod IP addresses.\n\nmy-services for the Service IP addresses.\n\nConsole\n\nCreate a network, subnet, and secondary ranges\n\nGo to the VPC networks page in the Google Cloud console.\n\nGo to VPC networks\n\nClick add_box Create VPC network.\n\nFor Name, enter my-net-2.\n\nFor Subnet creation mode , select Custom.\n\nIn the New subnet section, for Name, enter my-subnet-2.\n\nIn the Region list, select the region that you want.\n\nFor IP address range, enter 192.168.0.0/20.\n\nClick Create secondary IP range. For Subnet range name, enter my-services, and for Secondary IP range, enter 10.0.32.0/20.\n\nClick Add IP range. For Subnet range name, enter my-pods, and for Secondary IP range, enter 10.4.0.0/14.\n\nSet Private Google Access to On.\n\nClick Done.\n\nClick Create.\n\nCreate a private cluster\n\nCreate a private cluster that uses your subnet:\n\nGo to the Google Kubernetes Engine page in the Google Cloud console.\n\nGo to Google Kubernetes Engine\n\nClick add_box Create then in the Standard or Autopilot section, click Configure.\n\nFor the Name, enter private-cluster-2.\n\nFrom the navigation pane, click Networking.\n\nSelect the Private cluster radio button.\n\nTo create a control plane that is accessible from authorized external IP ranges, keep the Access control plane using its external IP address checkbox selected.\n\n(Optional for Autopilot) Set Control plane IP range to 172.16.0.16/28.\n\nIn the Network list, select my-net-2.\n\nIn the Node subnet list, select my-subnet-2.\n\nClear the Automatically create secondary ranges checkbox.\n\nIn the Pod secondary CIDR range list, select my-pods.\n\nIn the Services secondary CIDR range list, select my-services.\n\nSelect the Enable control plane authorized networks checkbox.\n\nClick Create.\n\ngcloud\n\nCreate a network\n\nFirst, create a network for your cluster. The following command creates a network, my-net-2:\n\ngcloud compute networks create my-net-2 \\ --subnet-mode custom\n\nCreate a subnet and secondary ranges\n\nNext, create a subnet, my-subnet-2, in the my-net-2 network, with secondary ranges my-pods for Pods and my-services for Services:\n\ngcloud compute networks subnets create my-subnet-2 \\ --network my-net-2 \\ --range 192.168.0.0/20 \\ --secondary-range my-pods=10.4.0.0/14,my-services=10.0.32.0/20 \\ --enable-private-ip-google-access\n\nCreate a private cluster\n\nNow, create a private cluster, private-cluster-2, using the network, subnet, and secondary ranges you created.\n\nFor Autopilot clusters, run the following command:\n\ngcloud container clusters create-auto private-cluster-2 \\ --enable-master-authorized-networks \\ --network my-net-2 \\ --subnetwork my-subnet-2 \\ --cluster-secondary-range-name my-pods \\ --services-secondary-range-name my-services \\ --enable-private-nodes\n\nFor Standard clusters, run the following command:\n\ngcloud container clusters create private-cluster-2 \\ --enable-master-authorized-networks \\ --network my-net-2 \\ --subnetwork my-subnet-2 \\ --cluster-secondary-range-name my-pods \\ --services-secondary-range-name my-services \\ --enable-private-nodes \\ --enable-ip-alias \\ --master-ipv4-cidr 172.16.0.16/28 \\ --no-enable-basic-auth \\ --no-issue-client-certificate\n\nAt this point, these are the only IP addresses that have access to the control plane:\n\nThe primary range of my-subnet-2.\n\nThe secondary range my-pods.\n\nSuppose you have a group of machines, outside of my-net-2, that have addresses in the range 203.0.113.0/29. You could authorize those machines to access the public endpoint by entering this command:\n\ngcloud container clusters update private-cluster-2 \\ --enable-master-authorized-networks \\ --master-authorized-networks 203.0.113.0/29\n\nAt this point, these are the only IP addresses that have access to the control plane:\n\nThe primary range of my-subnet-2.\n\nThe secondary range my-pods.\n\nAddress ranges that you have authorized, for example, 203.0.113.0/29.\n\nUsing Cloud Shell to access a private cluster\n\nThe private cluster you created in the Using an automatically generated subnet section, private-cluster-1, has a public endpoint and has authorized networks enabled. If you want to use Cloud Shell to access the cluster, you must add the external IP address of your Cloud Shell to the cluster's list of authorized networks.\n\nTo do this:\n\nIn your Cloud Shell command-line window, use dig to find the external IP address of your Cloud Shell:\n\ndig +short myip.opendns.com @resolver1.opendns.com\n\nAdd the external address of your Cloud Shell to your cluster's list of authorized networks:\n\ngcloud container clusters update private-cluster-1 \\ --enable-master-authorized-networks \\ --master-authorized-networks EXISTING_AUTH_NETS,SHELL_IP/32\n\nReplace the following:\n\nEXISTING_AUTH_NETS: the IP addresses of your existing list of authorized networks. You can find your authorized networks in the console or by running the following command:\n\ngcloud container clusters describe private-cluster-1 --format \"flattened(masterAuthorizedNetworksConfig.cidrBlocks[])\"\n\nSHELL_IP: the external IP address of your Cloud Shell.\n\nGet credentials, so that you can use kubectl to access the cluster:\n\ngcloud container clusters get-credentials private-cluster-1 \\ --project=PROJECT_ID \\ --internal-ip\n\nReplace PROJECT_ID with your project ID.\n\nUse kubectl, in Cloud Shell, to access your private cluster:\n\nkubectl get nodes\n\nThe output is similar to the following:\n\nNAME STATUS ROLES AGE VERSION gke-private-cluster-1-default-pool-7d914212-18jv Ready <none> 104m v1.21.5-gke.1302 gke-private-cluster-1-default-pool-7d914212-3d9p Ready <none> 104m v1.21.5-gke.1302 gke-private-cluster-1-default-pool-7d914212-wgqf Ready <none> 104m v1.21.5-gke.1302\n\nCreating a private cluster with unrestricted access to the public endpoint\n\nIn this section, you create a private cluster where any IP address can access the control plane.\n\nConsole\n\nGo to the Google Kubernetes Engine page in the Google Cloud console.\n\nGo to Google Kubernetes Engine\n\nClick add_box Create then in the Standard or Autopilot section, click Configure.\n\nFor the Name, enter private-cluster-3.\n\nIn the navigation pane, click Networking.\n\nSelect the Private cluster option.\n\nKeep the Access control plane using its external IP address checkbox selected.\n\n(Optional for Autopilot) Set Control plane IP range to 172.16.0.32/28.\n\nLeave Network and Node subnet set to default. This causes GKE to generate a subnet for your cluster.\n\nClear the Enable control plane authorized networks checkbox.\n\nClick Create.\n\ngcloud\n\nFor Autopilot clusters, run the following command:\n\ngcloud container clusters create-auto private-cluster-3 \\ --create-subnetwork name=my-subnet-3 \\ --no-enable-master-authorized-networks \\ --enable-private-nodes\n\nFor Standard clusters, run the following command:\n\ngcloud container clusters create private-cluster-3 \\ --create-subnetwork name=my-subnet-3 \\ --no-enable-master-authorized-networks \\ --enable-ip-alias \\ --enable-private-nodes \\ --master-ipv4-cidr 172.16.0.32/28\n\nwhere:\n\n--create-subnetwork name=my-subnet-3 causes GKE to automatically create a subnet named my-subnet-3.\n\n--no-enable-master-authorized-networks disables authorized networks for the cluster.\n\n--enable-ip-alias makes the cluster VPC-native (not required for Autopilot).\n\n--enable-private-nodes indicates that the cluster's nodes don't have external IP addresses.\n\n--master-ipv4-cidr 172.16.0.32/28 specifies an internal IP address range for the control plane (optional for Autopilot). This setting is permanent for this cluster and must be unique within the VPC. The use of non RFC 1918 internal IP addresses is supported.\n\nAdding firewall rules for specific use cases\n\nThis section explains how to add a firewall rule to a private cluster. By default, firewall rules restrict your cluster control plane to only initiate TCP connections to your nodes and Pods on ports 443 (HTTPS) and 10250 (kubelet). For some Kubernetes features, you might need to add firewall rules to allow access on additional ports. Don't create firewall rules or hierarchical firewall policy rules that have a higher priority than the automatically created firewall rules.\n\nKubernetes features that require additional firewall rules include:\n\nAdmission webhooks\n\nAggregated API servers\n\nWebhook conversion\n\nDynamic audit configuration\n\nGenerally, any API that has a ServiceReference field requires additional firewall rules.\n\nAdding a firewall rule allows traffic from the cluster control plane to all of the following:\n\nThe specified port of each node (hostPort).\n\nThe specified port of each Pod running on these nodes.\n\nThe specified port of each Service running on these nodes.\n\nTo learn about firewall rules, refer to Firewall rules in the Cloud Load Balancing documentation.\n\nTo add a firewall rule in a private cluster, you need to record the cluster control plane's CIDR block and the target used. After you have recorded this you can create the rule.\n\nStep 1. View control plane's CIDR block\n\nYou need the cluster control plane's CIDR block to add a firewall rule.\n\nConsole\n\nGo to the Google Kubernetes Engine page in the Google Cloud console.\n\nGo to Google Kubernetes Engine\n\nIn the cluster list, click the cluster name.\n\nIn the Details tab, under Networking, take note of the value in the Control plane address range field.\n\ngcloud\n\nRun the following command:\n\ngcloud container clusters describe CLUSTER_NAME\n\nReplace CLUSTER_NAME with the name of your private cluster.\n\nIn the command output, take note of the value in the masterIpv4CidrBlock field.\n\nStep 2. View existing firewall rules\n\nYou need to specify the target (in this case, the destination nodes) that the cluster's existing firewall rules use.\n\nConsole\n\nGo to the Firewall policies page in the Google Cloud console.\n\nGo to Firewall policies\n\nFor Filter table for VPC firewall rules, enter gke-CLUSTER_NAME.\n\nIn the results, take note of the value in the Targets field.\n\ngcloud\n\nRun the following command:\n\ngcloud compute firewall-rules list \\ --filter 'name~^gke-CLUSTER_NAME' \\ --format 'table( name, network, direction, sourceRanges.list():label=SRC_RANGES, allowed[].map().firewall_rule().list():label=ALLOW, targetTags.list():label=TARGET_TAGS )'\n\nIn the command output, take note of the value in the Targets field.\n\nStep 3. Add a firewall rule\n\nConsole\n\nGo to the Firewall policies page in the Google Cloud console.\n\nGo to Firewall policies\n\nClick add_box Create Firewall Rule.\n\nFor Name, enter the name for the firewall rule.\n\nIn the Network list, select the relevant network.\n\nIn Direction of traffic, click Ingress.\n\nIn Action on match, click Allow.\n\nIn the Targets list, select Specified target tags.\n\nFor Target tags, enter the target value that you noted previously.\n\nIn the Source filter list, select IPv4 ranges.\n\nFor Source IPv4 ranges, enter the cluster control plane's CIDR block.\n\nIn Protocols and ports, click Specified protocols and ports, select the checkbox for the relevant protocol (tcp or udp), and enter the port number in the protocol field.\n\nClick Create.\n\ngcloud\n\nRun the following command:\n\ngcloud compute firewall-rules create FIREWALL_RULE_NAME \\ --action ALLOW \\ --direction INGRESS \\ --source-ranges CONTROL_PLANE_RANGE \\ --rules PROTOCOL:PORT \\ --target-tags TARGET\n\nReplace the following:\n\nFIREWALL_RULE_NAME: the name you choose for the firewall rule.\n\nCONTROL_PLANE_RANGE: the cluster control plane's IP address range (masterIpv4CidrBlock) that you collected previously.\n\nPROTOCOL:PORT: the port and its protocol, tcp or udp.\n\nTARGET: the target (Targets) value that you collected previously.\n\nVerify that nodes don't have external IP addresses\n\nAfter you create a private cluster, verify that the cluster's nodes don't have external IP addresses.\n\nConsole\n\nGo to the Google Kubernetes Engine page in the Google Cloud console.\n\nGo to Google Kubernetes Engine\n\nIn the list of clusters, click the cluster name.\n\nFor Autopilot clusters, in the Cluster basics section, check the External endpoint field. The value is Disabled.\n\nFor Standard clusters, do the following:\n\nOn the Clusters page, click the Nodes tab.\n\nUnder Node Pools, click the node pool name.\n\nOn the Node pool details page, under Instance groups, click the name of your instance group. For example, gke-private-cluster-0-default-pool-5c5add1f-grp`.\n\nIn the list of instances, verify that your instances do not have external IP addresses.\n\ngcloud\n\nRun the following command:\n\nkubectl get nodes --output wide\n\nThe output's EXTERNAL-IP column is empty:\n\nSTATUS ... VERSION EXTERNAL-IP OS-IMAGE ... Ready v.8.7-gke.1 Container-Optimized OS Ready v1.8.7-gke.1 Container-Optimized OS Ready v1.8.7-gke.1 Container-Optimized OS\n\nVerify VPC peering reuse in cluster\n\nAny private clusters you create after January 15, 2020 reuse VPC Network Peering connections.\n\nYou can check if your private cluster reuses VPC Network Peering connections using the gcloud CLI or the Google Cloud console.\n\nConsole\n\nCheck the VPC peering row on the Cluster details page. If your cluster is reusing VPC peering connections, the output begins with gke-n. For example, gke-n34a117b968dee3b2221-93c6-40af-peer.\n\ngcloud\n\ngcloud container clusters describe CLUSTER_NAME \\ --format=\"value(privateClusterConfig.peeringName)\"\n\nIf your cluster is reusing VPC peering connections, the output begins with gke-n. For example, gke-n34a117b968dee3b2221-93c6-40af-peer.\n\nCleaning up\n\nAfter completing the tasks on this page, follow these steps to remove the resources to prevent unwanted charges incurring on your account:\n\nDelete the clusters\n\nConsole\n\nGo to the Google Kubernetes Engine page in the Google Cloud console.\n\nGo to Google Kubernetes Engine\n\nSelect each cluster.\n\nClick delete Delete.\n\ngcloud\n\ngcloud container clusters delete -q private-cluster-0 private-cluster-1 private-cluster-2 private-cluster-3\n\nDelete the network\n\nConsole\n\nGo to the VPC networks page in the Google Cloud console.\n\nGo to VPC networks\n\nIn the list of networks, click my-net-0.\n\nOn the VPC network details page, click delete Delete VPC Network.\n\nIn the Delete a network dialog, click Delete.\n\ngcloud\n\ngcloud compute networks delete my-net-0\n\nWhat's next\n\nLearn advanced configurations with private clusters.\n\nLearn how to create VPC-native clusters.\n\nLearn more about VPC Network Peering.\n\nFollow the tutorial about accessing private GKE clusters with Cloud Build private pools."
    }
}
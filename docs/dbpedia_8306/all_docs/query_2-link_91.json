{
    "id": "dbpedia_8306_2",
    "rank": 91,
    "data": {
        "url": "https://www.ibm.com/topics/apache-spark",
        "read_more_link": "",
        "language": "en",
        "title": "What Is Apache Spark?",
        "top_image": "https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/64/ea/content-hub-analytic-page-leadspace-short.png/_jcr_content/renditions/cq5dam.web.1280.1280.png",
        "meta_img": "https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/64/ea/content-hub-analytic-page-leadspace-short.png/_jcr_content/renditions/cq5dam.web.1280.1280.png",
        "images": [
            "https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/64/ea/content-hub-analytic-page-leadspace-short.component.xl.ts=1715276720020.png/content/adobe-cms/us/en/topics/apache-spark/_jcr_content/root/leadspace"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Spark",
            "Analytics"
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2021-09-22T00:00:00",
        "summary": "",
        "meta_description": "Apache Spark is an open-source data-processing engine for large data sets, designed to deliver the speed, scalability and programmability required for big data.",
        "meta_lang": "en",
        "meta_favicon": "/content/dam/adobe-cms/default-images/favicon.svg",
        "meta_site_name": "",
        "canonical_link": "https://www.ibm.com/topics/apache-spark",
        "text": "Apache Spark has a hierarchical primary/secondary architecture. The Spark Driver is the primary node that controls the cluster manager, which manages the secondary nodes and delivers data results to the application client.\n\nBased on the application code, Spark Driver generates the SparkContext, which works with the cluster manager—Spark’s Standalone Cluster Manager or other cluster managers such as Hadoop YARN, Kubernetes, or Mesos—to distribute and monitor execution across the nodes. It also creates Resilient Distributed Datasets (RDDs), which are the key to Spark’s remarkable processing speed.\n\nResilient Distributed Dataset (RDD)\n\nResilient Distributed Datasets (RDDs) are fault-tolerant collections of elements that can be distributed among multiple nodes in a cluster and worked on in parallel. RDDs are a fundamental structure in Apache Spark.\n\nSpark loads data by referencing a data source or by parallelizing an existing collection with the SparkContext parallelize method of caching data into an RDD for processing. Once data is loaded into an RDD, Spark performs transformations and actions on RDDs in memory—the key to Spark’s speed. Spark also stores the data in memory unless the system runs out of memory or the user decides to write the data to disk for persistence.\n\nEach dataset in an RDD is divided into logical partitions, which may be computed on different nodes of the cluster. And users can perform two types of RDD operations: transformations and actions. Transformations are operations applied to create a new RDD. Actions are used to instruct Apache Spark to apply computation and pass the result back to the driver.\n\nSpark supports a variety of actions and transformations on RDDs. This distribution is done by Spark, so users don’t have to worry about computing the right distribution.\n\nDirected Acyclic Graph (DAG)\n\nAs opposed to the two-stage execution process in MapReduce, Spark creates a Directed Acyclic Graph (DAG) to schedule tasks and the orchestration of worker nodes across the cluster. As Spark acts and transforms data in the task execution processes, the DAG scheduler facilitates efficiency by orchestrating the worker nodes across the cluster. This task-tracking makes fault tolerance possible, as it reapplies the recorded operations to the data from a previous state.\n\nDataFrames and Datasets\n\nIn addition to RDDs, Spark handles two other data types: DataFrames and Datasets.\n\nDataFrames are the most common structured application programming interfaces (APIs) and represent a table of data with rows and columns. Although RDD has been a critical feature to Spark, it is now in maintenance mode. Because of the popularity of Spark’s Machine Learning Library (MLlib), DataFrames have taken on the lead role as the primary API for MLlib (a set of machine learning algorithms for scalability plus tools for feature selection and building ML pipelines). This is important to note when using the MLlib API, as DataFrames provide uniformity across the different languages, such as Scala, Java, Python, and R.\n\nDatasets are an extension of DataFrames that provide a type-safe, object-oriented programming interface. Datasets are, by default, a collection of strongly typed JVM objects, unlike DataFrames.\n\nSpark SQL enables data to be queried from DataFrames and SQL data stores, such as Apache Hive. Spark SQL queries return a DataFrame or Dataset when they are run within another language.\n\nSpark Core\n\nSpark Core is the base for all parallel data processing and handles scheduling, optimization, RDD, and data abstraction. Spark Core provides the functional foundation for the Spark libraries, Spark SQL, Spark Streaming, the MLlib machine learning library, and GraphX graph data processing. The Spark Core and cluster manager distribute data across the Spark cluster and abstract it. This distribution and abstraction make handling Big Data very fast and user-friendly.\n\nSpark APIs"
    }
}
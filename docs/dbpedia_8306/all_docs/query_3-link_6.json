{
    "id": "dbpedia_8306_3",
    "rank": 6,
    "data": {
        "url": "https://dataaspirant.com/hierarchical-clustering-algorithm/",
        "read_more_link": "",
        "language": "en",
        "title": "How the Hierarchical Clustering Algorithm Works",
        "top_image": "https://dataaspirant.com/wp-content/uploads/2020/12/1-Hierarchical-Clustering.png",
        "meta_img": "https://dataaspirant.com/wp-content/uploads/2020/12/1-Hierarchical-Clustering.png",
        "images": [
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/1-Hierarchical-Clustering.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/2-What-Is-Unsupervised-Learning.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/3-What-Is-Clustering.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/5-Clustering-Vs-Classification-Example.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/5-Hierarchical-Clustering-Types-Agglomerative-and-Divisive.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/6-Agglomerative-approach-step-1-928x1024.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/7-Agglomerative-approach-step-2-887x1024.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/8-Agglomerative-approach-step-3-989x1024.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/9-Agglomerative-approach-step-4-965x1024.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/10-Simple-Linkage-Method-1024x791.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/11-Complete-Linkage-Method-1024x816.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/12-Average-Linkage-Method-1024x881.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/13-Centroid-Linkage-Method-1024x657.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/14-Wards-Linkage-Method-1024x761.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/15-Hierarchical-Clustering-Linkages.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/16-Hierarchical-Clustering-Dendrogram-1024x528.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/18-Input-data-overview-1024x715.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/19-dendrogram.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/20-hierarchical-clustering-result.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/17-Hierarchical-Divisive-Clustering-1024x761.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2023/02/1-1.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2023/02/2-1.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2023/02/3-1.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/1-Support-Vector-Machine-SVM-kernels-150x150.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://dataaspirant.com/wp-content/uploads/2020/12/1-Feature-Selection-Method-150x150.png",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/e39e61e0a34863e4145d3e279fd751ea?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/60658a138a1ef2ab2c4af99e65bcbb6e?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/7728be941b8d097bf744c8410d024580?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/9f367720a243fccbeb22154439c29678?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/8c8e77bea2d486a91856109361a72c01?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/7728be941b8d097bf744c8410d024580?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/6187127f11aa39e2c3ba7373afefe043?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/8c8e77bea2d486a91856109361a72c01?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/7728be941b8d097bf744c8410d024580?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/5111b23b31d36025ee0ae7ad0e3f35e0?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/8c8e77bea2d486a91856109361a72c01?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/7728be941b8d097bf744c8410d024580?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/f463afd1727d31dd879674fdda6aded7?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/8c8e77bea2d486a91856109361a72c01?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/7728be941b8d097bf744c8410d024580?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/b09de6f95d78383825a5bfaeb63e3fef?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/7728be941b8d097bf744c8410d024580?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/d910df509111cdf6ecd1a234f2c1ad3c?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/8c8e77bea2d486a91856109361a72c01?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/7728be941b8d097bf744c8410d024580?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/96771b960f02e856ce6906c69c9e602a?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/96771b960f02e856ce6906c69c9e602a?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/8c8e77bea2d486a91856109361a72c01?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/7728be941b8d097bf744c8410d024580?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/2a616854dc71e1a1367e06dae7cf8c5d?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/2a616854dc71e1a1367e06dae7cf8c5d?s=30&r=g",
            "https://dataaspirant.com/wp-content/plugins/lazy-load/images/1x1.trans.gif",
            "https://secure.gravatar.com/avatar/8c8e77bea2d486a91856109361a72c01?s=30&r=g",
            "https://dataaspirant.com/wp-content/uploads/2017/04/dataaspirant_award.png",
            "https://dataaspirant.com/wp-content/uploads/2017/04/dataaspirant_award.png",
            "https://dataaspirant.com/wp-content/uploads/2020/07/courseraplus-300x160.jpg",
            "https://dataaspirant.com/wp-content/uploads/2020/07/courseraplus-300x160.jpg",
            "https://dataaspirant.com/wp-content/uploads/2023/02/data-science-bootcamp-tyuxcv.webp",
            "https://dataaspirant.com/wp-content/uploads/2023/02/data-science-bootcamp-tyuxcv.webp",
            "https://dataaspirant.com/wp-content/uploads/2023/02/udacity_banner.png",
            "https://dataaspirant.com/wp-content/uploads/2023/02/udacity_banner.png",
            "https://dataaspirant.com/wp-content/uploads/2017/09/Deep_learning_coursera-277x300.jpg",
            "https://dataaspirant.com/wp-content/uploads/2017/09/Deep_learning_coursera-277x300.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Shaik Irfana Sultana"
        ],
        "publish_date": "2020-12-21T14:02:43+05:30",
        "summary": "",
        "meta_description": "Hierarchical Clustering algorithm is an unsupervised Learning Algorithm, and this is one of the most popular clustering technique in Machine Learning. Expectations of getting insights from machine learning algorithms is increasing abruptly. Initially, we were limited to predict the future by feeding historical data. This is easy when the expected results and the features in the historical",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "Dataaspirant - A Data Science Portal For Beginners",
        "canonical_link": "https://dataaspirant.com/hierarchical-clustering-algorithm/",
        "text": "Hierarchical Clustering algorithm is an unsupervised Learning Algorithm, and this is one of the most popular clustering technique in Machine Learning.\n\nExpectations of getting insights from machine learning algorithms is increasing abruptly. Initially, we were limited to predict the future by feeding historical data.\n\nThis is easy when the expected results and the features in the historical data are available to build the supervised learning models, which can predict the future.\n\nFor example predicting the email is spam or not, using the historical email data.\n\nLearn hierarchical clustering algorithm in detail also, learn about agglomeration and divisive way of hierarchical clustering. #clustering #hierarchicalclustering\n\nClick to Tweet\n\nBut the real world problems are not limited to supervised type, and we do get the unsupervised problems too.\n\nHow to build the models for such problems?\n\nWhere comes the unsupervised learning algorithms.\n\nIn this article, we are going to learn one such popular unsupervised learning algorithm which is hierarchical clustering algorithm.\n\nBefore we start learning, Let’s look at the topics you will learn in this article. Only if you read the complete article 🙂\n\nBefore we understand what hierarchical clustering algorithm is, its benefits, and how it works. Let us learn the unsupervised learning algorithm topic.\n\nWhat is Unsupervised Learning\n\nUnsupervised learning is training a machine using information that is neither classified nor labeled and allows the machine to act on that information without guidance.\n\nIn Unsupervised Learning, a machine’s task is to group unsorted information according to similarities, patterns, and differences without any prior data training. It is defined as\n\n“Unsupervised Learning Algorithm is a machine learning technique, where you don’t have to supervise the model. Rather, you need to allow the model to work on its own to discover information, and It mainly deals with unlabelled data.”\n\nIf you want to know more, we would suggest you to read the unsupervised learning algorithms article.\n\nTypes of Unsupervised Learning Algorithm\n\nUnsupervised Learning algorithms are classified into two categories.\n\nClustering Algorithms\n\nAssociation Rule Algorithms\n\nClustering Algorithms: Clustering is a technique of grouping objects into clusters. Objects with the most similarities remain in a group and have less or no similarities with another group’s objects.\n\nAssociation Rule Algorithms: Association rule in unsupervised learning method, which helps in finding the relationships between variables in a large database.\n\nUnsupervised Learning Algorithms\n\nThe list of some popular Unsupervised Learning algorithms are:\n\nK-means Clustering\n\nHierarchical Clustering\n\nPrincipal Component Analysis\n\nApriori Algorithm\n\nAnomaly detection\n\nIndependent Component Analysis\n\nSingular value decomposition\n\nBefore we learn about hierarchical clustering, we need to know about clustering and how it is different from classification.\n\nWhat is Clustering\n\nClustering is an important technique when it comes to the unsupervised learning algorithm. Clustering mainly deals with finding a structure or pattern in a collection of uncategorized data.\n\nIt is a technique that groups similar objects such that objects in the same group are identical to each other than the objects in the other groups. The group of similar objects is called a Cluster.\n\nHow is clustering different from classification?\n\nAs a data science beginner, the difference between clustering and classification is confusing. So as the initial step, let us understand the fundamental difference between classification and clustering.\n\nFor example,\n\nLet us say we have four categories:\n\nDog\n\nCat\n\nShark\n\nGoldfish\n\nIn this scenario, clustering would make 2 clusters. The one who lives on land and the other one lives in water.\n\nSo the entities of the first cluster would be dogs and cats. Similarly, for the second cluster, it would be sharks and goldfishes.\n\nBut in classification, it would classify the four categories into four different classes. One for each category.\n\nSo dogs would be classified under the class dog, and similarly, it would be for the rest.\n\nIn classification, we have labels to tell us and supervise whether the classification is right or not, and that is how we can classify them right. Thus making it a supervised learning algorithm.\n\nBut in clustering, despite distinctions, we cannot classify them because we don’t have labels for them. And that is why clustering is an unsupervised learning algorithm.\n\nIn real life, we can expect high volumes of data without labels. Because of such great use, clustering techniques have many real-time situations to help. Let us understand that.\n\nApplications of Clustering\n\nClustering has a large number of applications spread across various domains. Some of the most popular applications of clustering are:\n\nRecommendation Engines\n\nClustering similar news articles\n\nMedical Imaging\n\nImage Segmentation\n\nAnomaly detection\n\nPattern Recognition\n\nTill now, we got the in depth idea of what is unsupervised learning and its types. We also learned what clustering and various applications of the clustering algorithm.\n\nNow have a look at a detailed explanation of what is hierarchical clustering and why it is used?\n\nWhat is Hierarchical Clustering\n\nHierarchical clustering is one of the popular clustering techniques after K-means Clustering. It is also known as Hierarchical Clustering Analysis (HCA)\n\nWhich is used to group unlabelled datasets into a Cluster. This Hierarchical Clustering technique builds clusters based on the similarity between different objects in the set.\n\nIt goes through the various features of the data points and looks for the similarity between them.\n\nThis process will continue until the dataset has been grouped. Which creates a hierarchy for each of these clusters.\n\nHierarchical Clustering deals with the data in the form of a tree or a well-defined hierarchy.\n\nBecause of this reason, the algorithm is named as a hierarchical clustering algorithm.\n\nThis hierarchy way of clustering can be performed in two ways.\n\nAgglomerative: Hierarchy created from bottom to top.\n\nDivisive: Hierarchy created from top to bottom.\n\nIn the next section of this article, let’s learn about these two ways in detail. For now, the above image gives you a high level of understanding.\n\nIn the early sections of this article, we were given various algorithms to perform the clustering. But how is this hierarchical clustering different from other techniques?\n\nLet’s discuss that.\n\nWhy Hierarchical Clustering\n\nAs we already have some clustering algorithms such as K-Means Clustering, then why do we need Hierarchical Clustering?\n\nAs we have already seen in the K-Means Clustering algorithm article, it uses a pre-specified number of clusters. It requires advanced knowledge of K., i.e., how to define the number of clusters one wants to divide your data.\n\nStill, in hierarchical clustering no need to pre-specify the number of clusters as we did in the K-Means Clustering; one can stop at any number of clusters.\n\nFurthermore, Hierarchical Clustering has an advantage over K-Means Clustering. i.e., it results in an attractive tree-based representation of the observations, called a Dendrogram.\n\nTypes of Hierarchical Clustering\n\nThe Hierarchical Clustering technique has two types.\n\nAgglomerative Hierarchical Clustering\n\nStart with points as individual clusters.\n\nAt each step, it merges the closest pair of clusters until only one cluster ( or K clusters left).\n\nDivisive Hierarchical Clustering\n\nStart with one, all-inclusive cluster.\n\nAt each step, it splits a cluster until each cluster contains a point ( or there are clusters).\n\nAgglomerative Clustering\n\nIt is also known as AGNES ( Agglomerative Nesting) and follows the bottom-up approach.\n\nEach observation starts with its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n\nThat means the algorithm considers each data point as a single cluster initially and then starts combining the closest pair of clusters together.\n\nIt does the same process until all the clusters are merged into a single cluster that contains all the datasets.\n\nHow does Agglomerative Hierarchical Clustering work\n\nLet’s take a sample of data and learn how the agglomerative hierarchical clustering work step by step.\n\nStep 1\n\nFirst, make each data point a “single - cluster,” which forms N clusters. (let’s assume there are N numbers of clusters).\n\nStep 2\n\nTake the next two closest data points and make them one cluster; now, it forms N-1 clusters.\n\nStep 3\n\nAgain, take the two clusters and make them one cluster; now, it forms N-2 clusters.\n\nStep 4\n\nRepeat ‘Step 3’ until you are left with only one cluster.\n\nOnce all the clusters are combined into a big cluster. We develop the Dendrogram to divide the clusters.\n\nFor the divisive hierarchical clustering, it treats all the data points as one cluster and splits the clustering until it creates meaningful clusters.\n\nDifference ways to measure the distance between two clusters\n\nThere are several ways to measure the distance between in order to decide the rules for clustering, and they are often called Linkage Methods.\n\nSome of the popular linkage methods are:\n\nSimple Linkage\n\nComplete Linkage\n\nAverage Linkage\n\nCentroid Linkage\n\nWard’s Linkage\n\nSimple Linkage\n\nSimple Linkage is also known as the Minimum Linkage (MIN) method.\n\nIn the Single Linkage method, the distance of two clusters is defined as the minimum distance between an object (point) in one cluster and an object (point) in the other cluster. This method is also known as the nearest neighbor method.\n\nPros and Cons of Simple Linkage method\n\nPros of Simple Linkage\n\nSimple Linkage methods can handle non-elliptical shapes.\n\nSingle Linkage algorithms are the best for capturing clusters of different sizes.\n\nCons of Simple Linkage\n\nSimple Linkage methods are sensitive to noise and outliers.\n\nThat means Simple Linkage methods can not group clusters properly if there is any noise between the clusters.\n\nComplete Linkage\n\nThe complete Linkage method is also known as the Maximum Linkage (MAX) method.\n\nIn the Complete Linkage technique, the distance between two clusters is defined as the maximum distance between an object (point) in one cluster and an object (point) in the other cluster.\n\nAnd this method is also known as the furthest neighbor method.\n\nPros and Cons of Complete Linkage method\n\nPros of Complete Linkage\n\nComplete Linkage algorithms are less susceptible to noise and outliers.\n\nThat means the Complete Linkage method also does well in separating clusters if there is any noise between the clusters.\n\nCons of Complete Linkage\n\nComplete linkage methods tend to break large clusters.\n\nComplete Linkage is biased towards globular clusters.\n\nAverage Linkage\n\nIn the Average Linkage technique, the distance between two clusters is the average distance between each cluster’s point to every point in the other cluster.\n\nThis method is also known as the unweighted pair group method with arithmetic mean.\n\nPros and Cons of the Average Linkage method\n\nPros of Average Linkage\n\nThe average Linkage method also does well in separating clusters if there is any noise between the clusters.\n\nCons of Average Linkage\n\nThe average Linkage method is biased towards globular clusters.\n\nCentroid Linkage\n\nIn the Centroid Linkage approach, the distance between the two sets or clusters is the distance between two mean vectors of the sets (clusters).\n\nAt each stage, we combine the two sets that have the smallest centroid distance. In simple words, it is the distance between the centroids of the two sets.\n\nPros and Cons of Centroid Linkage method\n\nPros of Centroid Linkage\n\nThe Centroid Linkage method also does well in separating clusters if there is any noise between the clusters.\n\nCons of Centroid Linkage\n\nSimilar to Complete Linkage and Average Linkage methods, the Centroid Linkage method is also biased towards globular clusters.\n\nWard’s Linkage\n\nWard's Linkage method is the similarity of two clusters. Which is based on the increase in squared error when two clusters are merged, and it is similar to the group average if the distance between points is distance squared.\n\nPros and Cons of Ward’s Linkage method\n\nPros of Ward's Linkage\n\nIn many cases, Ward’s Linkage is preferred as it usually produces better cluster hierarchies\n\nWard’s method is less susceptible to noise and outliers.\n\nCons of Ward's Linkage\n\nWard’s linkage method is biased towards globular clusters.\n\nSome of the other linkage methods are:\n\nStrong Linkage\n\nFlexible linkage\n\nSimple Average\n\nThe Linkage method’s choice depends on you, and you can apply any of them according to the type of problem, and different linkage methods lead to different clusters.\n\nBelow is the comparison image, which shows all the linkage methods. We took this reference image from greatlearning platform blog.\n\nHierarchical Clustering algorithms generate clusters that are organized into hierarchical structures.\n\nThese hierarchical structures can be visualized using a tree-like diagram called Dendrogram.\n\nNow let us discuss Dendrogram.\n\nWhat is Dendrogram\n\nA Dendrogram is a diagram that represents the hierarchical relationship between objects. The Dendrogram is used to display the distance between each pair of sequentially merged objects.\n\nThese are commonly used in studying hierarchical clusters before deciding the number of clusters significant to the dataset.\n\nThe distance at which the two clusters combine is referred to as the dendrogram distance.\n\nThe primary use of a dendrogram is to work out the best way to allocate objects to clusters.\n\nThe key point to interpreting or implementing a dendrogram is to focus on the closest objects in the dataset.\n\nHence from the above figure, we can observe that the objects P6 and P5 are very close to each other, merging them into one cluster named C1, and followed by the object P4 is closed to the cluster C1, so combine these into a cluster (C2).\n\nAnd the objects P1 and P2 are close to each other so merge them into one cluster (C3), now cluster C3 is merged with the following object P0 and forms a cluster (C4), the object P3 is merged with the cluster C2, and finally the cluster C2 and C4 and merged into a single cluster (C6).\n\nTill now, we have a clear idea of the Agglomerative Hierarchical Clustering and Dendrograms.\n\nNow let us implement python code for the Agglomerative clustering technique.\n\nAgglomerative Clustering Algorithm Implementation in Python\n\nLet us have a look at how to apply a hierarchical cluster in python on a Mall_Customers dataset.\n\nIf you remembered, we have used the same dataset in the k-means clustering algorithms implementation too.\n\nPlease refer to k-means article for getting the dataset.\n\nImporting the libraries and loading the data\n\nWe are importing all the necessary libraries, then we will load the data.\n\nDendrogram to find the optimal number of clusters\n\nTraining the Hierarchical Clustering model on the dataset\n\nNow, we are training our dataset using Agglomerative Hierarchical Clustering.\n\nAdvantages and Disadvantages of Agglomerative Hierarchical Clustering Algorithm\n\nAdvantages\n\nThe agglomerative technique is easy to implement.\n\nIt can produce an ordering of objects, which may be informative for the display.\n\nIn agglomerative Clustering, there is no need to pre-specify the number of clusters.\n\nBy the Agglomerative Clustering approach, smaller clusters will be created, which may discover similarities in data.\n\nDisadvantages\n\nThe agglomerative technique gives the best result in some cases only.\n\nThe algorithm can never undo what was done previously, which means if the objects may have been incorrectly grouped at an earlier stage, and the same result should be close to ensure it.\n\nThe usage of various distance metrics for measuring distances between the clusters may produce different results. So performing multiple experiments and then comparing the result is recommended to help the actual results’ veracity.\n\nDivisive Hierarchical Clustering\n\nDivisive Hierarchical Clustering is also known as DIANA (Divisive Clustering Analysis.)\n\nIt is a top-down clustering approach. It works as similar as Agglomerative Clustering but in the opposite direction.\n\nThis approach starts with a single cluster containing all objects and then splits the cluster into two least similar clusters based on their characteristics. We proceed with the same process until there is one cluster for each observation.\n\nHere, the divisive approach method is known as rigid, i.e., once a splitting is done on clusters, we can't revert it.\n\nSteps to perform Divisive Clustering\n\nInitially, all the objects or points in the dataset belong to one single cluster.\n\nPartition the single cluster into two least similar clusters.\n\nAnd continue this process to form the new clusters until the desired number of clusters means one cluster for each observation.\n\nStrengths and Limitations of Hierarchical Clustering Algorithm\n\nFor every algorithm, we do have strengths and limitations. If we don't know about these, we end up using these algorithms in the cases where they are limited not to use. So let’s learn this as well.\n\nStrengths of Hierarchical Clustering\n\nIt is to understand and implement.\n\nWe don’t have to pre-specify any particular number of clusters.\n\nCan obtain any desired number of clusters by cutting the Dendrogram at the proper level.\n\nThey may correspond to meaningful classification.\n\nEasy to decide the number of clusters by merely looking at the Dendrogram.\n\nLimitations of Hierarchical Clustering\n\nHierarchical Clustering does not work well on vast amounts of data.\n\nAll the approaches to calculate the similarity between clusters have their own disadvantages.\n\nIn hierarchical Clustering, once a decision is made to combine two clusters, it can not be undone.\n\nDifferent measures have problems with one or more of the following.\n\nSensitivity to noise and outliers.\n\nFaces Difficulty when handling with different sizes of clusters.\n\nIt is breaking large clusters.\n\nIn this technique, the order of the data has an impact on the final results.\n\nConclusion\n\nIn this article, we discussed the hierarchical cluster algorithm’s in-depth intuition and approaches, such as the Agglomerative Clustering and Divisive Clustering approach.\n\nHierarchical Clustering is often used in the form of descriptive rather than predictive modeling.\n\nMostly we use Hierarchical Clustering algorithm when the application requires a hierarchy. The advantage of Hierarchical Clustering is we don’t have to pre-specify the clusters.\n\nHowever, it doesn’t work very well on vast amounts of data or huge datasets. And there are some disadvantages of the Hierarchical Clustering algorithm that it is not suitable for large datasets. And it gives the best results in some cases only.\n\nFrequently Asked Questions (FAQs) On Hierarchical Clustering Algorithm\n\n1. What is Hierarchical Clustering?\n\nHierarchical clustering is a method of cluster analysis that seeks to build a hierarchy of clusters by either successively splitting or merging them.\n\n2. How Does Hierarchical Clustering Differ from K-Means Clustering?\n\nUnlike K-means which requires specifying the number of clusters beforehand, hierarchical clustering doesn't need that. It builds a tree of clusters, allowing for a visual representation of the data hierarchy.\n\n3. What are Agglomerative and Divisive Hierarchical Clustering?\n\nAgglomerative (or bottom-up) starts with each data point as a separate cluster and merges them step by step. Divisive (or top-down) starts with all data points as one cluster and divides them.\n\n4. How Are Clusters Merged in the Agglomerative Approach?\n\nIn the agglomerative approach, at each step, the two clusters that are closest to each other are merged.\n\n5. Which Distance Metrics are Used in Hierarchical Clustering?\n\nCommon distance metrics include Euclidean, Manhattan, and Cosine. The choice of distance metric can influence the shape and structure of clusters.\n\n6. What is a Dendrogram in Hierarchical Clustering?\n\nA dendrogram is a tree-like diagram that illustrates the arrangement of clusters created by hierarchical clustering. It helps determine the number of clusters by cutting the tree at various levels.\n\n7. How Do I Determine the Optimal Number of Clusters?\n\nBy analyzing the dendrogram! Large jumps in distances in the dendrogram can suggest an optimal number of clusters.\n\n8. What are Linkage Methods in Hierarchical Clustering?\n\nLinkage methods determine the distance between clusters. Common methods are single linkage (minimum pairwise distance), complete linkage (maximum pairwise distance), average linkage, and Ward's linkage (minimize variance).\n\n9. How Scalable is Hierarchical Clustering?\n\nHierarchical clustering tends to be computationally intensive, especially for larger datasets. It's often more suited to smaller datasets or when the hierarchical structure is of specific interest.\n\n10. Is It Necessary to Standardize Data Before Hierarchical Clustering?\n\nOften, yes. Especially when the dataset's features have different units or scales. Standardizing ensures that each feature contributes equally to the distance computation.\n\n11. Can I Use Hierarchical Clustering for Large Datasets?\n\nWhile possible, it's computationally demanding. For very large datasets, one might consider sampling or using more scalable clustering algorithms.\n\n12. How Robust is Hierarchical Clustering to Outliers?\n\nIts sensitivity to outliers depends on the linkage method. For instance, a single linkage can be sensitive to noise and outliers, whereas Ward's method or complete linkage can be more robust.\n\nRecommended Courses\n\nRecommended\n\nMachine Learning Course\n\nRating: 4.6/5\n\nDeep Learning Course\n\nRating: 4.5/5\n\nNLP Course\n\nRating: 4.5/5"
    }
}
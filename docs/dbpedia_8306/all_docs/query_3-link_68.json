{
    "id": "dbpedia_8306_3",
    "rank": 68,
    "data": {
        "url": "https://www.modernstatisticswithr.com/eda.html",
        "read_more_link": "",
        "language": "en",
        "title": "4 Exploratory data analysis and unsupervised learning",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.modernstatisticswithr.com/Rlogo.png",
            "https://www.modernstatisticswithr.com/Rlogo.png",
            "https://www.modernstatisticswithr.com/_main_files/figure-html/pressureplot-1.png",
            "https://www.modernstatisticswithr.com/_main_files/figure-html/ggthemes-1.png",
            "https://www.modernstatisticswithr.com/_main_files/figure-html/diamondensity-1.png",
            "https://www.modernstatisticswithr.com/_main_files/figure-html/unnamed-chunk-309-1.png",
            "https://www.modernstatisticswithr.com/_main_files/figure-html/unnamed-chunk-343-1.png",
            "https://www.modernstatisticswithr.com/_main_files/figure-html/unnamed-chunk-345-1.png",
            "https://www.modernstatisticswithr.com/_main_files/figure-html/unnamed-chunk-354-1.png",
            "https://www.modernstatisticswithr.com/_main_files/figure-html/unnamed-chunk-380-1.png",
            "https://www.modernstatisticswithr.com/_main_files/figure-html/unnamed-chunk-387-1.png",
            "https://www.modernstatisticswithr.com/_main_files/figure-html/unnamed-chunk-389-1.png",
            "https://www.modernstatisticswithr.com/_main_files/figure-html/unnamed-chunk-393-1.png",
            "https://www.modernstatisticswithr.com/_main_files/figure-html/unnamed-chunk-402-1.png",
            "https://www.modernstatisticswithr.com/_main_files/figure-html/unnamed-chunk-408-1.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "MÃ¥ns Thulin"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "4 Exploratory data analysis and unsupervised learning | Modern Statistics with R",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "4.1 Reports with R Markdown\n\nR Markdown files can be used to create nicely formatted documents using R that are easy to export to other formats, like HTML, Word or PDF. They allow you to mix R code with results and text. They can be used to create reproducible reports that are easy to update with new data, because they include the code for making tables and figures. Additionally, they can be used as notebooks for keeping track of your work and your thoughts as you carry out an analysis. You can even use them for writing books; in fact, this entire book was written using R Markdown.\n\nIt is often a good idea to use R Markdown for exploratory analyses, as it allows you to write down your thoughts and comments as the analysis progresses, as well as to save the results of the exploratory journey. For that reason, weâll start this chapter by looking at some examples of what you can do using R Markdown. According to your preference, you can use either R Markdown or ordinary R scripts for the analyses in the remainder of the chapter. The R code used is the same and the results are identical; but, if you use R Markdown, you can also save the output of the analysis in a nicely formatted document.\n\n4.1.1 A first example\n\nWhen you create a new R Markdown document in RStudio by clicking File > New File > R Markdown in the menu, a document similar to the one below is created:\n\n--- title: \"Untitled\" author: \"MÃ¥ns Thulin\" date: \"10/20/2020\" output: html_document --- ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) ``` ## R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: ```{r cars} summary(cars) ``` ## Including Plots You can also embed plots, for example: ```{r pressure, echo=FALSE} plot(pressure) ``` Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.\n\nPress the Knit button at the top of the Script panel to create an HTML document using this Markdown file. It will be saved in the same folder as your Markdown file. Once the HTML document has been created, it will open so that you can see the results. You may have to install additional packages for this to work, in which case RStudio will prompt you.\n\nNow, letâs have a look at what the different parts of the Markdown document do. The first part is called the document header or YAML header. It contains information about the document, including its title, the name of its author, and the date on which it was first created:\n\n--- title: \"Untitled\" author: \"MÃ¥ns Thulin\" date: \"10/20/2020\" output: html_document ---\n\nThe part that says output: html_document specifies what type of document should be created when you press Knit. In this case, itâs set to html_document, meaning that an HTML document will be created. By changing this to output: word_document you can create a .docx Word document instead. By changing it to output: pdf_document, you can create a .pdf document using LaTeX (youâll have to install LaTeX if you havenât already â RStudio will notify you if that is the case).\n\nThe second part sets the default behaviour of code chunks included in the document, specifying that the output from running the chunks should be printed unless otherwise specified:\n\n```{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) ```\n\nThe third part contains the first proper section of the document. First, a header is created using ##. Then there is some text with formatting: < > is used to create a link and ** is used to get bold text. Finally, there is a code chunk, delimited by ```:\n\n## R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: ```{r cars} summary(cars) ```\n\nThe fourth and final part contains another section, this time with a figure created using R. A setting is added to the code chunk used to create the figure, which prevents the underlying code from being printed in the document:\n\n## Including Plots You can also embed plots, for example: ```{r pressure, echo=FALSE} plot(pressure) ``` Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.\n\nIn the next few sections, we will look at how formatting and code chunks work in R Markdown.\n\n4.1.2 Formatting text\n\nTo create plain text in a Markdown file, you simply have to write plain text. If you wish to add some formatting to your text, you can use the following:\n\n_italics_ or *italics*: to create text in italics.\n\n__bold__ or **bold**: to create bold text.\n\n[linked text](http://www.modernstatisticswithr.com): to create linked text.\n\n`code`: to include inline code in your document.\n\n$a^2 + b^2 = c^2$: to create inline equations like \\(a^2 + b^2 = c^2\\) using LaTeX syntax.\n\n$$a^2 + b^2 = c^2$$: to create a centred equation on a new line.\n\nTo add headers and sub-headers, and to divide your document into section, start a new line with #âs as follows:\n\n# Header text ## Sub-header text ### Sub-sub-header text ...and so on.\n\n4.1.3 Lists, tables, and images\n\nTo create a bullet list, you can use * as follows. Note that you need a blank line between your list and the previous paragraph to begin a list.\n\n* Item 1 * Item 2 + Sub-item 1 + Sub-item 2 * Item 3\n\nyielding:\n\nItem 1\n\nItem 2\n\nSub-item 1\n\nSub-item 2\n\nItem 3\n\nTo create an ordered list, use:\n\n1. First item 2. Second item i) Sub-item 1 ii) Sub-item 2 3. Item 3\n\nyielding:\n\nFirst item\n\nSecond item\n\nSub-item 1\n\nSub-item 2\n\nItem 3\n\nTo create a table, use | and --------- as follows:\n\nColumn 1 | Column 2 --------- | --------- Content | More content Even more | And some here Even more? | Yes!\n\nwhich yields the following output:\n\nColumn 1 Column 2 Content More content Even more And some here Even more? Yes!\n\nTo include an image, use the same syntax as when creating linked text with a link to the image path (either local or on the web), but with a ! in front:\n\n![](https://www.r-project.org/Rlogo.png) ![The R logo.](https://www.r-project.org/Rlogo.png)\n\nwhich yields:\n\n4.1.4 Code chunks\n\nThe simplest way to define a code chunk is to write:\n\n```{r} plot(pressure) ```\n\nIn RStudio, Ctrl+Alt+I is a keyboard shortcut for inserting this kind of code chunk.\n\nWe can add a name and a caption to the chunk, which lets us reference objects created by the chunk:\n\n```{r pressureplot, fig.cap = \"Plot of the pressure data.\"} plot(pressure) ``` As we can see in Figure \\@ref(fig:pressureplot), the relationship between temperature and pressure resembles a banana.\n\nThis yields the following output:\n\nplot(pressure)\n\nAs we can see in Figure 4.2, the relationship between temperature and pressure resembles a banana.\n\nIn addition, you can add settings to the chunk header to control its behaviour. For instance, you can include a code chunk without running it by adding echo = FALSE:\n\n```{r, eval = FALSE} plot(pressure) ```\n\nYou can add the following settings to your chunks:\n\necho = FALSE to run the code without printing it,\n\neval = FALSE to print the code without running it,\n\nresults = \"hide\" to hide printed output,\n\nfig.show = \"hide\" to hide plots,\n\nwarning = FALSE to suppress warning messages from being printed in your document,\n\nmessage = FALSE to suppress other messages from being printed in your document,\n\ninclude = FALSE to run a chunk without showing the code or results in the document,\n\nerror = TRUE to continue running your R Markdown document even if there is an error in the chunk (by default, the documentation creation stops if there is an error).\n\nData frames can be printed either as in the Console or as a nicely formatted table. For example,\n\n```{r, echo = FALSE} head(airquality) ```\n\nyields:\n\n## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6\n\nwhereas\n\n```{r, echo = FALSE} knitr::kable( head(airquality), caption = \"Some data I found.\" ) ```\n\nyields a nicely formatted table.\n\nFurther help and documentation for R Markdown can be found through the RStudio menus, by clicking Help > Cheatsheets > R Markdown Cheat Sheet or Help > Cheatsheets > R Markdown Reference Guide.\n\n4.2 Customising ggplot2 plots\n\nWeâll be using ggplot2 a lot in this chapter; so, before we get started with exploratory analyses, weâll take some time to learn how we can customise the look of ggplot2-plots.\n\nConsider the following facetted plot from Section 2.7.4:\n\nlibrary(ggplot2) ggplot(msleep, aes(brainwt, sleep_total)) + geom_point() + labs(x = \"Brain weight (logarithmic scale)\", y = \"Total sleep time\") + scale_x_log10() + facet_wrap(~ vore)\n\nIt looks nice, sure, but there may be things that youâd like to change. Maybe youâd like the plotâs background to be white instead of grey, or perhaps youâd like to use a different font. These, and many other things, can be modified using themes and palettes. Before we look at that, weâll take a quick look at how to modify the labels and axes of the plot.\n\n4.2.1 Modifying labels\n\nAs weâve already seen, the labs function allows us to change the labels for the aesthethics, like the header of the legend showing the different colours in the figure:\n\nggplot(msleep, aes(brainwt, sleep_total, colour = vore)) + geom_point() + labs(x = \"Brain weight (logarithmic scale)\", y = \"Total sleep time\", colour = \"Feeding behaviour\") + scale_x_log10()\n\n4.2.2 Modifying axis scales\n\nWeâve seen how functions with names beginning with scale_x, such as scale_x_log10 can be used to modify the scale of the x-axis (and scale_y-functions used to modify the y-axis). In addition to log transforms, we can for instance control where the tick marks are located by using the breaks argument. If we only want to modify the tick marks (without doing a log transform), we can use scale_x_continuous:\n\n# Default: ggplot(msleep, aes(brainwt, sleep_total, colour = vore)) + geom_point() + labs(x = \"Brain weight\", y = \"Total sleep time\", colour = \"Feeding behaviour\") # Tick marks at all integers from 0 to 6: ggplot(msleep, aes(brainwt, sleep_total, colour = vore)) + geom_point() + labs(x = \"Brain weight\", y = \"Total sleep time\", colour = \"Feeding behaviour\") + scale_x_continuous(breaks = 0:6) # Tick mark at specific values: ggplot(msleep, aes(brainwt, sleep_total, colour = vore)) + geom_point() + labs(x = \"Brain weight\", y = \"Total sleep time\", colour = \"Feeding behaviour\") + scale_x_continuous(breaks = c(0, 2.5, 4.25))\n\nThe y-axis can be modified analogously using scale_y_continuous.\n\n4.2.3 Using themes\n\nggplot2 comes with a number of basic themes. All are fairly similar but differ in things like background colour, grids, and borders. You can add them to your plot using theme_themeName, where themeName is the name of the theme . Here are some examples:\n\np <- ggplot(msleep, aes(brainwt, sleep_total, colour = vore)) + geom_point() + labs(x = \"Brain weight (logarithmic scale)\", y = \"Total sleep time\", colour = \"Feeding behaviour\") + scale_x_log10() + facet_wrap(~ vore) # Create plot with different themes: p + theme_grey() # The default theme p + theme_bw() p + theme_linedraw() p + theme_light() p + theme_dark() p + theme_minimal() p + theme_classic()\n\nThere are several packages available that contain additional themes. Letâs try a few:\n\ninstall.packages(\"ggthemes\") library(ggthemes) # Create plot with different themes from ggthemes: p + theme_tufte() # Minimalist Tufte theme p + theme_wsj() # Wall Street Journal p + theme_solarized() + scale_colour_solarized() # Solarized colours ############################## install.packages(\"hrbrthemes\") library(hrbrthemes) # Create plot with different themes from hrbrthemes: p + theme_ipsum() # Ipsum theme p + theme_ft_rc() # Suitable for use with dark RStudio themes p + theme_modern_rc() # Suitable for use with dark RStudio themes\n\n4.2.4 Colour palettes\n\nUnlike, e.g., background colours, the colour palette, i.e., the list of colours used for plotting, is not part of the theme that youâre using. Next, weâll have a look at how to change the colour palette used for your plot.\n\nLetâs start by creating a ggplot object:\n\np <- ggplot(msleep, aes(brainwt, sleep_total, colour = vore)) + geom_point() + labs(x = \"Brain weight (logarithmic scale)\", y = \"Total sleep time\", colour = \"Feeding behaviour\") + scale_x_log10()\n\nYou can change the colour palette using scale_colour_brewer. Three types of colour palettes are available:\n\nSequential palettes: these range from a colour to white. These are useful for representing ordinal (i.e., ordered) categorical variables and numerical variables.\n\nDiverging palettes: these range from one colour to another, with white in between. Diverging palettes are useful when there is a meaningful middle or 0 value (e.g., when your variables represent temperatures or profit/loss), which can be mapped to white.\n\nQualitative palettes: these contain multiple distinct colours. They are useful for nominal (i.e., with no natural ordering) categorical variables.\n\nSee ?scale_colour_brewer or http://www.colorbrewer2.org for a list of the available palettes. Here are some examples:\n\n# Sequential palette: p + scale_colour_brewer(palette = \"OrRd\") # Diverging palette: p + scale_colour_brewer(palette = \"RdBu\") # Qualitative palette: p + scale_colour_brewer(palette = \"Set1\")\n\nIn this case, because vore is a nominal categorical variable, a qualitative palette is arguably the best choice.\n\nIn addition to these ready-made palettes, you can create your own custom palettes. Hereâs an example with colours that have been recommended as being easy to distinguish for people who are colour-blind:\n\n# Create a vector with colours: colour_blind_palette <- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#0072B2\", \"#D55E00\", \"#CC79A7\") # Use the palette with the plot: p + scale_colour_manual(values = colour_blind_palette)\n\nUse scale_colour_manual to change the palette used for colour and scale_fill_manual to change the palette used for fill in your plots.\n\nFinally, if you want to use your custom palette in many different plots, and you donât want to have to add scale_colour_manual to every single plot, you can change the default colours for all ggplot2 plots in your R session as follows:\n\noptions(ggplot2.discrete.colour = colour_blind_palette, ggplot2.discrete.fill = colour_blind_palette)\n\nSome examples of themes are shown in Figure 4.3.\n\n4.2.5 Theme settings\n\nThe point of using a theme is that you get a combination of colours, fonts, and other choices that are supposed to go well together, meaning that you donât have to spend too much time picking combinations. But if you like, you can override the default options and customise any and all parts of a theme.\n\nThe theme controls all visual aspects of the plot not related to the aesthetics. You can change the theme settings using the theme function. For instance, you can use theme to remove the legend or change its position:\n\n# No legend: p + theme(legend.position = \"none\") # Legend below figure: p + theme(legend.position = \"bottom\") # Legend inside plot: p + theme(legend.position = c(0.9, 0.7))\n\nIn the last example, the vector c(0.9, 0.7) gives the relative coordinates of the legend, with c(0 0) representing the bottom left corner of the plot and c(1, 1) the upper right corner. Try to change the coordinates to different values between 0 and 1 and see what happens.\n\nThe base size of a theme controls the scale of the entire figure. This makes it useful when you want to rescale all elements of your plot at the same time:\n\np + theme_grey(base_size = 8) p + theme_grey(base_size = 20)\n\ntheme has a lot of other settings, including for the colours of the background, the grid, and the text in the plot. Here are a few examples that you can use as a starting point for experimenting with the settings:\n\np + theme(panel.grid.major = element_line(colour = \"black\"), panel.grid.minor = element_line(colour = \"purple\", linetype = \"dotted\"), panel.background = element_rect(colour = \"red\", size = 2), plot.background = element_rect(fill = \"yellow\"), axis.text = element_text(family = \"mono\", colour = \"blue\"), axis.title = element_text(family = \"serif\", size = 14))\n\nTo find a complete list of settings, see ?theme, ?element_line (lines), ?element_rect (borders and backgrounds), ?element_text (text), and element_blank (for suppressing plotting of elements). As before, you can use colors() to get a list of built-in colours, or use colour hex codes.\n\n\\[\\sim\\]\n\nExercise 4.1 Use the documentation for theme and the element_... functions to change the plot object p created above as follows:\n\nChange the background colour of the entire plot to lightblue.\n\nChange the font of the legend to serif.\n\nRemove the grid.\n\nChange the colour of the axis ticks to orange and make them thicker.\n\n(Click here to go to the solution.)\n\n4.3 Exploring distributions\n\nIt is often useful to visualise the distribution of a numerical variable. Comparing the distributions of different groups can lead to important insights. Visualising distributions is also essential when checking assumptions used for various statistical tests (sometimes called initial data analysis). In this section, we will illustrate how this can be done using the diamonds data from the ggplot2 package. You can read more about it by running ?diamonds.\n\n4.3.1 Density plots and frequency polygons\n\nWe already know how to visualise the distribution of the data by dividing it into bins and plotting a histogram:\n\nlibrary(ggplot2) ggplot(diamonds, aes(carat)) + geom_histogram(colour = \"black\")\n\nA similar plot is created using frequency polygons, which uses lines instead of bars to display the counts in the bins:\n\nggplot(diamonds, aes(carat)) + geom_freqpoly()\n\nAn advantage with frequency polygons is that they can be used to compare groups, e.g., diamonds with different cuts, without facetting:\n\nggplot(diamonds, aes(carat, colour = cut)) + geom_freqpoly()\n\nIt is clear from this figure that there are more diamonds with ideal cuts than diamonds with fair cuts in the data. The polygons have roughly the same shape, except perhaps for the polygon for diamonds with fair cuts.\n\nIn some cases, we are more interested in the shape of the distribution than in the actual counts in the different bins. Density plots are similar to frequency polygons but show an estimate of the density function of the underlying random variable. These estimates are smooth curves that are scaled so that the area below them is 1 (i.e., scaled to be proper density functions):\n\nggplot(diamonds, aes(carat, colour = cut)) + geom_density()\n\nFrom this figure, shown in Figure 4.4, it becomes clear that low-carat diamonds tend to have better cuts, which wasnât obvious from the frequency polygons. However, the plot does not provide any information about how common different cuts are. Use density plots if youâre more interested in the shape of a variableâs distribution, and frequency polygons if youâre more interested in counts.\n\nThere are several settings that can help improve the look of your density plot, which youâll explore in the next exercise.\n\n\\[\\sim\\]\n\nExercise 4.2 Using the density plot we just created above (Figure 4.4) and the documentation for geom_density, do the following:\n\nIncrease the smoothness of the density curves.\n\nFill the area under the density curves with the same colour as the curves themselves.\n\nMake the colours that fill the areas under the curves transparent.\n\nThe figure still isnât that easy to interpret. Install and load the ggridges package, an extension of ggplot2 that allows you to make so-called ridge plots (density plots that are separated along the y-axis, similar to facetting). Read the documentation for geom_density_ridges and use it to make a ridge plot of diamond prices for different cuts.\n\n(Click here to go to the solution.)\n\nExercise 4.3 Return to the histogram created by ggplot(diamonds, aes(carat)) + geom_histogram() above. As there are very few diamonds with carat greater than 3, cut the x-axis at 3. Then decrease the bin width to 0.01. Do any interesting patterns emerge?\n\n(Click here to go to the solution.)\n\n4.3.2 Asking questions\n\nExercise 4.3 causes us to ask why diamonds with carat values that are multiples of 0.25 are more common than others. Perhaps the price is involved? Unfortunately, a plot of carat versus price is not that informative:\n\nggplot(diamonds, aes(carat, price)) + geom_point(size = 0.05)\n\nMaybe we could compute the average price in each bin of the histogram? In that case, we need to extract the bin breaks from the histogram somehow. We could then create a new categorical variable using the breaks with cut (as we did in Exercise 2.27). It turns out that extracting the bins is much easier using base graphics than ggplot2, so letâs do that:\n\n# Extract information from a histogram with bin width 0.01, # which corresponds to 481 breaks: carat_br <- hist(diamonds$carat, breaks = 481, right = FALSE, plot = FALSE) # Of interest to us are: # carat_br$breaks, which contains the breaks for the bins # carat_br$mid, which contains the midpoints of the bins # (useful for plotting!) # Create categories for each bin: diamonds$carat_cat <- cut(diamonds$carat, 481, right = FALSE)\n\nWe now have a variable, carat_cat, that shows to which bin each observation belongs. Next, weâd like to compute the mean for each bin. This is a grouped summary â mean by category. After weâve computed the bin means, we could then plot them against the bin midpoints. Letâs try it:\n\nmeans <- aggregate(price ~ carat_cat, data = diamonds, FUN = mean) plot(carat_br$mid, means$price)\n\nThat didnât work as intended. We get an error message when attempting to plot the results:\n\nError in xy.coords(x, y, xlabel, ylabel, log) : 'x' and 'y' lengths differ\n\nThe error message implies that the number of bins and the number of mean values that have been computed differ. But weâve just computed the mean for each bin, havenât we? So whatâs going on?\n\nBy default, aggregate ignores groups for which there are no values when computing grouped summaries. In this case, there are a lot of empty bins â there is for instance no observation in the [4.99,5) bin. In fact, only 272 out of the 481 bins are non-empty.\n\nWe can solve this in different ways. One way is to remove the empty bins. We can do this using the match function, which returns the positions of matching values in two vectors. If we use it with the bins from the grouped summary and the vector containing all bins, we can find the indices of the non-empty bins. This requires the use of the levels function, which youâll learn more about in Section 5.4:\n\nmeans <- aggregate(price ~ carat_cat, data = diamonds, FUN = mean) id <- match(means$carat_cat, levels(diamonds$carat_cat))\n\nFinally, weâll also add some vertical lines to our plot, to call attention to multiples of 0.25.\n\nUsing base graphics is faster here:\n\nplot(carat_br$mid[id], means$price, cex = 0.5) # Add vertical lines at multiples # of 0.25: abline(v = c(0.5, 0.75, 1, 1.25, 1.5))\n\nBut we can of course stick to ggplot2 if we like:\n\nlibrary(ggplot2) d2 <- data.frame( bin = carat_br$mid[id], mean = means$price) ggplot(d2, aes(bin, mean)) + geom_point() + geom_vline(xintercept = c(0.5, 0.75, 1, 1.25, 1.5)) # geom_vline add vertical lines at # multiples of 0.25\n\nIt appears that there are small jumps in the prices at some of the 0.25-marks. This explains why there are more diamonds just above these marks than just below.\n\nThe above example illustrates three crucial things regarding exploratory data analysis:\n\nPlots (in our case, the histogram) often lead to new questions.\n\nOften, we must transform, summarise, or otherwise manipulate our data to answer a question. Sometimes this is straightforward, and sometimes it means diving deep into R code.\n\nSometimes the thing that weâre trying to do doesnât work straight away. There is almost always a solution though (and oftentimes more than one!). The more you work with R, the more problem-solving tricks you will learn.\n\n4.3.3 Violin plots\n\nDensity curves can also be used as alternatives to boxplots. In Exercise 2.17, you created boxplots to visualise price differences between diamonds of different cuts:\n\nggplot(diamonds, aes(cut, price)) + geom_boxplot()\n\nInstead of using a boxplot, we can use a violin plot. Each group is represented by a âviolinâ, given by a rotated and duplicated density plot:\n\nggplot(diamonds, aes(cut, price)) + geom_violin()\n\nCompared to boxplots, violin plots capture the entire distribution of the data rather than just a few numerical summaries. If you like numerical summaries (and you should) you can add the median and the quartiles (corresponding to the borders of the box in the boxplot) using the draw_quantiles argument:\n\nggplot(diamonds, aes(cut, price)) + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75))\n\n\\[\\sim\\]\n\nExercise 4.4 Using the first boxplot created above, i.e., ggplot(diamonds, aes(cut, price)) + geom_violin(), do the following:\n\nAdd some colour to the plot by giving different colours to each violin.\n\nBecause the categories are shown along the x-axis, we donât really need the legend. Remove it.\n\nBoth boxplots and violin plots are useful. Maybe we can have the best of both worlds? Add the corresponding boxplot inside each violin. Hint: the width and alpha arguments in geom_boxplot are useful for creating a nice-looking figure here.\n\nFlip the coordinate system to create horizontal violins and boxes instead.\n\n(Click here to go to the solution.)\n\n4.5 Outliers and missing data\n\n4.5.1 Detecting outliers\n\nBoth boxplots and scatterplots are helpful in detecting deviating observations â often called outliers. Outliers can be caused by measurement errors or errors in the data input but can also be interesting rare cases that can provide valuable insights about the process that generated the data. Either way, it is often of interest to detect outliers, for instance because that may influence the choice of what statistical tests to use.\n\nLetâs draw a scatterplot of diamond carats versus prices:\n\nggplot(diamonds, aes(carat, price)) + geom_point()\n\nThere are some outliers which we may want to study further. For instance, there is a surprisingly cheap 5-carat diamond, and some cheap 3-carat diamonds. Note that it is not just the prices nor just the carats of these diamonds that make them outliers, but the unusual combinations of prices and carats. But how can we identify those points?\n\nOne option is to use the plotly package to make an interactive version of the plot, where we can hover interesting points to see more information about them. Start by installing it:\n\ninstall.packages(\"plotly\")\n\nTo use plotly with a ggplot graphic, we store the graphic in a variable and then use it as input to the ggplotly function. The resulting (interactive!) plot takes a little longer than usual to load. Try hovering the points:\n\nmyPlot <- ggplot(diamonds, aes(carat, price)) + geom_point() library(plotly) ggplotly(myPlot)\n\nBy default, plotly only shows the carat and price of each diamond. But we can add more information to the box by adding a text aesthetic:\n\nmyPlot <- ggplot(diamonds, aes(carat, price, text = paste(\"Row:\", rownames(diamonds)))) + geom_point() ggplotly(myPlot)\n\nWe can now find the row numbers of the outliers visually, which is very useful when exploring data.\n\n\\[\\sim\\]\n\nExercise 4.5 The variables x and y in the diamonds data describe the length and width of the diamonds (in millimetres). Use an interactive scatterplot to identify outliers in these variables. Check prices, carat, and other information and think about if any of the outliers can be due to data errors.\n\n(Click here to go to the solution.)\n\n4.5.2 Labelling outliers\n\nInteractive plots are great when exploring a dataset but are not always possible to use in other contexts, e.g., for printed reports and some presentations. In these other cases, we can instead annotate the plot with notes about outliers. One way to do this is to use a geom called geom_text.\n\nFor instance, we may want to add the row numbers of outliers to a plot. To do so, we use geom_text along with a condition that specifies for which points we should add annotations. As in Section 2.11.3, if we, e.g., wish to add row numbers for diamonds with carats greater than four, our condition would be carat > 4. The ifelse function, which weâll look closer at in Section 6.3, is perfect to use here. The syntax will be ifelse(condition, what text to write if the condition is satisfied, what text to write else). To add row names for observations that fulfill the condition but not for other observations, we use ifelse(condition, rownames(diamonds), \"\"). If, instead, we wanted to print the price of the diamonds, weâd use ifelse(condition, price, \"\").\n\nHere are some different examples of conditions used to plot text:\n\n# Using the row number (the 5 carat diamond is on row 27,416) ggplot(diamonds, aes(carat, price)) + geom_point() + geom_text(aes(label = ifelse(rownames(diamonds) == 27416, rownames(diamonds), \"\")), hjust = 1.1) # (hjust=1.1 shifts the text to the left of the point) # Plot text next to all diamonds with carat>4 ggplot(diamonds, aes(carat, price)) + geom_point() + geom_text(aes(label = ifelse(carat > 4, rownames(diamonds), \"\")), hjust = 1.1) # Plot text next to 3 carat diamonds with a price below 7500 ggplot(diamonds, aes(carat, price)) + geom_point() + geom_text(aes(label = ifelse(carat == 3 & price < 7500, rownames(diamonds), \"\")), hjust = 1.1)\n\n\\[\\sim\\]\n\nExercise 4.6 Create a static (i.e., non-interactive) scatterplot of x versus y from the diamonds data. Label the diamonds with suspiciously high \\(y\\)-values.\n\n(Click here to go to the solution.)\n\n4.5.3 Missing data\n\nLike many datasets, the mammal sleep data msleep contains a lot of missing values, represented by NA (Not Available) in R. This becomes evident when we have a look at the data:\n\nlibrary(ggplot2) View(msleep)\n\nWe can check if a particular observation is missing using the is.na function:\n\nis.na(msleep$sleep_rem[4]) is.na(msleep$sleep_rem)\n\nWe can count the number of missing values for each variable using:\n\ncolSums(is.na(msleep))\n\nHere, colSums computes the sum of is.na(msleep) for each column of msleep (remember that in summation, TRUE counts as 1 and FALSE as 0), yielding the number of missing values for each variable. In total, there are 136 missing values in the dataset:\n\nsum(is.na(msleep))\n\nYouâll notice that ggplot2 prints a warning in the Console when you create a plot with missing data:\n\nggplot(msleep, aes(brainwt, sleep_total)) + geom_point() + scale_x_log10()\n\nSometimes, data are missing simply because the information is not yet available (for instance, the brain weight of the mountain beaver could be missing because no one has ever weighed the brain of a mountain beaver). In other cases, data can be missing because something about them is different (for instance, values for a male patient in a medical trial can be missing because the patient died, or because some values only were collected for female patients). Therefore, it is of interest to see if there are any differences in non-missing variables between subjects that have missing data and subjects that donât.\n\nIn msleep, all animals have recorded values for sleep_total and bodywt. To check if the animals that have missing brainwt values differ from the others, we can plot them in a different colour in a scatterplot:\n\nggplot(msleep, aes(bodywt, sleep_total, colour = is.na(brainwt))) + geom_point() + scale_x_log10()\n\n(If is.na(brainwt) is TRUE, then the brain weight is missing in the dataset.) In this case, there are no apparent differences between the animals with missing data and those without.\n\n\\[\\sim\\]\n\nExercise 4.7 Create a version of the diamonds dataset where the x value is missing for all diamonds with \\(x>9\\). Make a scatterplot of carat versus price, in which points where the x value is missing are plotted in a different colour. How would you interpret this plot?\n\n(Click here to go to the solution.)\n\n4.5.4 Exploring data\n\nThe nycflights13 package contains data about flights to and from three airports in New York, USA, in 2013. As a summary exercise, we will study a subset of these, namely all flights departing from New York on 1 January of that year:\n\ninstall.packages(\"nycflights13\") library(nycflights13) flights2 <- flights[flights$month == 1 & flights$day == 1,]\n\n\\[\\sim\\]\n\nExercise 4.8 Explore the flights2 dataset, focusing on delays and the amount of time spent in the air. Are there any differences between the different carriers? Are there missing data? Are there any outliers?\n\n(Click here to go to the solution.)\n\n4.7 Exploring time series\n\nBefore we have a look at time series, you should install four useful packages: forecast, nlme, fma and fpp2. The first contains useful functions for plotting time series data, and the latter three contain datasets that weâll use.\n\ninstall.packages(c(\"nlme\", \"forecast\", \"fma\", \"fpp2\"), dependencies = TRUE)\n\nThe a10 dataset contains information about the monthly anti-diabetic drug sales in Australia from July 1991 to June 2008. By checking its structure, we see that it is saved as a time series object :\n\nlibrary(fpp2) str(a10)\n\nggplot2 requires that data is saved as a data frame in order for it to be plotted. In order to plot the time series, we could first convert it to a data frame and then plot each point using geom_points:\n\na10_df <- data.frame(time = time(a10), sales = a10) ggplot(a10_df, aes(time, sales)) + geom_point()\n\nIt is however usually preferable to plot time series using lines instead of points. This is done using a different geom: geom_line:\n\nggplot(a10_df, aes(time, sales)) + geom_line()\n\nHaving to convert the time series object to a data frame is a little awkward. Luckily, there is a way around this. ggplot2 offers a function called autoplot that automatically draws an appropriate plot for certain types of data. forecast extends this function to time series objects:\n\nlibrary(forecast) autoplot(a10)\n\nWe can still add other geoms, axis labels, and other things just as before. autoplot has simply replaced the ggplot(data, aes()) + geom part that would be the first two rows of the ggplot2 figure and has implicitly converted the data to a data frame.\n\n\\[\\sim\\]\n\nExercise 4.10 Using the autoplot(a10) figure, do the following:\n\nAdd a smoothed line describing the trend in the data. Make sure that it is smooth enough not to capture the seasonal variation in the data.\n\nChange the label of the x-axis to âYearâ and the label of the y-axis to âSales ($ million)â.\n\nCheck the documentation for the ggtitle function. What does it do? Use it with the figure.\n\nChange the colour of the time series line to red.\n\n(Click here to go to the solution.)\n\n4.7.1 Annotations and reference lines\n\nWe sometimes wish to add text or symbols to plots, for instance to highlight interesting observations. Consider the following time series plot of daily morning gold prices, based on the gold data from the forecast package:\n\nlibrary(forecast) autoplot(gold)\n\nThere is a sharp spike a few weeks before day 800, which is due to an incorrect value in the data series. Weâd like to add a note about that to the plot. First, we wish to find out on which day the spike appears. This can be done by checking the data manually or using some code:\n\nspike_date <- which.max(gold)\n\nTo add a circle around that point, we add a call to annotate to the plot:\n\nautoplot(gold) + annotate(geom = \"point\", x = spike_date, y = gold[spike_date], size = 5, shape = 21, colour = \"red\", fill = \"transparent\")\n\nannotate can be used to annotate the plot with both geometrical objects and text (and can therefore be used as an alternative to geom_text).\n\n\\[\\sim\\]\n\nExercise 4.11 Using the figure created above and the documentation for annotate, do the following:\n\nAdd the text âIncorrect valueâ next to the circle.\n\nCreate a second plot where the incorrect value has been removed.\n\nRead the documentation for the geom geom_hline. Use it to add a red reference line to the plot, at \\(y=400\\).\n\n(Click here to go to the solution.)\n\n4.7.2 Longitudinal data\n\nMultiple time series with identical time points, known as longitudinal data or panel data, are common in many fields. One example of this is given by the elecdaily time series from the fpp2 package, which contains information about electricity demand in Victoria, Australia during 2014. As with a single time series, we can plot these data using autoplot:\n\nlibrary(fpp2) autoplot(elecdaily)\n\nIn this case, it is probably a good idea to facet the data, i.e., to plot each series in a different figure:\n\nautoplot(elecdaily, facets = TRUE)\n\n\\[\\sim\\]\n\nExercise 4.12 Make the following changes to the autoplot(elecdaily, facets = TRUE):\n\nRemove the WorkDay variable from the plot (it describes whether or not a given date is a workday, and while it is useful for modelling purposes, we do not wish to include it in our figure).\n\nAdd smoothed trend lines to the time series plots.\n\n(Click here to go to the solution.)\n\n4.7.3 Path plots\n\nAnother option for plotting multiple time series is path plots. A path plot is a scatterplot where the points are connected with lines in the order they appear in the data (which, for time series data, should correspond to time). The lines and points can be coloured to represent time.\n\nTo make a path plot of Temperature versus Demand for the elecdaily data, we first convert the time series object to a data frame and create a scatterplot:\n\nlibrary(fpp2) ggplot(as.data.frame(elecdaily), aes(Temperature, Demand)) + geom_point()\n\nNext, we connect the points by lines using the geom_path geom:\n\nggplot(as.data.frame(elecdaily), aes(Temperature, Demand)) + geom_point() + geom_path()\n\nThe resulting figure is quite messy. Using colour to indicate the passing of time helps a little. For this, we need to add the day of the year to the data frame. To get the values right, we use nrow, which gives us the number of rows in the data frame.\n\nelecdaily2 <- as.data.frame(elecdaily) elecdaily2$day <- 1:nrow(elecdaily2) ggplot(elecdaily2, aes(Temperature, Demand, colour = day)) + geom_point() + geom_path()\n\nIt becomes clear from the plot that temperatures were the highest at the beginning of the year and lower in the winter months (July-August).\n\n\\[\\sim\\]\n\nExercise 4.13 Make the following changes to the plot you created above:\n\nDecrease the size of the points.\n\nAdd text annotations showing the dates of the highest and lowest temperatures, next to the corresponding points in the figure.\n\n(Click here to go to the solution.)\n\n4.7.4 Spaghetti plots\n\nIn cases where weâve observed multiple subjects over time, we often wish to visualise their individual time series together using so-called spaghetti plots. With ggplot2 this is done using the geom_line geom. To illustrate this, we use the Oxboys data from the nlme package, showing the heights of 26 boys over time.\n\nlibrary(nlme) ggplot(Oxboys, aes(age, height, group = Subject)) + geom_point() + geom_line()\n\nThe first two aes arguments specify the x- and y-axes, and the third specifies that there should be one line per subject (i.e., per boy) rather than a single line interpolating all points. The latter would be a rather useless figure that looks like this:\n\nggplot(Oxboys, aes(age, height)) + geom_point() + geom_line() + ggtitle(\"A terrible plot\")\n\nReturning to the original plot, if we wish to be able to identify which time series corresponds to which boy, we can add a colour aesthetic:\n\nggplot(Oxboys, aes(age, height, group = Subject, colour = Subject)) + geom_point() + geom_line()\n\nNote that the boys are ordered by height, rather than subject number, in the legend.\n\nNow, imagine that we wish to add a trend line describing the general growth trend for all boys. The growth appears approximately linear, so it seems sensible to use geom_smooth(method = \"lm\") to add the trend:\n\nggplot(Oxboys, aes(age, height, group = Subject, colour = Subject)) + geom_point() + geom_line() + geom_smooth(method = \"lm\", colour = \"red\", se = FALSE)\n\nUnfortunately, because we have specified in the aesthetics that the data should be grouped by Subject, geom_smooth produces one trend line for each boy. The âproblemâ is that when we specify an aesthetic in the ggplot call, it is used for all geoms.\n\n\\[\\sim\\]\n\nExercise 4.14 Figure out how to produce a spaghetti plot of the Oxboys data with a single red trend line based on the data from all 26 boys.\n\n(Click here to go to the solution.)\n\n4.7.5 Seasonal plots and decompositions\n\nThe forecast package includes a number of useful functions when working with time series. One of them is ggseasonplot, which allows us to easily create a spaghetti plot of different periods of a time series with seasonality, i.e., with patterns that repeat seasonally over time. It works similar to the autoplot function, in that it replaces the ggplot(data, aes) + geom part of the code.\n\nlibrary(forecast) library(fpp2) ggseasonplot(a10)\n\nThis function is very useful when visually inspecting seasonal patterns.\n\nThe year.labels and year.labels.left arguments remove the legend in favour of putting the years at the end and beginning of the lines:\n\nggseasonplot(a10, year.labels = TRUE, year.labels.left = TRUE)\n\nAs always, we can add more things to our plot if we like:\n\nggseasonplot(a10, year.labels = TRUE, year.labels.left = TRUE) + labs(y = \"Sales ($ million)\") + ggtitle(\"Seasonal plot of anti-diabetic drug sales\")\n\nWhen working with seasonal time series, it is common to decompose the series into a seasonal component, a trend component, and a remainder. In R, this is typically done using the stl function, which uses repeated LOESS smoothing to decompose the series. There is an autoplot function for stl objects:\n\nautoplot(stl(a10, s.window = 365))\n\nThis plot can too be manipulated in the same way as other ggplot objects. You can access the different parts of the decomposition as follows:\n\nstl(a10, s.window = 365)$time.series[,\"seasonal\"] stl(a10, s.window = 365)$time.series[,\"trend\"] stl(a10, s.window = 365)$time.series[,\"remainder\"]\n\n\\[\\sim\\]\n\nExercise 4.15 Investigate the writing dataset from the fma package graphically. Make a time series plot with a smoothed trend line, a seasonal plot and an stl-decomposition plot. Add appropriate plot titles and labels to the axes. Can you see any interesting patterns?\n\n(Click here to go to the solution.)\n\n4.7.6 Detecting changepoints\n\nThe changepoint package contains a number of methods for detecting changepoints in time series, i.e., time points at which either the mean or the variance of the series changes. Finding changepoints can be important for detecting changes in the process underlying the time series. The ggfortify package extends ggplot2 by adding autoplot functions for a variety of tools, including those in changepoint. Letâs install the packages:\n\ninstall.packages(c(\"changepoint\", \"ggfortify\"))\n\nWe can now look at some examples with the anti-diabetic drug sales data:\n\nlibrary(forecast) library(fpp2) library(changepoint) library(ggfortify) # Plot the time series: autoplot(a10) # Remove the seasonal part and plot the series again: a10_ns <- a10 - stl(a10, s.window = 365)$time.series[,\"seasonal\"] autoplot(a10_ns) # Plot points where there are changes in the mean: autoplot(cpt.mean(a10_ns)) # Choosing a different method for finding changepoints # changes the result: autoplot(cpt.mean(a10_ns, method = \"BinSeg\")) # Plot points where there are changes in the variance: autoplot(cpt.var(a10_ns)) # Plot points where there are changes in either the mean or # the variance: autoplot(cpt.meanvar(a10_ns))\n\nAs you can see, the different methods from changepoint all yield different results. The results for changes in the mean are a bit dubious â which isnât all that strange as we are using a method that looks for jumps in the mean on a time series where the increase actually is more or less continuous. The changepoint for the variance looks more reliable â there is a clear change toward the end of the series where the sales become more volatile. We wonât go into details about the different methods here but mention that the documentation at ?cpt.mean, ?cpt.var, and ?cpt.meanvar contains descriptions of and references for the available methods.\n\n\\[\\sim\\]\n\nExercise 4.16 Are there any changepoints for variance in the Demand time series in elecdaily? Can you explain why the behaviour of the series changes?\n\n(Click here to go to the solution.)\n\n4.7.7 Interactive time series plots\n\nThe plotly packages can be used to create interactive time series plots. As before, you create a ggplot2 object as usual, assigning it to a variable and then call the ggplotly function. Here is an example with the elecdaily data:\n\nlibrary(plotly) library(fpp2) myPlot <- autoplot(elecdaily[,\"Demand\"]) ggplotly(myPlot)\n\nWhen you hover the mouse pointer over a point, a box appears, displaying information about that data point. Unfortunately, the date formatting isnât great in this example â dates are shown as weeks with decimal points. Weâll see how to fix this in Section 5.6.\n\n4.9 Visualising multiple variables\n\n4.9.1 Scatterplot matrices\n\nWhen we have a large enough number of numeric variables in our data, plotting scatterplots of all pairs of variables becomes tedious. Luckily there are some R functions that speed up this process.\n\nThe GGally package is an extension to ggplot2 which contains several functions for plotting multivariate data. They work similarly to the autoplot functions that we have used in previous sections. One of these is ggpairs, which creates a scatterplot matrix, a grid with scatterplots of all pairs of variables in data. In addition, it also plots density estimates (along the diagonal) and shows the (Pearson) correlation for each pair. Letâs start by installing GGally:\n\ninstall.packages(\"GGally\")\n\nTo create a scatterplot matrix for the airquality dataset, simply write:\n\nlibrary(GGally) ggpairs(airquality)\n\n(Enlarging your Plot window can make the figure look better.)\n\nIf we want to create a scatterplot matrix but only want to include some of the variables in a dataset, we can do so by providing a vector with variable names. Here is an example for the animal sleep data msleep.\n\nWithout pipes:\n\nggpairs(msleep[, c(\"sleep_total\", \"sleep_rem\", \"sleep_cycle\", \"awake\", \"brainwt\", \"bodywt\")])\n\nWith pipes:\n\nlibrary(dplyr) msleep |> select(sleep_total, sleep_rem, sleep_cycle, awake, brainwt, bodywt) |> ggpairs()\n\nOptionally, if we wish to create a scatterplot involving all numeric variables, we can replace the vector with variable names with some R code that extracts the columns containing numeric variables:\n\nWithout pipes:\n\nggpairs(msleep[, which(sapply(msleep, class) == \"numeric\")])\n\nWith pipes:\n\nmsleep |> select(where(is.numeric)) |> ggpairs()\n\nYouâll learn more about the sapply function in Section 6.5.\n\nThe resulting plot is identical to the previous one, because the list of names contained all numeric variables. The grab-all-numeric-variables approach is often convenient, because we donât have to write all the variable names. On the other hand, itâs not very helpful in case we only want to include some of the numeric variables.\n\nIf we include a categorical variable in the list of variables (such as the feeding behaviour vore), the matrix will include a bar plot of the categorical variable as well as boxplots and facetted histograms to show differences between different categories in the continuous variables:\n\nggpairs(msleep[, c(\"vore\", \"sleep_total\", \"sleep_rem\", \"sleep_cycle\", \"awake\", \"brainwt\", \"bodywt\")])\n\nAlternatively, we can use a categorical variable to colour points and density estimates using aes(colour = ...). The syntax for this follows the same pattern as that for a standard ggplot call - ggpairs(data, aes). The only exception is that if the categorical variable is not included in the data argument, we must specify which data frame it belongs to:\n\nggpairs(msleep[, c(\"sleep_total\", \"sleep_rem\", \"sleep_cycle\", \"awake\", \"brainwt\", \"bodywt\")], aes(colour = msleep$vore, alpha = 0.5))\n\nAs a side note, if all variables in your data frame are numeric, and if you only are looking for a quick-and-dirty scatterplot matrix without density estimates and correlations, you can also use the base R plot:\n\nplot(airquality)\n\n\\[\\sim\\]\n\nExercise 4.18 Create a scatterplot matrix for all numeric variables in diamonds. Differentiate different cuts by colour. Add a suitable title to the plot. (diamonds is a fairly large dataset, and it may take a minute or so for R to create the plot.)\n\n(Click here to go to the solution.)\n\n4.9.2 3D scatterplots\n\nThe plotly package lets us make three-dimensional scatterplots with the plot_ly function, which can be a useful alternative to scatterplot matrices in some cases. Here is an example using the airquality data:\n\nlibrary(plotly) plot_ly(airquality, x = ~Ozone, y = ~Wind, z = ~Temp, color = ~factor(Month))\n\nNote that you can drag and rotate the plot, to see it from different angles.\n\n4.9.3 Correlograms\n\nScatterplot matrices are not a good choice when we have too many variables, partially because the plot window needs to be very large to fit all variables and partially because it becomes difficult to get a good overview of the data. In such cases, a correlogram, where the strength of the correlation between each pair of variables is plotted instead of scatterplots, can be used instead. It is effectively a visualisation of the correlation matrix of the data, where the strengths and signs of the correlations are represented by different colours.\n\nThe GGally package contains the function ggcorr, which can be used to create a correlogram:\n\nggcorr(msleep[, c(\"sleep_total\", \"sleep_rem\", \"sleep_cycle\", \"awake\", \"brainwt\", \"bodywt\")])\n\n\\[\\sim\\]\n\nExercise 4.19 Using the diamonds dataset and the documentation for ggcorr, do the following:\n\nCreate a correlogram for all numeric variables in the dataset.\n\nThe Pearson correlation that ggcorr uses by default isnât always the best choice. A commonly used alternative is the Spearman correlation. Change the type of correlation used to create the plot to the Spearman correlation.\n\nChange the colour scale from a categorical scale with five categories.\n\nChange the colours on the scale to go from yellow (low correlation) to black (high correlation).\n\n(Click here to go to the solution.)\n\n4.9.4 Adding more variables to scatterplots\n\nWe have already seen how scatterplots can be used to visualise two continuous and one categorical variable by plotting the two continuous variables against each other and using the categorical variable to set the colours of the points. There are, however, more ways we can incorporate information about additional variables into a scatterplot.\n\nSo far, we have set three aesthetics in our scatterplots: x, y, and colour. Two other important aesthetics are shape and size, which, as youâd expect, allow us to control the shape and size of the points. As a first example using the msleep data, we use feeding behaviour (vore) to set the shapes used for the points:\n\nggplot(msleep, aes(brainwt, sleep_total, shape = vore)) + geom_point() + scale_x_log10()\n\nThe plot looks a little nicer if we increase the size of the points:\n\nggplot(msleep, aes(brainwt, sleep_total, shape = vore, size = 2)) + geom_point() + scale_x_log10()\n\nAnother option is to let size represent a continuous variable, in what is known as a bubble plot:\n\nggplot(msleep, aes(brainwt, sleep_total, colour = vore, size = bodywt)) + geom_point() + scale_x_log10()\n\nThe size of each âbubbleâ now represents the weight of the animal. Because some animals are much heavier (i.e., have higher bodywt values) than most others, almost all points are quite small. There are a couple of things we can do to remedy this. First, we can transform bodywt, e.g., using the square root transformation sqrt(bodywt), to decrease the differences between large and small animals. This can be done by adding scale_size(trans = \"sqrt\") to the plot. Second, we can also use scale_size to control the range of point sizes (e.g., from size 1 to size 20). This will cause some points to overlap, so we add alpha = 0.5 to the geom, to make the points transparent:\n\nggplot(msleep, aes(brainwt, sleep_total, colour = vore, size = bodywt)) + geom_point(alpha = 0.5) + scale_x_log10() + scale_size(range = c(1, 20), trans = \"sqrt\")\n\nThis produces a fairly nice-looking plot, but itâd look even better if we changed the axes labels and legend texts. We can change the legend text for the size scale by adding the argument name to labs. Including a \\n in the text lets us create a line break â youâll learn more tricks like that in Section 5.5.\n\nggplot(msleep, aes(brainwt, sleep_total, colour = vore, size = bodywt)) + geom_point(alpha = 0.5) + labs(x = \"Brain weight (logarithmic scale)\", y = \"Total sleep time\", size = \"Square root of\\nbody weight\", colour = \"Feeding behaviour\") + scale_x_log10() + scale_size(range = c(1, 20), trans = \"sqrt\")\n\n\\[\\sim\\]\n\nExercise 4.20 Using the bubble plot created above, do the following:\n\nReplace colour = vore in the aes by fill = vore and add colour = \"black\", shape = 21 to geom_point. What happens?\n\nUse ggplotly to create an interactive version of the bubble plot above, where variable information and the animal name are displayed when you hover a point.\n\n(Click here to go to the solution.)\n\n4.9.5 Overplotting\n\nLetâs make a scatterplot of table versus depth based on the diamonds dataset:\n\nggplot(diamonds, aes(table, depth)) + geom_point()\n\nThis plot is cluttered. There are too many points, which makes it difficult to see if, for instance, high table values are more common than low table values. In this section, weâll look at some ways to deal with this problem, known as overplotting.\n\nThe first thing we can try is to decrease the point size:\n\nggplot(diamonds, aes(table, depth)) + geom_point(size = 0.1)\n\nThis helps a little, but now the outliers become a bit difficult to spot. We can try changing the opacity using alpha instead:\n\nggplot(diamonds, aes(table, depth)) + geom_point(alpha = 0.2)\n\nThis is also better than the original plot, but neither plot is great. Instead of plotting each individual point, maybe we can try plotting the counts or densities in different regions of the plot instead? Effectively, this would be a two-dimensional version of a histogram. There are several ways of doing this in ggplot2.\n\nFirst, we bin the points and count the numbers in each bin, using geom_bin2d:\n\nggplot(diamonds, aes(table, depth)) + geom_bin2d()\n\nBy default, geom_bin2d uses 30 bins. Increasing that number can sometimes give us a better idea about the distribution of the data:\n\nggplot(diamonds, aes(table, depth)) + geom_bin2d(bins = 50)\n\nIf you prefer, you can get a similar plot with hexagonal bins by using geom_hex instead:\n\nggplot(diamonds, aes(table, depth)) + geom_hex(bins = 50)\n\nAs an alternative to bin counts, we could create a two-dimensional density estimate and create a contour plot showing the levels of the density:\n\nggplot(diamonds, aes(table, depth)) + stat_density_2d(aes(fill = ..level..), geom = \"polygon\", colour = \"white\")\n\nThe fill = ..level.. bit above probably looks a little strange to you. It means that an internal function (the level of the contours) is used to choose the fill colours. It also means that weâve reached a point where weâre reaching deep into the depths of ggplot2!\n\nWe can use a similar approach to show a summary statistic for a third variable in a plot. For instance, we may want to plot the average price as a function of table and depth. This is called a tile plot:\n\nggplot(diamonds, aes(table, depth, z = price)) + geom_tile(binwidth = 1, stat = \"summary_2d\", fun = mean) + ggtitle(\"Mean prices for diamonds with different depths and tables\")\n\n\\[\\sim\\]\n\nExercise 4.21 The following tasks involve the diamonds dataset:\n\nCreate a tile plot of table versus depth, showing the highest price for a diamond in each bin.\n\nCreate a bin plot of carat versus price. What type of diamonds have the highest bin counts?\n\n(Click here to go to the solution.)\n\n4.9.6 Categorical data\n\nWhen visualising a pair of categorical variables, plots similar to those in the previous section prove to be useful. One way of doing this is to use the geom_count geom. We illustrate this with an example using diamonds, showing how common different combinations of colours and cuts are:\n\nggplot(diamonds, aes(color, cut)) + geom_count()\n\nHowever, it is often better to use colour rather than point size to visualise counts, which we can do using a tile plot. First, we have to compute the counts though, using aggregate. We now wish to have two grouping variables, color and cut, which we can put on the right-hand side of the formula as follows:\n\ndiamonds2 <- aggregate(carat ~ cut + color, data = diamonds, FUN = length) diamonds2\n\ndiamonds2 is now a data frame containing the different combinations of color and cut along with counts of how many diamonds belong to each combination (labelled carat, because we put carat in our formula). Letâs change the name of the last column from carat to Count:\n\nnames(diamonds2)[3] <- \"Count\"\n\nNext, we can plot the counts using geom_tile:\n\nggplot(diamonds2, aes(color, cut, fill = Count)) + geom_tile()\n\nIt is also possible to combine point size and colours:\n\nggplot(diamonds2, aes(color, cut, colour = Count, size = Count)) + geom_count()\n\n\\[\\sim\\]\n\nExercise 4.22 Using the diamonds dataset, do the following:\n\nUse a plot to find out what the most common combination of cut and clarity is.\n\nUse a plot to find out which combination of cut and clarity has the highest average price.\n\n(Click here to go to the solution.)\n\n4.9.7 Putting it all together\n\nIn the next two exercises, you will repeat what you have learned so far by investigating the gapminder and planes datasets. First, load the corresponding libraries and have a look at the documentation for each dataset:\n\n\\[\\sim\\]\n\nExercise 4.23 Do the following using the gapminder dataset:\n\nCreate a scatterplot matrix showing life expectancy, population, and GDP per capita for all countries, using the data from the year 2007. Use colours to differentiate countries from different continents. Note: youâll probably need to add the argument upper = list(continuous = \"na\") when creating the scatterplot matrix. By default, correlations are shown above the diagonal, but the fact that there only are two countries from Oceania will cause a problem there â at least three points are needed for a correlation test.\n\nCreate an interactive bubble plot, showing information about each country when you hover the points. Use data from the year 2007. Put log(GDP per capita) on the x-axis and life expectancy on the y-axis. Let population determine point size. Plot each country in a different colour and facet by continent. Tip: the gapminder package provides a pretty colour scheme for different countries, called country_colors. You can use that scheme by adding scale_colour_manual(values = country_colors) to your plot.\n\n(Click here to go to the solution.)\n\nExercise 4.24 Use graphics to answer the following questions regarding the planes dataset:\n\nWhat is the most common combination of manufacturer and plane type in the dataset?\n\nWhich combination of manufacturer and plane type has the highest average number of seats?\n\nDo the numbers of seats on planes change over time? Which plane had the highest number of seats?\n\nDoes the type of engine used change over time?\n\n(Click here to go to the solution.)\n\n4.10 Sankey diagrams\n\nA Sankey diagram is a type of flow diagram used to show flows from one state to another. Weâll consider an example with data from a medical trial. Patients were recruited from four hospitals and assigned to one of two treatments: either surgery or physiotherapy. At the end of the study, they had either recovered or not. The data is in the surgphys.csv file, which can be downloaded from the bookâs web page.\n\nSet file_path to the path of surgphys.csv to load the data:\n\nsurgphys <- read.csv(file_path) View(surgphys)\n\nWeâll use the ggsankey package for creating the diagram, so letâs install that:\n\ninstall.packages(\"remotes\") remotes::install_github(\"davidsjoberg/ggsankey\")\n\nFirst, we need to reformat the data by pointing out the order of the different âstatesâ. In this case, patients are first recruited at a hospital, then assigned a treatment, and then an outcome is observed. The make_longer function from ggsankey formats the data to reflect this:\n\nlibrary(ggsankey) surgphys |> make_long(Hospital, Treatment, Outcome) -> surgphys_sankey\n\nNext, we create a ggplot using the variables we just created, and add the geom_sankey geom:\n\nggplot(surgphys_sankey, aes(x = x, next_x = next_x, node = node, next_node = next_node, fill = factor(node), label = node)) + geom_sankey()\n\nTo make this a little prettier, we can change some settings for geom_sankey, add labels with geom_sankey_label, and change the theme settings and colour palette:\n\nggplot(surgphys_sankey, aes(x = x, next_x = next_x, node = node, next_node = next_node, fill = factor(node), label = node)) + geom_sankey(flow.alpha = 0.5, node.color = \"black\", show.legend = FALSE) + geom_sankey_label(size = 3, colour = \"black\", fill = \"white\", hjust = -0.3) + theme_minimal() + theme(axis.title = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), panel.grid = element_blank()) + scale_fill_manual(values = c(\"darkorange\", \"skyblue\", \"forestgreen\", \"#00AFBB\", \"#E7B800\", \"#FC4E07\", \"deeppink\", \"purple\"))\n\nIf we want to show the number of patients in each state, we can add counts to the data, and then add them to the node labels as follows (see Section 5.12 for an explanation of what right_join does):\n\nlibrary(dplyr) surgphys_sankey |> group_by(node)|> count() |> right_join(surgphys_sankey, by = \"node\") -> surgphys_sankey ggplot(surgphys_sankey, aes(x = x, next_x = next_x, node = node, next_node = next_node, fill = factor(node), label = paste0(node,\" (n=\", n, \")\"))) + geom_sankey(flow.alpha = 0.5, node.color = \"black\", show.legend = FALSE) + geom_sankey_label(size = 3, colour = \"black\", fill = \"white\", hjust = -0.15) + theme_minimal() + theme(axis.title = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), panel.grid = element_blank()) + scale_fill_manual(values = c(\"darkorange\", \"skyblue\", \"forestgreen\", \"#00AFBB\", \"#E7B800\", \"#FC4E07\", \"deeppink\", \"purple\"))\n\n4.11 Principal component analysis\n\nIf there are many variables in your data, it can often be difficult to detect differences between groups or create a perspicuous visualisation. A useful tool in this context is principal component analysis (PCA), which can reduce high-dimensional data to a lower number of variables that can be visualised in one or two scatterplots. The idea is to compute new variables, called principal components, that are linear combinations of the original variables . These are constructed with two goals in mind: the principal components should capture as much of the variance in the data as possible, and each principal component should be uncorrelated to the other components. You can then plot the principal components to get a low-dimensional representation of your data, which hopefully captures most of its variation.\n\nBy design, the number of principal components computed are as many as the original number of variables, with the first having the largest variance, the second having the second largest variance, and so on. We hope that it will suffice to use just the first few of these to represent most of the variation in the data, but this is not guaranteed. Principal component analysis is more likely to yield a useful result if several variables are correlated.\n\n4.11.1 Running a principal component analysis\n\nTo illustrate the principles of PCA, we will use a dataset from Charytanowicz et al.Â (2010), containing measurements on wheat kernels for three varieties of wheat. A description of the variables is available at:\n\nhttp://archive.ics.uci.edu/ml/datasets/seeds\n\nWe are interested to find out if these measurements can be used to distinguish between the varieties. The data is stored in a .txt file, which we import using read.table (which works just like read.csv, but is tailored to text files) and convert the Variety column to a categorical factor variable (which youâll learn more about in Section 5.4):\n\n# The data is downloaded from the UCI Machine Learning Repository: # http://archive.ics.uci.edu/ml/datasets/seeds seeds <- read.table(\"https://tinyurl.com/seedsdata\", col.names = c(\"Area\", \"Perimeter\", \"Compactness\", \"Kernel_length\", \"Kernel_width\", \"Asymmetry\", \"Groove_length\", \"Variety\")) seeds$Variety <- factor(seeds$Variety)\n\nIf we make a scatterplot matrix of all variables, it becomes evident that there are differences between the varieties, but that no single pair of variables is enough to separate them:\n\nlibrary(ggplot2) library(GGally) ggpairs(seeds[, -8], aes(colour = seeds$Variety, alpha = 0.2))\n\nMoreover, for presentation purposes, the amount of information in the scatterplot matrix is a bit overwhelming. It would be nice to be able to present the data in a single scatterplot, without losing too much information. Weâll therefore compute the principal components using the prcomp function. It is usually recommended that PCA is performed using standardised data, i.e., using data that has been scaled to have mean 0 and standard deviation 1. The reason for this is that it puts all variables on the same scale. If we donât standardise our data, then variables with a high variance will completely dominate the principal components. This isnât desirable, as variance is affected by the scale of the measurements, meaning that the choice of measurement scale would influence the results (as an example, the variance of kernel length will be a million times greater if lengths are measured in millimetres instead of in metres).\n\nWe donât have to standardise the data ourselves, but can let prcomp do that for us using the arguments center = TRUE (to get mean 0) and scale. = TRUE (to get standard deviation 1):\n\n# Compute principal components: pca <- prcomp(seeds[,-8], center = TRUE, scale. = TRUE)\n\nTo see the loadings of the components, i.e., how much each variable contributes to the components, simply type the name of the object prcomp created:\n\nThe first principal component is more or less a weighted average of all variables but has stronger weights on Area, Perimeter, Kernel_length, Kernel_width, and Groove_length, all of which are measures of size. We can therefore interpret it as a size variable. The second component has higher loadings for Compactness and Asymmetry, meaning that it mainly measures those shape features. In Exercise 4.26 youâll see how the loadings can be visualised in a biplot.\n\n4.11.2 Choosing the number of components\n\nTo see how much of the variance each component represents, use summary:\n\nsummary(pca)\n\nThe first principal component accounts for 71.87% of the variance, and the first three combined account for 98.67%.\n\nTo visualise this, we can draw a scree plot, which shows the variance of each principal component â the total variance of the data is the sum of the variances of the principal components:\n\nscreeplot(pca, type = \"lines\")\n\nWe can use this to choose how many principal components to use when visualising or summarising our data. In that case, we look for an âelbowâ, i.e., a bend in the curve after which increasing the number of components doesnât increase the amount of variance explained much. In this case, we see an âelbowâ somewhere between two and four components.\n\n4.11.3 Plotting the results\n\nWe can access the values of the principal components using pca$x. Letâs check that the first two components really are uncorrelated:\n\ncor(pca$x[,1], pca$x[,2])\n\nIn this case, almost all of the variance is summarised by the first two or three principal components. It appears that we have successfully reduced the data from seven variables to between two and three, which should make visualisation much easier. The ggfortify package contains an autoplot function for PCA objects that creates a scatterplot of the first two principal components:\n\nlibrary(ggfortify) autoplot(pca, data = seeds, colour = \"Variety\")\n\nThat is much better! The groups are almost completely separated, which shows that the variables can be used to discriminate between the three varieties. The first principal component accounts for 71.87% of the total variance in the data, and the second for 17.11%.\n\nIf you like, you can plot other pairs of principal components than just components 1 and 2. In this case, component 3 may be of interest, as its variance is almost as high as that of component 2. You can specify which components to plot with the x and y arguments:\n\n# Plot 2nd and 3rd PC: autoplot(pca, data = seeds, colour = \"Variety\", x = 2, y = 3)\n\nHere, the separation is nowhere near as clear as in the previous figure. In this particular example, plotting the first two principal components is the better choice.\n\nJudging from these plots, it appears that the kernel measurements can be used to discriminate between the three varieties of wheat. In Chapters 7 and 11 youâll learn how to use R to build models that can be used to do just that, e.g., by predicting which variety of wheat a kernel comes from given its measurements. If we wanted to build a statistical model that could be used for this purpose, we could use the original measurements. But we could also try using the first two principal components as the only input to the model. Principal component analysis is very useful as a pre-processing tool used to create simpler models based on fewer variables (or ostensibly simpler, because the new variables are typically more difficult to interpret than the original ones).\n\n\\[\\sim\\]\n\nExercise 4.25 Use principal components on the carat, x, y, z, depth, and table variables in the diamonds data, and answer the following questions:\n\nHow much of the total variance does the first principal component account for? How many components are needed to account for at least 90% of the total variance?\n\nJudging by the loadings, what do the first two principal components measure?\n\nWhat is the correlation between the first principal component and price?\n\nCan the first two principal components be used to distinguish between diamonds with different cuts?\n\n(Click here to go to the solution.)\n\nExercise 4.26 Return to the scatterplot of the first two principal components for the seeds data created above. Adding the arguments loadings = TRUE and loadings.label = TRUE to the autoplot call creates a biplot, which shows the loadings for the principal components on top of the scatterplot. Create a biplot and compare the result to those obtained by looking at the loadings numerically. Do the conclusions from the two approaches agree?\n\n(Click here to go to the solution.)\n\n4.12 Cluster analysis\n\nCluster analysis is concerned with grouping observations into groups, clusters, that in some sense are similar. Numerous methods are available for this task, approaching the problem from different angles. Many of these are available in the cluster package, which ships with R. In this section, weâll look at a smorgasbord of clustering techniques.\n\n4.12.1 Hierarchical clustering\n\nAs a first example where clustering can be of interest, weâll consider the votes.repub data from cluster. It describes the proportion of votes for the Republican candidate in US presidential elections from 1856 to 1976 in 50 different states:\n\nlibrary(cluster) ?votes.repub View(votes.repub)\n\nWe are interested in finding subgroups â clusters â of states with similar voting patterns.\n\nTo find clusters of similar observations (states, in this case), we could start by assigning each observation to its own cluster. Weâd then start with 50 clusters, one for each observation. Next, we could merge the two clusters that are the most similar, yielding 49 clusters, one of which consisted of two observations and 48 consisting of a single observation. We could repeat this process, merging the two most similar clusters in each iteration until only a single cluster was left. This would give us a hierarchy of clusters, which could be plotted in a tree-like structure, where observations from the same cluster would be shown one the same branch. Like this:\n\nclusters_agnes <- agnes(votes.repub) plot(clusters_agnes, which = 2)\n\nThis type of plot is known as a dendrogram.\n\nWeâve just used agnes, a function from cluster that can be used to carry out hierarchical clustering in the manner described above. There are a couple of things that need to be clarified, though.\n\nFirst, how do we measure how similar two \\(p\\)-dimensional observations \\(x\\) and \\(y\\) are? agnes provides two measures of distance between points:\n\nmetric = \"euclidean\" (the default), uses the Euclidean \\(L_2\\) distance \\(||x-y||=\\sqrt{\\sum_{i=1}^p(x_i-y_i)^2}\\),\n\nmetric = \"manhattan\", uses the Manhattan \\(L_1\\) distance \\(||x-y||={\\sum_{i=1}^p|x_i-y_i|}\\).\n\nNote that neither of these work if you have categorical variables in your data. If all your variables are binary, i.e., categorical with two values, you can use mona instead of agnes for hierarchical clustering.\n\nSecond, how do we measure how similar two clusters of observations are? agnes offers a number of options here. Among them are:\n\nmethod = \"average\" (the default), unweighted average linkage, uses the average distance between points from the two clusters,\n\nmethod = \"single\", single linkage, uses the smallest distance between points from the two clusters,\n\nmethod = \"complete\", complete linkage, uses the largest distance between points from the two clusters,\n\nmethod = \"ward\", Wardâs method, uses the within-cluster variance to compare different possible clusterings, with the clustering with the lowest within-cluster variance chosen.\n\nRegardless of which of these you use, it is often a good idea to standardise the numeric variables in your dataset so that they all have the same variance. If you donât, your distance measure is likely to be dominated by variables with larger variance, while variables with low variances will have little or no impact on the clustering. To standardise your data, you can use scale:\n\n# Perform clustering on standardised data: clusters_agnes <- agnes(scale(votes.repub)) # Plot dendrogram: plot(clusters_agnes, which = 2)\n\nAt this point, weâre starting to use several functions one after another, and so this looks like a perfect job for a pipeline. To carry out the same analysis using the |> pipe, we write:\n\nvotes.repub |> scale() |> agnes() |> plot(which = 2)\n\nWe can now try changing the metric and clustering method used as described above. Letâs use the Manhattan distance and complete linkage:\n\nvotes.repub |> scale() |> agnes(metric = \"manhattan\", method = \"complete\") |> plot(which = 2)\n\nWe can change the look of the dendrogram by adding hang = -1, which causes all observations to be placed at the same level:\n\nvotes.repub |> scale() |> agnes(metric = \"manhattan\", method = \"complete\") |> plot(which = 2, hang = -1)\n\nAs an alternative to agnes, we can consider diana. agnes is an agglomerative method, which starts with a lot of clusters and then merge them step-by-step. diana, in contrast, is a divisive method, which starts with one large cluster and then step-by-step splits it into several smaller clusters.\n\nvotes.repub |> scale() |> diana() |> plot(which = 2)\n\nYou can change the distance measure used by setting metric in the diana call. Euclidean distance is the default.\n\nTo wrap this section up, weâll look at two packages that are useful for plotting the results of hierarchical clustering: dendextend and factoextra. We installed factoextra in the previous sectio, but still need to install dendextend:\n\ninstall.packages(\"dendextend\")\n\nTo compare the dendrograms produced by different methods (or the same method with different settings), in a tanglegram, where the dendrograms are plotted against each other, we can use tanglegram from dendextend:\n\nlibrary(dendextend) # Create clusters using agnes: votes.repub |> scale() |> agnes() -> clusters_agnes # Create clusters using diana: votes.repub |> scale() |> diana() -> clusters_diana # Compare the results: tanglegram(as.dendrogram(clusters_agnes), as.dendrogram(clusters_diana))\n\nSome clusters are quite similar here, whereas others are very different.\n\nOften, we are interested in finding a comparatively small number of clusters, \\(k\\). In hierarchical clustering, we can reduce the number of clusters by âcuttingâ the dendrogram tree. To do so using the factoextra package, we first use hcut to cut the tree into \\(k\\) parts, and then fviz_dend to plot the dendrogram, with each cluster plotted in a different colour. If, for instance, we want \\(k=5\\) clusters and want to use agnes with average linkage and Euclidean distance for the clustering, weâd do the following:\n\nlibrary(factoextra) votes.repub |> scale() |> hcut(k = 5, hc_func = \"agnes\", hc_method = \"average\", hc_metric = \"euclidean\") |> fviz_dend()\n\nThere is no inherent meaning to the colours â they are simply a way to visually distinguish between clusters.\n\nHierarchical clustering is especially suitable for data with named observations. For other types of data, other methods may be better. We will consider some alternatives next.\n\n\\[\\sim\\]\n\nExercise 4.27 Continue the last example above by changing the clustering method to complete linkage with the Manhattan distance.\n\nDo any of the five coloured clusters remain the same?\n\nHow can you produce a tanglegram with five coloured clusters, to better compare the results from the two clusterings?\n\n(Click here to go to the solution.)\n\nExercise 4.28 The USArrests data contains statistics on violent crime rates in 50 US states. Perform a hierarchical cluster analysis of the data. With which states is Maryland clustered?\n\n(Click here to go to the solution.)\n\n4.12.2 Heatmaps and clustering variables\n\nWhen looking at a dendrogram, you may ask why and how different observations are similar. Similarities between observations can be visualised using a heatmap, which displays the levels of different variables using colour hues or intensities. The heatmap function creates a heatmap from a matrix object. Letâs try it with the votes.repub voting data. Because votes.repub is a data.frame object, we have to convert it to a matrix with as.matrix first (see Section 2.10.2):\n\nlibrary(cluster) votes.repub |> as.matrix() |> heatmap()\n\nYou may want to increase the height of your Plot window so that the names of all states are displayed properly. Using the default colours, low values are represented by a light yellow and high values by a dark red. White represents missing values.\n\nYouâll notice that dendrograms are plotted along the margins. heatmap performs hierarchical clustering (by default, agglomerative with complete linkage) of the observations as well as of the variables. In the latter case, variables are grouped together based on similarities between observations, creating clusters of variables. In essence, this is just a hierarchical clustering of the transposed data matrix, but it does offer a different view of the data, which at times can be very revealing. The rows and columns are sorted according to the two hierarchical clusterings.\n\nAs per usual, it is a good idea to standardise the data before clustering, which can be done using the scale argument in heatmap. There are two options for scaling, either in the row direction (preferable if you wish to cluster variables) or the column direction (preferable if you wish to cluster observations):\n\n# Standardisation suitable for clustering variables: votes.repub |> as.matrix() |> heatmap(scale = \"row\") # Standardisation suitable for clustering observations: votes.repub |> as.matrix() |> heatmap(scale = \"col\")\n\nLooking at the first of these plots, we can see which elections (i.e., which variables) had similar outcomes in terms of Republican votes. For instance, we can see that the elections in 1960, 1976, 1888, 1884, 1880, and 1876 all had similar outcomes, with the large number of orange rows indicating that the Republicans neither did great nor did poorly.\n\nIf you like, you can change the colour palette used. As in Section 4.2.4, you can choose between palettes from http://www.colorbrewer2.org. heatmap is not a ggplot2 function, so this is done in a slightly different way than what youâre used to from other examples. Here are two examples, with the white-blue-purple sequential palette \"BuPu\" and the red-white-blue diverging palette \"RdBu\":\n\nlibrary(RColorBrewer) col_palette <- colorRampPalette(brewer.pal(8, \"BuPu\"))(25) votes.repub |> as.matrix() |> heatmap(scale = \"row\", col = col_palette) col_palette <- colorRampPalette(brewer.pal(8, \"RdBu\"))(25) votes.repub |> as.matrix() |> heatmap(scale = \"row\", col = col_palette)\n\n\\[\\sim\\]\n\nExercise 4.29 Draw a heatmap for the USArrests data. Have a look at Maryland and the states with which it is clustered. Do they have high or low crime rates?\n\n(Click here to go to the solution.)\n\n4.12.3 Centroid-based clustering\n\nLetâs return to the seeds data that we explored in Section 4.11:\n\n# Download the data: seeds <- read.table(\"https://tinyurl.com/seedsdata\", col.names = c(\"Area\", \"Perimeter\", \"Compactness\", \"Kernel_length\", \"Kernel_width\", \"Asymmetry\", \"Groove_length\", \"Variety\")) seeds$Variety <- factor(seeds$Variety)\n\nWe know that there are three varieties of seeds in this dataset, but what if we didnât? Or what if weâd lost the labels and didnât know what seeds are of what type? There are no row names for this data, and plotting a dendrogram may therefore not be that useful. Instead, we can use \\(k\\)-means clustering, where the points are clustered into \\(k\\) clusters based on their distances to the cluster means, or centroids.\n\nWhen performing \\(k\\)-means clustering (using the algorithm of Hartigan & Wong (1979) that is the default in the function that weâll use), the data is split into \\(k\\) clusters based on their distance to the mean of all points. Points are then moved between clusters, one at a time, based on how close they are (as measured by Euclidean distance) to the mean of each cluster. The algorithm finishes when no point can be moved between clusters without increasing the average distance between points and the means of their clusters.\n\nTo run a \\(k\\)-means clustering in R, we can use kmeans. Letâs start by using \\(k=3\\) clusters:\n\n# First, we standardise the data, and then we do a k-means # clustering. # We ignore variable 8, Variety, which is the group label. seeds[, -8] |> scale() |> kmeans(centers = 3) -> seeds_cluster seeds_cluster\n\nTo visualise the results, weâll plot the first two principal components. Weâll use colour to show the clusters. Moreover, weâll plot the different varieties in different shapes to see if the clusters found correspond to different varieties:\n\n# Compute principal components: pca <- prcomp(seeds[,-8]) library(ggfortify) autoplot(pca, data = seeds, colour = seeds_cluster$cluster, shape = \"Variety\", size = 2, alpha = 0.75)\n\nIn this case, the clusters more or less overlap with the varieties! Of course, in a lot of cases, we donât know the number of clusters beforehand. What happens if we change \\(k\\)?\n\nFirst, we try \\(k=2\\):\n\nseeds[, -8] |> scale() |> kmeans(centers = 2) -> seeds_cluster autoplot(pca, data = seeds, colour = seeds_cluster$cluster, shape = \"Variety\", size = 2, alpha = 0.75)\n\nNext, \\(k=4\\):\n\nseeds[, -8] |> scale() |> kmeans(centers = 4) -> seeds_cluster autoplot(pca, data = seeds, colour = seeds_cluster$cluster, shape = \"Variety\", size = 2, alpha = 0.75)\n\nAnd finally, a larger number of clusters, say \\(k=12\\):\n\nseeds[, -8] |> scale() |> kmeans(centers = 12) -> seeds_cluster autoplot(pca, data = seeds, colour = seeds_cluster$cluster, shape = \"Variety\", size = 2, alpha = 0.75)\n\nIf it werenât for the fact that the different varieties were shown as different shapes, weâd have no way to say, based on this plot alone, which choice of \\(k\\) is preferable here. Before we go into methods for choosing \\(k\\) though, weâll mention pam. pam is an alternative to \\(k\\)-means that works in the same way but uses median-like points, medoids instead of cluster means. This makes it more robust to outliers. Letâs try it with \\(k=3\\) clusters:\n\nseeds[, -8] |> scale() |> pam(k = 3) -> seeds_cluster autoplot(pca, data = seeds, colour = seeds_cluster$clustering, shape = \"Variety\", size = 2, alpha = 0.75)\n\nFor both kmeans and pam, there are visual tools that can help us choose the value of \\(k\\) in the factoextra package. Letâs install it:\n\ninstall.packages(\"factoextra\")\n\nThe fviz_nbclust function in factoextra can be used to obtain plots that can guide the choice of \\(k\\). It takes three arguments as input: the data, the clustering function (e.g., kmeans or pam) and the method used for evaluating different choices of \\(k\\). There are three options for the latter: \"wss\", \"silhouette\" and \"gap_stat\".\n\nmethod = \"wss\" yields a plot that relies on the within-cluster sum of squares, WSS, which is a measure of the within-cluster variation. The smaller this is, the more compact are the clusters. The WSS is plotted for several choices of \\(k\\), and we look for an âelbowâ, just as we did when using a scree plot for PCA. That is, we look for the value of \\(k\\) such that increasing \\(k\\) further doesnât improve the WSS much. Letâs have a look at an example, using pam for clustering:\n\nlibrary(factoextra) fviz_nbclust(scale(seeds[, -8]), pam, method = \"wss\") # Or, using a pipeline instead: seeds[, -8] |> scale() |> fviz_nbclust(pam, method = \"wss\")\n\n\\(k=3\\) seems like a good choice here.\n\nmethod = \"silhouette\" produces a silhouette plot. The silhouette value measures how similar a point is compared to other points in its cluster. The closer to 1 this value is, the better. In a silhouette plot, the average silhouette value for points in the data are plotted against \\(k\\):\n\nfviz_nbclust(scale(seeds[, -8]), pam, method = \"silhouette\")\n\nJudging by this plot, \\(k=2\\) appears to be the best choice.\n\nFinally, method = \"gap_stat\" yields a plot of the gap statistic (Tibshirani et al., 2001), which is based on comparing the WSS to its expected value under a null distribution obtained using the bootstrap (Section 7.4). Higher values of the gap statistic are preferable:\n\nfviz_nbclust(scale(seeds[, -8]), pam, method = \"gap_stat\")\n\nIn this case, \\(k=3\\) gives the best value.\n\nIn addition to plots for choosing \\(k\\), factoextra provides the function fviz_cluster for creating PCA-based plots, with an option to add convex hulls or ellipses around the clusters:\n\n# First, find the clusters: seeds[, -8] |> scale() |> kmeans(centers = 3) -> seeds_cluster # Plot clusters and their convex hulls: library(factoextra) fviz_cluster(seeds_cluster, data = seeds[, -8]) # Without row numbers: fviz_cluster(seeds_cluster, data = seeds[, -8], geom = \"point\") # With ellipses based on the multivariate normal distribution: fviz_cluster(seeds_cluster, data = seeds[, -8], geom = \"point\", ellipse.type = \"norm\")\n\nNote that in this plot, the shapes correspond to the clusters and not the varieties of seeds.\n\n\\[\\sim\\]\n\nExercise 4.30 The chorSub data from cluster contains measurements of 10 chemicals in 61 geological samples from the Kola Peninsula. Cluster this data using either kmeans or pam (does either seem to be a better choice here?). What is a good choice of \\(k\\) here? Visualise the results.\n\n(Click here to go to the solution.)\n\n4.12.4 Fuzzy clustering\n\nAn alternative to \\(k\\)-means clustering is fuzzy clustering, in which each point is âspread outâ over the \\(k\\) clusters instead of being placed in a single cluster. The more similar it is to other observations in a cluster, the higher is its membership in that cluster. Points can have a high degree of membership to several clusters, which is useful in applications where points should be allowed to belong to more than one cluster. An important example is genetics, where genes can encode proteins with more than one function. If each point corresponds to a gene, it then makes sense to allow the points to belong to several clusters, potentially associated with different functions. The opposite of fuzzy clustering is hard clustering, in which each point only belongs to one cluster.\n\nfanny from cluster can be used to perform fuzzy clustering:\n\nlibrary(cluster) seeds[, -8] |> scale() |> fanny(k = 3) -> seeds_cluster # Check membership of each cluster for the different points: seeds_cluster$membership # Plot the closest hard clustering: library(factoextra) fviz_cluster(seeds_cluster, geom = \"point\")\n\nAs for kmeans and pam, we can use fviz_nbclust to determine how many clusters to use:\n\nseeds[, -8] |> scale() |> fviz_nbclust(fanny, method = \"wss\") seeds[, -8] |> scale() |> fviz_nbclust(fanny, method = \"silhouette\") # Producing the gap statistic plot takes a while here, so # you may want to skip it in this case: seeds[, -8] |> scale() |> fviz_nbclust(fanny, method = \"gap\")\n\n\\[\\sim\\]\n\nExercise 4.31 Do a fuzzy clustering of the USArrests data. Is Maryland strongly associated with a single cluster, or with several clusters? What about New Jersey?\n\n(Click here to go to the solution.)\n\n4.12.5 Model-based clustering\n\nAs a last option, weâll consider model-based clustering, in which each cluster is assumed to come from a multivariate normal distribution. This will yield ellipsoidal clusters. Mclust from the mclust package fits such a model, called a Gaussian finite mixture model, using the EM-algorithm (Scrucca et al., 2016). First, letâs install the package:\n\ninstall.packages(\"mclust\")\n\nNow, letâs cluster the seeds data. The number of clusters is chosen as part of the clustering procedure. Weâll use a function from the factoextra for plotting the clusters with ellipsoids.\n\nlibrary(mclust) seeds_cluster <- Mclust(scale(seeds[, -8])) summary(seeds_cluster) # Plot results with ellipsoids: library(factoextra) fviz_cluster(seeds_cluster, geom = \"point\", ellipse.type = \"norm\")\n\nGaussian finite mixture models are based on the assumption that the data is numerical. For categorical data, we can use latent class analysis, which weâll discuss in Section 10.1.3, instead.\n\n\\[\\sim\\]\n\nExercise 4.32 Return to the chorSub data from Exercise 4.30. Cluster it using a Gaussian finite mixture model. How many clusters do you find?\n\n(Click here to go to the solution.)\n\n4.12.6 Comparing clusters\n\nHaving found some interesting clusters, we are often interested in exploring differences between the clusters. To do so, we must first extract the cluster labels from our clustering (which are contained in the variables clustering for methods with Western female names, cluster for kmeans, and classification for Mclust). We can then add those labels to our data frame and use them when plotting.\n\nFor instance, using the seeds data, we can compare the area of seeds from different clusters:\n\n# Cluster the seeds using k-means with k=3: library(cluster) seeds[, -8] |> scale() |> kmeans(centers = 3) -> seeds_cluster # Add the results to the data frame: seeds$clusters <- factor(seeds_cluster$cluster) # Instead of $cluster, we'd use $clustering for agnes, pam, and fanny # objects, and $classification for an Mclust object. # Compare the areas of the 3 clusters using boxplots: library(ggplot2) ggplot(seeds, aes(x = Area, group = clusters, fill = clusters)) + geom_boxplot() # Or using density estimates: ggplot(seeds, aes(x = Area, group = clusters, fill = clusters)) + geom_density(alpha = 0.7)\n\nWe can also create a scatterplot matrix to look at all variables simultaneously:\n\nlibrary(GGally) ggpairs(seeds[, -8], aes(colour = seeds$clusters, alpha = 0.2))\n\nIt may be tempting to run some statistical tests (e.g., a t-test) to see if there are differences between the clusters. Note, however, that in statistical hypothesis testing, it is typically assumed that the hypotheses that are being tested have been generated independently from the data. Double-dipping, where the data first is used to generate a hypothesis (âjudging from this boxplot, there seems to be a difference in means between these two groups!â or âI found these clusters, and now Iâll run a test to see if they are differentâ) and then test that hypothesis, is generally frowned upon, as that substantially inflates the risk of a type I error. Recently, however, there have been some advances in valid techniques for testing differences in means between clusters found using hierarchical clustering; see Gao et al.Â (2020)."
    }
}
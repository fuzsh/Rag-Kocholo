{
    "id": "dbpedia_8306_3",
    "rank": 66,
    "data": {
        "url": "https://medium.com/%40michalznalezniak/hierarchical-clustering-with-neural-networks-4ae0c01e1041",
        "read_more_link": "",
        "language": "en",
        "title": "Hierarchical Clustering with neural networks",
        "top_image": "https://miro.medium.com/v2/resize:fit:1200/1*Oj-hC0lxI60bflfmE0HOJw.png",
        "meta_img": "https://miro.medium.com/v2/resize:fit:1200/1*Oj-hC0lxI60bflfmE0HOJw.png",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/resize:fill:88:88/1*0GT4Tze8786rrC8Mc7PsQg.jpeg",
            "https://miro.medium.com/v2/resize:fill:144:144/1*0GT4Tze8786rrC8Mc7PsQg.jpeg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Michal Znalezniak",
            "medium.com"
        ],
        "publish_date": "2024-03-17T19:04:22.350000+00:00",
        "summary": "",
        "meta_description": "In this post, I would like to introduce you to CoHiClust, a Contrastive Hierarchical Clustering model based on deep neural networks, which can be applied to typical image data. CoHiClust distils the…",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19",
        "meta_site_name": "Medium",
        "canonical_link": "https://medium.com/@michalznalezniak/hierarchical-clustering-with-neural-networks-4ae0c01e1041",
        "text": "In this post, I would like to introduce you to CoHiClust, a Contrastive Hierarchical Clustering model based on deep neural networks, which can be applied to typical image data. CoHiClust distils the base network into a binary tree without access to any labelled data. CoHiClust outperforms the established and popular Agglomerative Clustering and generates a hierarchical structure of clusters consistent with human intuition and image semantics.\n\nYou can access the full paper of CoHiClust by clicking here.\n\nFor those who are interested in exploring the code, data, and additional resources associated with this study, you can find them in my GitHub repository here.\n\nClustering\n\nClustering, a fundamental branch of unsupervised learning is often one of the first steps in data analysis, which finds applications in anomaly detection [1], document clustering [2], bioinformatics [3] and many more. Initial approaches use representations taken from pre-trained models [4, 5] or employ autoencoders in joint training of the representation and the flat clustering model [6] or hierarchical clustering model [7]. Recent models designed to image data frequently follow the self-supervised learning principle, where the representation is trained on pairs of similar images generated automatically by data augmentations [8, 9]. Since augmentations used for image data are class-invariant, the latter techniques of ten obtain a very high similarity to the ground truth classes. However, we should be careful when comparing clustering techniques only by inspecting their accuracy with ground truth classes because the objective of clustering is not to perform classification.\n\nObjective of Clustering\n\nSo what’s the objective of clustering then? The primary objective of clustering is to discover structures and patterns in high-dimensional unlabeled data and group together data points with similar patterns. The above procedure reduces the complexity, facilitates the interpretation, and grants important insights into data. Let’s examine how much meaningful information a clustering algorithm can deliver when applied to a subset of the most widely recognized datasets in the field of computer vision and machine learning — ImageNet-10.\n\nThe algorithm reduces high-dimensional images into a hierarchy of groups that describes images and provides information that summarizes the dataset from high to low-level information. The hierarchy of groups is consistent with human intuition and image semantics, see Figure 1.\n\nThanks to the clustering we can understand that ImageNet-10 consists of two super-groups which can be categorised as ‘machines’ and ‘not machines’, four super-groups which can be categorised as ‘flying machines’, ‘not flying machines’, ‘animals’ and ‘not animals’, see Figure 2.\n\nHierarchical clustering\n\nHierarchical clustering organizes data into a tree-like structure where clusters are nested within each other, while flat clustering forms non-overlapping clusters without any hierarchy.\n\nHierarchical clustering can provide more detailed information about the relationships between data points, as it captures the hierarchical structure of the data. This can be useful for exploring nested relationships and understanding similarities at different levels of granularity.\n\nHierarchical clustering groups data based on similarity, forming a hierarchical structure of clusters. Approaches to hierarchical clustering typically belong to two main groups.\n\nAgglomerative: This is a “bottom-up” approach: Each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n\nDivisive: This is a “top-down” approach: All observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\n\nCoHiClust\n\nCoHiClust by backpropagation, jointly learns deep representation using SimCLR [10] framework and performs hierarchical clustering in a top-down manner. There are three key components of CoHiClust, see Figure 3.\n\nThe backbone neural network f(.), that generates the representation used by the hierarchical clustering head.\n\nThe hierarchical clustering head pi(.), which assigns data points to clusters by a sequence of decisions.\n\nThe regularized contrastive loss, which allows for training the whole framework.\n\nBackbone network\n\nCoHiClust utilizes ResNet architectures as its backbone network. This component projects images into an internal representation, which is then utilized by the clustering network. CoHiClust can rely on various architecture options. You can observe how a change of the backbone network influences the final quality of clustering in the full paper.\n\nHierarchical clustering head\n\nCoHiClust depends on a soft binary decision tree to create a hierarchical structure, where leaves play the role of clusters (similar to [11]). In contrast to hard decision trees, every internal node defines the probability of taking a left/right branch. The final assignment of the input examples to clusters involves partial decisions made by the internal nodes. Aggregating these decisions induces the posterior probability over the leaves.\n\nEach inner node is modelled by one neuron from the linear layer followed by the Sigmoid function, see Figure 4.\n\nLoss function\n\nCoHiClust is trained with a hierarchical contrastive loss function designed for trees — CoHiLoss. CoHiClust builds a hierarchical structure by maximizing the likelihood that similar data points will follow the same path. The more similar data points, the longer they should be routed through the same nodes. Clustering works in unsupervised setting, therefor CoHiClust uses a self-supervised approach and generates positives and negatives using data augmentations (SimCLR).\n\nMinimizing this loss maximizes the likelihood of similar data points sharing the same path (second term) and minimizes the likelihood of dissimilar ones being grouped together.\n\nResults\n\nCoHiClust was evaluated on several datasets of color images of various resolutions and with a diverse number of classes. In addition to reporting similarity scores with ground-truth partitions, the constructed hierarchies were analyzed, which is equally important in practical use-cases.\n\nTo measure the similarity of the constructed partition with the ground truth, four widely used clustering metrics were applied: normalized mutual information (NMI), clustering accuracy (ACC), adjusted rand index (ARI), and dendrogram purity (DP).\n\nComparison with deep hierarchical clustering methods\n\nIn the authors’ knowledge, DeepECT [7] is the only hierarchical clustering method based on deep neural networks. Following their experimental setup, we report the results on two popular image datasets, MNIST and F-MNIST, and consider classical hierarchical algorithms evaluated on the latent representation created by the autoencoder and IDEC [12]. The results summarized in Table 1 demonstrate that CoHiClust outperforms all baselines on both MNIST and F-MNIST datasets in terms of all metrics. Interestingly, DeepECT benefits from data augmentation in the case of MNIST, while on F-MNIST it deteriorates its performance. All methods except CoHiClust and DeepECT failed completely to create a hierarchy recovering true classes (see the DP measure), which confirms that there is a lack of powerful hierarchical clustering methods based on neural networks.\n\nWhile examining the MNIST and F-MNIST datasets, it’s apparent that CoHiClust produces clusters with a sensible structure. This structure is in line with our expectations and the inherent semantics of the images. Figure 6 and Figure 7 provide visual confirmation of this alignment.\n\nComparison with Agglomerative Hierarchical Clustering\n\nThe top layer responsible for constructing a decision tree is an important component of CoHiClust and cannot be replaced by alternative hierarchical clustering methods. For this purpose, a backbone network is first trained with a typical self-supervised SimCLR technique. Next, agglomeration clustering is applied to the resulting representation. As seen in Table 2, agglomerative clustering yields very low results, indicating that joint optimization of the backbone network and clustering tree using the proposed CoHiLoss is a significantly better choice. Consequently, the representation taken from a typical self-supervised learning model does not provide a representation that can be clustered accurately using simple methods.\n\nComparison with deep flat clustering methods\n\nCoHiClust was evaluated on typical benchmark datasets: CIFAR10, CIFAR-100, STL-10, ImageNet-Dogs, and ImageNet-10. The results presented in Table 3 show that CoHiClust outperforms the comparative methods in 3 out of 5 datasets. It gives extremely good results on CIFAR-10 and ImageNet-10, but is notably worse than CC on STL-10. Nevertheless, one should keep in mind that CoHiClust is the only hierarchical method in this comparison, and constructing a clustering hierarchy, which resembles ground truth classes, is more challenging than directly generating a flat partition.\n\nWhen analyzing the CIFAR10 and ImageNet-10 datasets, it becomes apparent that CoHiClust generates cluster structures that exhibit a high level of coherence, effectively capturing the underlying patterns within the data. This observation is particularly noteworthy as it aligns closely with our intuitive understanding of how these datasets should be organized. By examining Figure 1 and Figure 8, we can observe how the clusters formed by CoHiClust accurately reflect the semantic relationships present in the images, further reinforcing the validity of the clustering results.\n\nConclusions\n\nCoHiClust a contrastive hierarchical clustering suits well to clustering large-scale image databases. The hierarchical structure constructed by CoHiClust provides significantly more information about the data than typical flat clustering models. In particular, we can inspect the similarity between selected groups by measuring their distance in the hierarchy tree and, in consequence, find super-clusters. Experimental analysis performed on typical clustering benchmarks confirms that the produced partitions are highly similar to ground-truth classes. At the same time, CoHiClust allows us to discover important patterns that have not been encoded in the class labels.\n\nReferences\n\n[1] Liu, Hongfu, et al. “Clustering with outlier removal.” IEEE transactions on knowledge and data engineering 33.6 (2019): 2369–2379.\n\n[2] Steinbach, Michael, George Karypis, and Vipin Kumar. “A comparison of document clustering techniques.” (2000).\n\n[3] Oyelade, Jelili, et al. “Clustering algorithms: their application to gene expression data.” Bioinformatics and Biology insights 10 (2016): BBI-S38316.\n\n[4] Guérin, J., Gibaru, O., Thiery, S., Nyiri, E.: Cnn features are also great at unsupervised classification. arXiv preprint arXiv:1707.01700 (2017).\n\n[5] Naumov, S., Yaroslavtsev, G., Avdiukhin, D.: Objective-based hierarchical clustering of deep embedding vectors. In: AAAI. pp. 9055–9063 (2021).\n\n[6] Guo, X., Gao, L., Liu, X., Yin, J.: Improved deep embedded clustering with local structure preservation. In: Ijcai. pp. 1753–1759 (2017).\n\n[7] Mautz, D., Plant, C., Böhm, C.: Deep embedded cluster tree. In: 2019 IEEE International Conference on Data Mining (ICDM). pp. 1258–1263. IEEE (2019).\n\n[8] Li, Y., Hu, P., Liu, Z., Peng, D., Zhou, J.T., Peng, X.: Contrastive clustering.In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 35, pp. 8547–8555 (2021).\n\n[9] Dang, Z., Deng, C., Yang, X., Wei, K., Huang, H.: Nearest neighbor matching for deep clustering. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13693–13702 (2021).\n\n[10] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. In: III, H.D., Singh, A. (eds.) Proceedings of the 37th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 119, pp. 1597–1607. PMLR (13–18 Jul 2020).\n\n[11] Frosst, N., Hinton, G.: Distilling a neural network into a soft decision tree. arXiv preprint arXiv:1711.09784 (2017).\n\n[12] Guo, X., Gao, L., Liu, X., Yin, J.: Improved deep embedded clustering with local structure preservation. In: Ijcai. pp. 1753–1759 (2017)."
    }
}
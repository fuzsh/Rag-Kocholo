{
    "id": "dbpedia_8306_1",
    "rank": 61,
    "data": {
        "url": "https://docs.openshift.com/container-platform/4.12/release_notes/ocp-4-12-release-notes.html",
        "read_more_link": "",
        "language": "en",
        "title": "OpenShift Container Platform 4.12 release notes",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.redhat.com/cms/managed-files/Logo-Red_Hat-OpenShift-A-Standard-RGB_0_0.svg",
            "https://assets.openshift.net/content/img/Logo-Red_Hat-OpenShift-A-Reverse-RGB.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "https://assets.openshift.net/content/subdomain/touch-icon-precomposed.png",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Previously, instances were not set to respect the GCP infrastructure default option for automated restarts. As a result, instances could be created without using the infrastructure default for automatic restarts. This sometimes meant that instances were terminated in GCP but their associated machines were still listed in the Running state because they did not automatically restart. With this release, the code for passing the automatic restart option has been improved to better detect and pass on the default option selection from users. Instances now use the infrastructure default properly and are automatically restarted when the user requests the default functionality. (OCPBUGS-4504)\n\nThe v1beta1 version of the PodDisruptionBudget object is now deprecated in Kubernetes. With this release, internal references to v1beta1 are replaced with v1. This change is internal to the cluster autoscaler and does not require user action beyond the advice in the Preparing to upgrade to OpenShift Container Platform 4.12 Red Hat Knowledgebase Article. (OCPBUGS-1484)\n\nPreviously, the GCP machine controller reconciled the state of machines every 10 hours. Other providers set this value to 10 minutes so that changes that happen outside of the Machine API system are detected within a short period. The longer reconciliation period for GCP could cause unexpected issues such as missing certificate signing requests (CSR) approvals due to an external IP address being added but not detected for an extended period. With this release, the GCP machine controller is updated to reconcile every 10 minutes to be consistent with other platforms and so that external changes are picked up sooner. (OCPBUGS-4499)\n\nPreviously, due to a deployment misconfiguration for the Cluster Machine Approver Operator, enabling the TechPreviewNoUpgrade feature set caused errors and sporadic Operator degradation. Because clusters with the TechPreviewNoUpgrade feature set enabled use two instances of the Cluster Machine Approver Operator and both deployments used the same set of ports, there was a conflict that lead to errors for single-node topology. With this release, the Cluster Machine Approver Operator deployment is updated to use a different set of ports for different deployments. (OCPBUGS-2621)\n\nPreviously, the scale from zero functionality in Azure relied on a statically compiled list of instance types mapping the name of the instance type to the number of CPUs and the amount of memory allocated to the instance type. This list grew stale over time. With this release, information about instance type sizes is dynamically gathered from the Azure API directly to prevent the list from becoming stale. (OCPBUGS-2558)\n\nPreviously, Machine API termination handler pods did not start on spot instances. As a result, pods that were running on tainted spot instances did not receive a termination signal if the instance was terminated. This could result in loss of data in workload applications. With this release, the Machine API termination handler deployment is modified to tolerate the taints and pods running on spot instances with taints now receive termination signals. (OCPBUGS-1274)\n\nPreviously, error messages for Azure clusters did not explain that it is not possible to create new machines with public IP addresses for a disconnected install that uses only the internal publish strategy. With this release, the error message is updated for improved clarity. (OCPBUGS-519)\n\nPreviously, the Cloud Controller Manager Operator did not check the cloud-config configuration file for AWS clusters. As a result, it was not possible to pass additional settings to the AWS cloud controller manager component by using the configuration file. With this release, the Cloud Controller Manager Operator checks the infrastructure resource and parses references to the cloud-config configuration file so that users can configure additional settings. (BZ#2104373)\n\nPreviously, when Azure added new instance types and enabled accelerated networking support on instance types that previously did not have it, the list of Azure instances in the machine controller became outdated. As a result, the machine controller could not create machines with instance types that did not previously support accelerated networking, even if they support this feature on Azure. With this release, the required instance type information is retrieved from Azure API before the machine is created to keep it up to date so the machine controller is able to create machines with new and updated instance types. This fix also applies to any instance types that are added in the future. (BZ#2108647)\n\nPreviously, the cluster autoscaler did not respect the AWS, IBM Cloud, and Alibaba Cloud topology labels for the CSI drivers when using the Cluster API provider. As a result, nodes with the topology label were not processed properly by the autoscaler when attempting to balance nodes during a scale-out event. With this release, the autoscaler’s custom processors are updated so that it respects this label. The autoscaler can now balance similar node groups that are labeled by the AWS, IBM Cloud, or Alibaba CSI labels. (BZ#2001027)\n\nPreviously, Power VS cloud providers were not capable of fetching the machine IP address from a DHCP server. Changing the IP address did not update the node, which caused some inconsistencies, such as pending certificate signing requests. With this release, the Power VS cloud provider is updated to fetch the machine IP address from the DHCP server so that the IP addresses for the nodes are consistent with the machine IP address. (BZ#2111474)\n\nPreviously, machines created in early versions of OpenShift Container Platform with invalid configurations could not be deleted. With this release, the webhooks that prevent the creation of machines with invalid configurations no longer prevent the deletion of existing invalid machines. Users can now successfully remove these machines from their cluster by manually removing the finalizers on these machines. (BZ#2101736)\n\nPreviously, short DHCP lease times, caused by NetworkManager not being run as a daemon or in continuous mode, caused machines to become stuck during initial provisioning and never become nodes in the cluster. With this release, extra checks are added so that if a machine becomes stuck in this state it is deleted and recreated automatically. Machines that are affected by this network condition can become nodes after a reboot from the Machine API controller. (BZ#2115090)\n\nPreviously, when creating a new Machine resource using a machine profile that does not exist in IBM Cloud, the machines became stuck in the Provisioning phase. With this release, validation is added to the IBM Cloud Machine API provider to ensure that a machine profile exists, and machines with an invalid machine profile are rejected by the Machine API. (BZ#2062579)\n\nPreviously, the Machine API provider for AWS did not verify that the security group defined in the machine specification exists. Instead of returning an error in this case, it used a default security group, which should not be used for OpenShift Container Platform machines, and successfully created a machine without informing the user that the default group was used. With this release, the Machine API returns an error when users set either incorrect or empty security group names in the machine specification. (BZ#2060068)\n\nPreviously, the Machine API provider Azure did not treated user-provided values for instance types as case sensitive. This led to false-positive errors when instance types were correct but did not match the case. With this release, instance types are converted to the lowercase characters so that users get correct results without false-positive errors for mismatched case. (BZ#2085390)\n\nPreviously, there was no check for nil values in the annotations of a machine object before attempting to access the object. This situation was rare, but caused the machine controller to panic when reconciling the machine. With this release, nil values are checked and the machine controller is able to reconcile machines without annotations. (BZ#2106733)\n\nPreviously, the cluster autoscaler metrics for cluster CPU and memory usage would never reach, or exceed, the limits set by the ClusterAutoscaler resource. As a result, no alerts were fired when the cluster autoscaler could not scale due to resource limitations. With this release, a new metric called cluster_autoscaler_skipped_scale_events_count is added to the cluster autoscaler to more accurately detect when resource limits are reached or exceeded. Alerts will now fire when the cluster autoscaler is unable to scale the cluster up because it has reached the cluster resource limits. (BZ#1997396)\n\nPreviously, when the Machine API provider failed to fetch the machine IP address, it would not set the internal DNS name and the machine certificate signing requests were not automatically approved. With this release, the Power VS machine provider is updated to set the server name as the internal DNS name even when it fails to fetch the IP address. (BZ#2111467)\n\nPreviously, the Machine API vSphere machine controller set the PowerOn flag when cloning a VM. This created a PowerOn task that the machine controller was not aware of. If that PowerOn task failed, machines were stuck in the Provisioned phase but never powered on. With this release, the cloning sequence is altered to avoid the issue. Additionally, the machine controller now retries powering on the VM in case of failure and reports failures properly. (BZ#2087981, OCPBUGS-954)\n\nWith this release, AWS security groups are tagged immediately instead of after creation. This means that fewer requests are sent to AWS and the required user privileges are lowered. (BZ#2098054, OCPBUGS-3094)\n\nPreviously, a bug in the RHOSP legacy cloud provider resulted in a crash if certain RHOSP operations were attempted after authentication had failed. For example, shutting down a server causes the Kubernetes controller manager to fetch server information from RHOSP, which triggered this bug. As a result, if initial cloud authentication failed or was configured incorrectly, shutting down a server caused the Kubernetes controller manager to crash. With this release, the RHOSP legacy cloud provider is updated to not attempt any RHOSP API calls if it has not previously authenticated successfully. Now, shutting down a server with invalid cloud credentials no longer causes Kubernetes controller manager to crash. (BZ#2102383)\n\nPreviously, the number of supported user-defined tags was 8, and reserved OpenShift Container Platform tags were 2 for AWS resources. With this release, the number of supported user-defined tags is now 25 and reserved OpenShift Container Platform tags are 25 for AWS resources. You can now add up to 25 user tags during installation. (CFE#592)\n\nPreviously, installing a cluster on Amazon Web Services started and then failed when the IAM administrative user was not assigned the s3:GetBucketPolicy permission. This update adds this policy to checklist that the installation program uses to ensure that all of the required permissions are assigned. As a result, the installation program now stops the installation with a warning that the IAM administrative user is missing the s3:GetBucketPolicy permission. (BZ#2109388)\n\nPreviously, installing a cluster on Microsoft Azure failed when the Azure DCasv5-series or DCadsv5-series of confidential VMs were specified as control plane nodes. With this update, the installation program now stops the installation with an error, which states that confidential VMs are not yet supported. (BZ#2055247)\n\nPreviously, gathering bootstrap logs was not possible until the control plane machines were running. With this update, gathering bootstrap logs now only requires that the bootstrap machine be available. (BZ#2105341)\n\nPreviously, if a cluster failed to install on Google Cloud Platform because the service account had insufficient permissions, the resulting error message did not mention this as the cause of the failure. This update improves the error message, which now instructs users to check the permissions that are assigned to the service account. (BZ#2103236)\n\nPreviously, when an installation on Google Cloud provider (GCP) failed because an invalid GCP region was specified, the resulting error message did not mention this as the cause of the failure. This update improves the error message, which now states the region is not valid. (BZ#2102324)\n\nPreviously, cluster installations using Hive could fail if Hive used an older version of the install-config.yaml file. This update allows the installation program to accept older versions of the install-config.yaml file provided by Hive. (BZ#2098299)\n\nPreviously, the installation program would incorrectly allow the apiVIP and ingressVIP parameters to use the same IPv6 address if they represented the address differently, such as listing the address in an abbreviated format. In this update, the installer correctly validates these two parameters regardless of their formatting, requiring separate IP addresses for each parameter. (BZ#2103144)\n\nPreviously, uninstalling a cluster using the installation program failed to delete all resources in clusters installed on GCP if the cluster name was more than 22 characters long. In this update, uninstalling a cluster using the installation program correctly locates and deletes all GCP cluster resources in cases of long cluster names. (BZ#2076646)\n\nPreviously, when installing a cluster on Red Hat OpenStack Platform (RHOSP) with multiple networks defined in the machineNetwork parameter, the installation program only created security group rules for the first network. With this update, the installation program creates security group rules for all networks defined in the machineNetwork so that users no longer need to manually edit security group rules after installation. (BZ#2095323)\n\nPreviously, users could manually set the API and Ingress virtual IP addresses to values that conflicted with the allocation pool of the DHCP server when installing a cluster on OpenStack. This could cause the DHCP server to assign one of the VIP addresses to a new machine, which would fail to start. In this update, the installation program validates the user-provided VIP addresses to ensure that they do not conflict with any DHCP pools. (BZ#1944365)\n\nPreviously, when installing a cluster on vSphere using a datacenter that is embedded inside a folder, the installation program could not locate the datacenter object, causing the installation to fail. In this update, the installation program can traverse the directory that contains the datacenter object, allowing the installation to succeed. (BZ#2097691)\n\nPreviously, when installing a cluster on Azure using arm64 architecture with installer-provisioned infrastructure, the image definition resource for hyperVGeneration V1 incorrectly had an architecture value of x64. With this update, the image definition resource for hyperVGeneration V1 has the correct architecture value of Arm64. (OCPBUGS-3639)\n\nPreviously, when installing a cluster on VMware vSphere, the installation could fail if the user specified a user-defined folder in the failureDomain section of the install-config.yaml file. With this update, the installation program correctly validates user-defined folders in the failureDomain section of the install-config.yaml file. (OCPBUGS-3343)\n\nPreviously, when destroying a partially deployed cluster after an installation failed on VMware vSphere, some virtual machine folders were not destroyed. This error could occur in clusters configured with multiple vSphere datacenters or multiple vSphere clusters. With this update, all installer-provisioned infrastructure is correctly deleted when destroying a partially deployed cluster after an installation failure. (OCPBUGS-1489)\n\nPreviously, when installing a cluster on VMware vSphere, the installation failed if the user specified the platform.vsphere.vcenters parameter but did not specify the platform.vsphere.failureDomains.topology.networks parameter in the install-config.yaml file. With this update, the installation program alerts the user that the platform.vsphere.failureDomains.topology.networks field is required when specifying platform.vsphere.vcenters. (OCPBUGS-1698)\n\nPreviously, when installing a cluster on VMware vSphere, the installation failed if the user defined the platform.vsphere.vcenters and platform.vsphere.failureDomains parameters but did not define platform.vsphere.defaultMachinePlatform.zones, or compute.platform.vsphere.zones and controlPlane.platform.vsphere.zones. With this update, the installation program validates that the user has defined the zones parameter in multi-region or multi-zone deployments prior to installation. (OCPBUGS-1490)\n\nPreviously, the Operator details page attempted to display multiple error messages, but the error message component can only display a single error message at a time. As a result, relevant error messages were not displayed. With this update, the Operator details page displays only the first error message so the user sees a relevant error. (OCPBUGS-3927)\n\nPreviously, the product name for Azure Red Hat OpenShift was incorrect in Customer Case Management (CCM). As a result, the console had to use the same incorrect product name to correctly populate the fields in CCM. Once the product name in CCM was updated, the console needed to be updated as well. With this update, the same, correct product name as CCM is correctly populated with the correct Azure product name when following the link from the console. (OCPBUGS-869)\n\nPreviously, when a plugin page resulted in an error, the error did not reset when navigating away from the error page, and the error persisted after navigating to a page that was not the cause of the error. With this update, the error state is reset to its default when a user navigates to a new page, and the error no longer persists after navigating to a new page. (BZ#2117738, OCPBUGS-523)\n\nPreviously, the View it here link in the Operator details pane for installed Operators was incorrectly built when All Namespaces was selected. As a result, the link attempted to navigate to the Operator details page for a cluster service version (CSV) in All Projects, which is an invalid route. With this update, the View it here link to use the namespace where the CSV is installed now builds correctly and the link works as expected. (OCPBUGS-184)\n\nPreviously, line numbers with more than five digits resulted in a cosmetic issue where the line number overlaid the vertical divider between the line number and the line contents making it harder to read. With this update, the amount of space available for line numbers was increased to account for longer line numbers, and the line number no longer overlays the vertical divider. (OCPBUGS-183)\n\nPreviously, in the administrator perspective of the web console, the link to Learn more about the OpenShift local update services on the Default update server pop-up window in the Cluster Settings page produced a 404 error. With this update, the link works as expected. (BZ#2098234)\n\nPreviously, the MatchExpression component did not account for array-type values. As a result, only single values could be entered through forms using this component. With this update, the MatchExpression component accepts comma-separated values as an array. (BZ#207690)\n\nPreviously, there were redundant checks for the model resulting in tab reloading which occasionally resulted in a flickering of the tab contents where they rerendered. With this update, the redundant model check was removed, and the model is only checked once. As a result, the tab contents do not flicker and no longer rerender. (BZ#2037329)\n\nPreviously, when selecting the edit label from the action list on the OpenShift Dedicated node page, no response was elicited and a web hook error was returned. This issue has been fixed so that the error message is only returned when editing fails. (BZ#2102098)\n\nPreviously, if issues were pending, clicking on the Insights link would crash the page. As a workaround, you can wait for the variable to become initialized before clicking the Insights link. As a result, the Insights page will open as expected. (BZ#2052662)\n\nPreviously, when the MachineConfigPool resource was paused, the option to unpause said Resume rollouts. The wording has been updated so that it now says Resume updates. (BZ#2094240)\n\nPreviously, the wrong calculating method was used when counting master and worker nodes. With this update, the correct worker nodes are calculated when nodes have both the master and worker role. (BZ#1951901)\n\nPreviously, conflicting react-router routes for ImageManifestVuln resulted in attempts to render a details page for ImageManifestVuln with a ~new name. Now, the container security plugin has been updated to remove conflicting routes and to ensure dynamic lists and details page extensions are used on the Operator details page. As a result, the console renders the correct create, list, and details pages for ImageManifestVuln. (BZ#2080260)\n\nPreviously, incomplete YAML was not synced was occasionally displayed to users. With this update, synced YAML always displays. (BZ#2084453)\n\nPreviously, when installing an Operator that required a custom resource (CR) to be created for use, the Create resource button could fail to install the CR because it was pointing to the incorrect namespace. With this update, the Create resource button works as expected. (BZ#2094502)\n\nPreviously, the Cluster update modal was not displaying errors properly. As a result, the Cluster update modal did not display or explain errors when they occurred. With this update, the Cluster update modal correctly display errors. (BZ#2096350)\n\nPreviously, routers that were in the terminating state delayed the oc cp command which would delay the oc adm must-gather command until the pod was terminated. With this update, a timeout for each issued oc cp command is set to prevent delaying the must-gather command from running. As a result, terminating pods no longer delay must-gather commands. (BZ#2103283)\n\nPreviously, an Ingress Controller could not be configured with both the Private endpoint publishing strategy type and PROXY protocol. With this update, users can now configure an Ingress Controller with both the Private endpoint publishing strategy type and PROXY protocol. (BZ#2104481)\n\nPreviously, the routeSelector parameter cleared the route status of the Ingress Controller prior to the router deployment. Because of this, the route status repopulated incorrectly. To avoid using stale data, route status detection has been updated to no longer rely on the Kubernetes object cache. Additionally, this update includes a fix to check the generation ID on route deployment to determine the route status. As a result, the route status is consistently cleared with a routeSelector update. (BZ#2101878)\n\nPreviously, a cluster that was upgraded from a version of OpenShift Container Platform earlier than 4.8 could have orphaned Route objects. This was caused by earlier versions of OpenShift Container Platform translating Ingress objects into Route objects irrespective of a given Ingress object’s indicated IngressClass. With this update, an alert is sent to the cluster administrator about any orphaned Route objects still present in the cluster after Ingress-to-Route translation. This update also adds another alert that notifies the cluster administrator about any Ingress objects that do not specify an IngressClass. (BZ#1962502)\n\nPreviously, if a configmap that the router deployment depends on is not created, then the router deployment does not progress. With this update, the cluster Operator reports ingress progressing=true if the default ingress controller deployment is progressing. This results in users debugging issues with the ingress controller by using the command oc get co. (BZ#2066560)\n\nPreviously, when an incorrectly created network policy was added to the OVN-Kubernetes cache, it would cause the OVN-Kubernetes leader to enter crashloopbackoff status. With this update, OVN-Kubernetes leader does not enter crashloopbackoff status by skipping deleting nil policies. (BZ#2091238)\n\nPreviously, recreating an EgressIP pod with the same namespace or name within 60 seconds of deleting an older one with the same namespace or name causes the wrong SNAT to be configured. As a result, packets could go out with nodeIP instead of EgressIP SNAT. With this update, traffic leaves the pod with EgressIP instead of nodeIP. (BZ#2097243).\n\nPreviously, older Access Control Lists (ACL)s with arp produced unexpectedly found multiple equivalent ACLs (arp v/s arp||nd) errors due to a change in the ACL from arp to arp II nd. This prevented network policies from being created properly. With this update, older ACLs with just the arp match have been removed so that only ACLs with the new arp II nd match exist so that network policies can be created correctly and no errors will be observed on ovnkube-master. NOTE: This effects customers upgrading into 4.8.14, 4.9.32, 4.10.13 or higher from older versions. (BZ#2095852).\n\nWith this update, CoreDNS has been updated to version 1.10.0, which is based on Kubernetes 1.25. This keeps both the CoreDNS version and OpenShift Container Platform 4.12, which is also based on Kubernetes 1.25, in alignment with one another. (OCPBUGS-1731)\n\nWith this update, the OpenShift Container Platform router now uses k8s.io/client-go version 1.25.2, which supports Kubernetes 1.25. This keeps both the openshift-router and OpenShift Container Platform 4.12, which is also based on Kubernetes 1.25, in alignment with one another. (OCPBUGS-1730)\n\nWith this update, the Ingress Operator now uses k8s.io/client-go version 1.25.2, which supports Kubernetes 1.25. This keeps both the Ingress Operator and OpenShift Container Platform 4.12, which is also based on Kubernetes 1.25, in alignment with one another. (OCPBUGS-1554)\n\nPreviously, the DNS Operator did not reconcile the openshift-dns namespace. Because OpenShift Container Platform 4.12 requires the openshift-dns namespace to have pod-security labels, this caused the namespace to be missing those labels upon cluster update. Without the pod-security labels, the pods failed to start. With this update, the DNS Operator now reconciles the openshift-dns namespace, and the pod-security labels are now present. As a result, pods start as expected. (OCPBUGS-1549)\n\nPreviously, the ingresscontroller.spec.tuniningOptions.reloadInterval did not support decimal numerals as valid parameter values because the Ingress Operator internally converts the specified value into milliseconds, which was not a supported time unit. This prevented an Ingress Controller from being deleted. With this update, ingresscontroller.spec.tuningOptions.reloadInterval now supports decimal numerals and users can delete Ingress Controllers with reloadInterval parameter values which were previously unsupported. (OCPBUGS-236)\n\nPreviously, the Cluster DNS Operator used GO Kubernetes libraries that were based on Kubernetes 1.24 while OpenShift Container Platform 4.12 is based on Kubernetes 1.25. With this update, GO Kubernetes API is v1.25.2, which aligns the Cluster DNS Operator with OpenShift Container Platform 4.12 that uses Kubernetes 1.25 APIs. (link: OCPBUGS-1558)\n\nPreviously, setting the disableNetworkDiagnostics configuration to true did not persist when the network-operator pod was re-created. With this update, the disableNetworkDiagnostics configuration property of network operator.openshift.io/cluster no longer resets to its default value after network operator restart. (OCPBUGS-392)\n\nPreviously, ovn-kubernetes did not configure the correct MAC address of bonded interfaces in br-ex bridge. As a result, a node that uses bonding for the primary Kubernetes interface fails to join the cluster. With this update, ovn-kubernetes configures the correct MAC address of bonded interfaces in br-ex bridge, and nodes that use bonding for the primary Kubernetes interface successfully join the cluster. (BZ2096413)\n\nPreviously, when the Ingress Operator was configured to enable the use of mTLS, the Operator would not check if CRLs needed updating until some other event caused it to reconcile. As a result, CRLs used for mTLS could become out of date. With this update, the Ingress Operator now automatically reconciles when any CRL expires, and CRLs will be updated at the time specified by their nextUpdate field. (BZ#2117524)\n\nThe oc annotate command does not work for LDAP group names that contain an equal sign (=), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use oc patch or oc edit to add the annotation. (BZ#1917280)\n\nDue to the inclusion of old images in some image indexes, running oc adm catalog mirror and oc image mirror might result in the following error: error: unable to retrieve source image. As a temporary workaround, you can use the --skip-missing option to bypass the error and continue downloading the image index. For more information, see Service Mesh Operator mirroring failed.\n\nWhen using the egress IP address feature in OpenShift Container Platform on RHOSP, you can assign a floating IP address to a reservation port to have a predictable SNAT address for egress traffic. The floating IP address association must be created by the same user that installed the OpenShift Container Platform cluster. Otherwise any delete or move operation for the egress IP address hangs indefinitely because of insufficient privileges. When this issue occurs, a user with sufficient privileges must manually unset the floating IP address association to resolve the issue. (OCPBUGS-4902)\n\nThere is a known issue with Nutanix installation where the installation fails if you use 4096-bit certificates with Prism Central 2022.x. Instead, use 2048-bit certificates. (KCS)\n\nDeleting the bidirectional forwarding detection (BFD) profile and removing the bfdProfile added to the border gateway protocol (BGP) peer resource does not disable the BFD. Instead, the BGP peer starts using the default BFD profile. To disable BFD from a BGP peer resource, delete the BGP peer configuration and recreate it without a BFD profile. (BZ#2050824)\n\nDue to an unresolved metadata API issue, you cannot install clusters that use bare-metal workers on RHOSP 16.1. Clusters on RHOSP 16.2 are not impacted by this issue. (BZ#2033953)\n\nThe loadBalancerSourceRanges attribute is not supported, and is therefore ignored, in load-balancer type services in clusters that run on RHOSP and use the OVN Octavia provider. There is no workaround for this issue. (OCPBUGS-2789)\n\nAfter a catalog source update, it takes time for OLM to update the subscription status. This can mean that the status of the subscription policy may continue to show as compliant when Topology Aware Lifecycle Manager (TALM) decides whether remediation is needed. As a result the operator specified in the subscription policy does not get upgraded. As a workaround, include a status field in the spec section of the catalog source policy as follows:\n\nmetadata: name: redhat-operators-disconnected spec: displayName: disconnected-redhat-operators image: registry.example.com:5000/disconnected-redhat-operators/disconnected-redhat-operator-index:v4.11 status: connectionState: lastObservedState: READY\n\nThis mitigates the delay for OLM to pull the new index image and get the pod ready, reducing the time between completion of catalog source policy remediation and the update of the subscription status. If the issue persists and the subscription policy status update is still late you can apply another ClusterGroupUpdate CR with the same subscription policy, or an identical ClusterGroupUpdate CR with a different name. (OCPBUGS-2813)\n\nTALM skips remediating a policy if all selected clusters are compliant when the ClusterGroupUpdate CR is started. The update of operators with a modified catalog source policy and a subscription policy in the same ClusterGroupUpdate CR does not complete. The subscription policy is skipped as it is still compliant until the catalog source change is enforced. As a workaround, add the following change to one CR in the common-subscription policy, for example:\n\nmetadata.annotations.upgrade: \"1\"\n\nThis makes the policy non-compliant prior to the start of the ClusterGroupUpdate CR. (OCPBUGS-2812)\n\nOn a single-node OpenShift instance, rebooting without draining the node to remove all the running pods can cause issues with workload container recovery. After the reboot, the workload restarts before all the device plugins are ready, resulting in resources not being available or the workload running on the wrong NUMA node. The workaround is to restart the workload pods when all the device plugins have re-registered themselves during the reboot recovery procedure. (OCPBUGS-2180)\n\nThe default dataset_comparison is currently ieee1588. The recommended dataset_comparison is G.8275.x. It is planned to be fixed in a future version of OpenShift Container Platform. In the short term, you can manually update the ptp configuration to include the recommended dataset_comparison. (OCPBUGS-2336)\n\nThe default step_threshold is 0.0. The recommended step_threshold is 2.0. It is planned to be fixed in a future version of OpenShift Container Platform. In the short term, you can manually update the ptp configuration to include the recommended step_threshold. (OCPBUGS-3005)\n\nThe BMCEventSubscription CR fails to create a Redfish subscription for a spoke cluster in an ACM-deployed multi-cluster environment, where the metal3 service is only running on a hub cluster. The workaround is to create the subscription by calling the Redfish API directly, for example, by running the following command:\n\ncurl -X POST -i --insecure -u \"<BMC_username>:<BMC_password>\" https://<BMC_IP>/redfish/v1/EventService/Subscriptions \\ -H 'Content-Type: application/json' \\ --data-raw '{ \"Protocol\": \"Redfish\", \"Context\": \"any string is valid\", \"Destination\": \"https://hw-event-proxy-openshift-bare-metal-events.apps.example.com/webhook\", \"EventTypes\": [\"Alert\"] }'\n\nYou should receive a 201 Created response and a header with Location: /redfish/v1/EventService/Subscriptions/<sub_id> that indicates that the Redfish events subscription is successfully created. (OCPBUGSM-43707)\n\nWhen using the GitOps ZTP pipeline to install a single-node OpenShift cluster in a disconnected environment, there should be two CatalogSource CRs applied in the cluster. One of the CatalogSource CRs gets deleted following multiple node reboots. As a workaround, you can change the default names, such as certified-operators and redhat-operators, of the catalog sources. (OCPBUGSM-46245)\n\nIf an invalid subscription channel is specified in the subscription policy that is used to perform a cluster upgrade, the Topology Aware Lifecycle Manager indicates a successful upgrade right after the policy is enforced because the Subscription state remains AtLatestKnown. (OCPBUGSM-43618)\n\nThe SiteConfig disk partition definition fails when applied to multiple nodes in a cluster. When a SiteConfig CR is used to provision a compact cluster, creating a valid diskPartition config on multiple nodes fails with a Kustomize plugin error. (OCPBUGSM-44403)\n\nIf secure boot is currently disabled and you try to enable it using ZTP, the cluster installation does not start. When secure boot is enabled through ZTP, the boot options are configured before the virtual CD is attached. Therefore, the first boot from the existing hard disk has the secure boot turned on. The cluster installation gets stuck because the system never boots from the CD. (OCPBUGSM-45085)\n\nUsing Red Hat Advanced Cluster Management (RHACM), spoke cluster deployments on Dell PowerEdge R640 servers are blocked when the virtual media does not disconnect the ISO in the iDRAC console after writing the image to the disk. As a workaround, disconnect the ISO manually through the Virtual Media tab in the iDRAC console. (OCPBUGSM-45884)\n\nLow-latency applications that rely on high-resolution timers to wake up their threads might experience higher wake up latencies than expected. Although the expected wake up latency is under 20us, latencies exceeding this can occasionally be seen when running the cyclictest tool for long durations (24 hours or more). Testing has shown that wake up latencies are under 20us for over 99.999999% of the samples. (RHELPLAN-138733)\n\nA Chapman Beach NIC from Intel must be installed in a bifurcated PCIe slot to ensure that both ports are visible. A limitation also exists in the current devlink tooling in RHEL 8.6 which prevents the configuration of 2 ports in the bifurcated PCIe slot. (RHELPLAN-142458)\n\nDisabling an SR-IOV VF when a port goes down can cause a 3-4 second delay with Intel NICs. (RHELPLAN-126931)\n\nWhen using Intel NICs, IPV6 traffic stops when an SR-IOV VF is assigned an IPV6 address. (RHELPLAN-137741)\n\nWhen using VLAN strip offloading, the offload flag (ol_flag) is not consistently set correctly with the iavf driver. (RHELPLAN-141240)\n\nA deadlock can occur if an allocation fails during a configuration change with the ice driver. (RHELPLAN-130855)\n\nSR-IOV VFs send GARP packets with the wrong MAC address when using Intel NICs. (RHELPLAN-140971)\n\nWhen using the GitOps ZTP method of managing clusters and deleting a cluster which has not completed installation, the cleanup of the cluster namespace on the hub cluster might hang indefinitely. To complete the namespace deletion, remove the baremetalhost.metal3.io finalizer from two CRs in the cluster namespace:\n\nRemove the finalizer from the secret that is pointed to by the BareMetalHost CR .spec.bmc.credentialsName.\n\nRemove the finalizer from the BareMetalHost CR. When these finalizers are removed the namespace termination completes within a few seconds. (OCPBUGS-3029)\n\nThe addition of a new feature in OCP 4.12 that enables UDP GRO also causes all veth devices to have one RX queue per available CPU (previously each veth had one queue). Those queues are dynamically configured by OVN and there is no synchronization between latency tuning and this queue creation. The latency tuning logic monitors the veth NIC creation events and starts configuring the RPS queue cpu masks before all the queues are properly created. This means that some of the RPS queue masks are not configured. Since not all NIC queues are configured properly there is a chance of latency spikes in a real-time application that uses timing-sensitive cpus for communicating with services in other containers. Applications that do not use kernel networking stack are not affected. (OCPBUGS-4194)\n\nPlatform Operator and RukPak known issues:\n\nDeleting a platform Operator results in a cascading deletion of the underlying resources. This cascading deletion logic can only delete resources that are defined in the Operator Lifecycle Manager-based (OLM) Operator’s bundle format. In the case that a platform Operator creates resources that are defined outside of that bundle format, then the platform Operator is responsible for handling this cleanup interaction. This behavior can be observed when installing the cert-manager Operator as a platform Operator, and then removing it. The expected behavior is that a namespace is left behind that the cert-manager Operator created.\n\nThe platform Operators manager does not have any logic that compares the current and desired state of the cluster-scoped BundleDeployment resource it is managing. This leaves the possibility for a user who has sufficient role-based access control (RBAC) to manually modify that underlying BundleDeployment resource and can lead to situations where users can escalate their permissions to the cluster-admin role. By default, you should limit access to this resource to a small number of users that explicitly require access. The only supported client for the BundleDeployment resource during this Technology Preview release is the platform Operators manager component.\n\nOLM’s Marketplace component is an optional cluster capability that can be disabled. This has implications during the Technology Preview release because platform Operators are currently only sourced from the redhat-operators catalog source that is managed by the Marketplace component. As a workaround, a cluster administrator can create this catalog source manually.\n\nThe RukPak provisioner implementations do not have the ability to inspect the health or state of the resources that they are managing. This has implications for surfacing the generated BundleDeployment resource state to the PlatformOperator resource that owns it. If a registry+v1 bundle contains manifests that can be successfully applied to the cluster, but will fail at runtime, such as a Deployment object referencing a non-existent image, the result is a successful status being reflected in individual PlatformOperator and BundleDeployment resources.\n\nCluster administrators configuring PlatformOperator resources before cluster creation cannot easily determine the desired package name without leveraging an existing cluster or relying on documented examples. There is currently no validation logic that ensures an individually configured PlatformOperator resource will be able to successfully roll out to the cluster.\n\nWhen using the Technology Preview OCI feature with the oc-mirror CLI plugin, the mirrored catalog embeds all of the Operator bundles, instead of filtering only on those specified in the image set configuration file. (OCPBUGS-5085)\n\nThere is currently a known issue when you run the Agent-based OpenShift Container Platform Installer to generate an ISO image from a directory where the previous release was used for ISO image generation. An error message is displayed with the release version not matching. As a workaround, create and use a new directory. (OCPBUGS#5159)\n\nThe defined capabilities in the install-config.yaml file are not applied in the Agent-based OpenShift Container Platform installation. Currently, there is no workaround. (OCPBUGS#5129)\n\nFully populated load balancers on RHOSP that are created with the OVN driver can contain pools that are stuck in a pending creation status. This issue can cause problems for clusters that are deployed on RHOSP. To resolve the issue, update your RHOSP packages. (BZ#2042976)\n\nBulk load-balancer member updates on RHOSP can return a 500 code in response to PUT requests. This issue can cause problems for clusters that are deployed on RHOSP. To resolve the issue, update your RHOSP packages. (BZ#2100135)\n\nClusters that use external cloud providers can fail to retrieve updated credentials after rotation. The following platforms are affected:\n\nAlibaba Cloud\n\nIBM Cloud VPC\n\nIBM Power\n\nOpenShift Virtualization\n\nRHOSP\n\nAs a workaround, restart openshift-cloud-controller-manager pods by running the following command:\n\n$ oc delete pods --all -n openshift-cloud-controller-manager\n\nThere is a known issue when cloud-provider-openstack tries to create health monitors on OVN load balancers by using the API to create fully populated load balancers. These health monitors become stuck in a PENDING_CREATE status. After their deletion, associated load balancers are are stuck in a PENDING_UPDATE status. There is no workaround. (BZ#2143732)\n\nDue to a known issue, to use stateful IPv6 networks with cluster that run on RHOSP, you must include ip=dhcp,dhcpv6 in the kernel arguments of worker nodes. (OCPBUGS-2104)\n\nIt is not possible to create a macvlan on the physical function (PF) when a virtual function (VF) already exists. This issue affects the Intel E810 NIC. (BZ#2120585)\n\nThere is currently a known issue when manually configuring IPv6 addresses and routes on an IPv4 OpenShift Container Platform cluster. When converting to a dual-stack cluster, newly created pods remain in the ContainerCreating status. Currently, there is no workaround. This issue is planned to be addressed in a future OpenShift Container Platform release. (OCPBUGS-4411)\n\nWhen an OVN cluster installed on IBM Public Cloud has more than 60 worker nodes, simultaneously creating 2000 or more services and route objects can cause pods created at the same time to remain in the ContainerCreating status. If this problem occurs, entering the oc describe pod <podname> command shows events with the following warning: FailedCreatePodSandBox…​failed to configure pod interface: timed out waiting for OVS port binding (ovn-installed). There is currently no workaround for this issue. (OCPBUGS-3470)\n\nWhen a control plane machine is replaced on a cluster that uses the OVN-Kubernetes network provider, the pods related to OVN-Kubernetes might not start on the replacement machine. When this occurs, the lack of networking on the new machine prevents etcd from allowing it to replace the old machine. As a result, the cluster is stuck in this state and might become degraded. This behavior can occur when the control plane is replaced manually or by the control plane machine set.\n\nThere is currently no workaround to resolve this issue if encountered. To avoid this issue, disable the control plane machine set and do not replace control plane machines manually if your cluster uses the OVN-Kubernetes network provider. (OCPBUGS-5306)\n\nIf a cluster that was deployed through ZTP has policies that do not become compliant, and no ClusterGroupUpdates object is present, you must restart the TALM pods. Restarting TALM creates the proper ClusterGroupUpdates object, which enforces the policy compliance. (OCPBUGS-4065)\n\nCurrently, a certificate compliance issue, specifically outputted as x509: certificate is not standards compliant, exists when you run the installation program on macOS for the purposes of installing an OpenShift Container Platform cluster on VMware vSphere. This issue relates to a known issue with the golang compiler in that the compiler does not recognize newly supported macOS certificate standards. No workaround exists for this issue. (OSDOCS-5694)\n\nCurrently, when using a persistent volume (PV) that contains a very large number of files, the pod might not start or can take an excessive amount of time to start. For more information, see this knowledge base article. (BZ1987112)\n\nCreating pods with Azure File NFS volumes that are scheduled to the control plane node causes the mount to be denied. (OCPBUGS-18581)\n\nTo work around this issue: If your control plane nodes are schedulable, and the pods can run on worker nodes, use nodeSelector or Affinity to schedule the pod in worker nodes.\n\nWhen installing an OpenShift Container Platform cluster with static IP addressing and Tang encryption, nodes start without network settings. This condition prevents nodes from accessing the Tang server, causing installation to fail. To address this condition, you must set the network settings for each node as ip installer arguments.\n\nFor installer-provisioned infrastructure, before installation provide the network settings as ip installer arguments for each node by executing the following steps.\n\nCreate the manifests.\n\nFor each node, modify the BareMetalHost custom resource with annotations to include the network settings. For example:\n\n$cd ~/clusterconfigs/openshift $ vim openshift-worker-0.yaml\n\napiVersion: metal3.io/v1alpha1 kind: BareMetalHost metadata: annotations: bmac.agent-install.openshift.io/installer-args: '[\"--append-karg\",\"ip=<static_ip>::<gateway>:<netmask>:<hostname_1>:<interface>:none\",\"--save-partindex\",\"1\",\"-n\"]' (1) (2) (3) (4) (5) inspect.metal3.io: disabled bmac.agent-install.openshift.io/hostname: <fqdn> (6) bmac.agent-install.openshift.io/role: <role> (7) generation: 1 name: openshift-worker-0 namespace: mynamespace spec: automatedCleaningMode: disabled bmc: address: idrac-virtualmedia://<bmc_ip>/redfish/v1/Systems/System.Embedded.1 (8) credentialsName: bmc-secret-openshift-worker-0 disableCertificateVerification: true bootMACAddress: 94:6D:AE:AB:EE:E8 bootMode: \"UEFI\" rootDeviceHints: deviceName: /dev/sda\n\nFor the ip settings, replace:\n\n1 <static_ip> with the static IP address for the node, for example, 192.168.1.100 2 <gateway> with the IP address of your network’s gateway, for example, 192.168.1.1 3 <netmask> with the network mask, for example, 255.255.255.0 4 <hostname_1> with the node’s hostname, for example, node1.example.com 5 <interface> with the name of the network interface, for example, eth0 6 <fqdn> with the fully qualified domain name of the node 7 <role> with worker or master to reflect the node’s role 8 <bmc_ip> with with the BMC IP address and the protocol and path of the BMC, as needed.\n\nSave the file to the clusterconfigs/openshift directory.\n\nCreate the cluster.\n\nWhen installing with the Assisted Installer, before installation modify each node’s installer arguments using the API to append the network settings as ip installer arguments. For example:\n\n$ curl https://api.openshift.com/api/assisted-install/v2/infra-envs/${infra_env_id}/hosts/${host_id}/installer-args \\ -X PATCH \\ -H \"Authorization: Bearer ${API_TOKEN}\" \\ -H \"Content-Type: application/json\" \\ -d ' { \"args\": [ \"--append-karg\", \"ip=<static_ip>::<gateway>:<netmask>:<hostname_1>:<interface>:none\", (1) (2) (3) (4) (5) \"--save-partindex\", \"1\", \"-n\" ] } ' | jq\n\nFor the previous network settings, replace:\n\n1 <static_ip> with the static IP address for the node, for example, 192.168.1.100 2 <gateway> with the IP address of your network’s gateway, for example, 192.168.1.1 3 <netmask> with the network mask, for example, 255.255.255.0 4 <hostname_1> with the node’s hostname, for example, node1.example.com 5 <interface> with the name of the network interface, for example, eth0."
    }
}
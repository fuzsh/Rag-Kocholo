{
    "id": "wrong_mix_domainrange_leader_00076_1",
    "rank": 67,
    "data": {
        "url": "https://arxiv.org/html/2403.19318v2",
        "read_more_link": "",
        "language": "en",
        "title": "TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/2403.19318v2/figure/survey.png",
            "https://arxiv.org/html/extracted/2403.19318v2/figure/overview.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Large language model",
            "Tabular data manipulation"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Xiaokang Zhang1 , Jing Zhang1, Zeyao Ma1 , Yang Li1 , Bohan Zhang1 , Guanlin Li1 , Zijun Yao2, Kangli Xu2, Jinchang Zhou2, Daniel Zhang-Li2, Jifan Yu2, Shu Zhao3, Juanzi Li2, Jie Tang2\n\nAbstract.\n\nWe introduce TableLLM, a robust large language model (LLM) with 13 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted a benchmark tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs. We have publicly released the model checkpoint, source code, benchmarks, and a web application for user interaction .\n\nLarge language model, Tabular data manipulation\n\n††isbn: 978-1-4503-XXXX-X/18/06\n\n1. Introduction\n\nA substantial amount of data is routinely structured in tabular formats, a format widely embraced across various industries for different purposes. For instance, they enable bank employees to monitor transactions and detect fraud, assist human resources in managing employee information efficiently, and facilitate government agencies in conducting censuses and surveys for policy-making. While tabular data is ubiquitous, specific table-related tasks can be laborious, error-prone, and require specialized skills. Automating these tasks offers significant benefits to both academic and industrial sectors, attracting considerable interest (Badaro et al., 2023; Dong et al., 2022).\n\nConventional methods for processing tabular data predominantly focus on adapting language model architectures, incorporating elements like position embeddings, attention mechanisms, and learning objectives to encode the inherent structural attributes of tabular data (Yin et al., 2020; Gong et al., 2020; Herzig et al., 2020; Liu et al., 2022). However, a shift in paradigm has occurred with the rise of large language models (LLMs) like GPT-4 (OpenAI, 2023), GPT-3.5 (Ouyang et al., 2022), and PaLM2 (Anil et al., 2023). Recent research emphasizes crafting precise prompts that integrate crucial partial information from provided tabular data and leveraging external programming languages like SQL and Python. This approach facilitates a step-by-step Chain-of-thought (COT) (Wei et al., 2022) inference process by close-sourced LLMs (Cheng et al., 2023; Ye et al., 2023; Zhang et al., 2023a; Li et al., 2023b; Jiang et al., 2023). The availability of open-source LLMs, exemplified by Llama (Touvron et al., 2023), enables the fine-tuning of models for tabular data processing, as demonstrated by TableLlama (Zhang et al., 2023c).\n\nNumerous previous studies have focused on improving a model’s reasoning capabilities for table question answering (tableQA) (Cheng et al., 2023; Ye et al., 2023; Zhang et al., 2023a; Li et al., 2023a; Jiang et al., 2023; Yin et al., 2020; Zhang et al., 2023c; Herzig et al., 2020; Liu et al., 2022). Moving beyond tableQA, some of these endeavors have also tackled diverse table-related tasks, including fact verification (Ye et al., 2023; Zhang et al., 2023c, a; Jiang et al., 2023; Liu et al., 2022), column type annotation (Li et al., 2023a; Zhang et al., 2023c), table matching (Li et al., 2023a), schema augmentation (Li et al., 2023a; Zhang et al., 2023c), and more. However, many of these efforts, while valuable in an academic context, may fall short of reflecting the complete realism of individuals working with tabular data in real-world office scenarios.\n\nTo capture authentic insights from office users, we conduct an extensive user study utilizing a questionnaire focused on various tasks related to tables. The questionnaire is distributed to 507 participants across diverse professions, aiming to capture their specific requirements in real-world office scenarios. For further details on the user study, please refer to Section 3. The results, depicted in Figure 1, reveal a clear preference among respondents for tasks related to tableQA, table revision, chart creation, table matching. Notably, tables in Excel/CSV and Word/PDF formats, as well as long tables, emerged as the preferred choices among participants.\n\nChallenges. Compared to academically-focused table tasks, real-world office use of tabular data presents two primary challenges. (1) Diverse Operations: user preferred tasks involve a wide range of operations, including query, update, merge, and chart, which go beyond the query operations in tableQA. (2) Unique Processing Approaches for Different Formats: Word/PDF documents often contain contextual textual data alongside tabular information, allowing for hybrid querying. Excel/CSV spreadsheets, on the other hand, contain regular and long tables, enabling more intricate operations like update and merge.\n\nWhile existing works either focus on leveraging LLMs’ ability to derive answers directly from their internal parameters, particularly suitable for document-embedded tabular data, or specialize in writing and executing code to obtain answers from spreadsheet tabular data, they each have limitations. The former struggles with long tables and diverse operations in spreadsheets, while the latter fails to handle hybrid queries involving both text and tabular data. In summary, existing works have yet to effectively address both types of tabular data simultaneously, meeting the requirements of real-world office usage.\n\nOur Solution. We present TableLLM, specifically designed to handle a wide array of table operations encountered in spreadsheet and document usage scenarios, named tabular data manipulation in real office usage scenario. To facilitate model training, we introduce a distant supervision method that complements the reasoning process of existing benchmarks, aiding in training LLMs to understand reasoning patterns more effectively. Additionally, we validate the automatically generated questions and answers through a cross-way validation strategy, ensuring data quality. We also provide a theoretical analysis of the effectiveness of cross-way validation compared to single-answer sampling and same-way validation. Utilizing this distant supervision training data, we fine-tune CodeLlama (13B) (Rozière et al., 2023), resulting in the development of TableLLM. This model adeptly handles tabular data embedded within documents through an inner-parameter-driven approach and spreadsheet-embedded tabular data via a code-driven method.\n\nA rigorous performance assessment is conducted, involving the collection of primary tableQA test instances from existing benchmarks and the creation of additional table manipulation instances by an annotation team. Given the complex evaluation process under the two scenarios, we design a meticulous evaluation method that considers query, update, merge and chart operations with distinct metrics. TableLLM proves to be on par with GPT-3.5 and even outperforms the most capable commercial LLM GPT-4 in the spreadsheet-embedded scenario.\n\nImpact and Beneficial Groups In the realm of tabular data processing research, our contributions encompass: (1) Addressing a practical problem of tabular data manipulation in real-world office usage scenarios. (2) Presenting techniques that extend reasoning processing and integrate a cross-way validation strategy to enhance the quality of distant supervision training data. Theoretical proof is provided for the effectiveness of cross-way validation. We firmly believe that TableLLM holds significant potential to create a positive impact on both industrial developers and users, owing to the following contributions: (3) Delivering a high-quality open-source LLM tailored for tabular data manipulation in both 7B and 13B, thereby enhancing accessibility and fostering collaboration within the community. (4) Offering an online application service to facilitate convenient usage and improve the overall user experience.\n\n2. Related Work\n\nWe review table tasks, including basic analysis tasks represented by tableQA, table manipulation, and advanced table data analysis.\n\nTableQA-represented Basic Analysis. Beyond the primary tableQA task, various research endeavors tackle basic table analysis tasks like fact verification, column type annotation, schema augmentation, data-to-text, and more (Ye et al., 2023; Eisenschlos et al., 2021; Zhang et al., 2023a; Jiang et al., 2023; Yin et al., 2020; Yang et al., 2022; Herzig et al., 2020; Liu et al., 2022; Xie et al., 2022; Li et al., 2023a; Zhang et al., 2023c; Deng et al., 2020; Gong et al., 2020). These tasks commonly involve tables extracted from the web, typically of relatively short length and interspersed with textual content.\n\nRepresentation Learning. Many traditional methods, such as TaBERT (Yin et al., 2020), TAPAS (Herzig et al., 2020), TableGPT (Gong et al., 2020), Tableformer (Yang et al., 2022), MATE (Eisenschlos et al., 2021), TUTA (Wang et al., 2021), Tabbie (Iida et al., 2021), TABT5 (Andrejczuk et al., 2022), TAG-QA (Zhao et al., 2023b) and TURL (Deng et al., 2020), emphasize intricate encoder design, incorporating various positional encodings and dense/sparse attention mechanisms to represent tables. These methods also integrate reconstruction losses at token, cell, and column levels. TAPEX (Liu et al., 2022) and GraPPa(Yu et al., 2020) additionally integrate SQL execution as a pre-training task.\n\nFinetuning LLM. As LLM capabilities progress, researchers are shifting focus from intricate table encoding to gathering ample data for training unified LLMs capable of handling multiple table-related tasks. For instance, TableLlama (Zhang et al., 2023c) and TAT-LLM (Zhu et al., 2024) fine-tune the Llama2 (7B) model on various table-related benchmarks. UnifiedSKG (Xie et al., 2022) further integrates structured data-related benchmarks, like knowledge graph question answering, into the tuning process of the T5 (3B) (Raffel et al., 2020) model to address tasks requiring structured data.\n\nPrompting LLM. Due to the closed-source nature of GPT series LLMs and the high cost associated with fine-tuning these models, researchers have focused on designing effective prompts for GPT series LLMs to enable tableQA analysis tasks (Anonymous, 2024; Cheng et al., 2023; Ye et al., 2023; Jin and Lu, 2023; Zhang et al., 2023a; Jiang et al., 2023; Zhao et al., 2023a). The approach typically involves a multi-step inference process, breaking down the main question into subquestions, invoking external tools like SQL and Python to address these subquestions. For instance, DATER (Ye et al., 2023) employs SQL, StructGPT (Jiang et al., 2023) and Chain-of-Table (Anonymous, 2024) use self-defined interfaces/actions, and ReAcTable (Zhang et al., 2023a) employs SQL for querying tabular data and Python for handling string manipulation tasks. In contrast to incorporating tool execution results directly into the LLM, Binder (Cheng et al., 2023) integrates the LLM’s generated results back into SQL/Python.\n\nTable Manipulation. A new research direction aims to enhance table manipulation capabilities, particularly focusing on tasks such as insert, update, and delete operations within spreadsheet formats like Excel and CSV, as well as databases (Li et al., 2023b; Dong et al., 2023; Pourreza and Rafiei, 2023; Zhang et al., 2023b). Such tasks often involve working with lengthy and regular tables, making it practical to utilize LLMs alongside tools to address them. For instance, DB-GPT (Xue et al., 2023), ChatDB (Hu et al., 2023), C3 (Dong et al., 2023) and Din-SQL (Pourreza and Rafiei, 2023) translate questions into SQL queries. SheetCopilot (Li et al., 2023b) and DataCopilot (Zhang et al., 2023b) develop their atomic interfaces based on Excel’s embedded functions and various programming languages like C++ and Python, allowing LLMs to invoke them.\n\nAdvanced Analysis. Recently, researchers have redirected their focus towards advanced table data analysis tasks (Lai et al., 2023; Hu et al., 2024). These tasks involve intricate operations such as correlation analysis, feature engineering, and machine learning. The predominant methods in this area enable LLMs to generate Pandas code, which offers comprehensive support for advanced data analysis, facilitating the handling of these advanced analysis tasks.\n\nSummary. The majority of research still focuses on TableQA-represented basic analysis tasks, with some beginning to explore table manipulation and advanced analysis. Direct inference from LLM parameters is common for basic tasks, while inferring code/APIs and executing their results is favored for table manipulation and advanced analysis. However, current research seldom considers user scenarios. SQL-invoked tasks suit database administrators, while advanced analysis is for data analysts requiring in-depth pattern analysis. For office usage, people prefer both QA tasks on document-embedded tables and manipulation tasks on spreadsheet-embedded tables. Existing methods fall short of fully supporting these needs.\n\n3. User Study and Problem Definition\n\n3.1. User Study\n\nTo gather authentic insights from office users, we conduct an extensive user study involving a questionnaire, focusing on two key aspects: (1) inherent characteristics of tables and (2) exploration of table-related tasks. The questionnaire covers usage frequency of different tables, preferred length, and format options like Excel, CSV, Word, PDF, Markdown, and HTML. For tasks, we design 17 frequently mentioned ones, drawing inspiration from TableLlama (Zhang et al., 2023c) and Table-GPT (Li et al., 2023a). We’ve distributed the questionnaire to 507 diverse participants, including teachers, students, university administrators, marketing professionals, HR personnel, and R&D specialists. They’re asked about preferred table lengths, formats, and frequently required tasks.\n\nThe user study findings, depicted in Figure 1, indicate a clear preference among participants for tasks such as tableQA, table revision, chart creation, and table matching, followed by the data cleaning related tasks, including error detection, duplicate data removal, and missing value detection. Notably, table extraction, focused on format conversion, is in demand but can be handled efficiently by non-LLM tools, thus not considered LLM-related tasks. There’s relatively less demand for tasks like column type annotation, entity linking, and fact verification. Additionally, tables in Excel/CSV and Word/PDF formats, along with long tables (typically in Excel/CSV format), emerged as the preferred choices among participants. We also present the complete questionnaire in Appendix A.2.\n\n3.2. Problem Definition\n\nTabular Data refers to data organized in a table or grid format, with rows and columns facilitating efficient organization and access. Each row typically represents a different record, while each column represents a different attribute of the record. On top of it, document-embedded tabular data is tabular data integrated into documents, often in compact formats within Word or PDF files, accompanied by textual content for context and explanation, while spreadsheet-embedded tabular data refers to tables within spreadsheets, typically in Excel or CSV files, presented in regular and extensive formats.\n\nOperation Definition. In the user study, tableQA tasks are addressed by query operations, table revision tasks by update operations, table matching tasks by merge operations, and chart generation tasks are regarded as standalone operations. Furthermore, data cleaning tasks such as error detection, duplicate removal, and missing value detection can be handled using update operations. In summary, tabular data manipulation tasks can be categorized into four primary operations: query, update, merge, and chart, as detailed in Figure 2. The “query” operation selects desired data, encompassing filter, aggregate, group, sort, compute, and subquery functionalities, effectively addressing most tableQA instances. The “update” operation modifies or deletes existing data and adds new data. “Merge” operation combines two tables into one. Lastly, the “chart” operation visualizes table content using graphical representations such as bar, pie, or line charts. These operations serve as a guide for generating supervision data, as discussed in Section 4.1.\n\nProblem 1.\n\nTabular Data Manipulation in Real Office Usage Scenarios focuses on developing an LLM that can perform a range of query, update, merge, and chart operations with tabular data embedded in documents and spreadsheets.\n\nFor document-embedded tabular data, querying specific information is the primary user need, while for spreadsheet-embedded tabular data, users often require querying, data modification, and chart creation. Tasks for document-embedded data suit the LLM’s inner parameters due to its text and tabular data handling proficiency, whereas spreadsheet-embedded tasks demand a more intricate, code-driven approach for effective manipulation.\n\n4. TableLLM\n\nThe overview design of TableLLM is shown in Figure 3, which consists two primary aspects: (1) Distant Supervision Data Construction. The development of distant supervision data involves the integration of both existing benchmark training data and new questions and answers generated from available tabular data. To enhance the training of LLMs, we suggest expanding the reasoning processes within benchmark data. This includes text-based reasoning for queries on document-embedded tabular data and code-based reasoning for manipulations of spreadsheet-embedded tabular data. Additionally, to assure the quality of the automatically generated training data, we introduce a cross-way validation strategy. This strategy utilizes diverse solution methods for cross-validation, ensuring the reliability and accuracy of the data; (2) Model Training. The training of the model utilizes distinct prompts for document-embedded and spreadsheet-embedded tabular data.\n\n4.1. Distant Supervision Data Construction\n\nExtending Reasoning Process for Existing Benchmarks. While existing benchmarks offer ample training data for tableQA, the simple short answers provided by individual instances fall short for tackling complex tabular data manipulation tasks, which often demand intricate reasoning processes to derive answers effectively. Therefore, we augment existing benchmarks by enriching their reasoning processes to facilitate the training of LLMs.\n\nPrimarily, to address queries on document-embedded tabular data, we gather training data from widely-adopted tableQA benchmarks including WikiTQ (Pasupat and Liang, 2015), FeTaQA (Nan et al., 2022), and TAT-QA (Zhu et al., 2021). Inspired by CoT (Wei et al., 2022), We extend the provided short answers by presenting GPT-3.5 with the (question, answer) pairs and instructing it to enhance the reasoning process. This augmentation is represented in textual form, rather than as code, to align with the nature of queries involving hybrid text and tabular data inputs. We conjecture that expanding on the reasoning process beyond the short answers during training could enhance the reasoning ability of LLMs. Notably, for WikiTQ and FeTaQA that solely provide tabular data, we supplement them by generating table descriptions using GPT-3.5. Due to the inner-parameter-driven technique employed, we impose a constraint on the length of input tables, limiting them to a token count of fewer than 500. To validate the quality of these text-based reasoning processes, we utilize CritiqueLLM (Ke et al., 2023), an LLM model specialized in rating, to assess the consistency between the reasoning process and the answers provided in the benchmarks.\n\nFurthermore, to handle queries on spreadsheet-embedded tabular data, we compile training data from two Text2SQL benchmarks: WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018). Given that spreadsheet-embedded tabular data manipulation primarily involves pure tabular data inputs and complex table manipulations, it aligns more with code-driven techniques. Thus, we select training instances from WikiSQL and Spider, as they correspond to SQL queries. However, instead of directly using the provided SQL queries, we expand pandas code as the reasoning process for each (question, answer) pair by Deepseek (Bi et al., 2024), a recent powerful code LLM , as Pandas offers greater flexibility to support functionalities such as chart beyond table query, update, and merge. We ensure the quality of the generated code by validating that the executed outcomes align with the provided answers in the benchmarks. Note for Spider, in line with our focus on single-table operations typical in office scenarios, we exclude multi-table queries and those whose SQL queries yield null results, to better reflect real-world applications.\n\nAutomatically Generating Training Data by Cross-way Validation. While the training data derived from existing benchmarks is of high quality, the variety of questions and answers, especially the table update, merge, and chart operations they offer is limited. To address this, we introduce a cross-way validation strategy for automatically generating new questions and answers using only the provided tabular data. The detailed process is as follows:\n\n(1) Question Generation. We select 5,177 tables from WikiTQ, 5,000 from TAT-QA, and 4,019 from FeTaQA with less than 500 tokens to simulate document-embedded tabular data. For each table, GPT-3.5 generates questions involving single or multiple table query operations, as depicted in Figure 2. GPT-3.5 also creates contextual table descriptions for WikiTQ and FeTaQA-sourced tables, while TAT-QA tables retain their original text context. Furthermore, we select 1,300 long tables from GitTables (Hulsebos et al., 2023). For each table, we generate 20 questions involving various table manipulation operations, as illustrated in Figure 2. Existing benchmarks typically focus on table query operations, so update, chart, and merge operations are all generated. For query, update, and chart operations, we prompt GPT-3.5 for question generation. However, for the merge operation, given its well-defined nature, we directly construct templates to generate the merge question. Appendix A.7 provides the prompts and templates used for question generation.\n\n(2) Answer Generation and Cross-way Validation. For questions based on document-embedded tabular data, we employ GPT-3.5 for both answer generation through inner-parameter and code-driven solutions. The generated code is executed to produce an answer, which serves as the reference. CritiqueLLM (Ke et al., 2023) is used to evaluate the alignment between the inner-parameter-inferred answer and this reference, thereby improving answer quality. Such inner-parameter-driven and code-driven techniques offers diverse solutions, constituting a form of cross-way validation.\n\nThis cross-validation approach is inspired by ensemble learning (Dietterich et al., 2002), which combines multiple weak learners to create a strong learner. Building on this concept, we conduct an improved theoretical inference to ensure the quality of automatically generated data. Let’s denote Yasubscript𝑌𝑎Y_{a}italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT as the event that the first response is correct, Ybsubscript𝑌𝑏Y_{b}italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT as the event that the second response is correct, Y𝑌Yitalic_Y as the event that both responses are correct, and E𝐸Eitalic_E as the event that the two responses are consistent. Based on these definitions, we can establish the following theorem:\n\nTheorem 4.1.\n\n(1) If A𝐴Aitalic_A and B𝐵Bitalic_B are drawn from the same distribution such that P⁢(Ya)=P⁢(Yb)=p>1/2𝑃subscript𝑌𝑎𝑃subscript𝑌𝑏𝑝12P(Y_{a})=P(Y_{b})=p>1/2italic_P ( italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ) = italic_P ( italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ) = italic_p > 1 / 2, then consistency checking outperforms single inference, i.e., P⁢(Y|E)≥P⁢(Ya)𝑃conditional𝑌𝐸𝑃subscript𝑌𝑎P(Y|E)\\geq P(Y_{a})italic_P ( italic_Y | italic_E ) ≥ italic_P ( italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ).\n\n(2) If A𝐴Aitalic_A and B𝐵Bitalic_B are further drawn from independent distributions, the effect will be superior (in terms of expectation).\n\nThen for questions on spreadsheet-embedded tabular data, we employ GPT-3.5 to generate a pandas code solution, which is then followed by the generation of an alternative code solution using GPT-3.5 again. The accuracy of the executed outcomes from the first code are verified by comparing them with the outcomes of the second code. Given the potential diversity of two coding solutions resulting in the same answers, this dual-coding strategy can be regarded as stemming from different distributions. Thus it also functions as a cross-way validation method, ensuring the reliability of the solutions.\n\n4.2. Model Training\n\nIn the scenario of document-embedded tabular data, the input for LLMs includes both the text and the entire content of the table. However, in the case of spreadsheet-embedded tabular data, due to the typically extensive length of the table, only the header and a subset of rows are provided as input to the LLM. The prompt for the merge operation, involving two tables, is distinct and specifically designed. Figure 3 illustrates the specific prompts.\n\nGiven the prompt x𝑥xitalic_x as input, we enable LLMs to generate either the textural or code solution, collectively denoted as y𝑦yitalic_y. Subsequently, the training loss function is defined as:\n\nℒ⁢(x,y)=−∑i=1|y|log⁡TableLLM⁢(yi|x,yj<i),ℒ𝑥𝑦superscriptsubscript𝑖1𝑦TableLLMconditionalsubscript𝑦𝑖𝑥subscript𝑦𝑗𝑖\\mathcal{L}(x,y)=-\\sum_{i=1}^{|y|}\\log\\textsc{TableLLM}(y_{i}|x,y_{j<i}),caligraphic_L ( italic_x , italic_y ) = - ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_y | end_POSTSUPERSCRIPT roman_log TableLLM ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x , italic_y start_POSTSUBSCRIPT italic_j < italic_i end_POSTSUBSCRIPT ) ,\n\nwhere TableLLM⁢(yi|x,yj<i)TableLLMconditionalsubscript𝑦𝑖𝑥subscript𝑦𝑗𝑖\\textsc{TableLLM}(y_{i}|x,y_{j<i})TableLLM ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x , italic_y start_POSTSUBSCRIPT italic_j < italic_i end_POSTSUBSCRIPT ) represents the probability of generating the i𝑖iitalic_i-th token of y𝑦yitalic_y given x𝑥xitalic_x and the preceding tokens of y𝑦yitalic_y. This loss function resembles the standard language model loss but exclusively considers the loss computed on y𝑦yitalic_y. We hybridize the document-embedded and spreadsheet-embedded training data in a 1:1 ratio, thoroughly shuffle them, and then partition them into batches for training.\n\nThe trained single model addresses two types of data sources. Given that code models tend to excel in reasoning-intensive tasks compared to text models (Liang et al., 2023), combining the two data sources can enhance text-level reasoning with code-level reasoning. Moreover, a single model could alleviate deployment pressure.\n\n4.3. Model Deployment as Web Application\n\nWe recently launch our TableLLM as a web application , with a screenshot shown in Figure LABEL:fig:app. The typical workflow is as follows: Users begin by uploading their tabular data embedded in documents (with support for Word and PDF formats) and spreadsheets (supporting Excel and CSV formats). The system utilizes Grobid to parse PDF files and python-docx for Word files, converting them into CSV format for web visualization. Users then enter queries or instructions in the query box. Depending on the type of uploaded document or spreadsheet, appropriate prompts from Figure 3 guide the TableLLM to generate answers. The response could be a table, a chart, or a textual answer. Additionally, the application offers a feature for merging two tables, where users can upload two spreadsheets and specify the merging conditions in the query box .\n\nWe open the application for trial to a diverse group including teachers, students, administrators from universities, marketing professionals, human resources personnel, and research and development specialists. They are encouraged to provide feedback by clicking “Thumbs up” or “Thumbs down”. So far, we have collected 2,000 use cases from users, with 1,560 involving spreadsheet-embedded scenarios (1,869 for single table operations and 131 for double table operations) and 440 for document-embedded scenarios. Among these, we have received 1,473 feedbacks with 1,293 “Thumbs up” and 180 “Thumbs down”, closely aligning with the performance metrics reported in Table 2. We conduct an error analysis in Appendix A.4 for further improvement.\n\n5. Experiment\n\n5.1. Test Set Creation\n\nWe collect test sets from established benchmarks for document-embedded query tasks, including WikiTQ (Pasupat and Liang, 2015), FeTAQA (Nan et al., 2022), and TAT-QA (Zhu et al., 2021). Additionally, we incorporate the OTT-QA (Chen et al., 2021) dataset, which features distinct tables and questions compared to our training data, to evaluate the generalization capabilities of our model. For spreadsheet-embedded table tasks, we utilize test sets from WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018), which align with our query operation requirements. We extract the table, question, and answer from each instance, omitting the SQL statement. To ensure quality, we select clearly phrased questions with accurate answers from WikiSQL and discard instances with vague questions or flawed SQL resulting in multiple potential answers or incorrect results.\n\nAs no benchmarks exist for table update, merge, and chart operations, we create test set through human annotation. We choose 50 long tables from InfiAgent-DABench (Hu et al., 2024), ensuring they are entirely distinct from our training data. Following the process outlined in Section 4.1, we generate questions and answers. An annotator team verifies and corrects the generated content, including answers, codes, and operation types. They execute the code to ensure functionality and check if the answers align with the questions, making adjustments as needed. Since initial questions lack linguistic diversity, we utilize five prominent models from Huggingface for rewriting to enhance variety. This results in a composition of 10% original questions and 90% rewritten ones, with each model contributing to 18% of the rewrites. Human annotators also manually review the questions to retain essential information and avoid irrelevant additions. Table 1 displays the benchmark statistics.\n\n5.2. Evaluation Approach\n\nGiven the diverse range of operation types in our dataset, we have adopted a categorized evaluation approach to assess the performance of models across different operations:\n\n•\n\nQuery operations: For the answers obtained through code execution, we conduct an exact match comparison between the model’s output and the ground truth answers to determine correctness. However, for answers directly inferred via inner-parameters, we rely on CritiqueLLM (Ke et al., 2023) to assign a score from 1 to 10 by comparing the model’s output with the ground truth answers, with a score threshold of 7 considered correct. This is because the generated answers are often lengthy and challenging to precisely match. We also conduct a meta evaluation on CritiqueLLM’s rating scores by humans, obtaining 3% false positive percentage and 4.25% false negative percentage, which highlights the reliability of CritiqueLLM. Details about the meta evaluation is provided in Appendix A.9.\n\n•\n\nUpdate and merge operations: As these operations directly modify tables, we require the model’s output to be the complete modified table. We then perform an exact match comparison between the model’s output and the ground truth answers to determine correctness.\n\n•\n\nChart operations: Assessing charting operations is challenging through direct answer comparison. Instead, we compare code output by the model with the corresponding code from the ground truth answer. CritiqueLLM is once again employed to compare the model’s output code with the ground truth code, using a score threshold of 5 for evaluation.\n\nBased on the correctness determination, we assess accuracy.\n\n5.3. Comparison Methods\n\nThe comparison methods are categorized into four types:\n\n•\n\nPre-training and fine-tuning LLMs: This category encompasses models like TaPas (Herzig et al., 2020) (based on BERT) TAPEX (Liu et al., 2022) (based on BART), and TableLlama (Zhang et al., 2023c) (based on Llama2 (7B)).\n\n•\n\nGeneral LLMs: This group includes GPT-3.5 (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), and Llama2 (13B) (Touvron et al., 2023).\n\n•\n\nCoding-specific LLMs: This category contains LLMs tailored for coding tasks, including CodeLlama (Rozière et al., 2023) and DeepSeek (Bi et al., 2024).\n\n•\n\nPrompt-driven LLMs: This group includes StructGPT (Jiang et al., 2023), ReAcTable (Zhang et al., 2023a), Binder (Cheng et al., 2023), and DATER (Ye et al., 2023), focusing on creating sophisticated prompts to guide LLMs in processing tabular data.\n\nBaseline Selection Principle. We compare with methods featuring fully-maintained codes runnable under Linux Server, thus excluding ReAcTable (Zhang et al., 2023a) and SheetCopilot (Li et al., 2023b). DataCopilot (Zhang et al., 2023b) is also not considered due to its self-designed interfaces limited to certain areas like finance. DIN-SQL (Pourreza and Rafiei, 2023) and C3 (Dong et al., 2023) are excluded as they focus on generating SQL and rely on databases. Daagent (Hu et al., 2024), designed for advanced data analysis, is also excluded as its functionalities do not align with the intended scope of our assessment.\n\nImplementation. (1) TaPas and TAPEX have individual checkpoints trained on WikiTQ and WikiSQL. We assess their performance in document-embedded tabular data scenarios using the WikiTQ-trained versions and in spreadsheet-embedded tabular data scenarios using the WikiSQL-trained versions. As for TableLlama, we evaluate its single checkpoint directly. (2) For both general and coding-specific LLMs, we provide customized prompts for scenarios involving the processing of document-embedded and spreadsheet-embedded tabular data, as detailed in Appendix A.8. (3) Prompt-driven LLMs follow their established prompts. StructGPT, for instance, designs distinct prompts for WikiTQ, WikiSQL, and Spider. We standardize StructGPT’s prompts for WikiTQ, TAT-QA, FeTaQA, OTT-QA, and WikiSQL, aligning them with the prompts used for WikiTQ. Meanwhile, both Binder and DATER use a single unified set of prompts across all benchmarks. (4) TableLLM is trained using both CodeLlama (7B) and CodeLlama (13B) versions. During inference with our TableLLM , we consistently apply the same set of prompts used during its training phase. The generated distant supervision data is presented in Table 7 in Appendix A.6.\n\n5.4. Overall Experimental Results\n\nEffectiveness. Table 2 displays the overall evaluation in two scenarios. “–” in the table indicates that the method does not support the dataset or that the tested accuracy is too low. The results show that TableLLM generally surpasses others in the spreadsheet-embedded scenario and is on par with GPT-3.5 in the document-embedded scenario. Detailed findings include:\n\n(1) TaPEX and TaPas show limited performance due to their small model sizes. These two pre-training and fine-tuning models, utilizing BART and BERT respectively, only demonstrate relatively strong performance on WikiSQL and WikiTQ benchmarks when using their respective trained versions.\n\n(2) StructGPT, Binder, and DATER’s varying performance across datasets suggests a limitation in the generalization capability of prompt-driven LLMs. While these models, which generate prompts for tabular data QA tasks, consistently perform well in the WikiTQ benchmark, their performance weakens on other datasets. StructGPT stands out in the Spider benchmark due to its customized prompts tailored for this specific dataset.\n\n(3) DeepSeek (33B) excels in the spreadsheet-embedded tabular data scenario. This superior performance is attributed to DeepSeek’s extensive optimization for coding capabilities, enabling proficient code generation for processing spreadsheet-embedded tabular data. However, this specialization in coding proficiency comes at the expense of other abilities, such as direct answer inference from inner parameters.\n\n(4) Our TableLLM outperforms both GPT-3.5 and GPT-4 in the spreadsheet-embedded scenario. Moreover, in our created benchmark with entirely distinct tabular data and questions from the training data, TableLLM achieves an impressive 80.83% accuracy, showcasing robust generalization ability. Conversely, in the document-embedded scenario, TableLLM matches GPT-3.5 but slightly trails GPT-4, possibly due to the scenario’s demand for extensive commonsense reasoning with text data, where TableLLM could benefit from enhanced training in text QA. It’s noteworthy that OTT-QA features entirely different tabular data and questions from the training data, where TableLLM (7B) surpasses GPT-3.5 by 2.31% accuracy, further demonstrating its generalization prowess.\n\nEfficiency. All methods, except prompt-driven LLMs, require only one inference process per instance. However, Binder necessitates a one-step inference for each instance, requiring 50 samples per step for self-consistency validation. DATER requires four-step inferences for each instance, with self-consistency validation at each step, totaling 100 inferences per instance. StructGPT requires three inferences per question.\n\n5.5. Ablation Studies on Training Data\n\nEffect of Diverse Training Data Sources. We analyze the influence of different training datasets by comparing five distinct training configurations:\n\n•\n\nCodeLlama (13B): The base version without any training.\n\n•\n\nWith original training data of existing benchmarks: Train CodeLlama using 2,000 training instances from TAT-QA and WikiTQ, then evaluate on corresponding test sets.\n\n•\n\nWith extended training data of existing benchmarks: Train on 2,000 training instances from WikiTQ/TAT-QA, supplemented with extended reasoning process for each instance. Train on 2,000 training instances from Spider, supplemented with extended code, and evaluate on both Spider and our created test sets.\n\n•\n\nWith generated training data: Train on 2,000 generated instances based on WikiTQ/TAT-QA’s tabular data, then test on corresponding test sets. Train on 2,000 generated code-outputted instances based on GitLab’s tabular data, and evaluate on Spider and our created test sets.\n\n•\n\nWith mixed data: Train with a mix of 2,000 extended and 2,000 generated instances from TAT-QA/WikiTQ, then evaluate on corresponding test sets. Train with a mix of 2,000 extended Spider training instances and 2,000 generated code-outputted instances, and evaluate on both Spider and our created test sets.\n\nThe results presented in Table 3 demonstrate the effectiveness of incorporating extended reasoning processes, showcasing a performance boost of 3.8% and 9.2% on WikiTQ and TAT-QA respectively, compared to using solely original training data (i.e., question and answer pairs). This improvement is primarily attributed to the inclusion of detailed textual explanations of results, aiding LLMs in recognizing reasoning patterns. Furthermore, the addition of generated data yields an additional 1.6% and 6.4% enhancement in performance over the original training data on WikiTQ and TAT-QA, respectively, emphasizing the value of including answers with reasoning processes. Notably, the combination of both extended and generated training data leads to a significant 4.8% and 10.1% increase in performance relative to using only the original data, highlighting the advantages of integrating diverse data sources. The results observed on Spider and our created test sets further corroborate the benefits of extended and generated training data.\n\nEffect of Cross-way Validation. We examine the effectiveness of our proposed cross-way validation method, which assesses the consistency between direct answer generation and code generation solutions during the automatically generating training data process. We compare it against two other validation methods: Same-way validation, which generates two direct answers by the same inner-parameter technique of GPT-3.5 and assesses their alignment using CritiqueLLM, and Self-check validation, which enables GPT-3.5 to generate one textual solution and self-check its answer. According to the results presented in Table 4, our cross-way validation method outperforms the other methods. This superior performance is attributed to its use of two distinct responses, leading to more reliable validation results.\n\n5.6. Training Strategy Investigation\n\nWe investigate three training aspects: data size, the ratio between document- and spreadsheet-embedded data, and shuffling data strategy. We explore the following variants:\n\n•\n\nData size: Options include 1k, 2k, 5k, 10k, 20k, and 40k instances.\n\n•\n\nData ratio: The proportion of document- to spreadsheet-embedded data, explored in ratios of 0:10, 2:8, 4:6, 5:5, 6:4, 8:2, and 10:0.\n\n•\n\nShuffle strategy: Three approaches – full shuffle (instance-level shuffling), batch shuffle (keeping instances within a batch of the same type and shuffling batches), and epoch shuffle (keeping instances within an epoch of the same type and shuffling epochs).\n\nThe default configuration for our experiments is 10k training data instances, a 5:5 data ratio, and full shuffle. When evaluating one factor, the default settings are maintained for the other factors.\n\nFigure LABEL:fig:trainingstrategy illustrates the accuracies of TableLLM under various training data settings. As depicted in Figure LABEL:subfig:datasize, performance gains follow a log-linear relationship with the training data size, motivating us to stop early at 40K, which offers a cost-effective balance. Figure LABEL:subfig:dataratio indicates that a 5:5 ratio yields balanced performance across both data types. Lastly, Figure LABEL:subfig:shuffle demonstrates that full shuffle and batch shuffle lead to faster convergence than epoch shuffle because epoch lacks a sufficient mixture of the two data sources.\n\n6. Conclusion\n\nOur pioneering study introduces a TableLLM (13B) tailored for tabular data manipulation in real office scenarios. We gather actual requirements from office settings and identify document-embedded and spreadsheet-embedded scenarios. Ensuring high-quality data through extended reasoning processes and cross-way validation on automatically generated training data, the resulting TableLLM performs comparably to GPT-3.5 and even surpasses GPT-4 in the spreadsheet-embedded scenario. We anticipate that our published dataset, model checkpoint, and code will offer a cost-effective solution for researchers and developers aiming to enhance LLM capabilities for tables and develop diverse table-related applications.\n\nReferences\n\n(1)\n\nAndrejczuk et al. (2022) Ewa Andrejczuk, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, and Yasemin Altun. 2022. Table-to-text generation and pre-training with tabt5. arXiv preprint arXiv:2210.09162 (2022).\n\nAnil et al. (2023) Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, and et al. 2023. PaLM 2 Technical Report. CoRR abs/2305.10403 (2023). https://doi.org/10.48550/arXiv.2305.10403 arXiv:2305.10403\n\nAnonymous (2024) Anonymous. 2024. Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=4L0xnS4GQM\n\nBadaro et al. (2023) Gilbert Badaro, Mohammed Saeed, et al. 2023. Transformers for Tabular Data Representation: A Survey of Models and Applications. TACL (2023).\n\nBi et al. (2024) Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. 2024. DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. arXiv preprint arXiv:2401.02954 (2024).\n\nChen et al. (2021) Wenhu Chen, Ming-Wei Chang, Eva Schlinger, William Wang, and William W Cohen. 2021. Open Question Answering over Tables and Text. Proceedings of ICLR 2021 (2021).\n\nCheng et al. (2023) Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. 2023. Binding Language Models in Symbolic Languages. ICLR (2023).\n\nDeng et al. (2020) Xiang Deng, Huan Sun, Alyssa Lees, You Wu, and Cong Yu. 2020. TURL: table understanding through representation learning. Proceedings of the VLDB Endowment 14, 3 (2020), 307–319.\n\nDietterich et al. (2002) Thomas G Dietterich et al. 2002. Ensemble learning. The handbook of brain theory and neural networks 2, 1 (2002), 110–125.\n\nDong et al. (2022) Haoyu Dong, Zhoujun Cheng, et al. 2022. Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks. In IJCAI.\n\nDong et al. (2023) Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Lu Chen, Jinshu Lin, and Dongfang Lou. 2023. C3: Zero-shot Text-to-SQL with ChatGPT. CoRR abs/2307.07306 (2023). arXiv:2307.07306\n\nEisenschlos et al. (2021) Julian Eisenschlos, Maharshi Gor, Thomas Mueller, and William Cohen. 2021. MATE: Multi-view Attention for Table Transformer Efficiency. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 7606–7619.\n\nGong et al. (2020) Heng Gong, Yawei Sun, Xiaocheng Feng, Bing Qin, Wei Bi, Xiaojiang Liu, and Ting Liu. 2020. TableGPT: Few-shot Table-to-Text Generation with Table Structure Reconstruction and Content Matching. In Proceedings of the 28th International Conference on Computational Linguistics, Donia Scott, Nuria Bel, and Chengqing Zong (Eds.). International Committee on Computational Linguistics, Barcelona, Spain (Online), 1978–1988. https://doi.org/10.18653/v1/2020.coling-main.179\n\nHerzig et al. (2020) Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Eisenschlos. 2020. TaPas: Weakly Supervised Table Parsing via Pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Computational Linguistics, Online, 4320–4333. https://doi.org/10.18653/v1/2020.acl-main.398\n\nHu et al. (2023) Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. 2023. ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory. arXiv preprint arXiv:2306.03901 (2023).\n\nHu et al. (2024) Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, Jianbo Yuan, Kun Kuang, Yang Yang, Hongxia Yang, and Fei Wu. 2024. InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks. arXiv:2401.05507 [cs.CL]\n\nHulsebos et al. (2023) Madelon Hulsebos, Çagatay Demiralp, and Paul Groth. 2023. Gittables: A large-scale corpus of relational tables. Proceedings of the ACM on Management of Data 1, 1 (2023), 1–17.\n\nIida et al. (2021) Hiroshi Iida, Dung Thai, Varun Manjunatha, and Mohit Iyyer. 2021. Tabbie: Pretrained representations of tabular data. arXiv preprint arXiv:2105.02584 (2021).\n\nJiang et al. (2023) Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen. 2023. StructGPT: A General Framework for Large Language Model to Reason over Structured Data. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 9237–9251. https://doi.org/10.18653/v1/2023.emnlp-main.574\n\nJin and Lu (2023) Ziqi Jin and Wei Lu. 2023. Tab-CoT: Zero-shot Tabular Chain of Thought. arXiv preprint arXiv:2305.17812 (2023).\n\nKe et al. (2023) Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang. 2023. CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation. arXiv:2311.18702 [cs.CL]\n\nLai et al. (2023) Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-Tau Yih, Daniel Fried, Sida Wang, and Tao Yu. 2023. DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation. In Proceedings of the 40th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 18319–18345.\n\nLi et al. (2023b) Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, and Zhaoxiang Zhang. 2023b. SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models. In Thirty-seventh Conference on Neural Information Processing Systems. https://openreview.net/forum?id=tfyr2zRVoK\n\nLi et al. (2023a) Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, and Surajit Chaudhuri. 2023a. Table-GPT: Table-tuned GPT for Diverse Table Tasks. arXiv:2310.09263 [cs.CL]\n\nLiang et al. (2023) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2023. Holistic Evaluation of Language Models. arXiv:2211.09110 [cs.CL]\n\nLiu et al. (2022) Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. 2022. TAPEX: Table Pre-training via Learning a Neural SQL Executor. In International Conference on Learning Representations. https://openreview.net/forum?id=O50443AsCP\n\nNan et al. (2022) Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryściński, Hailey Schoelkopf, Riley Kong, Xiangru Tang, Mutethia Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, and Dragomir Radev. 2022. FeTaQA: Free-form Table Question Answering. Transactions of the Association for Computational Linguistics 10 (2022), 35–49.\n\nOpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023). https://doi.org/10.48550/arXiv.2303.08774 arXiv:2303.08774\n\nOuyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In NeurIPS. http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html\n\nPasupat and Liang (2015) Panupong Pasupat and Percy Liang. 2015. Compositional Semantic Parsing on Semi-Structured Tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Chengqing Zong and Michael Strube (Eds.). Association for Computational Linguistics, Beijing, China, 1470–1480. https://doi.org/10.3115/v1/P15-1142\n\nPourreza and Rafiei (2023) Mohammadreza Pourreza and Davood Rafiei. 2023. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. arXiv preprint arXiv:2304.11015 (2023).\n\nRaffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research 21, 140 (2020), 1–67. http://jmlr.org/papers/v21/20-074.html\n\nRozière et al. (2023) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, I. Evtimov, Joanna Bitton, Manish P Bhatt, Cristian Cantón Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D’efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code Llama: Open Foundation Models for Code. ArXiv abs/2308.12950 (2023). https://api.semanticscholar.org/CorpusID:261100919\n\nTouvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. CoRR abs/2307.09288 (2023). https://doi.org/10.48550/arXiv.2307.09288 arXiv:2307.09288\n\nWang et al. (2021) Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, and Dongmei Zhang. 2021. Tuta: Tree-based transformers for generally structured table pre-training. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 1780–1790.\n\nWei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In NeurIPS. http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html\n\nXie et al. (2022) Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2022. UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 602–631. https://doi.org/10.18653/v1/2022.emnlp-main.39\n\nXue et al. (2023) Siqiao Xue, Caigao Jiang, Wenhui Shi, Fangyin Cheng, Keting Chen, Hongjun Yang, Zhiping Zhang, Jianshan He, Hongyang Zhang, Ganglin Wei, et al. 2023. DB-GPT: Empowering Database Interactions with Private Large Language Models. arXiv preprint arXiv:2312.17449 (2023).\n\nYang et al. (2022) Jingfeng Yang, Aditya Gupta, Shyam Upadhyay, Luheng He, Rahul Goel, and Shachi Paul. 2022. TableFormer: Robust Transformer Modeling for Table-Text Encoding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 528–537. https://doi.org/10.18653/v1/2022.acl-long.40\n\nYe et al. (2023) Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. 2023. Large Language Models Are Versatile Decomposers: Decomposing Evidence and Questions for Table-Based Reasoning. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (¡conf-loc¿, ¡city¿Taipei¡/city¿, ¡country¿Taiwan¡/country¿, ¡/conf-loc¿) (SIGIR ’23). Association for Computing Machinery, New York, NY, USA, 174–184. https://doi.org/10.1145/3539618.3591708\n\nYin et al. (2020) Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. 2020. TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Computational Linguistics, Online, 8413–8426. https://doi.org/10.18653/v1/2020.acl-main.745\n\nYu et al. (2020) Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin Wang, Yi Chern Tan, Xinyi Yang, Dragomir Radev, Richard Socher, and Caiming Xiong. 2020. Grappa: Grammar-augmented pre-training for table semantic parsing. arXiv preprint arXiv:2009.13845 (2020).\n\nYu et al. (2018) Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2018. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium.\n\nZhang et al. (2023c) Tianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun. 2023c. TableLlama: Towards Open Large Generalist Models for Tables. arXiv:2311.09206 [cs.CL]\n\nZhang et al. (2023b) Wenqi Zhang, Yongliang Shen, Weiming Lu, and Yueting Zhuang. 2023b. Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow. arXiv preprint arXiv:2306.07209 (2023).\n\nZhang et al. (2023a) Yunjia Zhang, Jordan Henkel, Avrilia Floratou, Joyce Cahoon, Shaleen Deep, and Jignesh M. Patel. 2023a. ReAcTable: Enhancing ReAct for Table Question Answering. arXiv:2310.00815 [cs.DB]\n\nZhao et al. (2023a) Bowen Zhao, Changkai Ji, Yuejie Zhang, Wen He, Yingwen Wang, Qing Wang, Rui Feng, and Xiaobo Zhang. 2023a. Large Language Models are Complex Table Parsers. arXiv preprint arXiv:2312.11521 (2023).\n\nZhao et al. (2023b) Wenting Zhao, Ye Liu, Yao Wan, Yibo Wang, Zhongfen Deng, and Philip S Yu. 2023b. Localize, retrieve and fuse: A generalized framework for free-form question answering over tables. arXiv preprint arXiv:2309.11049 (2023).\n\nZhong et al. (2017) Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning. CoRR abs/1709.00103 (2017).\n\nZhu et al. (2021) Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online, 3277–3287. https://doi.org/10.18653/v1/2021.acl-long.254\n\nZhu et al. (2024) Fengbin Zhu, Ziyang Liu, Fuli Feng, Chao Wang, Moxin Li, and Tat-Seng Chua. 2024. TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data. arXiv preprint arXiv:2401.13223 (2024).\n\nAppendix A Appendix\n\n\\startcontents\n\n[appendix] \\printcontents[appendix] 1\n\nA.1. Web Application\n\nWe present a screenshot of the web application deployed with our proposed TableLLM in Figure LABEL:fig:app, where Figure LABEL:subfig:update shows an instruction with the update operation and Figure LABEL:subfig:chart shows an instruction with the chart operation.\n\nA.2. The Survey Details\n\nWe conduct a survey among universities and enterprises to assess users’ needs for tabular data-related tasks in real office environments. We obtain a total of 507 valid responses, representing various roles, including research and development specialists (36.69%), teachers (14.40%), administrators (14.00%), students (12.62%), marketing professionals (4.14%), and human resources personnel (1.97%).\n\nThe survey results, depicted in Figure 1, reveal: (a) Participants’ demands for various table-related tasks, with TableQA (80.28%) being the most sought-after, followed by Table revision (71.40%), which involves creating, updating, and deleting tables. Tasks with demand exceeding 200 include TableQA, Table revision, Chart creation, Table matching, Duplicate data removal, Error detection, Missing value detection, and Table extraction. (b) Participants prefer Excel, Word, PDF, and CSV formats, and (c) long tables with more than 50 rows.\n\nBelow is the complete survey on table usage:\n\n\\mdfdefinestyle\n\nexampledefaultlinecolor=black, frametitlerule = true\n\n(1)\n\nWhat is your occupation?\n\nA\n\nStudent\n\nB\n\nTeacher\n\nC\n\nAdministrator\n\nD\n\nHuman Resources Professional\n\nE\n\nMarketing Professional\n\nF\n\nResearch and Development Specialist\n\nG\n\nOthers[Fill in the Blank]\n\n(2)\n\nIn your daily work, how often do you work with tables (such as Excel, CSV, or direct access to databases)?\n\nA\n\nRarely use (less than once a day on average)\n\nB\n\nOccasionally use (1 to 5 times per day)\n\nC\n\nFrequently use (5 to 20 times per day)\n\nD\n\nIs my work theme (use more than 20 times a day)\n\n(3)\n\nIn your normal work, What are the sizes of tables that you typically work with?[Multiple choice question]\n\nA\n\nTables under 50 rows.\n\nB\n\nTables over 50 rows.\n\n(4)\n\nWhat types of tables do you typically encounter and handle in your daily work?[Multiple choice question]\n\nA\n\nExcel\n\nB\n\nWord\n\nC\n\nHTML\n\nD\n\nCSV\n\nE\n\nPDF\n\nF\n\nMarkdown\n\nG\n\nOthers[Fill in the Blank]\n\n(5)\n\nWhich Table manipulation tasks do you need to use in your work?[Multiple choice question]\n\nA\n\nTableQA, e.g.,\n\n–\n\nFind the number of people with grade above 90;\n\n–\n\nGroup them according to 90-100 points, 80-90 points, 60-80 points and 60 points, and count the number of people in each score segment;\n\n–\n\nFind all conferences held in Jiangsu in the second half of 2023;\n\nB\n\nTable revision, e.g.,\n\n–\n\nSort by height column;\n\n–\n\nConvert the Date column to Month/Day/year format;\n\n–\n\nInsert a column “total score”, representing the weighted sum of 60% and 40% from the first to the third column;\n\n–\n\nDelete the “normal score” column;\n\nC\n\nChart, e.g.,\n\n–\n\nDraw statistical drawings, such as line diagrams, column charts, pie charts;\n\nD\n\nNone.\n\n(6)\n\nWhich Table cleaning tasks do you need in your work?[Multiple choice question]\n\nA\n\nMissing value detection, e.g.,\n\n–\n\nDetect missing values and fill in the mean value of the corresponding column;\n\nB\n\nError detection, e.g.,\n\n–\n\nCheck a cell whose format is not “month/day/year” and convert it;\n\nC\n\nDelete duplicate data, e.g.,\n\n–\n\nTo filter duplicate data by name, only keep the first row with the same name;\n\nD\n\nNone.\n\n(7)\n\nWhich Table analysis tasks do you need in your work?[Multiple choice question]\n\nA\n\nColumn type annotation, e.g.,\n\n–\n\nGiven some examples of a column like 1,000 RMB, 1,500 RMB, and 2,000 RMB, name the column;\n\nB\n\nEntity linking, e.g.,\n\n–\n\nGiven a candidate combination of column names, assign an appropriate column name to each column in the table;\n\nC\n\nRow-to-row transform, e.g.,\n\n–\n\nPredict the rating of the fourth team based on the “win-loss rating” of the first three teams;\n\nD\n\nFact verification, e.g.,\n\n–\n\nBased on the content of the table, determine whether “profit growth in the first quarter of 2023 is 10%” is true;\n\nE\n\nNone.\n\n(8)\n\nWhich Table-to-Text tasks do you need in your work?[Multiple choice question]\n\nA\n\nSummarization, e.g.,\n\n–\n\nGenerate a title for the table;\n\nB\n\nDialogue generation, e.g.,\n\n–\n\nGiven the table and the history of the conversation, generate the next conversation;\n\nC\n\nNone.\n\n(9)\n\nWhich Table augmentation tasks do you need in your work? [Multiple choice question]\n\nA\n\nRow population, e.g.,\n\n–\n\nGiven “name”, “age”, “height”, generate specific row data;\n\nB\n\nSchema augmentation, e.g.,\n\n–\n\nGiven “Date”, “Growth rate”, “Net income”, expand the other columns;\n\nC\n\nNone.\n\n(10)\n\nDo you need Table matching at work? (For example, merge two tables as required)\n\nA\n\nYes, I need Table matching.\n\nB\n\nNo, I don’t need Table matching.\n\n(11)\n\nDo you need Table extraction at work? (For example, organize the Markdown format table into Excel, extract a table from the web page to Excel)\n\nA\n\nYes, I need Table extraction.\n\nB\n\nNo, I don’t need Table extraction.\n\n(12)\n\nDo you have tabular tasks that do not fit into the above categories? If yes, please give an example.[Fill in the Blank]\n\nA.3. Verification of Cross-way Validation\n\nWe use Yasubscript𝑌𝑎Y_{a}italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT to denote that the first response A𝐴Aitalic_A is correct, Ybsubscript𝑌𝑏Y_{b}italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT to denote that the second response B𝐵Bitalic_B is correct, Y𝑌Yitalic_Y to denote that both responses are correct, and E𝐸Eitalic_E to denote that the two responses are consistent. We will prove the following:\n\nTheorem A.1.\n\nIf A𝐴Aitalic_A and B𝐵Bitalic_B come from the same distribution D𝐷Ditalic_D, P⁢(Ya)=P⁢(Yb)=p>1/2𝑃subscript𝑌𝑎𝑃subscript𝑌𝑏𝑝12P(Y_{a})=P(Y_{b})=p>1/2italic_P ( italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ) = italic_P ( italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ) = italic_p > 1 / 2, then the consistency check is better than single inference, that is, P⁢(Y|E)≥P⁢(Ya)𝑃conditional𝑌𝐸𝑃subscript𝑌𝑎P(Y|E)\\geq P(Y_{a})italic_P ( italic_Y | italic_E ) ≥ italic_P ( italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ).\n\nTheorem A.2.\n\nIf P⁢(Ya)=P⁢(Yb)=p𝑃subscript𝑌𝑎𝑃subscript𝑌𝑏𝑝P(Y_{a})=P(Y_{b})=pitalic_P ( italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ) = italic_P ( italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ) = italic_p, A𝐴Aitalic_A and B𝐵Bitalic_B are sampled from independent distributions DAsubscript𝐷𝐴D_{A}italic_D start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT and DBsubscript𝐷𝐵D_{B}italic_D start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT respectively, the outcome will improve (in terms of expected value),\n\nE⁢[P⁢(Y|E)|Ya∼D,Yb∼D]≤E⁢[P⁢(Y|E)|Ya∼DA,Yb∼DB].𝐸delimited-[]formulae-sequencesimilar-toconditional𝑃conditional𝑌𝐸subscript𝑌𝑎𝐷similar-tosubscript𝑌𝑏𝐷𝐸delimited-[]formulae-sequencesimilar-toconditional𝑃conditional𝑌𝐸subscript𝑌𝑎subscript𝐷𝐴similar-tosubscript𝑌𝑏subscript𝐷𝐵E[P(Y|E)|Y_{a}\\sim D,Y_{b}\\sim D]\\leq E[P(Y|E)|Y_{a}\\sim D_{A},Y_{b}\\sim D_{B}].italic_E [ italic_P ( italic_Y | italic_E ) | italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ∼ italic_D , italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ∼ italic_D ] ≤ italic_E [ italic_P ( italic_Y | italic_E ) | italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ∼ italic_D start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT , italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ∼ italic_D start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT ] .\n\nLemma A.0.\n\nIf 12≤p≤112𝑝1\\frac{1}{2}\\leq p\\leq 1divide start_ARG 1 end_ARG start_ARG 2 end_ARG ≤ italic_p ≤ 1, then p2p2+(1−p)2≥psuperscript𝑝2superscript𝑝2superscript1𝑝2𝑝\\frac{p^{2}}{p^{2}+(1-p)^{2}}\\geq pdivide start_ARG italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( 1 - italic_p ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ≥ italic_p.\n\nProof.\n\np2p2+(1−p)2−psuperscript𝑝2superscript𝑝2superscript1𝑝2𝑝\\displaystyle\\frac{p^{2}}{p^{2}+(1-p)^{2}}-pdivide start_ARG italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( 1 - italic_p ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG - italic_p =\\displaystyle== p2−p⁢((1−p)2+p2)(1−p)2+p2superscript𝑝2𝑝superscript1𝑝2superscript𝑝2superscript1𝑝2superscript𝑝2\\displaystyle\\frac{p^{2}-p((1-p)^{2}+p^{2})}{(1-p)^{2}+p^{2}}divide start_ARG italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_p ( ( 1 - italic_p ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG start_ARG ( 1 - italic_p ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG =\\displaystyle== p⁢(−2⁢p2+3⁢p−1)(1−p)2+p2𝑝2superscript𝑝23𝑝1superscript1𝑝2superscript𝑝2\\displaystyle\\frac{p(-2p^{2}+3p-1)}{(1-p)^{2}+p^{2}}divide start_ARG italic_p ( - 2 italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 3 italic_p - 1 ) end_ARG start_ARG ( 1 - italic_p ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG =\\displaystyle== p⁢(1−p)⁢(2⁢p−1)(1−p)2+p2𝑝1𝑝2𝑝1superscript1𝑝2superscript𝑝2\\displaystyle\\frac{p(1-p)(2p-1)}{(1-p)^{2}+p^{2}}divide start_ARG italic_p ( 1 - italic_p ) ( 2 italic_p - 1 ) end_ARG start_ARG ( 1 - italic_p ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ≥\\displaystyle\\geq≥ 0.0\\displaystyle 0.0 .\n\n∎\n\nLemma A.0.\n\nIf x1+x2+x3+…+xk=Ssubscript𝑥1subscript𝑥2subscript𝑥3…subscript𝑥𝑘𝑆x_{1}+x_{2}+x_{3}+\\ldots+x_{k}=Sitalic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT + … + italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_S and x1,x2,…,xksubscript𝑥1subscript𝑥2…subscript𝑥𝑘x_{1},x_{2},\\ldots,x_{k}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT are non-negative numbers, then\n\nx12+x22+x32+…+xk2≥S2k.superscriptsubscript𝑥12superscriptsubscript𝑥22superscriptsubscript𝑥32…superscriptsubscript𝑥𝑘2superscript𝑆2𝑘x_{1}^{2}+x_{2}^{2}+x_{3}^{2}+\\ldots+x_{k}^{2}\\geq\\frac{S^{2}}{k}.italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + … + italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≥ divide start_ARG italic_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_k end_ARG .\n\nProof.\n\nAccording to the Cauchy-Schwarz inequality, we have\n\n(x12+x22+x32⁢…⁢xk2)superscriptsubscript𝑥12superscriptsubscript𝑥22superscriptsubscript𝑥32…superscriptsubscript𝑥𝑘2\\displaystyle(x_{1}^{2}+x_{2}^{2}+x_{3}^{2}\\ldots x_{k}^{2})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT … italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) =\\displaystyle== 1k⁢(1+1+1+…+1)⁢(x12+x22+x32+xk2)1𝑘111…1superscriptsubscript𝑥12superscriptsubscript𝑥22superscriptsubscript𝑥32superscriptsubscript𝑥𝑘2\\displaystyle\\frac{1}{k}(1+1+1+\\ldots+1)(x_{1}^{2}+x_{2}^{2}+x_{3}^{2}+x_{k}^{% 2})divide start_ARG 1 end_ARG start_ARG italic_k end_ARG ( 1 + 1 + 1 + … + 1 ) ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) ≥\\displaystyle\\geq≥ 1k⁢(x1+x2+x3+…+xk)21𝑘superscriptsubscript𝑥1subscript𝑥2subscript𝑥3…subscript𝑥𝑘2\\displaystyle\\frac{1}{k}(x_{1}+x_{2}+x_{3}+\\ldots+x_{k})^{2}divide start_ARG 1 end_ARG start_ARG italic_k end_ARG ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT + … + italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT =\\displaystyle== S2k.superscript𝑆2𝑘\\displaystyle\\frac{S^{2}}{k}.divide start_ARG italic_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_k end_ARG .\n\n∎\n\nLemma A.0.\n\nWe define x¯¯𝑥\\overline{x}over¯ start_ARG italic_x end_ARG to represent the mean of a set of numbers x1,x2,…,xnsubscript𝑥1subscript𝑥2…subscript𝑥𝑛x_{1},x_{2},\\ldots,x_{n}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT, that is, x¯=∑i=1nxin¯𝑥superscriptsubscript𝑖1𝑛subscript𝑥𝑖𝑛\\overline{x}=\\frac{\\sum_{i=1}^{n}{x_{i}}}{n}over¯ start_ARG italic_x end_ARG = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG italic_n end_ARG.\n\n∑i=1nxi⁢yi=∑i=1n(xi−x¯)⁢(yi−y¯)+n⁢x¯⁢y¯.superscriptsubscript𝑖1𝑛subscript𝑥𝑖subscript𝑦𝑖superscriptsubscript𝑖1𝑛subscript𝑥𝑖¯𝑥subscript𝑦𝑖¯𝑦𝑛¯𝑥¯𝑦\\sum_{i=1}^{n}{x_{i}y_{i}}=\\sum_{i=1}^{n}{(x_{i}-\\bar{x})(y_{i}-\\bar{y})}+n% \\bar{x}\\bar{y}.∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over¯ start_ARG italic_x end_ARG ) ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over¯ start_ARG italic_y end_ARG ) + italic_n over¯ start_ARG italic_x end_ARG over¯ start_ARG italic_y end_ARG .\n\nProof.\n\n∑i=1n(xi−x¯)⁢(yi−y¯)superscriptsubscript𝑖1𝑛subscript𝑥𝑖¯𝑥subscript𝑦𝑖¯𝑦\\displaystyle\\sum_{i=1}^{n}{(x_{i}-\\bar{x})(y_{i}-\\bar{y})}∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over¯ start_ARG italic_x end_ARG ) ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over¯ start_ARG italic_y end_ARG ) =\\displaystyle== ∑i=1nxi⁢yi−∑i=1n(xi)⁢y¯−∑i=1n(yi)⁢x¯+∑i=1nx¯⁢y¯superscriptsubscript𝑖1𝑛subscript𝑥𝑖subscript𝑦𝑖superscriptsubscript𝑖1𝑛subscript𝑥𝑖¯𝑦superscriptsubscript𝑖1𝑛subscript𝑦𝑖¯𝑥superscriptsubscript𝑖1𝑛¯𝑥¯𝑦\\displaystyle\\sum_{i=1}^{n}{x_{i}y_{i}}-\\sum_{i=1}^{n}{(x_{i})\\bar{y}}-\\sum_{i% =1}^{n}{(y_{i})\\bar{x}}+\\sum_{i=1}^{n}{\\bar{x}\\bar{y}}∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) over¯ start_ARG italic_y end_ARG - ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) over¯ start_ARG italic_x end_ARG + ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT over¯ start_ARG italic_x end_ARG over¯ start_ARG italic_y end_ARG =\\displaystyle== ∑i=1nxi⁢yi−y¯⁢∑i=1n(xi)−x¯⁢∑i=1n(yi)+n⁢x¯⁢y¯superscriptsubscript𝑖1𝑛subscript𝑥𝑖subscript𝑦𝑖¯𝑦superscriptsubscript𝑖1𝑛subscript𝑥𝑖¯𝑥superscriptsubscript𝑖1𝑛subscript𝑦𝑖𝑛¯𝑥¯𝑦\\displaystyle\\sum_{i=1}^{n}{x_{i}y_{i}}-\\bar{y}\\sum_{i=1}^{n}{(x_{i})}-\\bar{x}% \\sum_{i=1}^{n}{(y_{i})}+n{\\bar{x}\\bar{y}}∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over¯ start_ARG italic_y end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - over¯ start_ARG italic_x end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + italic_n over¯ start_ARG italic_x end_ARG over¯ start_ARG italic_y end_ARG =\\displaystyle== ∑i=1nxi⁢yi−n⁢y¯⁢x¯−n⁢x¯⁢y¯+n⁢x¯⁢y¯superscriptsubscript𝑖1𝑛subscript𝑥𝑖subscript𝑦𝑖𝑛¯𝑦¯𝑥𝑛¯𝑥¯𝑦𝑛¯𝑥¯𝑦\\displaystyle\\sum_{i=1}^{n}{x_{i}y_{i}}-n\\bar{y}\\bar{x}-n\\bar{x}\\bar{y}+n\\bar{% x}\\bar{y}∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_n over¯ start_ARG italic_y end_ARG over¯ start_ARG italic_x end_ARG - italic_n over¯ start_ARG italic_x end_ARG over¯ start_ARG italic_y end_ARG + italic_n over¯ start_ARG italic_x end_ARG over¯ start_ARG italic_y end_ARG =\\displaystyle== ∑i=1nxi⁢yi−n⁢x¯⁢y¯.superscriptsubscript𝑖1𝑛subscript𝑥𝑖subscript𝑦𝑖𝑛¯𝑥¯𝑦\\displaystyle\\sum_{i=1}^{n}{x_{i}y_{i}}-n\\bar{x}\\bar{y}.∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_n over¯ start_ARG italic_x end_ARG over¯ start_ARG italic_y end_ARG .\n\nMoving terms to the other side of the equation, thus proving. ∎\n\nProof of Theorem A.1.\n\nAccording to Bayes’ theorem,\n\nP⁢(Y|E)=P⁢(Y⁢E)P⁢(E)=P⁢(Y⁢E)P⁢(Y⁢E)+P⁢(Y¯⁢E).𝑃conditional𝑌𝐸𝑃𝑌𝐸𝑃𝐸𝑃𝑌𝐸𝑃𝑌𝐸𝑃¯𝑌𝐸P(Y|E)=\\frac{P(YE)}{P(E)}=\\frac{P(YE)}{P(YE)+P(\\bar{Y}E)}.italic_P ( italic_Y | italic_E ) = divide start_ARG italic_P ( italic_Y italic_E ) end_ARG start_ARG italic_P ( italic_E ) end_ARG = divide start_ARG italic_P ( italic_Y italic_E ) end_ARG start_ARG italic_P ( italic_Y italic_E ) + italic_P ( over¯ start_ARG italic_Y end_ARG italic_E ) end_ARG .\n\nSince the two samplings are independent, we have\n\nP⁢(Y)=P⁢(Ya⁢Yb)=P⁢(Ya)⋅P⁢(Yb)=p2,𝑃𝑌𝑃subscript𝑌𝑎subscript𝑌𝑏⋅𝑃subscript𝑌𝑎𝑃subscript𝑌𝑏superscript𝑝2P(Y)=P(Y_{a}Y_{b})=P(Y_{a})\\cdot P(Y_{b})=p^{2},italic_P ( italic_Y ) = italic_P ( italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ) = italic_P ( italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ) ⋅ italic_P ( italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ) = italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,\n\nP⁢(E⁢Y)=P⁢(E|Y)⋅P⁢(Y)=P⁢(Y)=p2,𝑃𝐸𝑌⋅𝑃conditional𝐸𝑌𝑃𝑌𝑃𝑌superscript𝑝2P(EY)=P(E|Y)\\cdot P(Y)=P(Y)=p^{2},italic_P ( italic_E italic_Y ) = italic_P ( italic_E | italic_Y ) ⋅ italic_P ( italic_Y ) = italic_P ( italic_Y ) = italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,\n\nP⁢(E⁢Y¯)𝑃𝐸¯𝑌\\displaystyle P(E\\bar{Y})italic_P ( italic_E over¯ start_ARG italic_Y end_ARG ) =\\displaystyle== P⁢(E⁢Ya¯⁢Yb¯)+P⁢(E⁢Ya⁢Yb¯)+P⁢(E⁢Ya¯⁢Yb)𝑃𝐸¯subscript𝑌𝑎¯subscript𝑌𝑏𝑃𝐸subscript𝑌𝑎¯subscript𝑌𝑏𝑃𝐸¯subscript𝑌𝑎subscript𝑌𝑏\\displaystyle P(E\\bar{Y_{a}}\\bar{Y_{b}})+P(EY_{a}\\bar{Y_{b}})+P(E\\bar{Y_{a}}Y_% {b})italic_P ( italic_E over¯ start_ARG italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT end_ARG over¯ start_ARG italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT end_ARG ) + italic_P ( italic_E italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT over¯ start_ARG italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT end_ARG ) + italic_P ( italic_E over¯ start_ARG italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT end_ARG italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ) =\\displaystyle== P⁢(E⁢Ya¯⁢Yb¯)𝑃𝐸¯subscript𝑌𝑎¯subscript𝑌𝑏\\displaystyle P(E\\bar{Y_{a}}\\bar{Y_{b}})italic_P ( italic_E over¯ start_ARG italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT end_ARG over¯ start_ARG italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT end_ARG ) =\\displaystyle== P⁢(Ya¯⁢Yb¯)⁢P⁢(E|Ya¯⁢Yb¯)𝑃¯subscript𝑌𝑎¯subscript𝑌𝑏𝑃conditional𝐸¯subscript𝑌𝑎¯subscript𝑌𝑏\\displaystyle P(\\bar{Y_{a}}\\bar{Y_{b}})P(E|\\bar{Y_{a}}\\bar{Y_{b}})italic_P ( over¯ start_ARG italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT end_ARG over¯ start_ARG italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT end_ARG ) italic_P ( italic_E | over¯ start_ARG italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT end_ARG over¯ start_ARG italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT end_ARG ) ≤\\displaystyle\\leq≤ P⁢(Ya¯⁢Yb¯)𝑃¯subscript𝑌𝑎¯subscript𝑌𝑏\\displaystyle P(\\bar{Y_{a}}\\bar{Y_{b}})italic_P ( over¯ start_ARG italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT end_ARG over¯ start_ARG italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT end_ARG ) =\\displaystyle== (1−p)2.superscript1𝑝2\\displaystyle(1-p)^{2}.( 1 - italic_p ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .\n\nThen:\n\nP⁢(Y|E)=P⁢(Y⁢E)P⁢(E)=P⁢(Y⁢E)P⁢(Y⁢E)+P⁢(Y¯⁢E)≥p2p2+(1−p)2≥p.𝑃conditional𝑌𝐸𝑃𝑌𝐸𝑃𝐸𝑃𝑌𝐸𝑃𝑌𝐸𝑃¯𝑌𝐸superscript𝑝2superscript𝑝2superscript1𝑝2𝑝P(Y|E)=\\frac{P(YE)}{P(E)}=\\frac{P(YE)}{P(YE)+P(\\bar{Y}E)}\\geq\\frac{p^{2}}{p^{2% }+(1-p)^{2}}\\geq p.italic_P ( italic_Y | italic_E ) = divide start_ARG italic_P ( italic_Y italic_E ) end_ARG start_ARG italic_P ( italic_E ) end_ARG = divide start_ARG italic_P ( italic_Y italic_E ) end_ARG start_ARG italic_P ( italic_Y italic_E ) + italic_P ( over¯ start_ARG italic_Y end_ARG italic_E ) end_ARG ≥ divide start_ARG italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( 1 - italic_p ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ≥ italic_p .\n\n∎\n\nProof of Theorem A.2.\n\nP⁢(Y|E)=P⁢(Y⁢E)P⁢(Y⁢E)+P⁢(Y¯⁢E)=11+P⁢(Y¯⁢E)P⁢(Y⁢E).𝑃conditional𝑌𝐸𝑃𝑌𝐸𝑃𝑌𝐸𝑃¯𝑌𝐸11𝑃¯𝑌𝐸𝑃𝑌𝐸P(Y|E)=\\frac{P(YE)}{P(YE)+P(\\bar{Y}E)}=\\frac{1}{{1+\\frac{P(\\bar{Y}E)}{P(YE)}}}.italic_P ( italic_Y | italic_E ) = divide start_ARG italic_P ( italic_Y italic_E ) end_ARG start_ARG italic_P ( italic_Y italic_E ) + italic_P ( over¯ start_ARG italic_Y end_ARG italic_E ) end_ARG = divide start_ARG 1 end_ARG start_ARG 1 + divide start_ARG italic_P ( over¯ start_ARG italic_Y end_ARG italic_E ) end_ARG start_ARG italic_P ( italic_Y italic_E ) end_ARG end_ARG .\n\nIn order to increase the value above, with the numerator fixed at 1, we need to reduce the denominator, which is as follows:\n\nP⁢(Y¯⁢E)P⁢(Y⁢E)𝑃¯𝑌𝐸𝑃𝑌𝐸\\displaystyle\\frac{P(\\bar{Y}E)}{P(YE)}divide start_ARG italic_P ( over¯ start_ARG italic_Y end_ARG italic_E ) end_ARG start_ARG italic_P ( italic_Y italic_E ) end_ARG =\\displaystyle== P⁢(E|Y¯)⋅P⁢(Y¯)P⁢(E|Y)⋅P⁢(Y)⋅𝑃conditional𝐸¯𝑌𝑃¯𝑌⋅𝑃conditional𝐸𝑌𝑃𝑌\\displaystyle\\frac{P(E|\\bar{Y})\\cdot P(\\bar{Y})}{P(E|Y)\\cdot P(Y)}divide start_ARG italic_P ( italic_E | over¯ start_ARG italic_Y end_ARG ) ⋅ italic_P ( over¯ start_ARG italic_Y end_ARG ) end_ARG start_ARG italic_P ( italic_E | italic_Y ) ⋅ italic_P ( italic_Y ) end_ARG =\\displaystyle== P⁢(E|Y¯)⋅P⁢(Y¯)P⁢(Y)⋅𝑃conditional𝐸¯𝑌𝑃¯𝑌𝑃𝑌\\displaystyle\\frac{P(E|\\bar{Y})\\cdot P(\\bar{Y})}{P(Y)}divide start_ARG italic_P ( italic_E | over¯ start_ARG italic_Y end_ARG ) ⋅ italic_P ( over¯ start_ARG italic_Y end_ARG ) end_ARG start_ARG italic_P ( italic_Y ) end_ARG =\\displaystyle== P⁢(E|Y¯)⋅P⁢(Y¯)P⁢(Y)⋅𝑃conditional𝐸¯𝑌𝑃¯𝑌𝑃𝑌\\displaystyle P(E|\\bar{Y})\\cdot\\frac{P(\\bar{Y})}{P(Y)}italic_P ( italic_E | over¯ start_ARG italic_Y end_ARG ) ⋅ divide start_ARG italic_P ( over¯ start_ARG italic_Y end_ARG ) end_ARG start_ARG italic_P ( italic_Y ) end_ARG =\\displaystyle== P⁢(E|Y¯)⋅1−P⁢(Y)P⁢(Y).⋅𝑃conditional𝐸¯𝑌1𝑃𝑌𝑃𝑌\\displaystyle P(E|\\bar{Y})\\cdot\\frac{1-P(Y)}{P(Y)}.italic_P ( italic_E | over¯ start_ARG italic_Y end_ARG ) ⋅ divide start_ARG 1 - italic_P ( italic_Y ) end_ARG start_ARG italic_P ( italic_Y ) end_ARG .\n\nIn the above process, P⁢(Y)𝑃𝑌P(Y)italic_P ( italic_Y ) represents the probability that the model provides two correct responses, which still equals to\n\nP⁢(Y)=P⁢(Ya⁢Yb)=P⁢(Ya)⋅P⁢(Yb)=p2.𝑃𝑌𝑃subscript𝑌𝑎subscript𝑌𝑏⋅𝑃subscript𝑌𝑎𝑃subscript𝑌𝑏superscript𝑝2P(Y)=P(Y_{a}Y_{b})=P(Y_{a})\\cdot P(Y_{b})=p^{2}.italic_P ( italic_Y ) = italic_P ( italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ) = italic_P ( italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ) ⋅ italic_P ( italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ) = italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .\n\nTherefore, we should minimize P⁢(E|Y¯)𝑃conditional𝐸¯𝑌P(E|\\bar{Y})italic_P ( italic_E | over¯ start_ARG italic_Y end_ARG ), meaning that if the generated responses are all incorrect, then the probability of them being consistent should be minimized as soon as possible.\n\nNow we only need to consider the probability of the model not producing the correct answer, i.e., the probability of making errors. Assuming there are a total of k𝑘kitalic_k types of errors generated by the model, e1,e2,e3,…,eksubscript𝑒1subscript𝑒2subscript𝑒3…subscript𝑒𝑘e_{1},e_{2},e_{3},\\ldots,e_{k}italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , … , italic_e start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, then Pa⁢(e1)+Pa⁢(e2)+Pa⁢(e3)+…+Pa⁢(ek)=Pb⁢(e1)+Pb⁢(e2)+Pb⁢(e3)+…+Pb⁢(ek)=1−psubscript𝑃𝑎subscript𝑒1subscript𝑃𝑎subscript𝑒2subscript𝑃𝑎subscript𝑒3…subscript𝑃𝑎subscript𝑒𝑘subscript𝑃𝑏subscript𝑒1subscript𝑃𝑏subscript𝑒2subscript𝑃𝑏subscript𝑒3…subscript𝑃𝑏subscript𝑒𝑘1𝑝P_{a}(e_{1})+P_{a}(e_{2})+P_{a}(e_{3})+\\ldots+P_{a}(e_{k})=P_{b}(e_{1})+P_{b}(% e_{2})+P_{b}(e_{3})+\\ldots+P_{b}(e_{k})=1-pitalic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) + italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) + italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) + … + italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) + italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) + italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) + … + italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = 1 - italic_p. Then,\n\nPa⁢(e)¯=Pb⁢(e)¯=∑i=1kPa⁢(ei)k=∑i=1kPb⁢(ei)k=1−pk.¯subscript𝑃𝑎𝑒¯subscript𝑃𝑏𝑒superscriptsubscript𝑖1𝑘subscript𝑃𝑎subscript𝑒𝑖𝑘superscriptsubscript𝑖1𝑘subscript𝑃𝑏subscript𝑒𝑖𝑘1𝑝𝑘\\overline{P_{a}(e)}=\\overline{P_{b}(e)}=\\frac{\\sum_{i=1}^{k}{P_{a}(e_{i})}}{k}% =\\frac{\\sum_{i=1}^{k}{P_{b}(e_{i})}}{k}=\\frac{1-p}{k}.over¯ start_ARG italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e ) end_ARG = over¯ start_ARG italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e ) end_ARG = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_k end_ARG = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_k end_ARG = divide start_ARG 1 - italic_p end_ARG start_ARG italic_k end_ARG .\n\nIf A𝐴Aitalic_A and B𝐵Bitalic_B come from the same distribution D𝐷Ditalic_D, then Pa⁢(ei)=Pb⁢(ei)subscript𝑃𝑎subscript𝑒𝑖subscript𝑃𝑏subscript𝑒𝑖P_{a}(e_{i})=P_{b}(e_{i})italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), we can obtain\n\nP⁢(E|Y¯)=(Pa⁢(e1))2+(Pa⁢(e2))2+…+(Pa⁢(ek))2.𝑃conditional𝐸¯𝑌superscriptsubscript𝑃𝑎subscript𝑒12superscriptsubscript𝑃𝑎subscript𝑒22…superscriptsubscript𝑃𝑎subscript𝑒𝑘2P(E|\\bar{Y})=(P_{a}(e_{1}))^{2}+(P_{a}(e_{2}))^{2}+\\ldots+(P_{a}(e_{k}))^{2}.italic_P ( italic_E | over¯ start_ARG italic_Y end_ARG ) = ( italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + … + ( italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .\n\nAccording to Lemma A.2 above, we see that the equation for the same distribution is greater than or equal to (1−p)2ksuperscript1𝑝2𝑘\\frac{{(1-p)^{2}}}{k}divide start_ARG ( 1 - italic_p ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_k end_ARG. Since for each distribution D, it is greater than or equal to this value, then its expected value should also be greater than this value. That is,\n\nE⁢[P⁢(E|Y¯)|Ya∼D,Yb∼D]≥(1−p)2k.𝐸delimited-[]formulae-sequencesimilar-toconditional𝑃conditional𝐸¯𝑌subscript𝑌𝑎𝐷similar-tosubscript𝑌𝑏𝐷superscript1𝑝2𝑘E[P(E|\\bar{Y})|Y_{a}\\sim D,Y_{b}\\sim D]\\geq\\frac{{(1-p)^{2}}}{k}.italic_E [ italic_P ( italic_E | over¯ start_ARG italic_Y end_ARG ) | italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ∼ italic_D , italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ∼ italic_D ] ≥ divide start_ARG ( 1 - italic_p ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_k end_ARG .\n\nHowever, when A𝐴Aitalic_A and B𝐵Bitalic_B come from independent distributions DAsubscript𝐷𝐴D_{A}italic_D start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT and DBsubscript𝐷𝐵D_{B}italic_D start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT respectively, P⁢(E|Y¯)=Pa⁢(e1)⁢Pb⁢(e1)+Pa⁢(e2)⁢Pb⁢(e2)+…+Pa⁢(ek)⁢Pb⁢(ek)=∑i=1kPa⁢(ei)⁢Pb⁢(ei)𝑃conditional𝐸¯𝑌subscript𝑃𝑎subscript𝑒1subscript𝑃𝑏subscript𝑒1subscript𝑃𝑎subscript𝑒2subscript𝑃𝑏subscript𝑒2…subscript𝑃𝑎subscript𝑒𝑘subscript𝑃𝑏subscript𝑒𝑘superscriptsubscript𝑖1𝑘subscript𝑃𝑎subscript𝑒𝑖subscript𝑃𝑏subscript𝑒𝑖P(E|\\bar{Y})=P_{a}(e_{1})P_{b}(e_{1})+P_{a}(e_{2})P_{b}(e_{2})+\\ldots+P_{a}(e_% {k})P_{b}(e_{k})=\\sum_{i=1}^{k}P_{a}(e_{i})P_{b}(e_{i})italic_P ( italic_E | over¯ start_ARG italic_Y end_ARG ) = italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) + italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) + … + italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ).\n\nAccording to Lemma A.3,\n\n∑i=1kPa⁢(ei)⁢Pb⁢(ei)superscriptsubscript𝑖1𝑘subscript𝑃𝑎subscript𝑒𝑖subscript𝑃𝑏subscript𝑒𝑖\\displaystyle\\sum_{i=1}^{k}P_{a}(e_{i})P_{b}(e_{i})∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) =k⋅Pa⁢(e)¯⋅Pb⁢(e)¯+∑i=1k((Pa⁢(ei)−Pa⁢(e)¯)⁢(Pb⁢(ei)−Pb⁢(e)¯))absent⋅𝑘¯subscript𝑃𝑎𝑒¯subscript𝑃𝑏𝑒superscriptsubscript𝑖1𝑘subscript𝑃𝑎subscript𝑒𝑖¯subscript𝑃𝑎𝑒subscript𝑃𝑏subscript𝑒𝑖¯subscript𝑃𝑏𝑒\\displaystyle=k\\cdot\\overline{P_{a}(e)}\\cdot\\overline{P_{b}(e)}+\\sum_{i=1}^{k}% \\left((P_{a}(e_{i})-\\overline{P_{a}(e)})(P_{b}(e_{i})-\\overline{P_{b}(e)})\\right)= italic_k ⋅ over¯ start_ARG italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e ) end_ARG ⋅ over¯ start_ARG italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e ) end_ARG + ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( ( italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - over¯ start_ARG italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e ) end_ARG ) ( italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - over¯ start_ARG italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e ) end_ARG ) ) =(1−p)2k+∑i=1k((Pa⁢(ei)−Pa⁢(e)¯)⁢(Pb⁢(ei)−Pb⁢(e)¯)),absentsuperscript1𝑝2𝑘superscriptsubscript𝑖1𝑘subscript𝑃𝑎subscript𝑒𝑖¯subscript𝑃𝑎𝑒subscript𝑃𝑏subscript𝑒𝑖¯subscript𝑃𝑏𝑒\\displaystyle=\\frac{(1-p)^{2}}{k}+\\sum_{i=1}^{k}\\left((P_{a}(e_{i})-\\overline{% P_{a}(e)})(P_{b}(e_{i})-\\overline{P_{b}(e)})\\right),= divide start_ARG ( 1 - italic_p ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_k end_ARG + ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( ( italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - over¯ start_ARG italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e ) end_ARG ) ( italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - over¯ start_ARG italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e ) end_ARG ) ) ,\n\nMeanwhile,\n\n∑i=1k((Pa⁢(ei)−Pa⁢(e)¯)⁢(Pb⁢(ei)−Pb⁢(e)¯))superscriptsubscript𝑖1𝑘subscript𝑃𝑎subscript𝑒𝑖¯subscript𝑃𝑎𝑒subscript𝑃𝑏subscript𝑒𝑖¯subscript𝑃𝑏𝑒\\displaystyle\\sum_{i=1}^{k}\\left((P_{a}(e_{i})-\\overline{P_{a}(e)})(P_{b}(e_{i% })-\\overline{P_{b}(e)})\\right)∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( ( italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - over¯ start_ARG italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e ) end_ARG ) ( italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - over¯ start_ARG italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e ) end_ARG ) ) =k×∑i=1k((Pa⁢(ei)−Pa⁢(e)¯)⁢(Pb⁢(ei)−Pb⁢(e)¯))kabsent𝑘superscriptsubscript𝑖1𝑘subscript𝑃𝑎subscript𝑒𝑖¯subscript𝑃𝑎𝑒subscript𝑃𝑏subscript𝑒𝑖¯subscript𝑃𝑏𝑒𝑘\\displaystyle=k\\times\\frac{\\sum_{i=1}^{k}\\left((P_{a}(e_{i})-\\overline{P_{a}(e% )})(P_{b}(e_{i})-\\overline{P_{b}(e)})\\right)}{k}= italic_k × divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( ( italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - over¯ start_ARG italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e ) end_ARG ) ( italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - over¯ start_ARG italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e ) end_ARG ) ) end_ARG start_ARG italic_k end_ARG =k×Cov⁢(Pa⁢(e),Pb⁢(e)),absent𝑘Covsubscript𝑃𝑎𝑒subscript𝑃𝑏𝑒\\displaystyle=k\\times\\text{Cov}(P_{a}(e),P_{b}(e)),= italic_k × Cov ( italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e ) , italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e ) ) ,\n\n(In the above formula, Cov⁢(X,Y)Cov𝑋𝑌\\text{Cov}(X,Y)Cov ( italic_X , italic_Y ) represents the covariance of two sets of numbers (X,Y)𝑋𝑌(X,Y)( italic_X , italic_Y )).\n\nE⁢[P⁢(E|Y¯)|Ya∼DA,Yb∼DB]𝐸delimited-[]formulae-sequencesimilar-toconditional𝑃conditional𝐸¯𝑌subscript𝑌𝑎subscript𝐷𝐴similar-tosubscript𝑌𝑏subscript𝐷𝐵\\displaystyle E[P(E|\\bar{Y})|Y_{a}\\sim D_{A},Y_{b}\\sim D_{B}]italic_E [ italic_P ( italic_E | over¯ start_ARG italic_Y end_ARG ) | italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ∼ italic_D start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT , italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ∼ italic_D start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT ] =(1−p)2k+k⋅E⁢[C⁢o⁢v⁢(Pa⁢(e),Pb⁢(e))|Ya∼DA,Yb∼DB].absentsuperscript1𝑝2𝑘⋅𝑘𝐸delimited-[]formulae-sequencesimilar-toconditional𝐶𝑜𝑣subscript𝑃𝑎𝑒subscript𝑃𝑏𝑒subscript𝑌𝑎subscript𝐷𝐴similar-tosubscript𝑌𝑏subscript𝐷𝐵\\displaystyle=\\frac{{(1-p)^{2}}}{k}+k\\cdot E[Cov(P_{a}(e),P_{b}(e))|Y_{a}\\sim D% _{A},Y_{b}\\sim D_{B}].= divide start_ARG ( 1 - italic_p ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_k end_ARG + italic_k ⋅ italic_E [ italic_C italic_o italic_v ( italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_e ) , italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_e ) ) | italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ∼ italic_D start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT , italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ∼ italic_D start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT ] .\n\nGiven that DA,DBsubscript𝐷𝐴subscript𝐷𝐵D_{A},D_{B}italic_D start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT are independently distributed, the expected value of the covariance is 0.\n\nE⁢[P⁢(E|Y¯)|Ya∼DA,Yb∼DB]=(1−p)2k.𝐸delimited-[]formulae-sequencesimilar-toconditional𝑃conditional𝐸¯𝑌subscript𝑌𝑎subscript𝐷𝐴similar-tosubscript𝑌𝑏subscript𝐷𝐵superscript1𝑝2𝑘E[P(E|\\bar{Y})|Y_{a}\\sim D_{A},Y_{b}\\sim D_{B}]=\\frac{{(1-p)^{2}}}{k}.italic_E [ italic_P ( italic_E | over¯ start_ARG italic_Y end_ARG ) | italic_Y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ∼ italic_D start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT , italic_Y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ∼ italic_D start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT ] = divide start_"
    }
}